{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML \n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train=pd.read_csv(\"train_data.csv\")\n",
    "test=pd.read_csv(\"test_data.csv\")\n",
    "import pandas_profiling as pdp\n",
    "df=pd.concat([train,test],sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pdp\n",
    "def preprocess():\n",
    "    train=pd.read_csv(\"train_data.csv\")\n",
    "    test=pd.read_csv(\"test_data.csv\")\n",
    "    df=pd.concat([train,test],sort=False)\n",
    "    df.age=df.age.map(lambda x:57 if x>58 else x)\n",
    "    df.num_child=df.num_child.map(lambda x:7 if x>7 else x)\n",
    "    df.study_time=df.study_time.map(lambda x:17 if x>17 else x)\n",
    "    df[\"familiy_num\"]=1+df.partner+df.num_child\n",
    "    arealist=list(train.groupby(\"area\").mean().salary.sort_values().index)\n",
    "    areadic={}\n",
    "    for i,area in enumerate(arealist):\n",
    "        areadic[area]=i+1\n",
    "    df.area=df.area.map(areadic)\n",
    "    df.position=df.position+1\n",
    "    df.sex=df.sex-1\n",
    "    df.salary=np.log(df.salary)\n",
    "    df[\"agexposition\"]=df.age*df.position.map(lambda x:1.5 if x==1 else x)\n",
    "    df.education=df.education+1\n",
    "#     df.drop([\"sex\"],axis=1)\n",
    "    train=df.dropna().drop([\"id\"],axis=1)\n",
    "    test=df[df.salary.isnull()].drop([\"id\"],axis=1)\n",
    "    test=test.drop([\"salary\",],axis=1)\n",
    "    X = train.drop([\"salary\"],axis=1)\n",
    "    y = train.salary\n",
    "    return X,y,test,df\n",
    "X,y,test,df=preprocess()\n",
    "#original 23.625521293118492\n",
    "train=pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.577470914009222, 22.33568473300726, 22.96702015021395, 22.86462929458672, 21.959873464946703]\n",
      "22.540935711352773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor(**{'booster': 'gblinear',\n",
    "                           'iterations': 309, 'depth': 16, 'learning_rate': 0.1,\n",
    "                           'random_strength': 48, 'bagging_temperature': 19.715729096205934, \n",
    "                           'od_type': 'Iter', 'od_wait': 26, 'lambda_l1': 0.726486176355415, \n",
    "                           'lambda_l2': 0.00044177449020498015, 'num_leaves': 188,\n",
    "                           'feature_fraction': 0.9443254919883529, 'bagging_fraction': 0.9271673814820428, \n",
    "                           'bagging_freq': 2, 'min_child_samples': 17})\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X,train_y, valid_y = train_test_split(X,y,test_size=0.2,random_state=43)\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pdp.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを生成する\n",
    "lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "lgbm_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 8,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'subsample' : 0.8,\n",
    "    'reg_alpha' : 1,\n",
    "    'reg_lambda' : 1,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'auc',\n",
    "    }\n",
    "\n",
    "# 上記のパラメータでモデルを学習する\n",
    "model = lgb.train(lgbm_params, lgb_train,\n",
    "                  # モデルの評価用データを渡す\n",
    "                  valid_sets=lgb_eval,\n",
    "                  # 最大で 1000 ラウンドまで学習する\n",
    "                  num_boost_round=1000,\n",
    "                  # 10 ラウンド経過しても性能が向上しないときは学習を打ち切る\n",
    "                  early_stopping_rounds=10)\n",
    "\n",
    "# 特徴量の重要度をプロットする\n",
    "lgb.plot_importance(model, figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9550358215611071\n",
      "MAE:23.610391\n",
      "Feature Importances:\n",
      "\tposition   : 0.010766\n",
      "\tage        : 0.003914\n",
      "\tarea       : 0.014661\n",
      "\tsex        : 0.001050\n",
      "\tpartner    : 0.013310\n",
      "\tnum_child  : 0.012098\n",
      "\teducation  : 0.045353\n",
      "\tservice_length : 0.004315\n",
      "\tstudy_time : 0.004520\n",
      "\tcommute    : 0.209512\n",
      "\tovertime   : 0.025041\n",
      "\tfamiliy_num : 0.024246\n",
      "\tagexposition : 0.631216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['agexposition', 'commute', 'education', 'overtime', 'familiy_num',\n",
       "       'area', 'partner', 'num_child', 'position', 'study_time',\n",
       "       'service_length', 'age', 'sex'], dtype=object)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rg = RFR(n_jobs=-1, random_state=2525)\n",
    " \n",
    "rg.fit(train_X,train_y)\n",
    "print(rg.score(valid_X,valid_y))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(rg.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "fti = rg.feature_importances_\n",
    "\n",
    "print('Feature Importances:')\n",
    "for i,feat in enumerate(valid_X.columns):\n",
    "    print('\\t{0:10s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "col_names = test.columns.values\n",
    "col_names_ = col_names[np.argsort(rg.feature_importances_)[::-1]]\n",
    "col_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.644725237611322, 22.79246432029882, 23.651437279803382, 23.300400935800944, 22.526815515676812]\n",
      "22.983168657838256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor()\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "#noadditional 23.625521293118492\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:55<00:08,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:25:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.87s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 特徴量を選択して、複数のモデルで精度を調査する\n",
    "from scipy.stats import mstats\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from sklearn import metrics,  feature_selection, ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\n",
    "# from imblearn import under_sampling, over_sampling\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "base_color = 'darkturquoise'\n",
    "base_color2 = 'gray'\n",
    "\n",
    "def generate_cmap(colors):\n",
    "    \n",
    "    values = range(len(colors))\n",
    "    vmax = np.ceil(np.max(values))\n",
    "    color_list = []\n",
    "    for v, c in zip(values, colors):\n",
    "        color_list.append((v/vmax, c))\n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list('custom_cmap', color_list)\n",
    "cm = generate_cmap([base_color2, 'white', base_color])\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    \"\"\"RMSE (Root Mean Square Error: 平均二乗誤差平方根) を計算する関数\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "params={'booster': 'dart', \n",
    "        'alpha': 0.009385870161072372, \n",
    "        'max_depth': 9, 'eta': 2.3698818355249718e-07,\n",
    "        'gamma': 3.167530221746867e-05, \n",
    "        'grow_policy': 'lossguide',\n",
    "        'sample_type': 'weighted',\n",
    "        'normalize_type': 'forest',\n",
    "        'rate_drop': 3.1207262366715483e-08,\n",
    "        'skip_drop': 1.2650261386504368e-05}\n",
    "\n",
    "\n",
    "models = [\n",
    " \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    " \n",
    "    #Gaussian Processes\n",
    "#     gaussian_process.GaussianProcessRegressor(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.Ridge(),\n",
    "\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeRegressor(),\n",
    "    tree.ExtraTreeRegressor(),\n",
    " \n",
    "    #xgboost\n",
    "    xgb. XGBRegressor(),\n",
    "    lgb.LGBMRegressor(),\n",
    "#     cat.CatBoostRegressor(),\n",
    "    \n",
    "]\n",
    " \n",
    "df_compare = pd.DataFrame(columns=['name', 'train_rmse', 'valid_rmse', 'time'])\n",
    "score_funcs = {\n",
    "    'rmse': make_scorer(rmse_score),\n",
    "}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    \n",
    "    cv_rlts = model_selection.cross_validate(model,X,y, scoring=score_funcs, cv=10, return_train_score=True)\n",
    " \n",
    "    for i in range(10):\n",
    "        s = pd.Series([name, cv_rlts['train_rmse'][i], cv_rlts['test_rmse'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name=name+str(i))\n",
    "        df_compare = df_compare.append(s)\n",
    "        \n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=df_compare, y='name', x='valid_rmse', orient='h', color=base_color, linewidth=0.5, width=0.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:42<00:16,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:26:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:56<00:00,  8.04s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = X.columns.tolist()\n",
    " \n",
    "# positive_cnt = int(df['salary'].sum())\n",
    "\n",
    "feature_importance_models = [\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    tree.DecisionTreeRegressor(),\n",
    "    xgb.XGBRegressor(),\n",
    "    lgb.LGBMRegressor()\n",
    "]\n",
    " \n",
    "scoring = ['rsme']\n",
    "df_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\n",
    "df_rfe_cols_cnt['cnt'] = 0\n",
    " \n",
    "for i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n",
    "    \n",
    "    rfe = feature_selection.RFECV(model, step=3)\n",
    "    rfe.fit(X, y)\n",
    "#     print(rfe.get_support())\n",
    "    rfe_cols = X[cols].columns.values[rfe.get_support()]\n",
    "    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n",
    "    \n",
    "df_rfe_cols_cnt.plot(kind='bar', color=base_color, figsize=(15, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df_rfe_cols_cnt[df_rfe_cols_cnt['cnt'] < 4].index\n",
    "X=X.drop(x_cols,axis=1)\n",
    "test=test.drop(x_cols,axis=1)\n",
    "train_X, valid_X,train_y, valid_y = train_test_split(X,y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_list = ['area', 'sex', 'partner','education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:28:36,513] Finished trial#0 resulted in value: 0.935196791185645. Current best value is 0.935196791185645 with parameters: {'booster': 'gbtree', 'alpha': 4.60033291811229e-06, 'max_depth': 18, 'eta': 0.008173817332634628, 'gamma': 0.8902721232594177, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:28:56,221] Finished trial#1 resulted in value: -1.5539921181251457. Current best value is 0.935196791185645 with parameters: {'booster': 'gbtree', 'alpha': 4.60033291811229e-06, 'max_depth': 18, 'eta': 0.008173817332634628, 'gamma': 0.8902721232594177, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:02,141] Finished trial#2 resulted in value: 0.9548856586697718. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:15,208] Finished trial#3 resulted in value: -29.40068058137117. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:16,951] Finished trial#4 resulted in value: 0.8069192137502167. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:23,603] Finished trial#5 resulted in value: 0.9554614147515625. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:41,435] Finished trial#6 resulted in value: -1.4777565969533113. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:47,520] Finished trial#7 resulted in value: 0.9546554963677492. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:52,570] Finished trial#8 resulted in value: 0.80763085397036. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:57,916] Finished trial#9 resulted in value: 0.9532127450886515. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[00:29:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:30:17,791] Finished trial#0 resulted in value: 0.9521134752192577. Current best value is 0.9521134752192577 with parameters: {'booster': 'dart', 'alpha': 0.0005498199457683365, 'max_depth': 5, 'eta': 2.673192533142511e-07, 'gamma': 1.8888217914863087e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.270287350141747e-05, 'skip_drop': 7.760338396503087e-08}.\n",
      "[I 2019-11-20 00:30:26,926] Finished trial#1 resulted in value: 0.9578058325024463. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:30:48,396] Finished trial#2 resulted in value: 0.7272354906592282. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:23,874] Finished trial#3 resulted in value: 0.9577140218613494. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:38,558] Finished trial#4 resulted in value: 0.9551363613997488. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:56,582] Finished trial#5 resulted in value: 0.9518838868793225. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:11,399] Finished trial#6 resulted in value: 0.9348990187528268. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:36,243] Finished trial#7 resulted in value: 0.9570785312201711. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:48,656] Finished trial#8 resulted in value: 0.9558221341072535. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:59,279] Finished trial#9 resulted in value: 0.9309553740696277. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[00:32:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:33:19,903] Finished trial#0 resulted in value: 0.9540782183180527. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:33:26,744] Finished trial#1 resulted in value: 0.9527068485401523. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:33:34,972] Finished trial#2 resulted in value: -509.39465015773476. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:26,887] Finished trial#3 resulted in value: 0.4727458439331976. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:40,564] Finished trial#4 resulted in value: 0.9545346369428991. Current best value is 0.9545346369428991 with parameters: {'booster': 'gbtree', 'alpha': 0.010153455864032377, 'max_depth': 14, 'eta': 0.0029264792450987144, 'gamma': 4.551552592372101e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:49,776] Finished trial#5 resulted in value: 0.9564613487344698. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:52,216] Finished trial#6 resulted in value: 0.8973922426646362. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:35:07,576] Finished trial#7 resulted in value: 0.9542409917899194. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:36:00,099] Finished trial#8 resulted in value: 0.957111945972502. Current best value is 0.957111945972502 with parameters: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}.\n",
      "[I 2019-11-20 00:36:11,372] Finished trial#9 resulted in value: 0.9314723147742662. Current best value is 0.957111945972502 with parameters: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}\n",
      "\n",
      "[00:36:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22.95643615911303, 23.146576434969464, 22.742527656745942]\n",
      "22.948513416942813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class RidgeCV():\n",
    "    model_cls = Ridge\n",
    "\n",
    "    def __init__(self, n_trials=100):\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.reset_index(drop=True)\n",
    "            y = y.reset_index(drop=True)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self, n_trials=self.n_trials)\n",
    "        self.best_trial = study.best_trial\n",
    "\n",
    "        print()\n",
    "        print(\"Best score:\", round(self.best_trial.value, 2))\n",
    "        print(\"Best params:\", self.best_trial.params)\n",
    "        print()\n",
    "\n",
    "        self.best_model = self.model_cls(**self.best_trial.params)\n",
    "        self.best_model.fit(self.X, self.y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = pd.DataFrame(X.values.reshape(1, -1))\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        return self.best_model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "\n",
    "        return self.best_model.score(X, y)\n",
    "\n",
    "    def kfold_cv(self, model, splits=5):\n",
    "        scores = []\n",
    "\n",
    "        kf = KFold(n_splits=splits, shuffle=True)\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            scores.append(r2_score(model.predict(X_test), y_test))\n",
    "\n",
    "        score = np.array(scores).mean()\n",
    "        return score\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class XGBRegressorCV(RidgeCV):\n",
    "    model_cls = xgb.XGBRegressor\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        booster = trial.suggest_categorical('booster', ['gbtree', 'dart'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 1e-8, 1.0)\n",
    "\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "        eta = trial.suggest_loguniform('eta', 1e-8, 1.0)\n",
    "        gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "        grow_policy = trial.suggest_categorical(\n",
    "            'grow_policy', ['depthwise', 'lossguide'])\n",
    "\n",
    "        if booster == 'gbtree':\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy)\n",
    "        elif booster == 'dart':\n",
    "            sample_type = trial.suggest_categorical('sample_type',\n",
    "                                                    ['uniform', 'weighted'])\n",
    "            normalize_type = trial.suggest_categorical('normalize_type',\n",
    "                                                       ['tree', 'forest'])\n",
    "            rate_drop = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
    "            skip_drop = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy,\n",
    "                                   sample_type=sample_type,\n",
    "                                   normalize_type=normalize_type,\n",
    "                                   rate_drop=rate_drop, skip_drop=skip_drop)\n",
    "\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "xgbr = XGBRegressorCV(n_trials=100)\n",
    "# xgbr.fit(train_X, train_y)\n",
    "\n",
    "# pred_y=xgbr.predict(valid_X)\n",
    "# score=mean_absolute_error(np.exp(valid_y),np.exp(xgbr.predict(valid_X)))\n",
    "# print(f'MAE:{score:4f}')\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "model = XGBRegressorCV(n_trials=10)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "#MAE:22.821581(drop)dummy\n",
    "#MAE:22.753628(nodrop)dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:49:33,001] Finished trial#0 resulted in value: 0.9444320369636955. Current best value is 0.9444320369636955 with parameters: {'booster': 'gbtree', 'iterations': 310, 'learning_rate': 0.5356748534001899, 'random_strength': 52, 'bagging_temperature': 0.17025341497792462, 'od_type': 'Iter', 'od_wait': 20, 'lambda_l1': 0.28916744743958905, 'lambda_l2': 2.6040951538148434e-07, 'num_leaves': 249, 'feature_fraction': 0.5161973959560417, 'bagging_fraction': 0.9864977088428162, 'bagging_freq': 2, 'min_child_samples': 69}.\n",
      "[I 2019-11-20 14:49:35,601] Finished trial#1 resulted in value: 0.6812421248745195. Current best value is 0.9444320369636955 with parameters: {'booster': 'gbtree', 'iterations': 310, 'learning_rate': 0.5356748534001899, 'random_strength': 52, 'bagging_temperature': 0.17025341497792462, 'od_type': 'Iter', 'od_wait': 20, 'lambda_l1': 0.28916744743958905, 'lambda_l2': 2.6040951538148434e-07, 'num_leaves': 249, 'feature_fraction': 0.5161973959560417, 'bagging_fraction': 0.9864977088428162, 'bagging_freq': 2, 'min_child_samples': 69}.\n",
      "[I 2019-11-20 14:49:37,737] Finished trial#2 resulted in value: 0.946479655692683. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:41,257] Finished trial#3 resulted in value: 0.9229633342970001. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:44,933] Finished trial#4 resulted in value: 0.9117554963499478. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:47,731] Finished trial#5 resulted in value: 0.9535737648164808. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:49,790] Finished trial#6 resulted in value: 0.9170101560951187. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:52,403] Finished trial#7 resulted in value: 0.9491913685191673. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:54,809] Finished trial#8 resulted in value: 0.952089428510396. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:57,576] Finished trial#9 resulted in value: 0.9535738228210354. Current best value is 0.9535738228210354 with parameters: {'booster': 'gbtree', 'iterations': 324, 'learning_rate': 0.19191133141833663, 'random_strength': 58, 'bagging_temperature': 1.3025324129298999, 'od_type': 'IncToDec', 'od_wait': 20, 'lambda_l1': 7.66143544724732e-05, 'lambda_l2': 0.0002223866325696207, 'num_leaves': 76, 'feature_fraction': 0.8249491788761739, 'bagging_fraction': 0.6621871695330719, 'bagging_freq': 5, 'min_child_samples': 13}.\n",
      "[I 2019-11-20 14:49:59,643] Finished trial#10 resulted in value: 0.9544397094191. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:01,689] Finished trial#11 resulted in value: 0.8912676270402354. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:04,101] Finished trial#12 resulted in value: 0.9501701287808487. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:06,404] Finished trial#13 resulted in value: 0.9551434497202755. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:07,828] Finished trial#14 resulted in value: 0.8912601043751215. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:09,855] Finished trial#15 resulted in value: 0.9482017929572482. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:13,464] Finished trial#16 resulted in value: 0.9452702720735692. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:14,839] Finished trial#17 resulted in value: 0.8082902384585313. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:17,131] Finished trial#18 resulted in value: 0.9498923087405512. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:20,225] Finished trial#19 resulted in value: 0.9515642996217105. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:50:22,203] Finished trial#0 resulted in value: 0.9190442462685363. Current best value is 0.9190442462685363 with parameters: {'booster': 'gbtree', 'iterations': 103, 'learning_rate': 0.7597808912677907, 'random_strength': 62, 'bagging_temperature': 50.66977343950774, 'od_type': 'Iter', 'od_wait': 22, 'lambda_l1': 7.434126748681443e-07, 'lambda_l2': 0.00351728452576551, 'num_leaves': 104, 'feature_fraction': 0.4508349977292094, 'bagging_fraction': 0.40809902352411337, 'bagging_freq': 3, 'min_child_samples': 73}.\n",
      "[I 2019-11-20 14:50:26,060] Finished trial#1 resulted in value: 0.9561786264207287. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:27,830] Finished trial#2 resulted in value: 0.7484399047076318. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:29,704] Finished trial#3 resulted in value: 0.9517489634448248. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:31,504] Finished trial#4 resulted in value: 0.9451362832560125. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:33,243] Finished trial#5 resulted in value: 0.9547846621786619. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:35,516] Finished trial#6 resulted in value: 0.950153906190244. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:38,358] Finished trial#7 resulted in value: 0.8794672564179061. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:41,705] Finished trial#8 resulted in value: 0.9463267555279613. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:45,019] Finished trial#9 resulted in value: 0.9491094153626438. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:49,833] Finished trial#10 resulted in value: 0.951389751571497. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:51,252] Finished trial#11 resulted in value: 0.9532864049605327. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:52,882] Finished trial#12 resulted in value: 0.8324258524376896. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:59,678] Finished trial#13 resulted in value: 0.9536680802440181. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:51:07,116] Finished trial#14 resulted in value: 0.9585452521008078. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:13,532] Finished trial#15 resulted in value: 0.957382636145273. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:21,090] Finished trial#16 resulted in value: 0.9561206288398209. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:23,635] Finished trial#17 resulted in value: 0.890840649928222. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:29,618] Finished trial#18 resulted in value: 0.9576068734294443. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-420-6f2c397560e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-420-6f2c397560e4>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     44\u001b[0m     }\n\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "def min_max_normalization(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    x_norm = (x - x_min) / ( x_max - x_min)\n",
    "    return x_norm\n",
    "def min_max_renormalization(x1,x2):\n",
    "    x_min = x2.min()\n",
    "    x_max = x2.max()\n",
    "#     x_norm = (x2 - x_min) / ( x_max - x_min)\n",
    "    x=x1*  ( x_max - x_min)+x_min\n",
    "    return x\n",
    "\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "cat_list = ['area', 'sex', 'partner','education']\n",
    "class LGBRegressorCV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'eval_metric':'mae',\n",
    "            'booster':trial.suggest_categorical('booster',['gbtree','gblinear']),\n",
    "            'loss_function': 'fair',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "#             'depth' : trial.suggest_int('depth', 4, 25),  \n",
    "            'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50),\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressorCV(n_trials=20)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "# model = XGBRegressorCV(n_trials=20)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "# checktesty=valid_y.sort_values()\n",
    "# checktestX=valid_X.ix[list(checktesty.index)]\n",
    "# checktesty=checktesty.reset_index(drop=True)#これと\n",
    "\n",
    "# checkpred=pd.DataFrame(model.predict(checktestX))#これ\n",
    "# check=pd.concat([checktesty,checkpred], axis=1)\n",
    "# check.columns=[\"actual\",\"predict\"]\n",
    "# check.plot(alpha=0.5)\n",
    "#MAE:23.144563 (nodrop)dummy\n",
    "#MAE:23.742147(drop)dummy\n",
    "#MAE:23.902507nodummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:48:38,249] Finished trial#0 resulted in value: 0.9546877750924008. Current best value is 0.9546877750924008 with parameters: {'iterations': 101, 'learning_rate': 0.05677338448922201, 'random_strength': 74, 'bagging_temperature': 2.0510543184036862, 'od_type': 'Iter', 'od_wait': 35}.\n",
      "[I 2019-11-20 14:48:40,445] Finished trial#1 resulted in value: 0.8725052547829868. Current best value is 0.9546877750924008 with parameters: {'iterations': 101, 'learning_rate': 0.05677338448922201, 'random_strength': 74, 'bagging_temperature': 2.0510543184036862, 'od_type': 'Iter', 'od_wait': 35}.\n",
      "[I 2019-11-20 14:48:41,974] Finished trial#2 resulted in value: 0.956279653164976. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:43,704] Finished trial#3 resulted in value: 0.9526568224980487. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:45,385] Finished trial#4 resulted in value: 0.724462304574427. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:47,379] Finished trial#5 resulted in value: 0.934567977655848. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:49,251] Finished trial#6 resulted in value: 0.8608944531751209. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:50,978] Finished trial#7 resulted in value: 0.9443375210424974. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:52,895] Finished trial#8 resulted in value: 0.6812147720085355. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:55,129] Finished trial#9 resulted in value: 0.9423476110039471. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:56,988] Finished trial#10 resulted in value: 0.9578334287160528. Current best value is 0.9578334287160528 with parameters: {'iterations': 52, 'learning_rate': 0.22744116181524698, 'random_strength': 93, 'bagging_temperature': 0.26035526103111334, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:48:59,048] Finished trial#11 resulted in value: 0.958161293343581. Current best value is 0.958161293343581 with parameters: {'iterations': 51, 'learning_rate': 0.24474452006467162, 'random_strength': 95, 'bagging_temperature': 0.27004141678600385, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:01,749] Finished trial#12 resulted in value: 0.9583523351809283. Current best value is 0.9583523351809283 with parameters: {'iterations': 50, 'learning_rate': 0.190908395790108, 'random_strength': 99, 'bagging_temperature': 0.3365228994292324, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:03,349] Finished trial#13 resulted in value: 0.9587674974781338. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:05,061] Finished trial#14 resulted in value: 0.9580861649431325. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:06,865] Finished trial#15 resulted in value: 0.9530357537100704. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:09,409] Finished trial#16 resulted in value: 0.9580107388968948. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-870cf0fd95d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLGBRegressor1CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_ylgm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_max_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_ylgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_max_renormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;31m# https://github.com/pfnet/optuna/pull/325.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    \n",
    "class LGBRegressor1CV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'xentropy loss',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 8, 20), \n",
    "#             'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressor1CV(n_trials=40)\n",
    "train_ylgm=min_max_normalization(train_y)\n",
    "model.fit(train_X,train_ylgm)\n",
    "\n",
    "pred_y=min_max_renormalization(model.predict(valid_X),train_y)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:48:22,192] Finished trial#0 resulted in value: 0.9540536681960733. Current best value is 0.9540536681960733 with parameters: {'iterations': 212, 'depth': 16, 'learning_rate': 0.4027814231140469, 'random_strength': 93, 'bagging_temperature': 0.11772803509772231, 'od_type': 'IncToDec', 'od_wait': 12}.\n",
      "[I 2019-11-20 14:48:24,962] Finished trial#1 resulted in value: 0.9552134775777515. Current best value is 0.9552134775777515 with parameters: {'iterations': 212, 'depth': 12, 'learning_rate': 0.360413050512503, 'random_strength': 28, 'bagging_temperature': 1.422386864890993, 'od_type': 'IncToDec', 'od_wait': 43}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-0d77aa182595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLGBRegressor2CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-417-0d77aa182595>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LGBRegressor2CV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            \n",
    "            'loss_function': 'regression_l1',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "            'depth' : trial.suggest_int('depth', 4, 25),                                      \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressor2CV(n_trials=40)\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost+optuna(non recomended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4311223\ttotal: 6.09ms\tremaining: 6.08s\n",
      "1:\tlearn: 0.4122959\ttotal: 15.3ms\tremaining: 7.62s\n",
      "2:\tlearn: 0.3929384\ttotal: 24.4ms\tremaining: 8.12s\n",
      "3:\tlearn: 0.3781482\ttotal: 30.1ms\tremaining: 7.5s\n",
      "4:\tlearn: 0.3617889\ttotal: 44.7ms\tremaining: 8.9s\n",
      "5:\tlearn: 0.3479133\ttotal: 58.7ms\tremaining: 9.72s\n",
      "6:\tlearn: 0.3305254\ttotal: 70.5ms\tremaining: 9.99s\n",
      "7:\tlearn: 0.3156954\ttotal: 83.6ms\tremaining: 10.4s\n",
      "8:\tlearn: 0.3049368\ttotal: 93.7ms\tremaining: 10.3s\n",
      "9:\tlearn: 0.2948312\ttotal: 103ms\tremaining: 10.2s\n",
      "10:\tlearn: 0.2851900\ttotal: 113ms\tremaining: 10.2s\n",
      "11:\tlearn: 0.2806031\ttotal: 123ms\tremaining: 10.1s\n",
      "12:\tlearn: 0.2729884\ttotal: 132ms\tremaining: 10.1s\n",
      "13:\tlearn: 0.2656313\ttotal: 142ms\tremaining: 10s\n",
      "14:\tlearn: 0.2601300\ttotal: 151ms\tremaining: 9.93s\n",
      "15:\tlearn: 0.2539201\ttotal: 170ms\tremaining: 10.5s\n",
      "16:\tlearn: 0.2485246\ttotal: 183ms\tremaining: 10.6s\n",
      "17:\tlearn: 0.2423629\ttotal: 195ms\tremaining: 10.6s\n",
      "18:\tlearn: 0.2380699\ttotal: 247ms\tremaining: 12.8s\n",
      "19:\tlearn: 0.2347966\ttotal: 288ms\tremaining: 14.1s\n",
      "20:\tlearn: 0.2311633\ttotal: 333ms\tremaining: 15.5s\n",
      "21:\tlearn: 0.2280010\ttotal: 357ms\tremaining: 15.9s\n",
      "22:\tlearn: 0.2217537\ttotal: 370ms\tremaining: 15.7s\n",
      "23:\tlearn: 0.2192560\ttotal: 382ms\tremaining: 15.5s\n",
      "24:\tlearn: 0.2161294\ttotal: 409ms\tremaining: 15.9s\n",
      "25:\tlearn: 0.2141041\ttotal: 419ms\tremaining: 15.7s\n",
      "26:\tlearn: 0.2092834\ttotal: 429ms\tremaining: 15.4s\n",
      "27:\tlearn: 0.2043057\ttotal: 436ms\tremaining: 15.1s\n",
      "28:\tlearn: 0.2012852\ttotal: 449ms\tremaining: 15s\n",
      "29:\tlearn: 0.2000804\ttotal: 479ms\tremaining: 15.5s\n",
      "30:\tlearn: 0.1983112\ttotal: 496ms\tremaining: 15.5s\n",
      "31:\tlearn: 0.1972527\ttotal: 505ms\tremaining: 15.3s\n",
      "32:\tlearn: 0.1942226\ttotal: 522ms\tremaining: 15.3s\n",
      "33:\tlearn: 0.1896919\ttotal: 532ms\tremaining: 15.1s\n",
      "34:\tlearn: 0.1858320\ttotal: 543ms\tremaining: 15s\n",
      "35:\tlearn: 0.1852328\ttotal: 552ms\tremaining: 14.8s\n",
      "36:\tlearn: 0.1845956\ttotal: 572ms\tremaining: 14.9s\n",
      "37:\tlearn: 0.1838172\ttotal: 580ms\tremaining: 14.7s\n",
      "38:\tlearn: 0.1826762\ttotal: 599ms\tremaining: 14.8s\n",
      "39:\tlearn: 0.1817929\ttotal: 615ms\tremaining: 14.8s\n",
      "40:\tlearn: 0.1800657\ttotal: 626ms\tremaining: 14.6s\n",
      "41:\tlearn: 0.1787221\ttotal: 633ms\tremaining: 14.4s\n",
      "42:\tlearn: 0.1780508\ttotal: 649ms\tremaining: 14.4s\n",
      "43:\tlearn: 0.1771337\ttotal: 663ms\tremaining: 14.4s\n",
      "44:\tlearn: 0.1767505\ttotal: 674ms\tremaining: 14.3s\n",
      "45:\tlearn: 0.1753077\ttotal: 687ms\tremaining: 14.3s\n",
      "46:\tlearn: 0.1728196\ttotal: 698ms\tremaining: 14.2s\n",
      "47:\tlearn: 0.1723998\ttotal: 713ms\tremaining: 14.1s\n",
      "48:\tlearn: 0.1698267\ttotal: 739ms\tremaining: 14.3s\n",
      "49:\tlearn: 0.1693407\ttotal: 771ms\tremaining: 14.7s\n",
      "50:\tlearn: 0.1670097\ttotal: 807ms\tremaining: 15s\n",
      "51:\tlearn: 0.1667453\ttotal: 840ms\tremaining: 15.3s\n",
      "52:\tlearn: 0.1653919\ttotal: 855ms\tremaining: 15.3s\n",
      "53:\tlearn: 0.1651249\ttotal: 867ms\tremaining: 15.2s\n",
      "54:\tlearn: 0.1651133\ttotal: 878ms\tremaining: 15.1s\n",
      "55:\tlearn: 0.1649510\ttotal: 891ms\tremaining: 15s\n",
      "56:\tlearn: 0.1646254\ttotal: 905ms\tremaining: 15s\n",
      "57:\tlearn: 0.1644612\ttotal: 915ms\tremaining: 14.9s\n",
      "58:\tlearn: 0.1643066\ttotal: 926ms\tremaining: 14.8s\n",
      "59:\tlearn: 0.1632530\ttotal: 936ms\tremaining: 14.7s\n",
      "60:\tlearn: 0.1614734\ttotal: 949ms\tremaining: 14.6s\n",
      "61:\tlearn: 0.1611170\ttotal: 966ms\tremaining: 14.6s\n",
      "62:\tlearn: 0.1607912\ttotal: 978ms\tremaining: 14.5s\n",
      "63:\tlearn: 0.1604157\ttotal: 987ms\tremaining: 14.4s\n",
      "64:\tlearn: 0.1595622\ttotal: 1s\tremaining: 14.4s\n",
      "65:\tlearn: 0.1591387\ttotal: 1.01s\tremaining: 14.3s\n",
      "66:\tlearn: 0.1590066\ttotal: 1.02s\tremaining: 14.3s\n",
      "67:\tlearn: 0.1588404\ttotal: 1.04s\tremaining: 14.3s\n",
      "68:\tlearn: 0.1588055\ttotal: 1.05s\tremaining: 14.2s\n",
      "69:\tlearn: 0.1586660\ttotal: 1.07s\tremaining: 14.2s\n",
      "70:\tlearn: 0.1582629\ttotal: 1.08s\tremaining: 14.1s\n",
      "71:\tlearn: 0.1581518\ttotal: 1.09s\tremaining: 14s\n",
      "72:\tlearn: 0.1580025\ttotal: 1.1s\tremaining: 14s\n",
      "73:\tlearn: 0.1573191\ttotal: 1.11s\tremaining: 13.9s\n",
      "74:\tlearn: 0.1565309\ttotal: 1.12s\tremaining: 13.8s\n",
      "75:\tlearn: 0.1561008\ttotal: 1.13s\tremaining: 13.8s\n",
      "76:\tlearn: 0.1558711\ttotal: 1.14s\tremaining: 13.7s\n",
      "77:\tlearn: 0.1557153\ttotal: 1.16s\tremaining: 13.7s\n",
      "78:\tlearn: 0.1550222\ttotal: 1.17s\tremaining: 13.7s\n",
      "79:\tlearn: 0.1543709\ttotal: 1.18s\tremaining: 13.6s\n",
      "80:\tlearn: 0.1541333\ttotal: 1.21s\tremaining: 13.7s\n",
      "81:\tlearn: 0.1540627\ttotal: 1.22s\tremaining: 13.7s\n",
      "82:\tlearn: 0.1536081\ttotal: 1.24s\tremaining: 13.7s\n",
      "83:\tlearn: 0.1530523\ttotal: 1.25s\tremaining: 13.6s\n",
      "84:\tlearn: 0.1529161\ttotal: 1.27s\tremaining: 13.6s\n",
      "85:\tlearn: 0.1528759\ttotal: 1.28s\tremaining: 13.6s\n",
      "86:\tlearn: 0.1527104\ttotal: 1.29s\tremaining: 13.6s\n",
      "87:\tlearn: 0.1525871\ttotal: 1.3s\tremaining: 13.5s\n",
      "88:\tlearn: 0.1525130\ttotal: 1.32s\tremaining: 13.5s\n",
      "89:\tlearn: 0.1523223\ttotal: 1.35s\tremaining: 13.6s\n",
      "90:\tlearn: 0.1520831\ttotal: 1.37s\tremaining: 13.7s\n",
      "91:\tlearn: 0.1517886\ttotal: 1.38s\tremaining: 13.6s\n",
      "92:\tlearn: 0.1510540\ttotal: 1.39s\tremaining: 13.6s\n",
      "93:\tlearn: 0.1509498\ttotal: 1.4s\tremaining: 13.5s\n",
      "94:\tlearn: 0.1505193\ttotal: 1.41s\tremaining: 13.5s\n",
      "95:\tlearn: 0.1503798\ttotal: 1.43s\tremaining: 13.4s\n",
      "96:\tlearn: 0.1503078\ttotal: 1.44s\tremaining: 13.4s\n",
      "97:\tlearn: 0.1502307\ttotal: 1.44s\tremaining: 13.3s\n",
      "98:\tlearn: 0.1498731\ttotal: 1.45s\tremaining: 13.2s\n",
      "99:\tlearn: 0.1487402\ttotal: 1.47s\tremaining: 13.2s\n",
      "100:\tlearn: 0.1485769\ttotal: 1.47s\tremaining: 13.1s\n",
      "101:\tlearn: 0.1482429\ttotal: 1.48s\tremaining: 13.1s\n",
      "102:\tlearn: 0.1477609\ttotal: 1.49s\tremaining: 13s\n",
      "103:\tlearn: 0.1477561\ttotal: 1.5s\tremaining: 12.9s\n",
      "104:\tlearn: 0.1477085\ttotal: 1.51s\tremaining: 12.9s\n",
      "105:\tlearn: 0.1476461\ttotal: 1.52s\tremaining: 12.8s\n",
      "106:\tlearn: 0.1476081\ttotal: 1.52s\tremaining: 12.7s\n",
      "107:\tlearn: 0.1476074\ttotal: 1.53s\tremaining: 12.6s\n",
      "108:\tlearn: 0.1471139\ttotal: 1.54s\tremaining: 12.6s\n",
      "109:\tlearn: 0.1470467\ttotal: 1.55s\tremaining: 12.5s\n",
      "110:\tlearn: 0.1469894\ttotal: 1.56s\tremaining: 12.5s\n",
      "111:\tlearn: 0.1469754\ttotal: 1.57s\tremaining: 12.4s\n",
      "112:\tlearn: 0.1469007\ttotal: 1.58s\tremaining: 12.4s\n",
      "113:\tlearn: 0.1468391\ttotal: 1.6s\tremaining: 12.4s\n",
      "114:\tlearn: 0.1467413\ttotal: 1.61s\tremaining: 12.4s\n",
      "115:\tlearn: 0.1459982\ttotal: 1.62s\tremaining: 12.3s\n",
      "116:\tlearn: 0.1459376\ttotal: 1.62s\tremaining: 12.2s\n",
      "117:\tlearn: 0.1458638\ttotal: 1.64s\tremaining: 12.2s\n",
      "118:\tlearn: 0.1456469\ttotal: 1.65s\tremaining: 12.2s\n",
      "119:\tlearn: 0.1455688\ttotal: 1.66s\tremaining: 12.1s\n",
      "120:\tlearn: 0.1443474\ttotal: 1.67s\tremaining: 12.1s\n",
      "121:\tlearn: 0.1442534\ttotal: 1.68s\tremaining: 12.1s\n",
      "122:\tlearn: 0.1442181\ttotal: 1.69s\tremaining: 12s\n",
      "123:\tlearn: 0.1441789\ttotal: 1.7s\tremaining: 12s\n",
      "124:\tlearn: 0.1441204\ttotal: 1.7s\tremaining: 11.9s\n",
      "125:\tlearn: 0.1432705\ttotal: 1.71s\tremaining: 11.9s\n",
      "126:\tlearn: 0.1431116\ttotal: 1.72s\tremaining: 11.8s\n",
      "127:\tlearn: 0.1430280\ttotal: 1.73s\tremaining: 11.8s\n",
      "128:\tlearn: 0.1430054\ttotal: 1.76s\tremaining: 11.9s\n",
      "129:\tlearn: 0.1428502\ttotal: 1.78s\tremaining: 11.9s\n",
      "130:\tlearn: 0.1427607\ttotal: 1.81s\tremaining: 12s\n",
      "131:\tlearn: 0.1413635\ttotal: 1.83s\tremaining: 12s\n",
      "132:\tlearn: 0.1412966\ttotal: 1.86s\tremaining: 12.1s\n",
      "133:\tlearn: 0.1407043\ttotal: 1.87s\tremaining: 12.1s\n",
      "134:\tlearn: 0.1405489\ttotal: 1.88s\tremaining: 12s\n",
      "135:\tlearn: 0.1404847\ttotal: 1.88s\tremaining: 12s\n",
      "136:\tlearn: 0.1404707\ttotal: 1.89s\tremaining: 11.9s\n",
      "137:\tlearn: 0.1399496\ttotal: 1.9s\tremaining: 11.9s\n",
      "138:\tlearn: 0.1396927\ttotal: 1.91s\tremaining: 11.8s\n",
      "139:\tlearn: 0.1394807\ttotal: 1.92s\tremaining: 11.8s\n",
      "140:\tlearn: 0.1394378\ttotal: 1.93s\tremaining: 11.8s\n",
      "141:\tlearn: 0.1388223\ttotal: 1.94s\tremaining: 11.7s\n",
      "142:\tlearn: 0.1388124\ttotal: 1.95s\tremaining: 11.7s\n",
      "143:\tlearn: 0.1379689\ttotal: 1.96s\tremaining: 11.6s\n",
      "144:\tlearn: 0.1374252\ttotal: 1.97s\tremaining: 11.6s\n",
      "145:\tlearn: 0.1368803\ttotal: 1.98s\tremaining: 11.6s\n",
      "146:\tlearn: 0.1359654\ttotal: 1.98s\tremaining: 11.5s\n",
      "147:\tlearn: 0.1348713\ttotal: 2s\tremaining: 11.5s\n",
      "148:\tlearn: 0.1344440\ttotal: 2s\tremaining: 11.5s\n",
      "149:\tlearn: 0.1341088\ttotal: 2.02s\tremaining: 11.4s\n",
      "150:\tlearn: 0.1335523\ttotal: 2.02s\tremaining: 11.4s\n",
      "151:\tlearn: 0.1322850\ttotal: 2.03s\tremaining: 11.3s\n",
      "152:\tlearn: 0.1313930\ttotal: 2.04s\tremaining: 11.3s\n",
      "153:\tlearn: 0.1310088\ttotal: 2.05s\tremaining: 11.3s\n",
      "154:\tlearn: 0.1300560\ttotal: 2.06s\tremaining: 11.3s\n",
      "155:\tlearn: 0.1295490\ttotal: 2.07s\tremaining: 11.2s\n",
      "156:\tlearn: 0.1292416\ttotal: 2.09s\tremaining: 11.2s\n",
      "157:\tlearn: 0.1277759\ttotal: 2.1s\tremaining: 11.2s\n",
      "158:\tlearn: 0.1270675\ttotal: 2.11s\tremaining: 11.2s\n",
      "159:\tlearn: 0.1266233\ttotal: 2.12s\tremaining: 11.1s\n",
      "160:\tlearn: 0.1262800\ttotal: 2.13s\tremaining: 11.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.1260394\ttotal: 2.13s\tremaining: 11s\n",
      "162:\tlearn: 0.1253762\ttotal: 2.14s\tremaining: 11s\n",
      "163:\tlearn: 0.1245294\ttotal: 2.15s\tremaining: 11s\n",
      "164:\tlearn: 0.1236547\ttotal: 2.16s\tremaining: 10.9s\n",
      "165:\tlearn: 0.1226551\ttotal: 2.17s\tremaining: 10.9s\n",
      "166:\tlearn: 0.1218153\ttotal: 2.19s\tremaining: 10.9s\n",
      "167:\tlearn: 0.1210012\ttotal: 2.2s\tremaining: 10.9s\n",
      "168:\tlearn: 0.1198229\ttotal: 2.21s\tremaining: 10.9s\n",
      "169:\tlearn: 0.1186641\ttotal: 2.21s\tremaining: 10.8s\n",
      "170:\tlearn: 0.1184452\ttotal: 2.22s\tremaining: 10.8s\n",
      "171:\tlearn: 0.1174584\ttotal: 2.23s\tremaining: 10.7s\n",
      "172:\tlearn: 0.1168594\ttotal: 2.24s\tremaining: 10.7s\n",
      "173:\tlearn: 0.1160382\ttotal: 2.25s\tremaining: 10.7s\n",
      "174:\tlearn: 0.1155777\ttotal: 2.26s\tremaining: 10.6s\n",
      "175:\tlearn: 0.1149415\ttotal: 2.27s\tremaining: 10.6s\n",
      "176:\tlearn: 0.1141265\ttotal: 2.29s\tremaining: 10.6s\n",
      "177:\tlearn: 0.1138118\ttotal: 2.31s\tremaining: 10.6s\n",
      "178:\tlearn: 0.1132746\ttotal: 2.32s\tremaining: 10.7s\n",
      "179:\tlearn: 0.1129659\ttotal: 2.34s\tremaining: 10.7s\n",
      "180:\tlearn: 0.1125093\ttotal: 2.35s\tremaining: 10.7s\n",
      "181:\tlearn: 0.1123111\ttotal: 2.37s\tremaining: 10.6s\n",
      "182:\tlearn: 0.1121614\ttotal: 2.38s\tremaining: 10.6s\n",
      "183:\tlearn: 0.1117889\ttotal: 2.39s\tremaining: 10.6s\n",
      "184:\tlearn: 0.1112220\ttotal: 2.4s\tremaining: 10.6s\n",
      "185:\tlearn: 0.1106817\ttotal: 2.41s\tremaining: 10.5s\n",
      "186:\tlearn: 0.1100388\ttotal: 2.42s\tremaining: 10.5s\n",
      "187:\tlearn: 0.1095495\ttotal: 2.43s\tremaining: 10.5s\n",
      "188:\tlearn: 0.1091457\ttotal: 2.44s\tremaining: 10.5s\n",
      "189:\tlearn: 0.1087893\ttotal: 2.44s\tremaining: 10.4s\n",
      "190:\tlearn: 0.1082639\ttotal: 2.45s\tremaining: 10.4s\n",
      "191:\tlearn: 0.1076691\ttotal: 2.46s\tremaining: 10.3s\n",
      "192:\tlearn: 0.1073450\ttotal: 2.47s\tremaining: 10.3s\n",
      "193:\tlearn: 0.1066572\ttotal: 2.48s\tremaining: 10.3s\n",
      "194:\tlearn: 0.1062926\ttotal: 2.49s\tremaining: 10.3s\n",
      "195:\tlearn: 0.1059169\ttotal: 2.5s\tremaining: 10.2s\n",
      "196:\tlearn: 0.1055321\ttotal: 2.5s\tremaining: 10.2s\n",
      "197:\tlearn: 0.1050727\ttotal: 2.51s\tremaining: 10.2s\n",
      "198:\tlearn: 0.1046798\ttotal: 2.52s\tremaining: 10.1s\n",
      "199:\tlearn: 0.1043478\ttotal: 2.53s\tremaining: 10.1s\n",
      "200:\tlearn: 0.1038878\ttotal: 2.54s\tremaining: 10.1s\n",
      "201:\tlearn: 0.1035696\ttotal: 2.55s\tremaining: 10.1s\n",
      "202:\tlearn: 0.1032663\ttotal: 2.56s\tremaining: 10.1s\n",
      "203:\tlearn: 0.1030018\ttotal: 2.57s\tremaining: 10s\n",
      "204:\tlearn: 0.1027316\ttotal: 2.59s\tremaining: 10s\n",
      "205:\tlearn: 0.1025337\ttotal: 2.6s\tremaining: 10s\n",
      "206:\tlearn: 0.1022917\ttotal: 2.61s\tremaining: 9.99s\n",
      "207:\tlearn: 0.1020399\ttotal: 2.62s\tremaining: 9.97s\n",
      "208:\tlearn: 0.1016837\ttotal: 2.63s\tremaining: 9.95s\n",
      "209:\tlearn: 0.1014508\ttotal: 2.64s\tremaining: 9.93s\n",
      "210:\tlearn: 0.1012881\ttotal: 2.65s\tremaining: 9.91s\n",
      "211:\tlearn: 0.1009841\ttotal: 2.66s\tremaining: 9.89s\n",
      "212:\tlearn: 0.1008057\ttotal: 2.67s\tremaining: 9.87s\n",
      "213:\tlearn: 0.1006113\ttotal: 2.68s\tremaining: 9.85s\n",
      "214:\tlearn: 0.1004135\ttotal: 2.69s\tremaining: 9.82s\n",
      "215:\tlearn: 0.1002820\ttotal: 2.7s\tremaining: 9.81s\n",
      "216:\tlearn: 0.1001487\ttotal: 2.71s\tremaining: 9.79s\n",
      "217:\tlearn: 0.0999552\ttotal: 2.72s\tremaining: 9.77s\n",
      "218:\tlearn: 0.0997712\ttotal: 2.73s\tremaining: 9.75s\n",
      "219:\tlearn: 0.0995869\ttotal: 2.74s\tremaining: 9.73s\n",
      "220:\tlearn: 0.0994833\ttotal: 2.77s\tremaining: 9.75s\n",
      "221:\tlearn: 0.0992720\ttotal: 2.8s\tremaining: 9.81s\n",
      "222:\tlearn: 0.0991120\ttotal: 2.81s\tremaining: 9.81s\n",
      "223:\tlearn: 0.0989766\ttotal: 2.84s\tremaining: 9.82s\n",
      "224:\tlearn: 0.0986919\ttotal: 2.86s\tremaining: 9.86s\n",
      "225:\tlearn: 0.0984873\ttotal: 2.88s\tremaining: 9.86s\n",
      "226:\tlearn: 0.0983647\ttotal: 2.89s\tremaining: 9.84s\n",
      "227:\tlearn: 0.0982691\ttotal: 2.9s\tremaining: 9.82s\n",
      "228:\tlearn: 0.0980235\ttotal: 2.91s\tremaining: 9.8s\n",
      "229:\tlearn: 0.0978615\ttotal: 2.92s\tremaining: 9.78s\n",
      "230:\tlearn: 0.0977867\ttotal: 2.93s\tremaining: 9.77s\n",
      "231:\tlearn: 0.0976870\ttotal: 2.94s\tremaining: 9.75s\n",
      "232:\tlearn: 0.0974900\ttotal: 2.96s\tremaining: 9.74s\n",
      "233:\tlearn: 0.0973135\ttotal: 2.97s\tremaining: 9.73s\n",
      "234:\tlearn: 0.0971796\ttotal: 2.98s\tremaining: 9.7s\n",
      "235:\tlearn: 0.0969793\ttotal: 2.99s\tremaining: 9.68s\n",
      "236:\tlearn: 0.0968243\ttotal: 3s\tremaining: 9.68s\n",
      "237:\tlearn: 0.0966600\ttotal: 3.01s\tremaining: 9.65s\n",
      "238:\tlearn: 0.0965809\ttotal: 3.02s\tremaining: 9.63s\n",
      "239:\tlearn: 0.0964334\ttotal: 3.03s\tremaining: 9.61s\n",
      "240:\tlearn: 0.0963443\ttotal: 3.04s\tremaining: 9.58s\n",
      "241:\tlearn: 0.0961940\ttotal: 3.05s\tremaining: 9.56s\n",
      "242:\tlearn: 0.0960926\ttotal: 3.06s\tremaining: 9.55s\n",
      "243:\tlearn: 0.0959967\ttotal: 3.08s\tremaining: 9.53s\n",
      "244:\tlearn: 0.0959136\ttotal: 3.09s\tremaining: 9.51s\n",
      "245:\tlearn: 0.0957592\ttotal: 3.1s\tremaining: 9.49s\n",
      "246:\tlearn: 0.0956402\ttotal: 3.11s\tremaining: 9.47s\n",
      "247:\tlearn: 0.0955753\ttotal: 3.12s\tremaining: 9.45s\n",
      "248:\tlearn: 0.0954457\ttotal: 3.13s\tremaining: 9.43s\n",
      "249:\tlearn: 0.0953398\ttotal: 3.14s\tremaining: 9.41s\n",
      "250:\tlearn: 0.0951616\ttotal: 3.15s\tremaining: 9.39s\n",
      "251:\tlearn: 0.0950456\ttotal: 3.16s\tremaining: 9.37s\n",
      "252:\tlearn: 0.0949378\ttotal: 3.17s\tremaining: 9.36s\n",
      "253:\tlearn: 0.0947669\ttotal: 3.18s\tremaining: 9.34s\n",
      "254:\tlearn: 0.0945274\ttotal: 3.19s\tremaining: 9.32s\n",
      "255:\tlearn: 0.0944292\ttotal: 3.2s\tremaining: 9.3s\n",
      "256:\tlearn: 0.0943059\ttotal: 3.21s\tremaining: 9.28s\n",
      "257:\tlearn: 0.0942131\ttotal: 3.22s\tremaining: 9.26s\n",
      "258:\tlearn: 0.0940935\ttotal: 3.23s\tremaining: 9.25s\n",
      "259:\tlearn: 0.0940064\ttotal: 3.24s\tremaining: 9.23s\n",
      "260:\tlearn: 0.0938856\ttotal: 3.25s\tremaining: 9.22s\n",
      "261:\tlearn: 0.0937738\ttotal: 3.27s\tremaining: 9.2s\n",
      "262:\tlearn: 0.0936748\ttotal: 3.28s\tremaining: 9.18s\n",
      "263:\tlearn: 0.0934597\ttotal: 3.29s\tremaining: 9.16s\n",
      "264:\tlearn: 0.0933010\ttotal: 3.3s\tremaining: 9.15s\n",
      "265:\tlearn: 0.0932088\ttotal: 3.31s\tremaining: 9.13s\n",
      "266:\tlearn: 0.0931566\ttotal: 3.32s\tremaining: 9.11s\n",
      "267:\tlearn: 0.0931127\ttotal: 3.33s\tremaining: 9.09s\n",
      "268:\tlearn: 0.0930273\ttotal: 3.34s\tremaining: 9.07s\n",
      "269:\tlearn: 0.0929134\ttotal: 3.35s\tremaining: 9.05s\n",
      "270:\tlearn: 0.0928538\ttotal: 3.36s\tremaining: 9.03s\n",
      "271:\tlearn: 0.0926644\ttotal: 3.37s\tremaining: 9.01s\n",
      "272:\tlearn: 0.0925175\ttotal: 3.38s\tremaining: 8.99s\n",
      "273:\tlearn: 0.0924366\ttotal: 3.38s\tremaining: 8.97s\n",
      "274:\tlearn: 0.0923379\ttotal: 3.4s\tremaining: 8.96s\n",
      "275:\tlearn: 0.0922851\ttotal: 3.41s\tremaining: 8.94s\n",
      "276:\tlearn: 0.0922020\ttotal: 3.42s\tremaining: 8.91s\n",
      "277:\tlearn: 0.0920693\ttotal: 3.43s\tremaining: 8.9s\n",
      "278:\tlearn: 0.0919757\ttotal: 3.44s\tremaining: 8.89s\n",
      "279:\tlearn: 0.0919015\ttotal: 3.45s\tremaining: 8.87s\n",
      "280:\tlearn: 0.0917332\ttotal: 3.46s\tremaining: 8.85s\n",
      "281:\tlearn: 0.0916767\ttotal: 3.47s\tremaining: 8.84s\n",
      "282:\tlearn: 0.0915522\ttotal: 3.48s\tremaining: 8.82s\n",
      "283:\tlearn: 0.0914976\ttotal: 3.49s\tremaining: 8.8s\n",
      "284:\tlearn: 0.0913882\ttotal: 3.5s\tremaining: 8.79s\n",
      "285:\tlearn: 0.0913048\ttotal: 3.51s\tremaining: 8.77s\n",
      "286:\tlearn: 0.0912239\ttotal: 3.52s\tremaining: 8.75s\n",
      "287:\tlearn: 0.0911468\ttotal: 3.53s\tremaining: 8.73s\n",
      "288:\tlearn: 0.0910387\ttotal: 3.54s\tremaining: 8.71s\n",
      "289:\tlearn: 0.0909935\ttotal: 3.56s\tremaining: 8.71s\n",
      "290:\tlearn: 0.0909160\ttotal: 3.56s\tremaining: 8.68s\n",
      "291:\tlearn: 0.0907403\ttotal: 3.57s\tremaining: 8.66s\n",
      "292:\tlearn: 0.0906629\ttotal: 3.59s\tremaining: 8.66s\n",
      "293:\tlearn: 0.0906138\ttotal: 3.6s\tremaining: 8.64s\n",
      "294:\tlearn: 0.0905115\ttotal: 3.61s\tremaining: 8.62s\n",
      "295:\tlearn: 0.0904531\ttotal: 3.62s\tremaining: 8.61s\n",
      "296:\tlearn: 0.0904027\ttotal: 3.63s\tremaining: 8.59s\n",
      "297:\tlearn: 0.0902462\ttotal: 3.64s\tremaining: 8.57s\n",
      "298:\tlearn: 0.0901883\ttotal: 3.65s\tremaining: 8.56s\n",
      "299:\tlearn: 0.0900959\ttotal: 3.66s\tremaining: 8.54s\n",
      "300:\tlearn: 0.0900545\ttotal: 3.67s\tremaining: 8.53s\n",
      "301:\tlearn: 0.0899826\ttotal: 3.68s\tremaining: 8.51s\n",
      "302:\tlearn: 0.0898530\ttotal: 3.69s\tremaining: 8.5s\n",
      "303:\tlearn: 0.0898037\ttotal: 3.71s\tremaining: 8.49s\n",
      "304:\tlearn: 0.0897543\ttotal: 3.72s\tremaining: 8.47s\n",
      "305:\tlearn: 0.0896982\ttotal: 3.73s\tremaining: 8.46s\n",
      "306:\tlearn: 0.0896365\ttotal: 3.74s\tremaining: 8.45s\n",
      "307:\tlearn: 0.0895549\ttotal: 3.75s\tremaining: 8.43s\n",
      "308:\tlearn: 0.0895109\ttotal: 3.76s\tremaining: 8.41s\n",
      "309:\tlearn: 0.0894220\ttotal: 3.78s\tremaining: 8.41s\n",
      "310:\tlearn: 0.0893716\ttotal: 3.8s\tremaining: 8.41s\n",
      "311:\tlearn: 0.0892957\ttotal: 3.83s\tremaining: 8.44s\n",
      "312:\tlearn: 0.0892328\ttotal: 3.84s\tremaining: 8.44s\n",
      "313:\tlearn: 0.0891675\ttotal: 3.86s\tremaining: 8.43s\n",
      "314:\tlearn: 0.0891162\ttotal: 3.89s\tremaining: 8.45s\n",
      "315:\tlearn: 0.0890710\ttotal: 3.9s\tremaining: 8.44s\n",
      "316:\tlearn: 0.0890343\ttotal: 3.91s\tremaining: 8.43s\n",
      "317:\tlearn: 0.0890106\ttotal: 3.92s\tremaining: 8.41s\n",
      "318:\tlearn: 0.0889672\ttotal: 3.93s\tremaining: 8.39s\n",
      "319:\tlearn: 0.0889135\ttotal: 3.94s\tremaining: 8.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320:\tlearn: 0.0888810\ttotal: 3.95s\tremaining: 8.36s\n",
      "321:\tlearn: 0.0888278\ttotal: 3.96s\tremaining: 8.35s\n",
      "322:\tlearn: 0.0887025\ttotal: 3.98s\tremaining: 8.34s\n",
      "323:\tlearn: 0.0886652\ttotal: 3.98s\tremaining: 8.32s\n",
      "324:\tlearn: 0.0885920\ttotal: 4s\tremaining: 8.3s\n",
      "325:\tlearn: 0.0885365\ttotal: 4.01s\tremaining: 8.29s\n",
      "326:\tlearn: 0.0884819\ttotal: 4.02s\tremaining: 8.27s\n",
      "327:\tlearn: 0.0884438\ttotal: 4.03s\tremaining: 8.26s\n",
      "328:\tlearn: 0.0883378\ttotal: 4.04s\tremaining: 8.24s\n",
      "329:\tlearn: 0.0883066\ttotal: 4.05s\tremaining: 8.23s\n",
      "330:\tlearn: 0.0882703\ttotal: 4.07s\tremaining: 8.22s\n",
      "331:\tlearn: 0.0882276\ttotal: 4.08s\tremaining: 8.2s\n",
      "332:\tlearn: 0.0881350\ttotal: 4.08s\tremaining: 8.18s\n",
      "333:\tlearn: 0.0880910\ttotal: 4.1s\tremaining: 8.17s\n",
      "334:\tlearn: 0.0880187\ttotal: 4.11s\tremaining: 8.16s\n",
      "335:\tlearn: 0.0879648\ttotal: 4.12s\tremaining: 8.14s\n",
      "336:\tlearn: 0.0878786\ttotal: 4.13s\tremaining: 8.12s\n",
      "337:\tlearn: 0.0878516\ttotal: 4.13s\tremaining: 8.1s\n",
      "338:\tlearn: 0.0877520\ttotal: 4.14s\tremaining: 8.07s\n",
      "339:\tlearn: 0.0876903\ttotal: 4.15s\tremaining: 8.06s\n",
      "340:\tlearn: 0.0876500\ttotal: 4.16s\tremaining: 8.04s\n",
      "341:\tlearn: 0.0876138\ttotal: 4.17s\tremaining: 8.03s\n",
      "342:\tlearn: 0.0875670\ttotal: 4.18s\tremaining: 8.01s\n",
      "343:\tlearn: 0.0875234\ttotal: 4.19s\tremaining: 8s\n",
      "344:\tlearn: 0.0874622\ttotal: 4.2s\tremaining: 7.98s\n",
      "345:\tlearn: 0.0873691\ttotal: 4.21s\tremaining: 7.97s\n",
      "346:\tlearn: 0.0872930\ttotal: 4.22s\tremaining: 7.95s\n",
      "347:\tlearn: 0.0872649\ttotal: 4.23s\tremaining: 7.93s\n",
      "348:\tlearn: 0.0872308\ttotal: 4.25s\tremaining: 7.93s\n",
      "349:\tlearn: 0.0871984\ttotal: 4.26s\tremaining: 7.92s\n",
      "350:\tlearn: 0.0871124\ttotal: 4.28s\tremaining: 7.91s\n",
      "351:\tlearn: 0.0870668\ttotal: 4.29s\tremaining: 7.9s\n",
      "352:\tlearn: 0.0870248\ttotal: 4.3s\tremaining: 7.88s\n",
      "353:\tlearn: 0.0870034\ttotal: 4.31s\tremaining: 7.87s\n",
      "354:\tlearn: 0.0869812\ttotal: 4.32s\tremaining: 7.85s\n",
      "355:\tlearn: 0.0869234\ttotal: 4.33s\tremaining: 7.84s\n",
      "356:\tlearn: 0.0868903\ttotal: 4.34s\tremaining: 7.82s\n",
      "357:\tlearn: 0.0868521\ttotal: 4.35s\tremaining: 7.8s\n",
      "358:\tlearn: 0.0868118\ttotal: 4.36s\tremaining: 7.78s\n",
      "359:\tlearn: 0.0867864\ttotal: 4.37s\tremaining: 7.76s\n",
      "360:\tlearn: 0.0867424\ttotal: 4.38s\tremaining: 7.75s\n",
      "361:\tlearn: 0.0866474\ttotal: 4.39s\tremaining: 7.74s\n",
      "362:\tlearn: 0.0865953\ttotal: 4.4s\tremaining: 7.72s\n",
      "363:\tlearn: 0.0865630\ttotal: 4.41s\tremaining: 7.7s\n",
      "364:\tlearn: 0.0864793\ttotal: 4.42s\tremaining: 7.69s\n",
      "365:\tlearn: 0.0864307\ttotal: 4.43s\tremaining: 7.67s\n",
      "366:\tlearn: 0.0863789\ttotal: 4.43s\tremaining: 7.65s\n",
      "367:\tlearn: 0.0863551\ttotal: 4.44s\tremaining: 7.63s\n",
      "368:\tlearn: 0.0863379\ttotal: 4.45s\tremaining: 7.62s\n",
      "369:\tlearn: 0.0862524\ttotal: 4.46s\tremaining: 7.6s\n",
      "370:\tlearn: 0.0862242\ttotal: 4.47s\tremaining: 7.58s\n",
      "371:\tlearn: 0.0861402\ttotal: 4.48s\tremaining: 7.57s\n",
      "372:\tlearn: 0.0860873\ttotal: 4.49s\tremaining: 7.55s\n",
      "373:\tlearn: 0.0860542\ttotal: 4.5s\tremaining: 7.53s\n",
      "374:\tlearn: 0.0860117\ttotal: 4.51s\tremaining: 7.51s\n",
      "375:\tlearn: 0.0859653\ttotal: 4.52s\tremaining: 7.5s\n",
      "376:\tlearn: 0.0859325\ttotal: 4.53s\tremaining: 7.49s\n",
      "377:\tlearn: 0.0859120\ttotal: 4.54s\tremaining: 7.48s\n",
      "378:\tlearn: 0.0858936\ttotal: 4.56s\tremaining: 7.47s\n",
      "379:\tlearn: 0.0858622\ttotal: 4.57s\tremaining: 7.45s\n",
      "380:\tlearn: 0.0857976\ttotal: 4.58s\tremaining: 7.43s\n",
      "381:\tlearn: 0.0857182\ttotal: 4.58s\tremaining: 7.42s\n",
      "382:\tlearn: 0.0856770\ttotal: 4.59s\tremaining: 7.4s\n",
      "383:\tlearn: 0.0856496\ttotal: 4.61s\tremaining: 7.39s\n",
      "384:\tlearn: 0.0856212\ttotal: 4.62s\tremaining: 7.38s\n",
      "385:\tlearn: 0.0855579\ttotal: 4.63s\tremaining: 7.36s\n",
      "386:\tlearn: 0.0855148\ttotal: 4.64s\tremaining: 7.34s\n",
      "387:\tlearn: 0.0854991\ttotal: 4.64s\tremaining: 7.32s\n",
      "388:\tlearn: 0.0854674\ttotal: 4.66s\tremaining: 7.31s\n",
      "389:\tlearn: 0.0853876\ttotal: 4.66s\tremaining: 7.29s\n",
      "390:\tlearn: 0.0853267\ttotal: 4.67s\tremaining: 7.27s\n",
      "391:\tlearn: 0.0853057\ttotal: 4.68s\tremaining: 7.26s\n",
      "392:\tlearn: 0.0851840\ttotal: 4.69s\tremaining: 7.24s\n",
      "393:\tlearn: 0.0851531\ttotal: 4.7s\tremaining: 7.23s\n",
      "394:\tlearn: 0.0851218\ttotal: 4.71s\tremaining: 7.22s\n",
      "395:\tlearn: 0.0850721\ttotal: 4.72s\tremaining: 7.2s\n",
      "396:\tlearn: 0.0850420\ttotal: 4.73s\tremaining: 7.19s\n",
      "397:\tlearn: 0.0849972\ttotal: 4.75s\tremaining: 7.18s\n",
      "398:\tlearn: 0.0849675\ttotal: 4.75s\tremaining: 7.16s\n",
      "399:\tlearn: 0.0849146\ttotal: 4.76s\tremaining: 7.14s\n",
      "400:\tlearn: 0.0848798\ttotal: 4.77s\tremaining: 7.12s\n",
      "401:\tlearn: 0.0848480\ttotal: 4.78s\tremaining: 7.11s\n",
      "402:\tlearn: 0.0847711\ttotal: 4.79s\tremaining: 7.1s\n",
      "403:\tlearn: 0.0847476\ttotal: 4.82s\tremaining: 7.1s\n",
      "404:\tlearn: 0.0847151\ttotal: 4.84s\tremaining: 7.11s\n",
      "405:\tlearn: 0.0846537\ttotal: 4.86s\tremaining: 7.11s\n",
      "406:\tlearn: 0.0846081\ttotal: 4.9s\tremaining: 7.14s\n",
      "407:\tlearn: 0.0845414\ttotal: 4.92s\tremaining: 7.14s\n",
      "408:\tlearn: 0.0844902\ttotal: 4.93s\tremaining: 7.12s\n",
      "409:\tlearn: 0.0844378\ttotal: 4.94s\tremaining: 7.11s\n",
      "410:\tlearn: 0.0843975\ttotal: 4.94s\tremaining: 7.08s\n",
      "411:\tlearn: 0.0843649\ttotal: 4.96s\tremaining: 7.08s\n",
      "412:\tlearn: 0.0843352\ttotal: 4.97s\tremaining: 7.06s\n",
      "413:\tlearn: 0.0842200\ttotal: 4.98s\tremaining: 7.04s\n",
      "414:\tlearn: 0.0841636\ttotal: 4.99s\tremaining: 7.03s\n",
      "415:\tlearn: 0.0841251\ttotal: 5s\tremaining: 7.02s\n",
      "416:\tlearn: 0.0840708\ttotal: 5.01s\tremaining: 7.01s\n",
      "417:\tlearn: 0.0840443\ttotal: 5.02s\tremaining: 6.99s\n",
      "418:\tlearn: 0.0839953\ttotal: 5.03s\tremaining: 6.97s\n",
      "419:\tlearn: 0.0839660\ttotal: 5.03s\tremaining: 6.95s\n",
      "420:\tlearn: 0.0839119\ttotal: 5.04s\tremaining: 6.94s\n",
      "421:\tlearn: 0.0838529\ttotal: 5.05s\tremaining: 6.92s\n",
      "422:\tlearn: 0.0838342\ttotal: 5.06s\tremaining: 6.9s\n",
      "423:\tlearn: 0.0838077\ttotal: 5.07s\tremaining: 6.89s\n",
      "424:\tlearn: 0.0837879\ttotal: 5.08s\tremaining: 6.88s\n",
      "425:\tlearn: 0.0837679\ttotal: 5.09s\tremaining: 6.86s\n",
      "426:\tlearn: 0.0837338\ttotal: 5.1s\tremaining: 6.85s\n",
      "427:\tlearn: 0.0836964\ttotal: 5.11s\tremaining: 6.83s\n",
      "428:\tlearn: 0.0836569\ttotal: 5.12s\tremaining: 6.82s\n",
      "429:\tlearn: 0.0836258\ttotal: 5.13s\tremaining: 6.81s\n",
      "430:\tlearn: 0.0836091\ttotal: 5.14s\tremaining: 6.79s\n",
      "431:\tlearn: 0.0835654\ttotal: 5.15s\tremaining: 6.77s\n",
      "432:\tlearn: 0.0835105\ttotal: 5.16s\tremaining: 6.76s\n",
      "433:\tlearn: 0.0834885\ttotal: 5.17s\tremaining: 6.75s\n",
      "434:\tlearn: 0.0834522\ttotal: 5.18s\tremaining: 6.73s\n",
      "435:\tlearn: 0.0834083\ttotal: 5.19s\tremaining: 6.72s\n",
      "436:\tlearn: 0.0833847\ttotal: 5.2s\tremaining: 6.7s\n",
      "437:\tlearn: 0.0833385\ttotal: 5.21s\tremaining: 6.69s\n",
      "438:\tlearn: 0.0833168\ttotal: 5.22s\tremaining: 6.67s\n",
      "439:\tlearn: 0.0832846\ttotal: 5.23s\tremaining: 6.66s\n",
      "440:\tlearn: 0.0832561\ttotal: 5.24s\tremaining: 6.64s\n",
      "441:\tlearn: 0.0831736\ttotal: 5.24s\tremaining: 6.62s\n",
      "442:\tlearn: 0.0831516\ttotal: 5.26s\tremaining: 6.61s\n",
      "443:\tlearn: 0.0831345\ttotal: 5.27s\tremaining: 6.59s\n",
      "444:\tlearn: 0.0830984\ttotal: 5.28s\tremaining: 6.58s\n",
      "445:\tlearn: 0.0830802\ttotal: 5.29s\tremaining: 6.57s\n",
      "446:\tlearn: 0.0830497\ttotal: 5.29s\tremaining: 6.55s\n",
      "447:\tlearn: 0.0829569\ttotal: 5.3s\tremaining: 6.53s\n",
      "448:\tlearn: 0.0829454\ttotal: 5.31s\tremaining: 6.52s\n",
      "449:\tlearn: 0.0828988\ttotal: 5.32s\tremaining: 6.5s\n",
      "450:\tlearn: 0.0828788\ttotal: 5.33s\tremaining: 6.49s\n",
      "451:\tlearn: 0.0828386\ttotal: 5.34s\tremaining: 6.48s\n",
      "452:\tlearn: 0.0827881\ttotal: 5.35s\tremaining: 6.46s\n",
      "453:\tlearn: 0.0827593\ttotal: 5.36s\tremaining: 6.44s\n",
      "454:\tlearn: 0.0826700\ttotal: 5.36s\tremaining: 6.42s\n",
      "455:\tlearn: 0.0826496\ttotal: 5.37s\tremaining: 6.41s\n",
      "456:\tlearn: 0.0826252\ttotal: 5.38s\tremaining: 6.39s\n",
      "457:\tlearn: 0.0825832\ttotal: 5.39s\tremaining: 6.37s\n",
      "458:\tlearn: 0.0825642\ttotal: 5.39s\tremaining: 6.36s\n",
      "459:\tlearn: 0.0825418\ttotal: 5.4s\tremaining: 6.34s\n",
      "460:\tlearn: 0.0825012\ttotal: 5.41s\tremaining: 6.33s\n",
      "461:\tlearn: 0.0824771\ttotal: 5.43s\tremaining: 6.32s\n",
      "462:\tlearn: 0.0824474\ttotal: 5.43s\tremaining: 6.3s\n",
      "463:\tlearn: 0.0824105\ttotal: 5.44s\tremaining: 6.29s\n",
      "464:\tlearn: 0.0823821\ttotal: 5.45s\tremaining: 6.27s\n",
      "465:\tlearn: 0.0823563\ttotal: 5.46s\tremaining: 6.26s\n",
      "466:\tlearn: 0.0823041\ttotal: 5.47s\tremaining: 6.24s\n",
      "467:\tlearn: 0.0822883\ttotal: 5.47s\tremaining: 6.22s\n",
      "468:\tlearn: 0.0822703\ttotal: 5.48s\tremaining: 6.2s\n",
      "469:\tlearn: 0.0822349\ttotal: 5.49s\tremaining: 6.19s\n",
      "470:\tlearn: 0.0821939\ttotal: 5.5s\tremaining: 6.18s\n",
      "471:\tlearn: 0.0821791\ttotal: 5.51s\tremaining: 6.17s\n",
      "472:\tlearn: 0.0821593\ttotal: 5.52s\tremaining: 6.15s\n",
      "473:\tlearn: 0.0821131\ttotal: 5.53s\tremaining: 6.14s\n",
      "474:\tlearn: 0.0820654\ttotal: 5.54s\tremaining: 6.12s\n",
      "475:\tlearn: 0.0820420\ttotal: 5.55s\tremaining: 6.11s\n",
      "476:\tlearn: 0.0820181\ttotal: 5.56s\tremaining: 6.09s\n",
      "477:\tlearn: 0.0819950\ttotal: 5.57s\tremaining: 6.08s\n",
      "478:\tlearn: 0.0819424\ttotal: 5.58s\tremaining: 6.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479:\tlearn: 0.0819126\ttotal: 5.59s\tremaining: 6.05s\n",
      "480:\tlearn: 0.0818823\ttotal: 5.59s\tremaining: 6.04s\n",
      "481:\tlearn: 0.0818578\ttotal: 5.6s\tremaining: 6.02s\n",
      "482:\tlearn: 0.0818371\ttotal: 5.61s\tremaining: 6s\n",
      "483:\tlearn: 0.0818036\ttotal: 5.62s\tremaining: 5.99s\n",
      "484:\tlearn: 0.0817769\ttotal: 5.63s\tremaining: 5.97s\n",
      "485:\tlearn: 0.0817513\ttotal: 5.64s\tremaining: 5.96s\n",
      "486:\tlearn: 0.0817293\ttotal: 5.64s\tremaining: 5.95s\n",
      "487:\tlearn: 0.0817076\ttotal: 5.65s\tremaining: 5.93s\n",
      "488:\tlearn: 0.0816822\ttotal: 5.67s\tremaining: 5.92s\n",
      "489:\tlearn: 0.0816284\ttotal: 5.68s\tremaining: 5.91s\n",
      "490:\tlearn: 0.0815935\ttotal: 5.69s\tremaining: 5.9s\n",
      "491:\tlearn: 0.0815864\ttotal: 5.7s\tremaining: 5.88s\n",
      "492:\tlearn: 0.0815727\ttotal: 5.71s\tremaining: 5.87s\n",
      "493:\tlearn: 0.0815573\ttotal: 5.72s\tremaining: 5.86s\n",
      "494:\tlearn: 0.0815413\ttotal: 5.73s\tremaining: 5.84s\n",
      "495:\tlearn: 0.0815091\ttotal: 5.82s\tremaining: 5.92s\n",
      "496:\tlearn: 0.0814835\ttotal: 5.87s\tremaining: 5.94s\n",
      "497:\tlearn: 0.0814680\ttotal: 5.99s\tremaining: 6.04s\n",
      "498:\tlearn: 0.0814180\ttotal: 6s\tremaining: 6.02s\n",
      "499:\tlearn: 0.0813822\ttotal: 6.01s\tremaining: 6.01s\n",
      "500:\tlearn: 0.0813528\ttotal: 6.03s\tremaining: 6s\n",
      "501:\tlearn: 0.0813036\ttotal: 6.04s\tremaining: 5.99s\n",
      "502:\tlearn: 0.0812317\ttotal: 6.05s\tremaining: 5.98s\n",
      "503:\tlearn: 0.0812048\ttotal: 6.06s\tremaining: 5.96s\n",
      "504:\tlearn: 0.0811812\ttotal: 6.07s\tremaining: 5.95s\n",
      "505:\tlearn: 0.0811490\ttotal: 6.08s\tremaining: 5.94s\n",
      "506:\tlearn: 0.0811164\ttotal: 6.09s\tremaining: 5.92s\n",
      "507:\tlearn: 0.0810764\ttotal: 6.1s\tremaining: 5.91s\n",
      "508:\tlearn: 0.0810499\ttotal: 6.11s\tremaining: 5.9s\n",
      "509:\tlearn: 0.0810321\ttotal: 6.12s\tremaining: 5.88s\n",
      "510:\tlearn: 0.0809927\ttotal: 6.13s\tremaining: 5.87s\n",
      "511:\tlearn: 0.0809573\ttotal: 6.14s\tremaining: 5.86s\n",
      "512:\tlearn: 0.0809425\ttotal: 6.16s\tremaining: 5.85s\n",
      "513:\tlearn: 0.0809219\ttotal: 6.17s\tremaining: 5.84s\n",
      "514:\tlearn: 0.0809031\ttotal: 6.18s\tremaining: 5.82s\n",
      "515:\tlearn: 0.0808569\ttotal: 6.19s\tremaining: 5.81s\n",
      "516:\tlearn: 0.0808090\ttotal: 6.21s\tremaining: 5.8s\n",
      "517:\tlearn: 0.0807885\ttotal: 6.22s\tremaining: 5.79s\n",
      "518:\tlearn: 0.0807536\ttotal: 6.23s\tremaining: 5.78s\n",
      "519:\tlearn: 0.0807337\ttotal: 6.25s\tremaining: 5.76s\n",
      "520:\tlearn: 0.0807274\ttotal: 6.26s\tremaining: 5.76s\n",
      "521:\tlearn: 0.0807019\ttotal: 6.27s\tremaining: 5.74s\n",
      "522:\tlearn: 0.0806751\ttotal: 6.29s\tremaining: 5.73s\n",
      "523:\tlearn: 0.0806360\ttotal: 6.3s\tremaining: 5.73s\n",
      "524:\tlearn: 0.0806197\ttotal: 6.31s\tremaining: 5.71s\n",
      "525:\tlearn: 0.0805936\ttotal: 6.32s\tremaining: 5.7s\n",
      "526:\tlearn: 0.0805561\ttotal: 6.33s\tremaining: 5.68s\n",
      "527:\tlearn: 0.0805115\ttotal: 6.34s\tremaining: 5.66s\n",
      "528:\tlearn: 0.0804984\ttotal: 6.35s\tremaining: 5.65s\n",
      "529:\tlearn: 0.0804846\ttotal: 6.38s\tremaining: 5.65s\n",
      "530:\tlearn: 0.0804406\ttotal: 6.4s\tremaining: 5.66s\n",
      "531:\tlearn: 0.0804208\ttotal: 6.42s\tremaining: 5.65s\n",
      "532:\tlearn: 0.0803814\ttotal: 6.44s\tremaining: 5.64s\n",
      "533:\tlearn: 0.0803719\ttotal: 6.46s\tremaining: 5.64s\n",
      "534:\tlearn: 0.0803371\ttotal: 6.47s\tremaining: 5.63s\n",
      "535:\tlearn: 0.0803234\ttotal: 6.5s\tremaining: 5.62s\n",
      "536:\tlearn: 0.0803055\ttotal: 6.51s\tremaining: 5.61s\n",
      "537:\tlearn: 0.0802737\ttotal: 6.54s\tremaining: 5.61s\n",
      "538:\tlearn: 0.0802533\ttotal: 6.57s\tremaining: 5.62s\n",
      "539:\tlearn: 0.0802171\ttotal: 6.58s\tremaining: 5.61s\n",
      "540:\tlearn: 0.0801933\ttotal: 6.6s\tremaining: 5.6s\n",
      "541:\tlearn: 0.0801828\ttotal: 6.62s\tremaining: 5.59s\n",
      "542:\tlearn: 0.0801546\ttotal: 6.64s\tremaining: 5.59s\n",
      "543:\tlearn: 0.0801369\ttotal: 6.66s\tremaining: 5.58s\n",
      "544:\tlearn: 0.0801216\ttotal: 6.68s\tremaining: 5.57s\n",
      "545:\tlearn: 0.0800756\ttotal: 6.69s\tremaining: 5.57s\n",
      "546:\tlearn: 0.0800590\ttotal: 6.71s\tremaining: 5.56s\n",
      "547:\tlearn: 0.0800463\ttotal: 6.73s\tremaining: 5.55s\n",
      "548:\tlearn: 0.0800344\ttotal: 6.77s\tremaining: 5.56s\n",
      "549:\tlearn: 0.0799853\ttotal: 6.79s\tremaining: 5.55s\n",
      "550:\tlearn: 0.0799620\ttotal: 6.8s\tremaining: 5.54s\n",
      "551:\tlearn: 0.0799300\ttotal: 6.83s\tremaining: 5.54s\n",
      "552:\tlearn: 0.0799045\ttotal: 6.84s\tremaining: 5.53s\n",
      "553:\tlearn: 0.0798548\ttotal: 6.85s\tremaining: 5.51s\n",
      "554:\tlearn: 0.0798100\ttotal: 6.86s\tremaining: 5.5s\n",
      "555:\tlearn: 0.0797886\ttotal: 6.88s\tremaining: 5.49s\n",
      "556:\tlearn: 0.0797685\ttotal: 6.9s\tremaining: 5.49s\n",
      "557:\tlearn: 0.0797439\ttotal: 6.92s\tremaining: 5.48s\n",
      "558:\tlearn: 0.0797191\ttotal: 6.97s\tremaining: 5.5s\n",
      "559:\tlearn: 0.0797152\ttotal: 7.05s\tremaining: 5.54s\n",
      "560:\tlearn: 0.0796906\ttotal: 7.06s\tremaining: 5.53s\n",
      "561:\tlearn: 0.0796687\ttotal: 7.07s\tremaining: 5.51s\n",
      "562:\tlearn: 0.0796497\ttotal: 7.08s\tremaining: 5.49s\n",
      "563:\tlearn: 0.0796359\ttotal: 7.09s\tremaining: 5.48s\n",
      "564:\tlearn: 0.0796113\ttotal: 7.1s\tremaining: 5.47s\n",
      "565:\tlearn: 0.0795979\ttotal: 7.11s\tremaining: 5.45s\n",
      "566:\tlearn: 0.0795701\ttotal: 7.12s\tremaining: 5.43s\n",
      "567:\tlearn: 0.0795427\ttotal: 7.12s\tremaining: 5.42s\n",
      "568:\tlearn: 0.0794877\ttotal: 7.13s\tremaining: 5.4s\n",
      "569:\tlearn: 0.0794728\ttotal: 7.13s\tremaining: 5.38s\n",
      "570:\tlearn: 0.0794609\ttotal: 7.14s\tremaining: 5.37s\n",
      "571:\tlearn: 0.0794204\ttotal: 7.15s\tremaining: 5.35s\n",
      "572:\tlearn: 0.0793795\ttotal: 7.16s\tremaining: 5.33s\n",
      "573:\tlearn: 0.0793541\ttotal: 7.16s\tremaining: 5.32s\n",
      "574:\tlearn: 0.0793418\ttotal: 7.17s\tremaining: 5.3s\n",
      "575:\tlearn: 0.0793243\ttotal: 7.17s\tremaining: 5.28s\n",
      "576:\tlearn: 0.0792863\ttotal: 7.18s\tremaining: 5.26s\n",
      "577:\tlearn: 0.0792728\ttotal: 7.19s\tremaining: 5.25s\n",
      "578:\tlearn: 0.0792589\ttotal: 7.19s\tremaining: 5.23s\n",
      "579:\tlearn: 0.0792171\ttotal: 7.2s\tremaining: 5.21s\n",
      "580:\tlearn: 0.0791834\ttotal: 7.21s\tremaining: 5.2s\n",
      "581:\tlearn: 0.0791603\ttotal: 7.21s\tremaining: 5.18s\n",
      "582:\tlearn: 0.0791275\ttotal: 7.22s\tremaining: 5.16s\n",
      "583:\tlearn: 0.0790970\ttotal: 7.22s\tremaining: 5.15s\n",
      "584:\tlearn: 0.0790719\ttotal: 7.23s\tremaining: 5.13s\n",
      "585:\tlearn: 0.0790426\ttotal: 7.24s\tremaining: 5.11s\n",
      "586:\tlearn: 0.0789974\ttotal: 7.24s\tremaining: 5.09s\n",
      "587:\tlearn: 0.0789760\ttotal: 7.25s\tremaining: 5.08s\n",
      "588:\tlearn: 0.0789121\ttotal: 7.27s\tremaining: 5.07s\n",
      "589:\tlearn: 0.0788806\ttotal: 7.28s\tremaining: 5.06s\n",
      "590:\tlearn: 0.0788544\ttotal: 7.29s\tremaining: 5.05s\n",
      "591:\tlearn: 0.0788065\ttotal: 7.3s\tremaining: 5.03s\n",
      "592:\tlearn: 0.0787829\ttotal: 7.33s\tremaining: 5.03s\n",
      "593:\tlearn: 0.0787580\ttotal: 7.34s\tremaining: 5.02s\n",
      "594:\tlearn: 0.0787454\ttotal: 7.35s\tremaining: 5s\n",
      "595:\tlearn: 0.0787277\ttotal: 7.37s\tremaining: 4.99s\n",
      "596:\tlearn: 0.0786995\ttotal: 7.38s\tremaining: 4.98s\n",
      "597:\tlearn: 0.0786746\ttotal: 7.39s\tremaining: 4.97s\n",
      "598:\tlearn: 0.0786603\ttotal: 7.41s\tremaining: 4.96s\n",
      "599:\tlearn: 0.0786409\ttotal: 7.42s\tremaining: 4.94s\n",
      "600:\tlearn: 0.0786139\ttotal: 7.42s\tremaining: 4.93s\n",
      "601:\tlearn: 0.0785900\ttotal: 7.45s\tremaining: 4.92s\n",
      "602:\tlearn: 0.0785455\ttotal: 7.46s\tremaining: 4.91s\n",
      "603:\tlearn: 0.0785155\ttotal: 7.47s\tremaining: 4.9s\n",
      "604:\tlearn: 0.0785006\ttotal: 7.48s\tremaining: 4.88s\n",
      "605:\tlearn: 0.0784870\ttotal: 7.5s\tremaining: 4.88s\n",
      "606:\tlearn: 0.0784738\ttotal: 7.51s\tremaining: 4.86s\n",
      "607:\tlearn: 0.0784330\ttotal: 7.53s\tremaining: 4.85s\n",
      "608:\tlearn: 0.0784148\ttotal: 7.54s\tremaining: 4.84s\n",
      "609:\tlearn: 0.0783928\ttotal: 7.55s\tremaining: 4.83s\n",
      "610:\tlearn: 0.0783665\ttotal: 7.56s\tremaining: 4.81s\n",
      "611:\tlearn: 0.0783254\ttotal: 7.57s\tremaining: 4.8s\n",
      "612:\tlearn: 0.0782812\ttotal: 7.58s\tremaining: 4.79s\n",
      "613:\tlearn: 0.0782614\ttotal: 7.6s\tremaining: 4.78s\n",
      "614:\tlearn: 0.0782272\ttotal: 7.61s\tremaining: 4.76s\n",
      "615:\tlearn: 0.0781968\ttotal: 7.62s\tremaining: 4.75s\n",
      "616:\tlearn: 0.0781732\ttotal: 7.63s\tremaining: 4.74s\n",
      "617:\tlearn: 0.0781469\ttotal: 7.64s\tremaining: 4.72s\n",
      "618:\tlearn: 0.0781228\ttotal: 7.65s\tremaining: 4.71s\n",
      "619:\tlearn: 0.0780675\ttotal: 7.67s\tremaining: 4.7s\n",
      "620:\tlearn: 0.0780482\ttotal: 7.68s\tremaining: 4.69s\n",
      "621:\tlearn: 0.0780278\ttotal: 7.69s\tremaining: 4.67s\n",
      "622:\tlearn: 0.0780064\ttotal: 7.71s\tremaining: 4.67s\n",
      "623:\tlearn: 0.0779807\ttotal: 7.72s\tremaining: 4.65s\n",
      "624:\tlearn: 0.0779556\ttotal: 7.74s\tremaining: 4.64s\n",
      "625:\tlearn: 0.0779394\ttotal: 7.75s\tremaining: 4.63s\n",
      "626:\tlearn: 0.0779219\ttotal: 7.77s\tremaining: 4.62s\n",
      "627:\tlearn: 0.0778995\ttotal: 7.79s\tremaining: 4.61s\n",
      "628:\tlearn: 0.0778697\ttotal: 7.79s\tremaining: 4.6s\n",
      "629:\tlearn: 0.0778469\ttotal: 7.8s\tremaining: 4.58s\n",
      "630:\tlearn: 0.0778251\ttotal: 7.81s\tremaining: 4.57s\n",
      "631:\tlearn: 0.0778123\ttotal: 7.82s\tremaining: 4.55s\n",
      "632:\tlearn: 0.0777862\ttotal: 7.83s\tremaining: 4.54s\n",
      "633:\tlearn: 0.0777746\ttotal: 7.84s\tremaining: 4.53s\n",
      "634:\tlearn: 0.0777663\ttotal: 7.85s\tremaining: 4.51s\n",
      "635:\tlearn: 0.0777571\ttotal: 7.86s\tremaining: 4.5s\n",
      "636:\tlearn: 0.0777474\ttotal: 7.87s\tremaining: 4.48s\n",
      "637:\tlearn: 0.0777252\ttotal: 7.87s\tremaining: 4.47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638:\tlearn: 0.0777096\ttotal: 7.88s\tremaining: 4.45s\n",
      "639:\tlearn: 0.0776699\ttotal: 7.89s\tremaining: 4.44s\n",
      "640:\tlearn: 0.0776200\ttotal: 7.89s\tremaining: 4.42s\n",
      "641:\tlearn: 0.0776051\ttotal: 7.9s\tremaining: 4.41s\n",
      "642:\tlearn: 0.0775866\ttotal: 7.91s\tremaining: 4.39s\n",
      "643:\tlearn: 0.0775585\ttotal: 7.91s\tremaining: 4.37s\n",
      "644:\tlearn: 0.0775452\ttotal: 7.92s\tremaining: 4.36s\n",
      "645:\tlearn: 0.0775320\ttotal: 7.92s\tremaining: 4.34s\n",
      "646:\tlearn: 0.0775059\ttotal: 7.93s\tremaining: 4.33s\n",
      "647:\tlearn: 0.0774929\ttotal: 7.94s\tremaining: 4.32s\n",
      "648:\tlearn: 0.0774865\ttotal: 7.96s\tremaining: 4.31s\n",
      "649:\tlearn: 0.0774516\ttotal: 7.98s\tremaining: 4.3s\n",
      "650:\tlearn: 0.0774122\ttotal: 8s\tremaining: 4.29s\n",
      "651:\tlearn: 0.0773967\ttotal: 8.01s\tremaining: 4.27s\n",
      "652:\tlearn: 0.0773711\ttotal: 8.04s\tremaining: 4.27s\n",
      "653:\tlearn: 0.0773426\ttotal: 8.06s\tremaining: 4.27s\n",
      "654:\tlearn: 0.0773192\ttotal: 8.07s\tremaining: 4.25s\n",
      "655:\tlearn: 0.0773029\ttotal: 8.09s\tremaining: 4.24s\n",
      "656:\tlearn: 0.0772901\ttotal: 8.09s\tremaining: 4.22s\n",
      "657:\tlearn: 0.0772819\ttotal: 8.1s\tremaining: 4.21s\n",
      "658:\tlearn: 0.0772725\ttotal: 8.12s\tremaining: 4.2s\n",
      "659:\tlearn: 0.0772432\ttotal: 8.12s\tremaining: 4.18s\n",
      "660:\tlearn: 0.0772263\ttotal: 8.13s\tremaining: 4.17s\n",
      "661:\tlearn: 0.0772132\ttotal: 8.15s\tremaining: 4.16s\n",
      "662:\tlearn: 0.0771799\ttotal: 8.15s\tremaining: 4.14s\n",
      "663:\tlearn: 0.0771457\ttotal: 8.16s\tremaining: 4.13s\n",
      "664:\tlearn: 0.0771234\ttotal: 8.17s\tremaining: 4.12s\n",
      "665:\tlearn: 0.0771060\ttotal: 8.18s\tremaining: 4.1s\n",
      "666:\tlearn: 0.0770902\ttotal: 8.18s\tremaining: 4.09s\n",
      "667:\tlearn: 0.0770560\ttotal: 8.19s\tremaining: 4.07s\n",
      "668:\tlearn: 0.0770449\ttotal: 8.2s\tremaining: 4.05s\n",
      "669:\tlearn: 0.0770282\ttotal: 8.2s\tremaining: 4.04s\n",
      "670:\tlearn: 0.0770093\ttotal: 8.21s\tremaining: 4.03s\n",
      "671:\tlearn: 0.0769989\ttotal: 8.21s\tremaining: 4.01s\n",
      "672:\tlearn: 0.0769564\ttotal: 8.22s\tremaining: 3.99s\n",
      "673:\tlearn: 0.0769431\ttotal: 8.23s\tremaining: 3.98s\n",
      "674:\tlearn: 0.0769326\ttotal: 8.23s\tremaining: 3.96s\n",
      "675:\tlearn: 0.0769088\ttotal: 8.24s\tremaining: 3.95s\n",
      "676:\tlearn: 0.0768881\ttotal: 8.25s\tremaining: 3.93s\n",
      "677:\tlearn: 0.0768738\ttotal: 8.25s\tremaining: 3.92s\n",
      "678:\tlearn: 0.0768464\ttotal: 8.26s\tremaining: 3.9s\n",
      "679:\tlearn: 0.0768317\ttotal: 8.26s\tremaining: 3.89s\n",
      "680:\tlearn: 0.0768169\ttotal: 8.27s\tremaining: 3.87s\n",
      "681:\tlearn: 0.0768044\ttotal: 8.28s\tremaining: 3.86s\n",
      "682:\tlearn: 0.0767878\ttotal: 8.28s\tremaining: 3.84s\n",
      "683:\tlearn: 0.0767786\ttotal: 8.29s\tremaining: 3.83s\n",
      "684:\tlearn: 0.0767702\ttotal: 8.29s\tremaining: 3.81s\n",
      "685:\tlearn: 0.0767463\ttotal: 8.3s\tremaining: 3.8s\n",
      "686:\tlearn: 0.0767269\ttotal: 8.31s\tremaining: 3.79s\n",
      "687:\tlearn: 0.0766968\ttotal: 8.31s\tremaining: 3.77s\n",
      "688:\tlearn: 0.0766837\ttotal: 8.32s\tremaining: 3.76s\n",
      "689:\tlearn: 0.0766587\ttotal: 8.34s\tremaining: 3.74s\n",
      "690:\tlearn: 0.0766180\ttotal: 8.34s\tremaining: 3.73s\n",
      "691:\tlearn: 0.0765969\ttotal: 8.35s\tremaining: 3.72s\n",
      "692:\tlearn: 0.0765806\ttotal: 8.36s\tremaining: 3.7s\n",
      "693:\tlearn: 0.0765598\ttotal: 8.36s\tremaining: 3.69s\n",
      "694:\tlearn: 0.0765395\ttotal: 8.37s\tremaining: 3.67s\n",
      "695:\tlearn: 0.0765144\ttotal: 8.38s\tremaining: 3.66s\n",
      "696:\tlearn: 0.0764811\ttotal: 8.39s\tremaining: 3.65s\n",
      "697:\tlearn: 0.0764635\ttotal: 8.4s\tremaining: 3.63s\n",
      "698:\tlearn: 0.0764518\ttotal: 8.4s\tremaining: 3.62s\n",
      "699:\tlearn: 0.0764376\ttotal: 8.41s\tremaining: 3.6s\n",
      "700:\tlearn: 0.0764179\ttotal: 8.42s\tremaining: 3.59s\n",
      "701:\tlearn: 0.0764048\ttotal: 8.43s\tremaining: 3.58s\n",
      "702:\tlearn: 0.0763854\ttotal: 8.44s\tremaining: 3.56s\n",
      "703:\tlearn: 0.0763715\ttotal: 8.44s\tremaining: 3.55s\n",
      "704:\tlearn: 0.0763514\ttotal: 8.45s\tremaining: 3.54s\n",
      "705:\tlearn: 0.0763217\ttotal: 8.46s\tremaining: 3.52s\n",
      "706:\tlearn: 0.0763023\ttotal: 8.47s\tremaining: 3.51s\n",
      "707:\tlearn: 0.0762999\ttotal: 8.47s\tremaining: 3.49s\n",
      "708:\tlearn: 0.0762836\ttotal: 8.48s\tremaining: 3.48s\n",
      "709:\tlearn: 0.0762699\ttotal: 8.49s\tremaining: 3.47s\n",
      "710:\tlearn: 0.0762595\ttotal: 8.49s\tremaining: 3.45s\n",
      "711:\tlearn: 0.0762516\ttotal: 8.5s\tremaining: 3.44s\n",
      "712:\tlearn: 0.0762107\ttotal: 8.51s\tremaining: 3.42s\n",
      "713:\tlearn: 0.0761960\ttotal: 8.52s\tremaining: 3.41s\n",
      "714:\tlearn: 0.0761791\ttotal: 8.52s\tremaining: 3.4s\n",
      "715:\tlearn: 0.0761672\ttotal: 8.53s\tremaining: 3.38s\n",
      "716:\tlearn: 0.0761466\ttotal: 8.54s\tremaining: 3.37s\n",
      "717:\tlearn: 0.0761375\ttotal: 8.55s\tremaining: 3.36s\n",
      "718:\tlearn: 0.0761218\ttotal: 8.55s\tremaining: 3.34s\n",
      "719:\tlearn: 0.0760951\ttotal: 8.56s\tremaining: 3.33s\n",
      "720:\tlearn: 0.0760796\ttotal: 8.57s\tremaining: 3.31s\n",
      "721:\tlearn: 0.0760658\ttotal: 8.58s\tremaining: 3.3s\n",
      "722:\tlearn: 0.0760520\ttotal: 8.58s\tremaining: 3.29s\n",
      "723:\tlearn: 0.0760395\ttotal: 8.59s\tremaining: 3.27s\n",
      "724:\tlearn: 0.0760197\ttotal: 8.6s\tremaining: 3.26s\n",
      "725:\tlearn: 0.0760092\ttotal: 8.61s\tremaining: 3.25s\n",
      "726:\tlearn: 0.0759931\ttotal: 8.62s\tremaining: 3.23s\n",
      "727:\tlearn: 0.0759688\ttotal: 8.62s\tremaining: 3.22s\n",
      "728:\tlearn: 0.0759539\ttotal: 8.63s\tremaining: 3.21s\n",
      "729:\tlearn: 0.0759354\ttotal: 8.64s\tremaining: 3.19s\n",
      "730:\tlearn: 0.0759198\ttotal: 8.65s\tremaining: 3.18s\n",
      "731:\tlearn: 0.0759042\ttotal: 8.65s\tremaining: 3.17s\n",
      "732:\tlearn: 0.0758927\ttotal: 8.66s\tremaining: 3.15s\n",
      "733:\tlearn: 0.0758721\ttotal: 8.67s\tremaining: 3.14s\n",
      "734:\tlearn: 0.0758595\ttotal: 8.67s\tremaining: 3.13s\n",
      "735:\tlearn: 0.0758430\ttotal: 8.68s\tremaining: 3.11s\n",
      "736:\tlearn: 0.0758250\ttotal: 8.69s\tremaining: 3.1s\n",
      "737:\tlearn: 0.0758133\ttotal: 8.69s\tremaining: 3.09s\n",
      "738:\tlearn: 0.0758022\ttotal: 8.7s\tremaining: 3.07s\n",
      "739:\tlearn: 0.0757638\ttotal: 8.71s\tremaining: 3.06s\n",
      "740:\tlearn: 0.0757280\ttotal: 8.71s\tremaining: 3.04s\n",
      "741:\tlearn: 0.0757106\ttotal: 8.72s\tremaining: 3.03s\n",
      "742:\tlearn: 0.0756816\ttotal: 8.73s\tremaining: 3.02s\n",
      "743:\tlearn: 0.0756677\ttotal: 8.73s\tremaining: 3s\n",
      "744:\tlearn: 0.0756514\ttotal: 8.74s\tremaining: 2.99s\n",
      "745:\tlearn: 0.0756347\ttotal: 8.74s\tremaining: 2.98s\n",
      "746:\tlearn: 0.0756092\ttotal: 8.75s\tremaining: 2.96s\n",
      "747:\tlearn: 0.0755877\ttotal: 8.76s\tremaining: 2.95s\n",
      "748:\tlearn: 0.0755654\ttotal: 8.76s\tremaining: 2.94s\n",
      "749:\tlearn: 0.0755516\ttotal: 8.77s\tremaining: 2.92s\n",
      "750:\tlearn: 0.0755343\ttotal: 8.78s\tremaining: 2.91s\n",
      "751:\tlearn: 0.0755156\ttotal: 8.78s\tremaining: 2.9s\n",
      "752:\tlearn: 0.0754881\ttotal: 8.79s\tremaining: 2.88s\n",
      "753:\tlearn: 0.0754723\ttotal: 8.8s\tremaining: 2.87s\n",
      "754:\tlearn: 0.0754630\ttotal: 8.8s\tremaining: 2.86s\n",
      "755:\tlearn: 0.0754511\ttotal: 8.81s\tremaining: 2.84s\n",
      "756:\tlearn: 0.0754238\ttotal: 8.82s\tremaining: 2.83s\n",
      "757:\tlearn: 0.0754055\ttotal: 8.82s\tremaining: 2.82s\n",
      "758:\tlearn: 0.0753959\ttotal: 8.83s\tremaining: 2.8s\n",
      "759:\tlearn: 0.0753867\ttotal: 8.84s\tremaining: 2.79s\n",
      "760:\tlearn: 0.0753752\ttotal: 8.85s\tremaining: 2.78s\n",
      "761:\tlearn: 0.0753550\ttotal: 8.85s\tremaining: 2.76s\n",
      "762:\tlearn: 0.0753244\ttotal: 8.86s\tremaining: 2.75s\n",
      "763:\tlearn: 0.0753087\ttotal: 8.86s\tremaining: 2.74s\n",
      "764:\tlearn: 0.0752814\ttotal: 8.87s\tremaining: 2.72s\n",
      "765:\tlearn: 0.0752603\ttotal: 8.88s\tremaining: 2.71s\n",
      "766:\tlearn: 0.0752386\ttotal: 8.88s\tremaining: 2.7s\n",
      "767:\tlearn: 0.0752259\ttotal: 8.89s\tremaining: 2.68s\n",
      "768:\tlearn: 0.0752010\ttotal: 8.89s\tremaining: 2.67s\n",
      "769:\tlearn: 0.0751817\ttotal: 8.9s\tremaining: 2.66s\n",
      "770:\tlearn: 0.0751518\ttotal: 8.9s\tremaining: 2.64s\n",
      "771:\tlearn: 0.0751412\ttotal: 8.91s\tremaining: 2.63s\n",
      "772:\tlearn: 0.0751243\ttotal: 8.92s\tremaining: 2.62s\n",
      "773:\tlearn: 0.0751079\ttotal: 8.92s\tremaining: 2.6s\n",
      "774:\tlearn: 0.0750939\ttotal: 8.93s\tremaining: 2.59s\n",
      "775:\tlearn: 0.0750738\ttotal: 8.93s\tremaining: 2.58s\n",
      "776:\tlearn: 0.0750555\ttotal: 8.95s\tremaining: 2.57s\n",
      "777:\tlearn: 0.0750529\ttotal: 8.96s\tremaining: 2.56s\n",
      "778:\tlearn: 0.0750276\ttotal: 8.99s\tremaining: 2.55s\n",
      "779:\tlearn: 0.0750157\ttotal: 9.01s\tremaining: 2.54s\n",
      "780:\tlearn: 0.0750070\ttotal: 9.07s\tremaining: 2.54s\n",
      "781:\tlearn: 0.0749876\ttotal: 9.09s\tremaining: 2.53s\n",
      "782:\tlearn: 0.0749839\ttotal: 9.12s\tremaining: 2.53s\n",
      "783:\tlearn: 0.0749612\ttotal: 9.15s\tremaining: 2.52s\n",
      "784:\tlearn: 0.0749518\ttotal: 9.16s\tremaining: 2.51s\n",
      "785:\tlearn: 0.0749224\ttotal: 9.18s\tremaining: 2.5s\n",
      "786:\tlearn: 0.0749008\ttotal: 9.18s\tremaining: 2.48s\n",
      "787:\tlearn: 0.0748933\ttotal: 9.19s\tremaining: 2.47s\n",
      "788:\tlearn: 0.0748690\ttotal: 9.2s\tremaining: 2.46s\n",
      "789:\tlearn: 0.0748534\ttotal: 9.21s\tremaining: 2.45s\n",
      "790:\tlearn: 0.0748190\ttotal: 9.22s\tremaining: 2.44s\n",
      "791:\tlearn: 0.0748088\ttotal: 9.22s\tremaining: 2.42s\n",
      "792:\tlearn: 0.0747845\ttotal: 9.23s\tremaining: 2.41s\n",
      "793:\tlearn: 0.0747635\ttotal: 9.24s\tremaining: 2.4s\n",
      "794:\tlearn: 0.0747498\ttotal: 9.25s\tremaining: 2.38s\n",
      "795:\tlearn: 0.0747187\ttotal: 9.27s\tremaining: 2.37s\n",
      "796:\tlearn: 0.0747099\ttotal: 9.29s\tremaining: 2.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797:\tlearn: 0.0747011\ttotal: 9.31s\tremaining: 2.36s\n",
      "798:\tlearn: 0.0746856\ttotal: 9.32s\tremaining: 2.35s\n",
      "799:\tlearn: 0.0746578\ttotal: 9.33s\tremaining: 2.33s\n",
      "800:\tlearn: 0.0746125\ttotal: 9.34s\tremaining: 2.32s\n",
      "801:\tlearn: 0.0745864\ttotal: 9.35s\tremaining: 2.31s\n",
      "802:\tlearn: 0.0745643\ttotal: 9.36s\tremaining: 2.29s\n",
      "803:\tlearn: 0.0745479\ttotal: 9.36s\tremaining: 2.28s\n",
      "804:\tlearn: 0.0745386\ttotal: 9.37s\tremaining: 2.27s\n",
      "805:\tlearn: 0.0745288\ttotal: 9.38s\tremaining: 2.26s\n",
      "806:\tlearn: 0.0745143\ttotal: 9.39s\tremaining: 2.25s\n",
      "807:\tlearn: 0.0745050\ttotal: 9.4s\tremaining: 2.23s\n",
      "808:\tlearn: 0.0744827\ttotal: 9.4s\tremaining: 2.22s\n",
      "809:\tlearn: 0.0744600\ttotal: 9.41s\tremaining: 2.21s\n",
      "810:\tlearn: 0.0744410\ttotal: 9.42s\tremaining: 2.2s\n",
      "811:\tlearn: 0.0744283\ttotal: 9.44s\tremaining: 2.19s\n",
      "812:\tlearn: 0.0743981\ttotal: 9.45s\tremaining: 2.17s\n",
      "813:\tlearn: 0.0743852\ttotal: 9.46s\tremaining: 2.16s\n",
      "814:\tlearn: 0.0743754\ttotal: 9.47s\tremaining: 2.15s\n",
      "815:\tlearn: 0.0743592\ttotal: 9.48s\tremaining: 2.14s\n",
      "816:\tlearn: 0.0743276\ttotal: 9.5s\tremaining: 2.13s\n",
      "817:\tlearn: 0.0743185\ttotal: 9.5s\tremaining: 2.11s\n",
      "818:\tlearn: 0.0743101\ttotal: 9.52s\tremaining: 2.1s\n",
      "819:\tlearn: 0.0742954\ttotal: 9.53s\tremaining: 2.09s\n",
      "820:\tlearn: 0.0742836\ttotal: 9.54s\tremaining: 2.08s\n",
      "821:\tlearn: 0.0742575\ttotal: 9.54s\tremaining: 2.07s\n",
      "822:\tlearn: 0.0742396\ttotal: 9.55s\tremaining: 2.05s\n",
      "823:\tlearn: 0.0742288\ttotal: 9.56s\tremaining: 2.04s\n",
      "824:\tlearn: 0.0742178\ttotal: 9.57s\tremaining: 2.03s\n",
      "825:\tlearn: 0.0742039\ttotal: 9.57s\tremaining: 2.02s\n",
      "826:\tlearn: 0.0741962\ttotal: 9.58s\tremaining: 2s\n",
      "827:\tlearn: 0.0741569\ttotal: 9.59s\tremaining: 1.99s\n",
      "828:\tlearn: 0.0741324\ttotal: 9.6s\tremaining: 1.98s\n",
      "829:\tlearn: 0.0741055\ttotal: 9.6s\tremaining: 1.97s\n",
      "830:\tlearn: 0.0740924\ttotal: 9.61s\tremaining: 1.95s\n",
      "831:\tlearn: 0.0740841\ttotal: 9.62s\tremaining: 1.94s\n",
      "832:\tlearn: 0.0740638\ttotal: 9.63s\tremaining: 1.93s\n",
      "833:\tlearn: 0.0740395\ttotal: 9.63s\tremaining: 1.92s\n",
      "834:\tlearn: 0.0740308\ttotal: 9.64s\tremaining: 1.91s\n",
      "835:\tlearn: 0.0740050\ttotal: 9.65s\tremaining: 1.89s\n",
      "836:\tlearn: 0.0739872\ttotal: 9.66s\tremaining: 1.88s\n",
      "837:\tlearn: 0.0739724\ttotal: 9.66s\tremaining: 1.87s\n",
      "838:\tlearn: 0.0739668\ttotal: 9.67s\tremaining: 1.85s\n",
      "839:\tlearn: 0.0739432\ttotal: 9.67s\tremaining: 1.84s\n",
      "840:\tlearn: 0.0739221\ttotal: 9.68s\tremaining: 1.83s\n",
      "841:\tlearn: 0.0739102\ttotal: 9.69s\tremaining: 1.82s\n",
      "842:\tlearn: 0.0738938\ttotal: 9.69s\tremaining: 1.8s\n",
      "843:\tlearn: 0.0738802\ttotal: 9.7s\tremaining: 1.79s\n",
      "844:\tlearn: 0.0738772\ttotal: 9.71s\tremaining: 1.78s\n",
      "845:\tlearn: 0.0738512\ttotal: 9.72s\tremaining: 1.77s\n",
      "846:\tlearn: 0.0738432\ttotal: 9.73s\tremaining: 1.76s\n",
      "847:\tlearn: 0.0738354\ttotal: 9.74s\tremaining: 1.75s\n",
      "848:\tlearn: 0.0738290\ttotal: 9.75s\tremaining: 1.73s\n",
      "849:\tlearn: 0.0738115\ttotal: 9.76s\tremaining: 1.72s\n",
      "850:\tlearn: 0.0737895\ttotal: 9.77s\tremaining: 1.71s\n",
      "851:\tlearn: 0.0737754\ttotal: 9.78s\tremaining: 1.7s\n",
      "852:\tlearn: 0.0737615\ttotal: 9.79s\tremaining: 1.69s\n",
      "853:\tlearn: 0.0737443\ttotal: 9.8s\tremaining: 1.68s\n",
      "854:\tlearn: 0.0737344\ttotal: 9.81s\tremaining: 1.66s\n",
      "855:\tlearn: 0.0737162\ttotal: 9.82s\tremaining: 1.65s\n",
      "856:\tlearn: 0.0736935\ttotal: 9.83s\tremaining: 1.64s\n",
      "857:\tlearn: 0.0736839\ttotal: 9.84s\tremaining: 1.63s\n",
      "858:\tlearn: 0.0736678\ttotal: 9.85s\tremaining: 1.62s\n",
      "859:\tlearn: 0.0736436\ttotal: 9.86s\tremaining: 1.6s\n",
      "860:\tlearn: 0.0736280\ttotal: 9.87s\tremaining: 1.59s\n",
      "861:\tlearn: 0.0736074\ttotal: 9.88s\tremaining: 1.58s\n",
      "862:\tlearn: 0.0735884\ttotal: 9.89s\tremaining: 1.57s\n",
      "863:\tlearn: 0.0735601\ttotal: 9.9s\tremaining: 1.56s\n",
      "864:\tlearn: 0.0735383\ttotal: 9.91s\tremaining: 1.55s\n",
      "865:\tlearn: 0.0735268\ttotal: 9.92s\tremaining: 1.53s\n",
      "866:\tlearn: 0.0735193\ttotal: 9.93s\tremaining: 1.52s\n",
      "867:\tlearn: 0.0734958\ttotal: 9.94s\tremaining: 1.51s\n",
      "868:\tlearn: 0.0734862\ttotal: 9.95s\tremaining: 1.5s\n",
      "869:\tlearn: 0.0734684\ttotal: 9.96s\tremaining: 1.49s\n",
      "870:\tlearn: 0.0734581\ttotal: 9.98s\tremaining: 1.48s\n",
      "871:\tlearn: 0.0734490\ttotal: 10s\tremaining: 1.47s\n",
      "872:\tlearn: 0.0734314\ttotal: 10s\tremaining: 1.46s\n",
      "873:\tlearn: 0.0734204\ttotal: 10s\tremaining: 1.45s\n",
      "874:\tlearn: 0.0734178\ttotal: 10.1s\tremaining: 1.44s\n",
      "875:\tlearn: 0.0734035\ttotal: 10.1s\tremaining: 1.44s\n",
      "876:\tlearn: 0.0733879\ttotal: 10.2s\tremaining: 1.42s\n",
      "877:\tlearn: 0.0733638\ttotal: 10.2s\tremaining: 1.41s\n",
      "878:\tlearn: 0.0733549\ttotal: 10.2s\tremaining: 1.4s\n",
      "879:\tlearn: 0.0733323\ttotal: 10.2s\tremaining: 1.39s\n",
      "880:\tlearn: 0.0733231\ttotal: 10.2s\tremaining: 1.38s\n",
      "881:\tlearn: 0.0733122\ttotal: 10.2s\tremaining: 1.37s\n",
      "882:\tlearn: 0.0732956\ttotal: 10.2s\tremaining: 1.35s\n",
      "883:\tlearn: 0.0732718\ttotal: 10.2s\tremaining: 1.34s\n",
      "884:\tlearn: 0.0732528\ttotal: 10.2s\tremaining: 1.33s\n",
      "885:\tlearn: 0.0732438\ttotal: 10.3s\tremaining: 1.32s\n",
      "886:\tlearn: 0.0732303\ttotal: 10.3s\tremaining: 1.31s\n",
      "887:\tlearn: 0.0732029\ttotal: 10.3s\tremaining: 1.29s\n",
      "888:\tlearn: 0.0731722\ttotal: 10.3s\tremaining: 1.28s\n",
      "889:\tlearn: 0.0731504\ttotal: 10.3s\tremaining: 1.27s\n",
      "890:\tlearn: 0.0731476\ttotal: 10.3s\tremaining: 1.26s\n",
      "891:\tlearn: 0.0731291\ttotal: 10.3s\tremaining: 1.25s\n",
      "892:\tlearn: 0.0731204\ttotal: 10.3s\tremaining: 1.24s\n",
      "893:\tlearn: 0.0731038\ttotal: 10.3s\tremaining: 1.22s\n",
      "894:\tlearn: 0.0730914\ttotal: 10.3s\tremaining: 1.21s\n",
      "895:\tlearn: 0.0730790\ttotal: 10.4s\tremaining: 1.2s\n",
      "896:\tlearn: 0.0730503\ttotal: 10.4s\tremaining: 1.19s\n",
      "897:\tlearn: 0.0730432\ttotal: 10.4s\tremaining: 1.18s\n",
      "898:\tlearn: 0.0730154\ttotal: 10.4s\tremaining: 1.17s\n",
      "899:\tlearn: 0.0729969\ttotal: 10.4s\tremaining: 1.16s\n",
      "900:\tlearn: 0.0729672\ttotal: 10.4s\tremaining: 1.14s\n",
      "901:\tlearn: 0.0729586\ttotal: 10.4s\tremaining: 1.13s\n",
      "902:\tlearn: 0.0729493\ttotal: 10.4s\tremaining: 1.12s\n",
      "903:\tlearn: 0.0729332\ttotal: 10.4s\tremaining: 1.11s\n",
      "904:\tlearn: 0.0729173\ttotal: 10.4s\tremaining: 1.09s\n",
      "905:\tlearn: 0.0729010\ttotal: 10.4s\tremaining: 1.08s\n",
      "906:\tlearn: 0.0728826\ttotal: 10.4s\tremaining: 1.07s\n",
      "907:\tlearn: 0.0728696\ttotal: 10.5s\tremaining: 1.06s\n",
      "908:\tlearn: 0.0728531\ttotal: 10.5s\tremaining: 1.05s\n",
      "909:\tlearn: 0.0728275\ttotal: 10.5s\tremaining: 1.03s\n",
      "910:\tlearn: 0.0728151\ttotal: 10.5s\tremaining: 1.02s\n",
      "911:\tlearn: 0.0727973\ttotal: 10.5s\tremaining: 1.01s\n",
      "912:\tlearn: 0.0727707\ttotal: 10.5s\tremaining: 999ms\n",
      "913:\tlearn: 0.0727609\ttotal: 10.5s\tremaining: 987ms\n",
      "914:\tlearn: 0.0727320\ttotal: 10.5s\tremaining: 975ms\n",
      "915:\tlearn: 0.0727224\ttotal: 10.5s\tremaining: 963ms\n",
      "916:\tlearn: 0.0727092\ttotal: 10.5s\tremaining: 951ms\n",
      "917:\tlearn: 0.0726906\ttotal: 10.5s\tremaining: 940ms\n",
      "918:\tlearn: 0.0726855\ttotal: 10.5s\tremaining: 928ms\n",
      "919:\tlearn: 0.0726762\ttotal: 10.5s\tremaining: 916ms\n",
      "920:\tlearn: 0.0726545\ttotal: 10.5s\tremaining: 904ms\n",
      "921:\tlearn: 0.0726393\ttotal: 10.5s\tremaining: 892ms\n",
      "922:\tlearn: 0.0726294\ttotal: 10.6s\tremaining: 880ms\n",
      "923:\tlearn: 0.0726178\ttotal: 10.6s\tremaining: 868ms\n",
      "924:\tlearn: 0.0726075\ttotal: 10.6s\tremaining: 857ms\n",
      "925:\tlearn: 0.0725914\ttotal: 10.6s\tremaining: 845ms\n",
      "926:\tlearn: 0.0725583\ttotal: 10.6s\tremaining: 833ms\n",
      "927:\tlearn: 0.0725487\ttotal: 10.6s\tremaining: 821ms\n",
      "928:\tlearn: 0.0725352\ttotal: 10.6s\tremaining: 809ms\n",
      "929:\tlearn: 0.0725168\ttotal: 10.6s\tremaining: 798ms\n",
      "930:\tlearn: 0.0725093\ttotal: 10.6s\tremaining: 786ms\n",
      "931:\tlearn: 0.0724831\ttotal: 10.6s\tremaining: 774ms\n",
      "932:\tlearn: 0.0724747\ttotal: 10.6s\tremaining: 762ms\n",
      "933:\tlearn: 0.0724692\ttotal: 10.6s\tremaining: 751ms\n",
      "934:\tlearn: 0.0724590\ttotal: 10.6s\tremaining: 739ms\n",
      "935:\tlearn: 0.0724448\ttotal: 10.6s\tremaining: 727ms\n",
      "936:\tlearn: 0.0724377\ttotal: 10.6s\tremaining: 715ms\n",
      "937:\tlearn: 0.0724268\ttotal: 10.6s\tremaining: 704ms\n",
      "938:\tlearn: 0.0724161\ttotal: 10.7s\tremaining: 692ms\n",
      "939:\tlearn: 0.0724049\ttotal: 10.7s\tremaining: 680ms\n",
      "940:\tlearn: 0.0723901\ttotal: 10.7s\tremaining: 669ms\n",
      "941:\tlearn: 0.0723816\ttotal: 10.7s\tremaining: 657ms\n",
      "942:\tlearn: 0.0723682\ttotal: 10.7s\tremaining: 645ms\n",
      "943:\tlearn: 0.0723517\ttotal: 10.7s\tremaining: 634ms\n",
      "944:\tlearn: 0.0723441\ttotal: 10.7s\tremaining: 622ms\n",
      "945:\tlearn: 0.0723072\ttotal: 10.7s\tremaining: 611ms\n",
      "946:\tlearn: 0.0723046\ttotal: 10.7s\tremaining: 599ms\n",
      "947:\tlearn: 0.0722796\ttotal: 10.7s\tremaining: 587ms\n",
      "948:\tlearn: 0.0722617\ttotal: 10.7s\tremaining: 576ms\n",
      "949:\tlearn: 0.0722493\ttotal: 10.7s\tremaining: 564ms\n",
      "950:\tlearn: 0.0722287\ttotal: 10.7s\tremaining: 553ms\n",
      "951:\tlearn: 0.0722165\ttotal: 10.7s\tremaining: 541ms\n",
      "952:\tlearn: 0.0721959\ttotal: 10.7s\tremaining: 530ms\n",
      "953:\tlearn: 0.0721830\ttotal: 10.7s\tremaining: 518ms\n",
      "954:\tlearn: 0.0721702\ttotal: 10.8s\tremaining: 507ms\n",
      "955:\tlearn: 0.0721555\ttotal: 10.8s\tremaining: 495ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956:\tlearn: 0.0721371\ttotal: 10.8s\tremaining: 484ms\n",
      "957:\tlearn: 0.0721162\ttotal: 10.8s\tremaining: 472ms\n",
      "958:\tlearn: 0.0720910\ttotal: 10.8s\tremaining: 461ms\n",
      "959:\tlearn: 0.0720777\ttotal: 10.8s\tremaining: 450ms\n",
      "960:\tlearn: 0.0720675\ttotal: 10.8s\tremaining: 438ms\n",
      "961:\tlearn: 0.0720430\ttotal: 10.8s\tremaining: 427ms\n",
      "962:\tlearn: 0.0720237\ttotal: 10.8s\tremaining: 415ms\n",
      "963:\tlearn: 0.0720122\ttotal: 10.8s\tremaining: 404ms\n",
      "964:\tlearn: 0.0719990\ttotal: 10.8s\tremaining: 392ms\n",
      "965:\tlearn: 0.0719877\ttotal: 10.8s\tremaining: 381ms\n",
      "966:\tlearn: 0.0719730\ttotal: 10.8s\tremaining: 370ms\n",
      "967:\tlearn: 0.0719579\ttotal: 10.8s\tremaining: 358ms\n",
      "968:\tlearn: 0.0719401\ttotal: 10.8s\tremaining: 347ms\n",
      "969:\tlearn: 0.0719309\ttotal: 10.9s\tremaining: 336ms\n",
      "970:\tlearn: 0.0719133\ttotal: 10.9s\tremaining: 324ms\n",
      "971:\tlearn: 0.0718980\ttotal: 10.9s\tremaining: 313ms\n",
      "972:\tlearn: 0.0718867\ttotal: 10.9s\tremaining: 302ms\n",
      "973:\tlearn: 0.0718669\ttotal: 10.9s\tremaining: 290ms\n",
      "974:\tlearn: 0.0718536\ttotal: 10.9s\tremaining: 279ms\n",
      "975:\tlearn: 0.0718463\ttotal: 10.9s\tremaining: 268ms\n",
      "976:\tlearn: 0.0718319\ttotal: 10.9s\tremaining: 257ms\n",
      "977:\tlearn: 0.0718212\ttotal: 10.9s\tremaining: 245ms\n",
      "978:\tlearn: 0.0717963\ttotal: 10.9s\tremaining: 234ms\n",
      "979:\tlearn: 0.0717877\ttotal: 10.9s\tremaining: 223ms\n",
      "980:\tlearn: 0.0717773\ttotal: 10.9s\tremaining: 212ms\n",
      "981:\tlearn: 0.0717681\ttotal: 10.9s\tremaining: 200ms\n",
      "982:\tlearn: 0.0717484\ttotal: 10.9s\tremaining: 189ms\n",
      "983:\tlearn: 0.0717327\ttotal: 10.9s\tremaining: 178ms\n",
      "984:\tlearn: 0.0717254\ttotal: 10.9s\tremaining: 167ms\n",
      "985:\tlearn: 0.0717078\ttotal: 11s\tremaining: 156ms\n",
      "986:\tlearn: 0.0716913\ttotal: 11s\tremaining: 144ms\n",
      "987:\tlearn: 0.0716748\ttotal: 11s\tremaining: 133ms\n",
      "988:\tlearn: 0.0716605\ttotal: 11s\tremaining: 122ms\n",
      "989:\tlearn: 0.0716469\ttotal: 11s\tremaining: 111ms\n",
      "990:\tlearn: 0.0716133\ttotal: 11s\tremaining: 99.8ms\n",
      "991:\tlearn: 0.0715970\ttotal: 11s\tremaining: 88.7ms\n",
      "992:\tlearn: 0.0715885\ttotal: 11s\tremaining: 77.6ms\n",
      "993:\tlearn: 0.0715712\ttotal: 11s\tremaining: 66.5ms\n",
      "994:\tlearn: 0.0715645\ttotal: 11s\tremaining: 55.4ms\n",
      "995:\tlearn: 0.0715516\ttotal: 11s\tremaining: 44.3ms\n",
      "996:\tlearn: 0.0715379\ttotal: 11s\tremaining: 33.2ms\n",
      "997:\tlearn: 0.0715095\ttotal: 11.1s\tremaining: 22.2ms\n",
      "998:\tlearn: 0.0714965\ttotal: 11.1s\tremaining: 11.1ms\n",
      "999:\tlearn: 0.0714915\ttotal: 11.1s\tremaining: 0us\n",
      "MAE:22.720843\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoost\n",
    "# from sklearn.metrics import log_loss\n",
    "\n",
    "class CatRegressorCV(RidgeCV):\n",
    "    model_cls = CatBoost\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'MAE',\n",
    "#             'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 6, 10),                                      \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "# model=CatRegressorCV(n_trials=10)\n",
    "model=CatBoost({'depth': 7, 'learning_rate': 0.074638569770399, 'random_strength': 59, 'bagging_temperature': 0.011038111194790014, 'od_type': 'IncToDec', 'od_wait': 12})\n",
    "# model=CatBoost({'depth':16,'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, })\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(model.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engeneering for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "categorical_variable_names = [\"position\",\"sex\",\"education\"]\n",
    "# カテゴリ変数に一括変換\n",
    "# x_dummy = pd.get_dummies(df[categorical_variable_names], drop_first=True,columns=categorical_variable_names)\n",
    "# X_nn=df.drop(categorical_variable_names, axis=1)\n",
    "X_nn=df\n",
    "sscaler =StandardScaler()\n",
    "sscaler.fit(X_nn)  \n",
    "\n",
    "x_datas_std =sscaler.transform(X_nn)\n",
    "x_datas_std = pd.DataFrame(x_datas_std, columns=X_nn.columns)\n",
    "X_nn= pd.concat([x_datas_std,], axis=1)\n",
    "\n",
    "\n",
    "train_X_nn=X_nn.drop(\"id\",axis=1)\n",
    "test_nn=X_nn[X_nn.salary.isnull()].drop(\"id\",axis=1)\n",
    "test_nn= test_nn.drop([\"salary\",],axis=1)\n",
    "train_X_nn= train_X_nn.dropna().drop([\"salary\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>1.027084</td>\n",
       "      <td>1.544468</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>1.097992</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>0.818742</td>\n",
       "      <td>-0.243349</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>-0.192567</td>\n",
       "      <td>0.292875</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>0.069682</td>\n",
       "      <td>1.584501</td>\n",
       "      <td>-0.539406</td>\n",
       "      <td>0.283116</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.070094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>0.145629</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>1.023458</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.691715</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>-0.753363</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.755483</td>\n",
       "      <td>-0.737849</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.678180</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>-0.950787</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.567845</td>\n",
       "      <td>-0.516979</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.491214</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.760128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.470845</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.447636</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>1.463169</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>1.249976</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.388501</td>\n",
       "      <td>0.332472</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.876595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.130761</td>\n",
       "      <td>0.808237</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-1.145594</td>\n",
       "      <td>-0.840184</td>\n",
       "      <td>-0.086690</td>\n",
       "      <td>0.036335</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.902451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>-0.286387</td>\n",
       "      <td>-1.621326</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>1.707344</td>\n",
       "      <td>-0.584697</td>\n",
       "      <td>-1.143269</td>\n",
       "      <td>-0.841217</td>\n",
       "      <td>1.500567</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.451760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position       age      area       sex   partner  num_child  education  \\\n",
       "0     -0.176420  1.027084  1.544468  1.009041  0.998734   0.708483  -0.085928   \n",
       "1      0.643375 -0.192567  0.292875 -0.991040 -1.001267  -0.708058  -0.982563   \n",
       "2      0.643375  0.276529  0.145629 -0.991040 -1.001267  -0.708058   0.810708   \n",
       "3     -0.996214 -1.036941  1.691715  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "4     -0.996214 -0.755483 -0.737849  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "...         ...       ...       ...       ...       ...        ...        ...   \n",
       "20995 -0.996214 -0.567845 -0.516979  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "20996 -0.996214 -1.036941  1.470845  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "20997  1.463169  0.276529  1.249976  1.009041 -1.001267  -0.708058   0.810708   \n",
       "20998 -0.996214 -1.130761  0.808237  1.009041  0.998734   0.708483  -0.085928   \n",
       "20999 -0.176420 -0.286387 -1.621326 -0.991040 -1.001267  -0.708058   1.707344   \n",
       "\n",
       "       service_length  study_time   commute  overtime  familiy_num  \\\n",
       "0            1.097992   -0.537098  0.818742 -0.243349     0.832908   \n",
       "1            0.069682    1.584501 -0.539406  0.283116    -0.833279   \n",
       "2            0.163165    0.069073 -0.992122  1.023458    -0.833279   \n",
       "3           -0.771663   -0.234012 -0.992122 -0.753363    -0.833279   \n",
       "4           -0.678180   -0.234012 -1.293932 -0.950787    -0.833279   \n",
       "...               ...         ...       ...       ...          ...   \n",
       "20995       -0.491214   -0.537098 -1.293932  0.793129    -0.833279   \n",
       "20996       -0.771663   -0.234012 -1.293932  0.447636    -0.833279   \n",
       "20997        0.163165    0.069073 -0.388501  0.332472    -0.833279   \n",
       "20998       -1.145594   -0.840184 -0.086690  0.036335     0.832908   \n",
       "20999       -0.584697   -1.143269 -0.841217  1.500567    -0.833279   \n",
       "\n",
       "       agexposition  \n",
       "0         -0.008975  \n",
       "1          0.070094  \n",
       "2          0.307300  \n",
       "3         -0.878731  \n",
       "4         -0.807569  \n",
       "...             ...  \n",
       "20995     -0.760128  \n",
       "20996     -0.878731  \n",
       "20997      0.876595  \n",
       "20998     -0.902451  \n",
       "20999     -0.451760  \n",
       "\n",
       "[21000 rows x 13 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16800 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "16800/16800 [==============================] - ETA: 31s - loss: 32.925 - ETA: 13s - loss: 24.315 - ETA: 9s - loss: 16.529 - ETA: 6s - loss: 12.08 - ETA: 5s - loss: 9.5510 - ETA: 4s - loss: 7.818 - ETA: 4s - loss: 6.887 - ETA: 3s - loss: 6.059 - ETA: 3s - loss: 5.394 - ETA: 3s - loss: 4.912 - ETA: 3s - loss: 4.621 - ETA: 3s - loss: 4.368 - ETA: 2s - loss: 4.060 - ETA: 2s - loss: 3.879 - ETA: 2s - loss: 3.705 - ETA: 2s - loss: 3.481 - ETA: 2s - loss: 3.279 - ETA: 2s - loss: 3.098 - ETA: 2s - loss: 2.938 - ETA: 2s - loss: 2.839 - ETA: 2s - loss: 2.749 - ETA: 2s - loss: 2.664 - ETA: 2s - loss: 2.587 - ETA: 2s - loss: 2.509 - ETA: 2s - loss: 2.438 - ETA: 1s - loss: 2.339 - ETA: 1s - loss: 2.276 - ETA: 1s - loss: 2.218 - ETA: 1s - loss: 2.162 - ETA: 1s - loss: 2.109 - ETA: 1s - loss: 2.035 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.924 - ETA: 1s - loss: 1.882 - ETA: 1s - loss: 1.823 - ETA: 1s - loss: 1.785 - ETA: 1s - loss: 1.749 - ETA: 1s - loss: 1.698 - ETA: 1s - loss: 1.665 - ETA: 0s - loss: 1.634 - ETA: 0s - loss: 1.588 - ETA: 0s - loss: 1.559 - ETA: 0s - loss: 1.531 - ETA: 0s - loss: 1.491 - ETA: 0s - loss: 1.453 - ETA: 0s - loss: 1.429 - ETA: 0s - loss: 1.393 - ETA: 0s - loss: 1.371 - ETA: 0s - loss: 1.338 - ETA: 0s - loss: 1.307 - ETA: 0s - loss: 1.287 - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 1.248 - ETA: 0s - loss: 1.230 - 4s 221us/step - loss: 1.2191 - val_loss: 0.0552\n",
      "Epoch 2/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.061 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.055 - ETA: 3s - loss: 0.052 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.049 - ETA: 3s - loss: 0.047 - ETA: 3s - loss: 0.046 - ETA: 2s - loss: 0.045 - ETA: 2s - loss: 0.044 - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.038 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 1s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - 4s 247us/step - loss: 0.0292 - val_loss: 0.0201\n",
      "Epoch 3/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 219us/step - loss: 0.0206 - val_loss: 0.0210\n",
      "Epoch 4/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 223us/step - loss: 0.0201 - val_loss: 0.0187\n",
      "Epoch 5/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 202us/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 6/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 201us/step - loss: 0.0185 - val_loss: 0.0249\n",
      "Epoch 7/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 4s 217us/step - loss: 0.0189 - val_loss: 0.0243\n",
      "Epoch 8/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 242us/step - loss: 0.0175 - val_loss: 0.0164\n",
      "Epoch 9/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 296us/step - loss: 0.0173 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 314us/step - loss: 0.0173 - val_loss: 0.0235\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 7s - loss: 0.015 - ETA: 8s - loss: 0.019 - ETA: 5s - loss: 0.019 - ETA: 4s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 4s 256us/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 12/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 281us/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 13/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 316us/step - loss: 0.0168 - val_loss: 0.0299\n",
      "Epoch 14/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.031 - ETA: 2s - loss: 0.024 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 231us/step - loss: 0.0178 - val_loss: 0.0198\n",
      "Epoch 15/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 214us/step - loss: 0.0149 - val_loss: 0.0159\n",
      "Epoch 16/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 234us/step - loss: 0.0145 - val_loss: 0.0136\n",
      "Epoch 17/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - 4s 247us/step - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 18/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 10s - loss: 0.01 - ETA: 8s - loss: 0.0118 - ETA: 8s - loss: 0.012 - ETA: 8s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 320us/step - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 5s 321us/step - loss: 0.0140 - val_loss: 0.0179\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.016 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 6s 329us/step - loss: 0.0147 - val_loss: 0.0158\n",
      "Epoch 21/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 235us/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 22/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 205us/step - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 23/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 3s 182us/step - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 24/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 209us/step - loss: 0.0145 - val_loss: 0.0188\n",
      "Epoch 25/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.023 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.021 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 5s 304us/step - loss: 0.0131 - val_loss: 0.0220\n",
      "Epoch 26/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - 5s 278us/step - loss: 0.0120 - val_loss: 0.0129\n",
      "Epoch 27/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 257us/step - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 28/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 255us/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 240us/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 30/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 232us/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 31/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 299us/step - loss: 0.0111 - val_loss: 0.0151\n",
      "Epoch 32/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 317us/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 33/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 276us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 34/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 238us/step - loss: 0.0131 - val_loss: 0.0284\n",
      "Epoch 35/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.024 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 224us/step - loss: 0.0126 - val_loss: 0.0142\n",
      "Epoch 36/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 6s 334us/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 6s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 384us/step - loss: 0.0107 - val_loss: 0.0207\n",
      "Epoch 38/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 7s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 7s 429us/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 39/100\n",
      "16800/16800 [==============================] - ETA: 7s - loss: 0.009 - ETA: 9s - loss: 0.009 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.009 - ETA: 7s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 346us/step - loss: 0.0108 - val_loss: 0.0198\n",
      "Epoch 40/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.022 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 219us/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 41/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 292us/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 42/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 218us/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 43/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 238us/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 44/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 239us/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 45/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 223us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 244us/step - loss: 0.0114 - val_loss: 0.0130\n",
      "Epoch 47/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 195us/step - loss: 0.0107 - val_loss: 0.0170\n",
      "Epoch 48/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 250us/step - loss: 0.0115 - val_loss: 0.0137\n",
      "Epoch 49/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 7s - loss: 0.009 - ETA: 7s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 212us/step - loss: 0.0098 - val_loss: 0.0130\n",
      "Epoch 50/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 300us/step - loss: 0.0115 - val_loss: 0.0210\n",
      "Epoch 51/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 256us/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 52/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 256us/step - loss: 0.0094 - val_loss: 0.0201\n",
      "Epoch 53/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 247us/step - loss: 0.0104 - val_loss: 0.0181\n",
      "Epoch 54/100\n",
      "16800/16800 [==============================] - ETA: 14s - loss: 0.01 - ETA: 8s - loss: 0.0142 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 278us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 277us/step - loss: 0.0094 - val_loss: 0.0138\n",
      "Epoch 56/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 322us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 57/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 280us/step - loss: 0.0100 - val_loss: 0.0185\n",
      "Epoch 58/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 6s - loss: 0.013 - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 339us/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 59/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.005 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 7s 422us/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 60/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 320us/step - loss: 0.0095 - val_loss: 0.0103\n",
      "Epoch 61/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 280us/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 62/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 312us/step - loss: 0.0107 - val_loss: 0.0154\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 311us/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 64/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 297us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 65/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0109\n",
      "Epoch 66/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 291us/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 67/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0110\n",
      "Epoch 68/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 289us/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 69/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 271us/step - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 70/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 171us/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 71/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 170us/step - loss: 0.0092 - val_loss: 0.0109\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 224us/step - loss: 0.0092 - val_loss: 0.0121\n",
      "Epoch 73/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 336us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 74/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 338us/step - loss: 0.0091 - val_loss: 0.0104\n",
      "Epoch 75/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 318us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 76/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 293us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 77/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 285us/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 78/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 292us/step - loss: 0.0082 - val_loss: 0.0094\n",
      "Epoch 79/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 8s - loss: 0.007 - ETA: 7s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 274us/step - loss: 0.0087 - val_loss: 0.0161\n",
      "Epoch 81/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 3s 183us/step - loss: 0.0085 - val_loss: 0.0107\n",
      "Epoch 82/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 173us/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 83/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 318us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 84/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 312us/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 85/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 86/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0082 - val_loss: 0.0103\n",
      "Epoch 87/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 281us/step - loss: 0.0083 - val_loss: 0.0112\n",
      "Epoch 88/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 280us/step - loss: 0.0079 - val_loss: 0.0103\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 296us/step - loss: 0.0082 - val_loss: 0.0127\n",
      "Epoch 90/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0111\n",
      "Epoch 91/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 286us/step - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 92/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 298us/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 93/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 294us/step - loss: 0.0086 - val_loss: 0.0109\n",
      "Epoch 94/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0085 - val_loss: 0.0095\n",
      "Epoch 95/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0114\n",
      "Epoch 96/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 301us/step - loss: 0.0081 - val_loss: 0.0104\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 296us/step - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 98/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0080 - val_loss: 0.0106\n",
      "Epoch 99/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 293us/step - loss: 0.0086 - val_loss: 0.0169\n",
      "Epoch 100/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 279us/step - loss: 0.0081 - val_loss: 0.0091\n"
     ]
    }
   ],
   "source": [
    "#ニューラルネットワークモデルの生成\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(800, activation = 'relu'))\n",
    "model.add(Dense(100, activation =  'relu'))\n",
    "model.add(Dense(1))\n",
    "# モデルをコンパイル \n",
    "from keras.optimizers import Adam\n",
    "# ylgm=min_max_normalization(y)\n",
    "train_X_nn, valid_X_nn,train_y_nn, valid_y_nn = train_test_split(train_X_nn,y,test_size=0.2,random_state=43)\n",
    "\n",
    "model.compile(Adam(lr=1e-3), loss=\"mean_squared_error\")\n",
    "#トレーニングデータで学習し，テストデータで評価（平均2乗誤差を用いる）\n",
    "history = model.fit(np.array(train_X_nn), np.array(train_y_nn), batch_size=128, epochs=100, verbose=1, \n",
    "          validation_data=(np.array(valid_X_nn), np.array(valid_y_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 138us/step\n",
      "0.009095980622583912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134851e10>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#結果の表示\n",
    "import matplotlib.pyplot as plt #プロット用のライブラリを利用\n",
    "\n",
    "print(model.evaluate(valid_X_nn, valid_y_nn))\n",
    "\n",
    "train_acc = history.history['loss']\n",
    "test_acc = history.history['val_loss']\n",
    "x = np.arange(len(train_acc))\n",
    "plt.plot(x, train_acc, label = 'train mse')\n",
    "plt.plot(x, test_acc, label = 'test mse')\n",
    "plt.legend() #グラフの線の説明を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:25.749117\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(valid_X_nn)\n",
    "score=mean_absolute_error(np.exp(valid_y_nn),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>230.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>147.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  age  area  sex  partner  num_child  education  service_length  \\\n",
       "0            4   39    14    1        1          5          2              19   \n",
       "1            2   31    27    0        0          0          5               0   \n",
       "2            1   20    45    1        1          2          1               2   \n",
       "3            1   28    37    1        0          0          1              10   \n",
       "4            2   41    16    1        0          0          1              23   \n",
       "...        ...  ...   ...  ...      ...        ...        ...             ...   \n",
       "8995         2   43    14    1        0          0          1              25   \n",
       "8996         3   40     5    1        0          0          1              22   \n",
       "8997         5   46    24    0        0          0          1              28   \n",
       "8998         1   22    13    0        0          0          1               4   \n",
       "8999         3   49    14    0        1          0          3              27   \n",
       "\n",
       "      study_time  commute  overtime  familiy_num  agexposition  cluster  \n",
       "0            1.0      1.8      14.2            7         156.0       19  \n",
       "1            0.0      0.5      18.6            1          62.0        0  \n",
       "2            2.0      1.2       2.3            4          30.0       14  \n",
       "3            3.0      0.3       0.0            1          42.0        9  \n",
       "4            3.0      0.5      10.1            1          82.0        4  \n",
       "...          ...      ...       ...          ...           ...      ...  \n",
       "8995         3.0      0.7       0.0            1          86.0        4  \n",
       "8996         8.0      0.7       5.7            1         120.0        7  \n",
       "8997         2.0      0.8       0.0            1         230.0       17  \n",
       "8998         0.0      0.1       0.7            1          33.0       12  \n",
       "8999         0.0      1.7      11.0            2         147.0       19  \n",
       "\n",
       "[9000 rows x 14 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:17:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:18:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:18:30,045] Finished trial#0 resulted in value: 0.6043565819234415. Current best value is 0.6043565819234415 with parameters: {'booster': 'gblinear', 'iterations': 122, 'depth': 20, 'learning_rate': 0.011708411691531534, 'random_strength': 43, 'bagging_temperature': 24.461831978594116, 'od_type': 'Iter', 'od_wait': 47, 'lambda_l1': 0.0003256224275922188, 'lambda_l2': 4.238936922941192e-05, 'num_leaves': 43, 'feature_fraction': 0.7707248327545355, 'bagging_fraction': 0.8906911477099924, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:18:32,208] Finished trial#1 resulted in value: 0.9415110780513425. Current best value is 0.9415110780513425 with parameters: {'booster': 'gbtree', 'iterations': 63, 'depth': 9, 'learning_rate': 0.032220248836418316, 'random_strength': 70, 'bagging_temperature': 21.15100928817564, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.2958777513373701, 'lambda_l2': 1.0311880248242327e-07, 'num_leaves': 63, 'feature_fraction': 0.7224174599715385, 'bagging_fraction': 0.6391599746303983, 'bagging_freq': 2, 'min_child_samples': 25}.\n",
      "[I 2019-11-20 14:18:36,339] Finished trial#2 resulted in value: 0.9530811099517871. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:38,686] Finished trial#3 resulted in value: 0.9305080833646834. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:40,879] Finished trial#4 resulted in value: 0.9383860228549192. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:44,539] Finished trial#5 resulted in value: 0.955420168697476. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:48,111] Finished trial#6 resulted in value: 0.5256091000865739. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:50,255] Finished trial#7 resulted in value: 0.7874055549283296. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:54,479] Finished trial#8 resulted in value: 0.8835014084365532. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:56,836] Finished trial#9 resulted in value: 0.8831033143208874. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:18:59,299] Finished trial#0 resulted in value: 0.956099778308922. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:01,342] Finished trial#1 resulted in value: 0.938048402195524. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:03,021] Finished trial#2 resulted in value: 0.9412773184926577. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:05,075] Finished trial#3 resulted in value: 0.9383871941380209. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:07,496] Finished trial#4 resulted in value: 0.9530004129290692. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:09,693] Finished trial#5 resulted in value: 0.9517698894095877. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:12,551] Finished trial#6 resulted in value: 0.9526862633492964. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:14,684] Finished trial#7 resulted in value: 0.9256051497057376. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:17,752] Finished trial#8 resulted in value: 0.953977499885235. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:20,307] Finished trial#9 resulted in value: 0.9521265541453993. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:19:23,115] Finished trial#0 resulted in value: 0.9425957163349349. Current best value is 0.9425957163349349 with parameters: {'booster': 'gbtree', 'iterations': 174, 'depth': 14, 'learning_rate': 0.5421405403866358, 'random_strength': 80, 'bagging_temperature': 0.04017628837898865, 'od_type': 'IncToDec', 'od_wait': 22, 'lambda_l1': 3.452892041612629e-08, 'lambda_l2': 0.00014667709942794993, 'num_leaves': 103, 'feature_fraction': 0.9507822358048381, 'bagging_fraction': 0.6487603668695829, 'bagging_freq': 6, 'min_child_samples': 90}.\n",
      "[I 2019-11-20 14:19:24,625] Finished trial#1 resulted in value: 0.9495955420462524. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:26,237] Finished trial#2 resulted in value: 0.8348422613017638. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:27,708] Finished trial#3 resulted in value: 0.9357670660837147. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:31,823] Finished trial#4 resulted in value: 0.9451798740530399. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:33,505] Finished trial#5 resulted in value: 0.9512997991105854. Current best value is 0.9512997991105854 with parameters: {'booster': 'gbtree', 'iterations': 337, 'depth': 20, 'learning_rate': 0.07729673940129299, 'random_strength': 37, 'bagging_temperature': 0.3589275470407121, 'od_type': 'IncToDec', 'od_wait': 18, 'lambda_l1': 0.011211890401376249, 'lambda_l2': 1.082503848783411e-06, 'num_leaves': 39, 'feature_fraction': 0.5931287505247991, 'bagging_fraction': 0.44324275382737355, 'bagging_freq': 7, 'min_child_samples': 50}.\n",
      "[I 2019-11-20 14:19:37,170] Finished trial#6 resulted in value: 0.9535112447534292. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:39,650] Finished trial#7 resulted in value: 0.5789989267837456. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:41,276] Finished trial#8 resulted in value: 0.8707083123082388. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:44,821] Finished trial#9 resulted in value: 0.9134533367641637. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.95\n",
      "Best params: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:19:49,342] Finished trial#0 resulted in value: 0.9436375317909936. Current best value is 0.9436375317909936 with parameters: {'booster': 'gbtree', 'iterations': 140, 'depth': 16, 'learning_rate': 0.032043648081768836, 'random_strength': 22, 'bagging_temperature': 38.75767354433763, 'od_type': 'IncToDec', 'od_wait': 11, 'lambda_l1': 0.06919274572425878, 'lambda_l2': 0.043941525494697556, 'num_leaves': 70, 'feature_fraction': 0.8153141563251047, 'bagging_fraction': 0.44211734721374485, 'bagging_freq': 1, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:19:52,782] Finished trial#1 resulted in value: 0.4833872013348449. Current best value is 0.9436375317909936 with parameters: {'booster': 'gbtree', 'iterations': 140, 'depth': 16, 'learning_rate': 0.032043648081768836, 'random_strength': 22, 'bagging_temperature': 38.75767354433763, 'od_type': 'IncToDec', 'od_wait': 11, 'lambda_l1': 0.06919274572425878, 'lambda_l2': 0.043941525494697556, 'num_leaves': 70, 'feature_fraction': 0.8153141563251047, 'bagging_fraction': 0.44211734721374485, 'bagging_freq': 1, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:19:55,263] Finished trial#2 resulted in value: 0.953354739204389. Current best value is 0.953354739204389 with parameters: {'booster': 'gbtree', 'iterations': 377, 'depth': 4, 'learning_rate': 0.28817006864353856, 'random_strength': 38, 'bagging_temperature': 0.1978806336288733, 'od_type': 'IncToDec', 'od_wait': 31, 'lambda_l1': 1.0539365288391849e-05, 'lambda_l2': 0.37718058055527987, 'num_leaves': 50, 'feature_fraction': 0.49656668255830977, 'bagging_fraction': 0.8895635902633579, 'bagging_freq': 3, 'min_child_samples': 75}.\n",
      "[I 2019-11-20 14:19:58,094] Finished trial#3 resulted in value: 0.9337457353389416. Current best value is 0.953354739204389 with parameters: {'booster': 'gbtree', 'iterations': 377, 'depth': 4, 'learning_rate': 0.28817006864353856, 'random_strength': 38, 'bagging_temperature': 0.1978806336288733, 'od_type': 'IncToDec', 'od_wait': 31, 'lambda_l1': 1.0539365288391849e-05, 'lambda_l2': 0.37718058055527987, 'num_leaves': 50, 'feature_fraction': 0.49656668255830977, 'bagging_fraction': 0.8895635902633579, 'bagging_freq': 3, 'min_child_samples': 75}.\n",
      "[I 2019-11-20 14:20:04,365] Finished trial#4 resulted in value: 0.9556549036561319. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:06,370] Finished trial#5 resulted in value: 0.9339572594792331. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:08,185] Finished trial#6 resulted in value: 0.9491532523010999. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:11,093] Finished trial#7 resulted in value: 0.9139825604491474. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:12,790] Finished trial#8 resulted in value: 0.7389463650744487. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:14,756] Finished trial#9 resulted in value: 0.6717983969536553. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}\n",
      "\n",
      "0:\tlearn: 0.4379078\ttotal: 9.48ms\tremaining: 9.47s\n",
      "1:\tlearn: 0.4275838\ttotal: 17.8ms\tremaining: 8.88s\n",
      "2:\tlearn: 0.4172781\ttotal: 23.2ms\tremaining: 7.72s\n",
      "3:\tlearn: 0.4074042\ttotal: 30.5ms\tremaining: 7.6s\n",
      "4:\tlearn: 0.3977807\ttotal: 35.5ms\tremaining: 7.06s\n",
      "5:\tlearn: 0.3885448\ttotal: 44.1ms\tremaining: 7.31s\n",
      "6:\tlearn: 0.3792993\ttotal: 51.9ms\tremaining: 7.36s\n",
      "7:\tlearn: 0.3708988\ttotal: 61.5ms\tremaining: 7.62s\n",
      "8:\tlearn: 0.3627070\ttotal: 73.1ms\tremaining: 8.05s\n",
      "9:\tlearn: 0.3547298\ttotal: 84.8ms\tremaining: 8.39s\n",
      "10:\tlearn: 0.3470742\ttotal: 97.9ms\tremaining: 8.8s\n",
      "11:\tlearn: 0.3391469\ttotal: 109ms\tremaining: 8.97s\n",
      "12:\tlearn: 0.3316150\ttotal: 119ms\tremaining: 9.02s\n",
      "13:\tlearn: 0.3246369\ttotal: 127ms\tremaining: 8.92s\n",
      "14:\tlearn: 0.3178956\ttotal: 143ms\tremaining: 9.38s\n",
      "15:\tlearn: 0.3110082\ttotal: 150ms\tremaining: 9.22s\n",
      "16:\tlearn: 0.3045172\ttotal: 155ms\tremaining: 8.96s\n",
      "17:\tlearn: 0.2981711\ttotal: 161ms\tremaining: 8.79s\n",
      "18:\tlearn: 0.2922796\ttotal: 166ms\tremaining: 8.58s\n",
      "19:\tlearn: 0.2863589\ttotal: 171ms\tremaining: 8.39s\n",
      "20:\tlearn: 0.2806532\ttotal: 183ms\tremaining: 8.52s\n",
      "21:\tlearn: 0.2752674\ttotal: 189ms\tremaining: 8.4s\n",
      "22:\tlearn: 0.2696920\ttotal: 196ms\tremaining: 8.34s\n",
      "23:\tlearn: 0.2646486\ttotal: 202ms\tremaining: 8.22s\n",
      "24:\tlearn: 0.2596521\ttotal: 208ms\tremaining: 8.1s\n",
      "25:\tlearn: 0.2551176\ttotal: 213ms\tremaining: 7.97s\n",
      "26:\tlearn: 0.2505730\ttotal: 220ms\tremaining: 7.94s\n",
      "27:\tlearn: 0.2463309\ttotal: 225ms\tremaining: 7.82s\n",
      "28:\tlearn: 0.2422309\ttotal: 231ms\tremaining: 7.73s\n",
      "29:\tlearn: 0.2378867\ttotal: 235ms\tremaining: 7.61s\n",
      "30:\tlearn: 0.2337069\ttotal: 241ms\tremaining: 7.52s\n",
      "31:\tlearn: 0.2296324\ttotal: 246ms\tremaining: 7.43s\n",
      "32:\tlearn: 0.2256271\ttotal: 251ms\tremaining: 7.35s\n",
      "33:\tlearn: 0.2220109\ttotal: 256ms\tremaining: 7.27s\n",
      "34:\tlearn: 0.2186837\ttotal: 261ms\tremaining: 7.19s\n",
      "35:\tlearn: 0.2155680\ttotal: 266ms\tremaining: 7.13s\n",
      "36:\tlearn: 0.2122006\ttotal: 272ms\tremaining: 7.07s\n",
      "37:\tlearn: 0.2088621\ttotal: 276ms\tremaining: 7s\n",
      "38:\tlearn: 0.2058567\ttotal: 282ms\tremaining: 6.94s\n",
      "39:\tlearn: 0.2030110\ttotal: 288ms\tremaining: 6.9s\n",
      "40:\tlearn: 0.2003221\ttotal: 293ms\tremaining: 6.85s\n",
      "41:\tlearn: 0.1976403\ttotal: 298ms\tremaining: 6.79s\n",
      "42:\tlearn: 0.1952482\ttotal: 303ms\tremaining: 6.74s\n",
      "43:\tlearn: 0.1925376\ttotal: 308ms\tremaining: 6.69s\n",
      "44:\tlearn: 0.1902030\ttotal: 313ms\tremaining: 6.64s\n",
      "45:\tlearn: 0.1880066\ttotal: 318ms\tremaining: 6.6s\n",
      "46:\tlearn: 0.1858651\ttotal: 324ms\tremaining: 6.56s\n",
      "47:\tlearn: 0.1837542\ttotal: 329ms\tremaining: 6.52s\n",
      "48:\tlearn: 0.1817112\ttotal: 334ms\tremaining: 6.48s\n",
      "49:\tlearn: 0.1794884\ttotal: 339ms\tremaining: 6.44s\n",
      "50:\tlearn: 0.1775916\ttotal: 344ms\tremaining: 6.4s\n",
      "51:\tlearn: 0.1758250\ttotal: 353ms\tremaining: 6.43s\n",
      "52:\tlearn: 0.1739601\ttotal: 360ms\tremaining: 6.43s\n",
      "53:\tlearn: 0.1720406\ttotal: 367ms\tremaining: 6.43s\n",
      "54:\tlearn: 0.1705263\ttotal: 372ms\tremaining: 6.4s\n",
      "55:\tlearn: 0.1688571\ttotal: 381ms\tremaining: 6.42s\n",
      "56:\tlearn: 0.1671213\ttotal: 389ms\tremaining: 6.43s\n",
      "57:\tlearn: 0.1654609\ttotal: 394ms\tremaining: 6.4s\n",
      "58:\tlearn: 0.1640678\ttotal: 399ms\tremaining: 6.37s\n",
      "59:\tlearn: 0.1625170\ttotal: 404ms\tremaining: 6.33s\n",
      "60:\tlearn: 0.1611926\ttotal: 409ms\tremaining: 6.3s\n",
      "61:\tlearn: 0.1597531\ttotal: 415ms\tremaining: 6.27s\n",
      "62:\tlearn: 0.1586154\ttotal: 420ms\tremaining: 6.24s\n",
      "63:\tlearn: 0.1574752\ttotal: 425ms\tremaining: 6.21s\n",
      "64:\tlearn: 0.1561201\ttotal: 430ms\tremaining: 6.19s\n",
      "65:\tlearn: 0.1550788\ttotal: 436ms\tremaining: 6.17s\n",
      "66:\tlearn: 0.1537834\ttotal: 443ms\tremaining: 6.17s\n",
      "67:\tlearn: 0.1526765\ttotal: 449ms\tremaining: 6.15s\n",
      "68:\tlearn: 0.1516859\ttotal: 454ms\tremaining: 6.13s\n",
      "69:\tlearn: 0.1508132\ttotal: 460ms\tremaining: 6.11s\n",
      "70:\tlearn: 0.1498110\ttotal: 465ms\tremaining: 6.08s\n",
      "71:\tlearn: 0.1489518\ttotal: 472ms\tremaining: 6.08s\n",
      "72:\tlearn: 0.1481080\ttotal: 479ms\tremaining: 6.08s\n",
      "73:\tlearn: 0.1470856\ttotal: 486ms\tremaining: 6.08s\n",
      "74:\tlearn: 0.1463174\ttotal: 493ms\tremaining: 6.07s\n",
      "75:\tlearn: 0.1454997\ttotal: 499ms\tremaining: 6.07s\n",
      "76:\tlearn: 0.1448378\ttotal: 504ms\tremaining: 6.04s\n",
      "77:\tlearn: 0.1441396\ttotal: 509ms\tremaining: 6.02s\n",
      "78:\tlearn: 0.1433696\ttotal: 514ms\tremaining: 6s\n",
      "79:\tlearn: 0.1427612\ttotal: 519ms\tremaining: 5.97s\n",
      "80:\tlearn: 0.1421226\ttotal: 525ms\tremaining: 5.95s\n",
      "81:\tlearn: 0.1415375\ttotal: 530ms\tremaining: 5.93s\n",
      "82:\tlearn: 0.1409578\ttotal: 537ms\tremaining: 5.94s\n",
      "83:\tlearn: 0.1402608\ttotal: 542ms\tremaining: 5.91s\n",
      "84:\tlearn: 0.1397326\ttotal: 549ms\tremaining: 5.91s\n",
      "85:\tlearn: 0.1391547\ttotal: 555ms\tremaining: 5.9s\n",
      "86:\tlearn: 0.1385481\ttotal: 560ms\tremaining: 5.88s\n",
      "87:\tlearn: 0.1380731\ttotal: 566ms\tremaining: 5.86s\n",
      "88:\tlearn: 0.1373662\ttotal: 571ms\tremaining: 5.85s\n",
      "89:\tlearn: 0.1367306\ttotal: 577ms\tremaining: 5.83s\n",
      "90:\tlearn: 0.1360538\ttotal: 582ms\tremaining: 5.81s\n",
      "91:\tlearn: 0.1353600\ttotal: 587ms\tremaining: 5.79s\n",
      "92:\tlearn: 0.1348423\ttotal: 592ms\tremaining: 5.78s\n",
      "93:\tlearn: 0.1344051\ttotal: 597ms\tremaining: 5.76s\n",
      "94:\tlearn: 0.1339423\ttotal: 602ms\tremaining: 5.74s\n",
      "95:\tlearn: 0.1335740\ttotal: 608ms\tremaining: 5.72s\n",
      "96:\tlearn: 0.1331749\ttotal: 614ms\tremaining: 5.71s\n",
      "97:\tlearn: 0.1328333\ttotal: 619ms\tremaining: 5.7s\n",
      "98:\tlearn: 0.1321947\ttotal: 624ms\tremaining: 5.68s\n",
      "99:\tlearn: 0.1317014\ttotal: 629ms\tremaining: 5.66s\n",
      "100:\tlearn: 0.1313400\ttotal: 635ms\tremaining: 5.65s\n",
      "101:\tlearn: 0.1309153\ttotal: 640ms\tremaining: 5.63s\n",
      "102:\tlearn: 0.1304477\ttotal: 646ms\tremaining: 5.63s\n",
      "103:\tlearn: 0.1301293\ttotal: 651ms\tremaining: 5.61s\n",
      "104:\tlearn: 0.1297833\ttotal: 656ms\tremaining: 5.59s\n",
      "105:\tlearn: 0.1294095\ttotal: 661ms\tremaining: 5.58s\n",
      "106:\tlearn: 0.1289525\ttotal: 666ms\tremaining: 5.56s\n",
      "107:\tlearn: 0.1285886\ttotal: 671ms\tremaining: 5.54s\n",
      "108:\tlearn: 0.1282437\ttotal: 676ms\tremaining: 5.52s\n",
      "109:\tlearn: 0.1278965\ttotal: 681ms\tremaining: 5.51s\n",
      "110:\tlearn: 0.1276418\ttotal: 686ms\tremaining: 5.5s\n",
      "111:\tlearn: 0.1272142\ttotal: 691ms\tremaining: 5.48s\n",
      "112:\tlearn: 0.1269173\ttotal: 696ms\tremaining: 5.46s\n",
      "113:\tlearn: 0.1266602\ttotal: 701ms\tremaining: 5.45s\n",
      "114:\tlearn: 0.1263385\ttotal: 706ms\tremaining: 5.43s\n",
      "115:\tlearn: 0.1259109\ttotal: 711ms\tremaining: 5.42s\n",
      "116:\tlearn: 0.1256775\ttotal: 716ms\tremaining: 5.41s\n",
      "117:\tlearn: 0.1254524\ttotal: 722ms\tremaining: 5.39s\n",
      "118:\tlearn: 0.1250409\ttotal: 727ms\tremaining: 5.38s\n",
      "119:\tlearn: 0.1248424\ttotal: 732ms\tremaining: 5.37s\n",
      "120:\tlearn: 0.1244909\ttotal: 737ms\tremaining: 5.36s\n",
      "121:\tlearn: 0.1243022\ttotal: 743ms\tremaining: 5.35s\n",
      "122:\tlearn: 0.1240458\ttotal: 748ms\tremaining: 5.33s\n",
      "123:\tlearn: 0.1235485\ttotal: 753ms\tremaining: 5.32s\n",
      "124:\tlearn: 0.1232401\ttotal: 759ms\tremaining: 5.31s\n",
      "125:\tlearn: 0.1230764\ttotal: 764ms\tremaining: 5.3s\n",
      "126:\tlearn: 0.1227724\ttotal: 771ms\tremaining: 5.3s\n",
      "127:\tlearn: 0.1226080\ttotal: 778ms\tremaining: 5.3s\n",
      "128:\tlearn: 0.1223530\ttotal: 784ms\tremaining: 5.29s\n",
      "129:\tlearn: 0.1220298\ttotal: 788ms\tremaining: 5.28s\n",
      "130:\tlearn: 0.1218838\ttotal: 793ms\tremaining: 5.26s\n",
      "131:\tlearn: 0.1217269\ttotal: 798ms\tremaining: 5.25s\n",
      "132:\tlearn: 0.1213280\ttotal: 804ms\tremaining: 5.24s\n",
      "133:\tlearn: 0.1209985\ttotal: 809ms\tremaining: 5.23s\n",
      "134:\tlearn: 0.1207907\ttotal: 815ms\tremaining: 5.22s\n",
      "135:\tlearn: 0.1206111\ttotal: 821ms\tremaining: 5.21s\n",
      "136:\tlearn: 0.1204028\ttotal: 826ms\tremaining: 5.2s\n",
      "137:\tlearn: 0.1202731\ttotal: 833ms\tremaining: 5.2s\n",
      "138:\tlearn: 0.1201308\ttotal: 838ms\tremaining: 5.19s\n",
      "139:\tlearn: 0.1200278\ttotal: 843ms\tremaining: 5.17s\n",
      "140:\tlearn: 0.1197411\ttotal: 848ms\tremaining: 5.16s\n",
      "141:\tlearn: 0.1193948\ttotal: 853ms\tremaining: 5.15s\n",
      "142:\tlearn: 0.1191470\ttotal: 858ms\tremaining: 5.14s\n",
      "143:\tlearn: 0.1187580\ttotal: 863ms\tremaining: 5.13s\n",
      "144:\tlearn: 0.1184650\ttotal: 869ms\tremaining: 5.12s\n",
      "145:\tlearn: 0.1182621\ttotal: 873ms\tremaining: 5.11s\n",
      "146:\tlearn: 0.1179877\ttotal: 878ms\tremaining: 5.1s\n",
      "147:\tlearn: 0.1177549\ttotal: 883ms\tremaining: 5.08s\n",
      "148:\tlearn: 0.1175510\ttotal: 889ms\tremaining: 5.08s\n",
      "149:\tlearn: 0.1172671\ttotal: 894ms\tremaining: 5.06s\n",
      "150:\tlearn: 0.1170068\ttotal: 899ms\tremaining: 5.05s\n",
      "151:\tlearn: 0.1167415\ttotal: 905ms\tremaining: 5.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152:\tlearn: 0.1165467\ttotal: 910ms\tremaining: 5.04s\n",
      "153:\tlearn: 0.1163725\ttotal: 915ms\tremaining: 5.03s\n",
      "154:\tlearn: 0.1162362\ttotal: 920ms\tremaining: 5.01s\n",
      "155:\tlearn: 0.1160617\ttotal: 925ms\tremaining: 5s\n",
      "156:\tlearn: 0.1159258\ttotal: 929ms\tremaining: 4.99s\n",
      "157:\tlearn: 0.1158152\ttotal: 934ms\tremaining: 4.98s\n",
      "158:\tlearn: 0.1157085\ttotal: 939ms\tremaining: 4.97s\n",
      "159:\tlearn: 0.1155783\ttotal: 944ms\tremaining: 4.96s\n",
      "160:\tlearn: 0.1154813\ttotal: 950ms\tremaining: 4.95s\n",
      "161:\tlearn: 0.1152914\ttotal: 955ms\tremaining: 4.94s\n",
      "162:\tlearn: 0.1151079\ttotal: 961ms\tremaining: 4.93s\n",
      "163:\tlearn: 0.1148463\ttotal: 966ms\tremaining: 4.92s\n",
      "164:\tlearn: 0.1147656\ttotal: 971ms\tremaining: 4.92s\n",
      "165:\tlearn: 0.1145215\ttotal: 978ms\tremaining: 4.91s\n",
      "166:\tlearn: 0.1143694\ttotal: 984ms\tremaining: 4.91s\n",
      "167:\tlearn: 0.1141285\ttotal: 988ms\tremaining: 4.89s\n",
      "168:\tlearn: 0.1138725\ttotal: 994ms\tremaining: 4.89s\n",
      "169:\tlearn: 0.1137617\ttotal: 999ms\tremaining: 4.88s\n",
      "170:\tlearn: 0.1135394\ttotal: 1s\tremaining: 4.87s\n",
      "171:\tlearn: 0.1133940\ttotal: 1.01s\tremaining: 4.86s\n",
      "172:\tlearn: 0.1133171\ttotal: 1.02s\tremaining: 4.88s\n",
      "173:\tlearn: 0.1132710\ttotal: 1.03s\tremaining: 4.89s\n",
      "174:\tlearn: 0.1131041\ttotal: 1.04s\tremaining: 4.89s\n",
      "175:\tlearn: 0.1128832\ttotal: 1.05s\tremaining: 4.91s\n",
      "176:\tlearn: 0.1127658\ttotal: 1.06s\tremaining: 4.92s\n",
      "177:\tlearn: 0.1126166\ttotal: 1.07s\tremaining: 4.94s\n",
      "178:\tlearn: 0.1124073\ttotal: 1.08s\tremaining: 4.97s\n",
      "179:\tlearn: 0.1122908\ttotal: 1.09s\tremaining: 4.98s\n",
      "180:\tlearn: 0.1121027\ttotal: 1.1s\tremaining: 4.98s\n",
      "181:\tlearn: 0.1119248\ttotal: 1.11s\tremaining: 4.97s\n",
      "182:\tlearn: 0.1118283\ttotal: 1.11s\tremaining: 4.96s\n",
      "183:\tlearn: 0.1116846\ttotal: 1.12s\tremaining: 4.95s\n",
      "184:\tlearn: 0.1116067\ttotal: 1.12s\tremaining: 4.94s\n",
      "185:\tlearn: 0.1114694\ttotal: 1.13s\tremaining: 4.93s\n",
      "186:\tlearn: 0.1113518\ttotal: 1.13s\tremaining: 4.92s\n",
      "187:\tlearn: 0.1112752\ttotal: 1.14s\tremaining: 4.92s\n",
      "188:\tlearn: 0.1111222\ttotal: 1.14s\tremaining: 4.91s\n",
      "189:\tlearn: 0.1110053\ttotal: 1.15s\tremaining: 4.89s\n",
      "190:\tlearn: 0.1108860\ttotal: 1.15s\tremaining: 4.89s\n",
      "191:\tlearn: 0.1106839\ttotal: 1.16s\tremaining: 4.88s\n",
      "192:\tlearn: 0.1106293\ttotal: 1.17s\tremaining: 4.88s\n",
      "193:\tlearn: 0.1104835\ttotal: 1.17s\tremaining: 4.87s\n",
      "194:\tlearn: 0.1104137\ttotal: 1.18s\tremaining: 4.87s\n",
      "195:\tlearn: 0.1102824\ttotal: 1.18s\tremaining: 4.86s\n",
      "196:\tlearn: 0.1102174\ttotal: 1.19s\tremaining: 4.85s\n",
      "197:\tlearn: 0.1101139\ttotal: 1.19s\tremaining: 4.84s\n",
      "198:\tlearn: 0.1099180\ttotal: 1.2s\tremaining: 4.83s\n",
      "199:\tlearn: 0.1098204\ttotal: 1.21s\tremaining: 4.82s\n",
      "200:\tlearn: 0.1097206\ttotal: 1.21s\tremaining: 4.81s\n",
      "201:\tlearn: 0.1096678\ttotal: 1.22s\tremaining: 4.83s\n",
      "202:\tlearn: 0.1095754\ttotal: 1.23s\tremaining: 4.83s\n",
      "203:\tlearn: 0.1095171\ttotal: 1.24s\tremaining: 4.83s\n",
      "204:\tlearn: 0.1093734\ttotal: 1.25s\tremaining: 4.84s\n",
      "205:\tlearn: 0.1093031\ttotal: 1.26s\tremaining: 4.86s\n",
      "206:\tlearn: 0.1092272\ttotal: 1.27s\tremaining: 4.85s\n",
      "207:\tlearn: 0.1091550\ttotal: 1.27s\tremaining: 4.84s\n",
      "208:\tlearn: 0.1090476\ttotal: 1.28s\tremaining: 4.83s\n",
      "209:\tlearn: 0.1089124\ttotal: 1.28s\tremaining: 4.83s\n",
      "210:\tlearn: 0.1087761\ttotal: 1.29s\tremaining: 4.82s\n",
      "211:\tlearn: 0.1087183\ttotal: 1.29s\tremaining: 4.81s\n",
      "212:\tlearn: 0.1085947\ttotal: 1.3s\tremaining: 4.81s\n",
      "213:\tlearn: 0.1085531\ttotal: 1.31s\tremaining: 4.8s\n",
      "214:\tlearn: 0.1084887\ttotal: 1.31s\tremaining: 4.79s\n",
      "215:\tlearn: 0.1084259\ttotal: 1.32s\tremaining: 4.79s\n",
      "216:\tlearn: 0.1083573\ttotal: 1.32s\tremaining: 4.78s\n",
      "217:\tlearn: 0.1082960\ttotal: 1.33s\tremaining: 4.77s\n",
      "218:\tlearn: 0.1082389\ttotal: 1.33s\tremaining: 4.76s\n",
      "219:\tlearn: 0.1081318\ttotal: 1.34s\tremaining: 4.75s\n",
      "220:\tlearn: 0.1079800\ttotal: 1.35s\tremaining: 4.75s\n",
      "221:\tlearn: 0.1078802\ttotal: 1.35s\tremaining: 4.74s\n",
      "222:\tlearn: 0.1077598\ttotal: 1.36s\tremaining: 4.73s\n",
      "223:\tlearn: 0.1076390\ttotal: 1.36s\tremaining: 4.72s\n",
      "224:\tlearn: 0.1075600\ttotal: 1.37s\tremaining: 4.71s\n",
      "225:\tlearn: 0.1075062\ttotal: 1.37s\tremaining: 4.7s\n",
      "226:\tlearn: 0.1073832\ttotal: 1.38s\tremaining: 4.69s\n",
      "227:\tlearn: 0.1072460\ttotal: 1.38s\tremaining: 4.69s\n",
      "228:\tlearn: 0.1071706\ttotal: 1.39s\tremaining: 4.68s\n",
      "229:\tlearn: 0.1071110\ttotal: 1.39s\tremaining: 4.67s\n",
      "230:\tlearn: 0.1070673\ttotal: 1.4s\tremaining: 4.66s\n",
      "231:\tlearn: 0.1070022\ttotal: 1.41s\tremaining: 4.65s\n",
      "232:\tlearn: 0.1069216\ttotal: 1.41s\tremaining: 4.65s\n",
      "233:\tlearn: 0.1068052\ttotal: 1.42s\tremaining: 4.64s\n",
      "234:\tlearn: 0.1067170\ttotal: 1.42s\tremaining: 4.63s\n",
      "235:\tlearn: 0.1066061\ttotal: 1.43s\tremaining: 4.62s\n",
      "236:\tlearn: 0.1064745\ttotal: 1.43s\tremaining: 4.61s\n",
      "237:\tlearn: 0.1063795\ttotal: 1.44s\tremaining: 4.61s\n",
      "238:\tlearn: 0.1063168\ttotal: 1.45s\tremaining: 4.6s\n",
      "239:\tlearn: 0.1062153\ttotal: 1.45s\tremaining: 4.59s\n",
      "240:\tlearn: 0.1061137\ttotal: 1.46s\tremaining: 4.59s\n",
      "241:\tlearn: 0.1060883\ttotal: 1.46s\tremaining: 4.58s\n",
      "242:\tlearn: 0.1060551\ttotal: 1.47s\tremaining: 4.58s\n",
      "243:\tlearn: 0.1059931\ttotal: 1.47s\tremaining: 4.57s\n",
      "244:\tlearn: 0.1059321\ttotal: 1.48s\tremaining: 4.55s\n",
      "245:\tlearn: 0.1058868\ttotal: 1.48s\tremaining: 4.54s\n",
      "246:\tlearn: 0.1057802\ttotal: 1.49s\tremaining: 4.54s\n",
      "247:\tlearn: 0.1057110\ttotal: 1.49s\tremaining: 4.53s\n",
      "248:\tlearn: 0.1056161\ttotal: 1.5s\tremaining: 4.51s\n",
      "249:\tlearn: 0.1055168\ttotal: 1.5s\tremaining: 4.5s\n",
      "250:\tlearn: 0.1054498\ttotal: 1.51s\tremaining: 4.5s\n",
      "251:\tlearn: 0.1053853\ttotal: 1.51s\tremaining: 4.49s\n",
      "252:\tlearn: 0.1052637\ttotal: 1.52s\tremaining: 4.49s\n",
      "253:\tlearn: 0.1051578\ttotal: 1.52s\tremaining: 4.48s\n",
      "254:\tlearn: 0.1051060\ttotal: 1.53s\tremaining: 4.48s\n",
      "255:\tlearn: 0.1050697\ttotal: 1.54s\tremaining: 4.47s\n",
      "256:\tlearn: 0.1050390\ttotal: 1.54s\tremaining: 4.47s\n",
      "257:\tlearn: 0.1050003\ttotal: 1.55s\tremaining: 4.46s\n",
      "258:\tlearn: 0.1049055\ttotal: 1.56s\tremaining: 4.45s\n",
      "259:\tlearn: 0.1048494\ttotal: 1.56s\tremaining: 4.44s\n",
      "260:\tlearn: 0.1047498\ttotal: 1.57s\tremaining: 4.43s\n",
      "261:\tlearn: 0.1046900\ttotal: 1.57s\tremaining: 4.43s\n",
      "262:\tlearn: 0.1046625\ttotal: 1.58s\tremaining: 4.42s\n",
      "263:\tlearn: 0.1046055\ttotal: 1.58s\tremaining: 4.41s\n",
      "264:\tlearn: 0.1044852\ttotal: 1.59s\tremaining: 4.4s\n",
      "265:\tlearn: 0.1044331\ttotal: 1.59s\tremaining: 4.39s\n",
      "266:\tlearn: 0.1043092\ttotal: 1.6s\tremaining: 4.38s\n",
      "267:\tlearn: 0.1042316\ttotal: 1.6s\tremaining: 4.38s\n",
      "268:\tlearn: 0.1041530\ttotal: 1.61s\tremaining: 4.37s\n",
      "269:\tlearn: 0.1041170\ttotal: 1.61s\tremaining: 4.36s\n",
      "270:\tlearn: 0.1040686\ttotal: 1.62s\tremaining: 4.35s\n",
      "271:\tlearn: 0.1039467\ttotal: 1.62s\tremaining: 4.34s\n",
      "272:\tlearn: 0.1039230\ttotal: 1.63s\tremaining: 4.33s\n",
      "273:\tlearn: 0.1038683\ttotal: 1.63s\tremaining: 4.33s\n",
      "274:\tlearn: 0.1037840\ttotal: 1.64s\tremaining: 4.32s\n",
      "275:\tlearn: 0.1037568\ttotal: 1.64s\tremaining: 4.31s\n",
      "276:\tlearn: 0.1036709\ttotal: 1.65s\tremaining: 4.31s\n",
      "277:\tlearn: 0.1036199\ttotal: 1.65s\tremaining: 4.3s\n",
      "278:\tlearn: 0.1035570\ttotal: 1.66s\tremaining: 4.29s\n",
      "279:\tlearn: 0.1034806\ttotal: 1.66s\tremaining: 4.28s\n",
      "280:\tlearn: 0.1034093\ttotal: 1.67s\tremaining: 4.27s\n",
      "281:\tlearn: 0.1033820\ttotal: 1.68s\tremaining: 4.27s\n",
      "282:\tlearn: 0.1033350\ttotal: 1.68s\tremaining: 4.26s\n",
      "283:\tlearn: 0.1032612\ttotal: 1.69s\tremaining: 4.25s\n",
      "284:\tlearn: 0.1032125\ttotal: 1.69s\tremaining: 4.24s\n",
      "285:\tlearn: 0.1031400\ttotal: 1.7s\tremaining: 4.24s\n",
      "286:\tlearn: 0.1030921\ttotal: 1.7s\tremaining: 4.23s\n",
      "287:\tlearn: 0.1030364\ttotal: 1.71s\tremaining: 4.22s\n",
      "288:\tlearn: 0.1030044\ttotal: 1.71s\tremaining: 4.22s\n",
      "289:\tlearn: 0.1028850\ttotal: 1.72s\tremaining: 4.21s\n",
      "290:\tlearn: 0.1028379\ttotal: 1.72s\tremaining: 4.2s\n",
      "291:\tlearn: 0.1027740\ttotal: 1.73s\tremaining: 4.19s\n",
      "292:\tlearn: 0.1027106\ttotal: 1.73s\tremaining: 4.19s\n",
      "293:\tlearn: 0.1026736\ttotal: 1.74s\tremaining: 4.18s\n",
      "294:\tlearn: 0.1026069\ttotal: 1.75s\tremaining: 4.18s\n",
      "295:\tlearn: 0.1024561\ttotal: 1.75s\tremaining: 4.17s\n",
      "296:\tlearn: 0.1023794\ttotal: 1.76s\tremaining: 4.17s\n",
      "297:\tlearn: 0.1022726\ttotal: 1.76s\tremaining: 4.16s\n",
      "298:\tlearn: 0.1022417\ttotal: 1.77s\tremaining: 4.15s\n",
      "299:\tlearn: 0.1021255\ttotal: 1.78s\tremaining: 4.14s\n",
      "300:\tlearn: 0.1020903\ttotal: 1.78s\tremaining: 4.14s\n",
      "301:\tlearn: 0.1020381\ttotal: 1.79s\tremaining: 4.13s\n",
      "302:\tlearn: 0.1019913\ttotal: 1.79s\tremaining: 4.12s\n",
      "303:\tlearn: 0.1019216\ttotal: 1.8s\tremaining: 4.11s\n",
      "304:\tlearn: 0.1018910\ttotal: 1.8s\tremaining: 4.11s\n",
      "305:\tlearn: 0.1018240\ttotal: 1.81s\tremaining: 4.1s\n",
      "306:\tlearn: 0.1017286\ttotal: 1.81s\tremaining: 4.09s\n",
      "307:\tlearn: 0.1016354\ttotal: 1.82s\tremaining: 4.08s\n",
      "308:\tlearn: 0.1015120\ttotal: 1.82s\tremaining: 4.08s\n",
      "309:\tlearn: 0.1014410\ttotal: 1.83s\tremaining: 4.07s\n",
      "310:\tlearn: 0.1013964\ttotal: 1.83s\tremaining: 4.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311:\tlearn: 0.1013417\ttotal: 1.84s\tremaining: 4.05s\n",
      "312:\tlearn: 0.1012289\ttotal: 1.84s\tremaining: 4.05s\n",
      "313:\tlearn: 0.1011535\ttotal: 1.85s\tremaining: 4.04s\n",
      "314:\tlearn: 0.1011357\ttotal: 1.85s\tremaining: 4.03s\n",
      "315:\tlearn: 0.1010483\ttotal: 1.86s\tremaining: 4.03s\n",
      "316:\tlearn: 0.1009671\ttotal: 1.86s\tremaining: 4.02s\n",
      "317:\tlearn: 0.1008972\ttotal: 1.87s\tremaining: 4.01s\n",
      "318:\tlearn: 0.1008379\ttotal: 1.88s\tremaining: 4s\n",
      "319:\tlearn: 0.1007997\ttotal: 1.88s\tremaining: 4s\n",
      "320:\tlearn: 0.1007392\ttotal: 1.89s\tremaining: 4s\n",
      "321:\tlearn: 0.1007043\ttotal: 1.89s\tremaining: 3.99s\n",
      "322:\tlearn: 0.1006687\ttotal: 1.9s\tremaining: 3.98s\n",
      "323:\tlearn: 0.1006323\ttotal: 1.91s\tremaining: 3.98s\n",
      "324:\tlearn: 0.1006008\ttotal: 1.91s\tremaining: 3.97s\n",
      "325:\tlearn: 0.1005380\ttotal: 1.92s\tremaining: 3.96s\n",
      "326:\tlearn: 0.1004865\ttotal: 1.92s\tremaining: 3.95s\n",
      "327:\tlearn: 0.1004422\ttotal: 1.93s\tremaining: 3.95s\n",
      "328:\tlearn: 0.1003798\ttotal: 1.93s\tremaining: 3.94s\n",
      "329:\tlearn: 0.1002908\ttotal: 1.94s\tremaining: 3.93s\n",
      "330:\tlearn: 0.1002673\ttotal: 1.94s\tremaining: 3.92s\n",
      "331:\tlearn: 0.1001878\ttotal: 1.95s\tremaining: 3.92s\n",
      "332:\tlearn: 0.1001370\ttotal: 1.95s\tremaining: 3.91s\n",
      "333:\tlearn: 0.1001003\ttotal: 1.96s\tremaining: 3.9s\n",
      "334:\tlearn: 0.1000575\ttotal: 1.97s\tremaining: 3.91s\n",
      "335:\tlearn: 0.1000214\ttotal: 1.98s\tremaining: 3.91s\n",
      "336:\tlearn: 0.0999484\ttotal: 1.99s\tremaining: 3.91s\n",
      "337:\tlearn: 0.0998697\ttotal: 2s\tremaining: 3.91s\n",
      "338:\tlearn: 0.0997939\ttotal: 2.02s\tremaining: 3.93s\n",
      "339:\tlearn: 0.0997201\ttotal: 2.04s\tremaining: 3.96s\n",
      "340:\tlearn: 0.0996772\ttotal: 2.05s\tremaining: 3.95s\n",
      "341:\tlearn: 0.0996202\ttotal: 2.05s\tremaining: 3.95s\n",
      "342:\tlearn: 0.0996008\ttotal: 2.06s\tremaining: 3.94s\n",
      "343:\tlearn: 0.0995334\ttotal: 2.06s\tremaining: 3.93s\n",
      "344:\tlearn: 0.0994661\ttotal: 2.07s\tremaining: 3.92s\n",
      "345:\tlearn: 0.0993954\ttotal: 2.07s\tremaining: 3.92s\n",
      "346:\tlearn: 0.0993662\ttotal: 2.08s\tremaining: 3.91s\n",
      "347:\tlearn: 0.0993094\ttotal: 2.08s\tremaining: 3.9s\n",
      "348:\tlearn: 0.0992805\ttotal: 2.09s\tremaining: 3.89s\n",
      "349:\tlearn: 0.0992451\ttotal: 2.09s\tremaining: 3.89s\n",
      "350:\tlearn: 0.0991972\ttotal: 2.1s\tremaining: 3.88s\n",
      "351:\tlearn: 0.0991746\ttotal: 2.1s\tremaining: 3.87s\n",
      "352:\tlearn: 0.0991296\ttotal: 2.11s\tremaining: 3.87s\n",
      "353:\tlearn: 0.0990904\ttotal: 2.12s\tremaining: 3.86s\n",
      "354:\tlearn: 0.0990680\ttotal: 2.12s\tremaining: 3.85s\n",
      "355:\tlearn: 0.0990062\ttotal: 2.13s\tremaining: 3.85s\n",
      "356:\tlearn: 0.0989452\ttotal: 2.13s\tremaining: 3.85s\n",
      "357:\tlearn: 0.0989155\ttotal: 2.14s\tremaining: 3.84s\n",
      "358:\tlearn: 0.0988815\ttotal: 2.15s\tremaining: 3.83s\n",
      "359:\tlearn: 0.0988393\ttotal: 2.15s\tremaining: 3.83s\n",
      "360:\tlearn: 0.0987442\ttotal: 2.16s\tremaining: 3.82s\n",
      "361:\tlearn: 0.0987040\ttotal: 2.17s\tremaining: 3.82s\n",
      "362:\tlearn: 0.0986424\ttotal: 2.17s\tremaining: 3.81s\n",
      "363:\tlearn: 0.0985916\ttotal: 2.18s\tremaining: 3.81s\n",
      "364:\tlearn: 0.0985549\ttotal: 2.18s\tremaining: 3.8s\n",
      "365:\tlearn: 0.0985285\ttotal: 2.19s\tremaining: 3.79s\n",
      "366:\tlearn: 0.0985038\ttotal: 2.19s\tremaining: 3.78s\n",
      "367:\tlearn: 0.0984746\ttotal: 2.2s\tremaining: 3.77s\n",
      "368:\tlearn: 0.0984215\ttotal: 2.21s\tremaining: 3.77s\n",
      "369:\tlearn: 0.0983827\ttotal: 2.21s\tremaining: 3.77s\n",
      "370:\tlearn: 0.0983592\ttotal: 2.22s\tremaining: 3.76s\n",
      "371:\tlearn: 0.0983258\ttotal: 2.23s\tremaining: 3.76s\n",
      "372:\tlearn: 0.0982904\ttotal: 2.27s\tremaining: 3.81s\n",
      "373:\tlearn: 0.0982600\ttotal: 2.28s\tremaining: 3.81s\n",
      "374:\tlearn: 0.0982179\ttotal: 2.29s\tremaining: 3.81s\n",
      "375:\tlearn: 0.0981679\ttotal: 2.3s\tremaining: 3.82s\n",
      "376:\tlearn: 0.0981436\ttotal: 2.31s\tremaining: 3.82s\n",
      "377:\tlearn: 0.0981110\ttotal: 2.32s\tremaining: 3.81s\n",
      "378:\tlearn: 0.0980718\ttotal: 2.33s\tremaining: 3.81s\n",
      "379:\tlearn: 0.0980397\ttotal: 2.33s\tremaining: 3.81s\n",
      "380:\tlearn: 0.0980075\ttotal: 2.34s\tremaining: 3.81s\n",
      "381:\tlearn: 0.0979813\ttotal: 2.36s\tremaining: 3.82s\n",
      "382:\tlearn: 0.0979272\ttotal: 2.37s\tremaining: 3.82s\n",
      "383:\tlearn: 0.0978898\ttotal: 2.38s\tremaining: 3.82s\n",
      "384:\tlearn: 0.0978235\ttotal: 2.39s\tremaining: 3.82s\n",
      "385:\tlearn: 0.0977887\ttotal: 2.4s\tremaining: 3.82s\n",
      "386:\tlearn: 0.0977431\ttotal: 2.41s\tremaining: 3.82s\n",
      "387:\tlearn: 0.0977108\ttotal: 2.44s\tremaining: 3.84s\n",
      "388:\tlearn: 0.0976326\ttotal: 2.45s\tremaining: 3.84s\n",
      "389:\tlearn: 0.0976019\ttotal: 2.46s\tremaining: 3.84s\n",
      "390:\tlearn: 0.0975718\ttotal: 2.46s\tremaining: 3.84s\n",
      "391:\tlearn: 0.0975436\ttotal: 2.48s\tremaining: 3.85s\n",
      "392:\tlearn: 0.0974982\ttotal: 2.49s\tremaining: 3.84s\n",
      "393:\tlearn: 0.0974484\ttotal: 2.5s\tremaining: 3.84s\n",
      "394:\tlearn: 0.0973813\ttotal: 2.5s\tremaining: 3.84s\n",
      "395:\tlearn: 0.0973563\ttotal: 2.51s\tremaining: 3.83s\n",
      "396:\tlearn: 0.0973081\ttotal: 2.52s\tremaining: 3.83s\n",
      "397:\tlearn: 0.0972857\ttotal: 2.54s\tremaining: 3.83s\n",
      "398:\tlearn: 0.0972555\ttotal: 2.54s\tremaining: 3.83s\n",
      "399:\tlearn: 0.0972252\ttotal: 2.56s\tremaining: 3.83s\n",
      "400:\tlearn: 0.0971994\ttotal: 2.56s\tremaining: 3.83s\n",
      "401:\tlearn: 0.0971714\ttotal: 2.57s\tremaining: 3.82s\n",
      "402:\tlearn: 0.0971397\ttotal: 2.58s\tremaining: 3.82s\n",
      "403:\tlearn: 0.0971151\ttotal: 2.58s\tremaining: 3.81s\n",
      "404:\tlearn: 0.0970862\ttotal: 2.59s\tremaining: 3.8s\n",
      "405:\tlearn: 0.0970646\ttotal: 2.59s\tremaining: 3.79s\n",
      "406:\tlearn: 0.0969873\ttotal: 2.6s\tremaining: 3.79s\n",
      "407:\tlearn: 0.0969347\ttotal: 2.6s\tremaining: 3.78s\n",
      "408:\tlearn: 0.0969027\ttotal: 2.61s\tremaining: 3.77s\n",
      "409:\tlearn: 0.0968507\ttotal: 2.61s\tremaining: 3.76s\n",
      "410:\tlearn: 0.0968156\ttotal: 2.62s\tremaining: 3.75s\n",
      "411:\tlearn: 0.0967665\ttotal: 2.63s\tremaining: 3.75s\n",
      "412:\tlearn: 0.0967182\ttotal: 2.63s\tremaining: 3.74s\n",
      "413:\tlearn: 0.0966725\ttotal: 2.64s\tremaining: 3.74s\n",
      "414:\tlearn: 0.0966267\ttotal: 2.65s\tremaining: 3.73s\n",
      "415:\tlearn: 0.0965915\ttotal: 2.65s\tremaining: 3.73s\n",
      "416:\tlearn: 0.0965465\ttotal: 2.66s\tremaining: 3.72s\n",
      "417:\tlearn: 0.0964731\ttotal: 2.67s\tremaining: 3.71s\n",
      "418:\tlearn: 0.0964388\ttotal: 2.67s\tremaining: 3.7s\n",
      "419:\tlearn: 0.0964077\ttotal: 2.68s\tremaining: 3.7s\n",
      "420:\tlearn: 0.0963769\ttotal: 2.68s\tremaining: 3.69s\n",
      "421:\tlearn: 0.0963229\ttotal: 2.69s\tremaining: 3.68s\n",
      "422:\tlearn: 0.0962993\ttotal: 2.69s\tremaining: 3.67s\n",
      "423:\tlearn: 0.0962803\ttotal: 2.7s\tremaining: 3.67s\n",
      "424:\tlearn: 0.0962529\ttotal: 2.7s\tremaining: 3.66s\n",
      "425:\tlearn: 0.0962031\ttotal: 2.71s\tremaining: 3.65s\n",
      "426:\tlearn: 0.0961805\ttotal: 2.71s\tremaining: 3.64s\n",
      "427:\tlearn: 0.0961471\ttotal: 2.72s\tremaining: 3.63s\n",
      "428:\tlearn: 0.0961254\ttotal: 2.72s\tremaining: 3.63s\n",
      "429:\tlearn: 0.0960782\ttotal: 2.73s\tremaining: 3.62s\n",
      "430:\tlearn: 0.0960585\ttotal: 2.73s\tremaining: 3.61s\n",
      "431:\tlearn: 0.0960178\ttotal: 2.74s\tremaining: 3.6s\n",
      "432:\tlearn: 0.0959653\ttotal: 2.75s\tremaining: 3.6s\n",
      "433:\tlearn: 0.0959394\ttotal: 2.75s\tremaining: 3.59s\n",
      "434:\tlearn: 0.0958894\ttotal: 2.76s\tremaining: 3.58s\n",
      "435:\tlearn: 0.0958367\ttotal: 2.77s\tremaining: 3.58s\n",
      "436:\tlearn: 0.0957886\ttotal: 2.77s\tremaining: 3.57s\n",
      "437:\tlearn: 0.0957639\ttotal: 2.78s\tremaining: 3.56s\n",
      "438:\tlearn: 0.0957455\ttotal: 2.78s\tremaining: 3.56s\n",
      "439:\tlearn: 0.0957219\ttotal: 2.79s\tremaining: 3.55s\n",
      "440:\tlearn: 0.0956734\ttotal: 2.8s\tremaining: 3.54s\n",
      "441:\tlearn: 0.0956627\ttotal: 2.8s\tremaining: 3.54s\n",
      "442:\tlearn: 0.0956433\ttotal: 2.81s\tremaining: 3.53s\n",
      "443:\tlearn: 0.0956059\ttotal: 2.81s\tremaining: 3.52s\n",
      "444:\tlearn: 0.0955576\ttotal: 2.82s\tremaining: 3.52s\n",
      "445:\tlearn: 0.0955139\ttotal: 2.83s\tremaining: 3.51s\n",
      "446:\tlearn: 0.0954903\ttotal: 2.83s\tremaining: 3.5s\n",
      "447:\tlearn: 0.0954128\ttotal: 2.84s\tremaining: 3.5s\n",
      "448:\tlearn: 0.0953894\ttotal: 2.84s\tremaining: 3.49s\n",
      "449:\tlearn: 0.0953718\ttotal: 2.85s\tremaining: 3.48s\n",
      "450:\tlearn: 0.0953433\ttotal: 2.85s\tremaining: 3.48s\n",
      "451:\tlearn: 0.0952842\ttotal: 2.86s\tremaining: 3.47s\n",
      "452:\tlearn: 0.0952652\ttotal: 2.87s\tremaining: 3.46s\n",
      "453:\tlearn: 0.0952180\ttotal: 2.87s\tremaining: 3.45s\n",
      "454:\tlearn: 0.0951512\ttotal: 2.88s\tremaining: 3.45s\n",
      "455:\tlearn: 0.0950725\ttotal: 2.88s\tremaining: 3.44s\n",
      "456:\tlearn: 0.0950311\ttotal: 2.89s\tremaining: 3.43s\n",
      "457:\tlearn: 0.0949898\ttotal: 2.89s\tremaining: 3.42s\n",
      "458:\tlearn: 0.0949460\ttotal: 2.9s\tremaining: 3.42s\n",
      "459:\tlearn: 0.0949038\ttotal: 2.9s\tremaining: 3.41s\n",
      "460:\tlearn: 0.0948443\ttotal: 2.91s\tremaining: 3.4s\n",
      "461:\tlearn: 0.0948260\ttotal: 2.92s\tremaining: 3.4s\n",
      "462:\tlearn: 0.0947961\ttotal: 2.93s\tremaining: 3.4s\n",
      "463:\tlearn: 0.0947702\ttotal: 2.94s\tremaining: 3.4s\n",
      "464:\tlearn: 0.0947524\ttotal: 2.95s\tremaining: 3.4s\n",
      "465:\tlearn: 0.0947111\ttotal: 2.96s\tremaining: 3.4s\n",
      "466:\tlearn: 0.0946888\ttotal: 2.97s\tremaining: 3.39s\n",
      "467:\tlearn: 0.0946452\ttotal: 2.98s\tremaining: 3.39s\n",
      "468:\tlearn: 0.0946046\ttotal: 3s\tremaining: 3.39s\n",
      "469:\tlearn: 0.0945715\ttotal: 3s\tremaining: 3.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470:\tlearn: 0.0945620\ttotal: 3.01s\tremaining: 3.38s\n",
      "471:\tlearn: 0.0945444\ttotal: 3.01s\tremaining: 3.37s\n",
      "472:\tlearn: 0.0945121\ttotal: 3.02s\tremaining: 3.36s\n",
      "473:\tlearn: 0.0944936\ttotal: 3.02s\tremaining: 3.35s\n",
      "474:\tlearn: 0.0944686\ttotal: 3.03s\tremaining: 3.35s\n",
      "475:\tlearn: 0.0944266\ttotal: 3.03s\tremaining: 3.34s\n",
      "476:\tlearn: 0.0944065\ttotal: 3.04s\tremaining: 3.33s\n",
      "477:\tlearn: 0.0943807\ttotal: 3.04s\tremaining: 3.33s\n",
      "478:\tlearn: 0.0943570\ttotal: 3.05s\tremaining: 3.32s\n",
      "479:\tlearn: 0.0943197\ttotal: 3.06s\tremaining: 3.31s\n",
      "480:\tlearn: 0.0942893\ttotal: 3.06s\tremaining: 3.3s\n",
      "481:\tlearn: 0.0942501\ttotal: 3.07s\tremaining: 3.29s\n",
      "482:\tlearn: 0.0942131\ttotal: 3.07s\tremaining: 3.29s\n",
      "483:\tlearn: 0.0941968\ttotal: 3.08s\tremaining: 3.28s\n",
      "484:\tlearn: 0.0941738\ttotal: 3.08s\tremaining: 3.27s\n",
      "485:\tlearn: 0.0941185\ttotal: 3.09s\tremaining: 3.27s\n",
      "486:\tlearn: 0.0940913\ttotal: 3.09s\tremaining: 3.26s\n",
      "487:\tlearn: 0.0940785\ttotal: 3.1s\tremaining: 3.25s\n",
      "488:\tlearn: 0.0940637\ttotal: 3.11s\tremaining: 3.25s\n",
      "489:\tlearn: 0.0940315\ttotal: 3.12s\tremaining: 3.25s\n",
      "490:\tlearn: 0.0939809\ttotal: 3.13s\tremaining: 3.24s\n",
      "491:\tlearn: 0.0939608\ttotal: 3.13s\tremaining: 3.23s\n",
      "492:\tlearn: 0.0939441\ttotal: 3.14s\tremaining: 3.23s\n",
      "493:\tlearn: 0.0938831\ttotal: 3.14s\tremaining: 3.22s\n",
      "494:\tlearn: 0.0938692\ttotal: 3.15s\tremaining: 3.21s\n",
      "495:\tlearn: 0.0938323\ttotal: 3.15s\tremaining: 3.21s\n",
      "496:\tlearn: 0.0937981\ttotal: 3.16s\tremaining: 3.2s\n",
      "497:\tlearn: 0.0937822\ttotal: 3.16s\tremaining: 3.19s\n",
      "498:\tlearn: 0.0937555\ttotal: 3.17s\tremaining: 3.18s\n",
      "499:\tlearn: 0.0937252\ttotal: 3.17s\tremaining: 3.17s\n",
      "500:\tlearn: 0.0937093\ttotal: 3.18s\tremaining: 3.17s\n",
      "501:\tlearn: 0.0936740\ttotal: 3.19s\tremaining: 3.16s\n",
      "502:\tlearn: 0.0936492\ttotal: 3.19s\tremaining: 3.15s\n",
      "503:\tlearn: 0.0936211\ttotal: 3.2s\tremaining: 3.15s\n",
      "504:\tlearn: 0.0935945\ttotal: 3.21s\tremaining: 3.14s\n",
      "505:\tlearn: 0.0935614\ttotal: 3.21s\tremaining: 3.13s\n",
      "506:\tlearn: 0.0935103\ttotal: 3.21s\tremaining: 3.13s\n",
      "507:\tlearn: 0.0934829\ttotal: 3.22s\tremaining: 3.12s\n",
      "508:\tlearn: 0.0934557\ttotal: 3.23s\tremaining: 3.11s\n",
      "509:\tlearn: 0.0934225\ttotal: 3.23s\tremaining: 3.1s\n",
      "510:\tlearn: 0.0934085\ttotal: 3.24s\tremaining: 3.1s\n",
      "511:\tlearn: 0.0933916\ttotal: 3.24s\tremaining: 3.09s\n",
      "512:\tlearn: 0.0933671\ttotal: 3.25s\tremaining: 3.08s\n",
      "513:\tlearn: 0.0933439\ttotal: 3.25s\tremaining: 3.08s\n",
      "514:\tlearn: 0.0933284\ttotal: 3.26s\tremaining: 3.07s\n",
      "515:\tlearn: 0.0933059\ttotal: 3.27s\tremaining: 3.06s\n",
      "516:\tlearn: 0.0932914\ttotal: 3.27s\tremaining: 3.06s\n",
      "517:\tlearn: 0.0932800\ttotal: 3.28s\tremaining: 3.05s\n",
      "518:\tlearn: 0.0932611\ttotal: 3.28s\tremaining: 3.04s\n",
      "519:\tlearn: 0.0932449\ttotal: 3.29s\tremaining: 3.03s\n",
      "520:\tlearn: 0.0932169\ttotal: 3.29s\tremaining: 3.03s\n",
      "521:\tlearn: 0.0932016\ttotal: 3.3s\tremaining: 3.02s\n",
      "522:\tlearn: 0.0931765\ttotal: 3.31s\tremaining: 3.01s\n",
      "523:\tlearn: 0.0931493\ttotal: 3.31s\tremaining: 3.01s\n",
      "524:\tlearn: 0.0931338\ttotal: 3.31s\tremaining: 3s\n",
      "525:\tlearn: 0.0931007\ttotal: 3.32s\tremaining: 2.99s\n",
      "526:\tlearn: 0.0930745\ttotal: 3.33s\tremaining: 2.98s\n",
      "527:\tlearn: 0.0930515\ttotal: 3.33s\tremaining: 2.98s\n",
      "528:\tlearn: 0.0930191\ttotal: 3.34s\tremaining: 2.97s\n",
      "529:\tlearn: 0.0929956\ttotal: 3.34s\tremaining: 2.96s\n",
      "530:\tlearn: 0.0929251\ttotal: 3.35s\tremaining: 2.96s\n",
      "531:\tlearn: 0.0928484\ttotal: 3.35s\tremaining: 2.95s\n",
      "532:\tlearn: 0.0928195\ttotal: 3.36s\tremaining: 2.94s\n",
      "533:\tlearn: 0.0927941\ttotal: 3.37s\tremaining: 2.94s\n",
      "534:\tlearn: 0.0927724\ttotal: 3.37s\tremaining: 2.93s\n",
      "535:\tlearn: 0.0927366\ttotal: 3.38s\tremaining: 2.92s\n",
      "536:\tlearn: 0.0927261\ttotal: 3.38s\tremaining: 2.92s\n",
      "537:\tlearn: 0.0926916\ttotal: 3.39s\tremaining: 2.91s\n",
      "538:\tlearn: 0.0926856\ttotal: 3.39s\tremaining: 2.9s\n",
      "539:\tlearn: 0.0926358\ttotal: 3.4s\tremaining: 2.89s\n",
      "540:\tlearn: 0.0926217\ttotal: 3.4s\tremaining: 2.89s\n",
      "541:\tlearn: 0.0925991\ttotal: 3.41s\tremaining: 2.88s\n",
      "542:\tlearn: 0.0925820\ttotal: 3.41s\tremaining: 2.87s\n",
      "543:\tlearn: 0.0925450\ttotal: 3.42s\tremaining: 2.87s\n",
      "544:\tlearn: 0.0925202\ttotal: 3.42s\tremaining: 2.86s\n",
      "545:\tlearn: 0.0925018\ttotal: 3.43s\tremaining: 2.85s\n",
      "546:\tlearn: 0.0924830\ttotal: 3.44s\tremaining: 2.85s\n",
      "547:\tlearn: 0.0924619\ttotal: 3.44s\tremaining: 2.84s\n",
      "548:\tlearn: 0.0924521\ttotal: 3.45s\tremaining: 2.83s\n",
      "549:\tlearn: 0.0924362\ttotal: 3.45s\tremaining: 2.83s\n",
      "550:\tlearn: 0.0924139\ttotal: 3.46s\tremaining: 2.82s\n",
      "551:\tlearn: 0.0923773\ttotal: 3.46s\tremaining: 2.81s\n",
      "552:\tlearn: 0.0923466\ttotal: 3.47s\tremaining: 2.8s\n",
      "553:\tlearn: 0.0923195\ttotal: 3.47s\tremaining: 2.8s\n",
      "554:\tlearn: 0.0922918\ttotal: 3.48s\tremaining: 2.79s\n",
      "555:\tlearn: 0.0922596\ttotal: 3.48s\tremaining: 2.78s\n",
      "556:\tlearn: 0.0922440\ttotal: 3.49s\tremaining: 2.78s\n",
      "557:\tlearn: 0.0922226\ttotal: 3.5s\tremaining: 2.77s\n",
      "558:\tlearn: 0.0921998\ttotal: 3.51s\tremaining: 2.77s\n",
      "559:\tlearn: 0.0921857\ttotal: 3.52s\tremaining: 2.76s\n",
      "560:\tlearn: 0.0921531\ttotal: 3.52s\tremaining: 2.76s\n",
      "561:\tlearn: 0.0921223\ttotal: 3.53s\tremaining: 2.75s\n",
      "562:\tlearn: 0.0921068\ttotal: 3.53s\tremaining: 2.74s\n",
      "563:\tlearn: 0.0920927\ttotal: 3.54s\tremaining: 2.74s\n",
      "564:\tlearn: 0.0920786\ttotal: 3.54s\tremaining: 2.73s\n",
      "565:\tlearn: 0.0920638\ttotal: 3.55s\tremaining: 2.72s\n",
      "566:\tlearn: 0.0920437\ttotal: 3.56s\tremaining: 2.72s\n",
      "567:\tlearn: 0.0920119\ttotal: 3.57s\tremaining: 2.71s\n",
      "568:\tlearn: 0.0919885\ttotal: 3.58s\tremaining: 2.71s\n",
      "569:\tlearn: 0.0919613\ttotal: 3.58s\tremaining: 2.7s\n",
      "570:\tlearn: 0.0919367\ttotal: 3.59s\tremaining: 2.69s\n",
      "571:\tlearn: 0.0919243\ttotal: 3.59s\tremaining: 2.69s\n",
      "572:\tlearn: 0.0918820\ttotal: 3.6s\tremaining: 2.68s\n",
      "573:\tlearn: 0.0918631\ttotal: 3.61s\tremaining: 2.68s\n",
      "574:\tlearn: 0.0918224\ttotal: 3.61s\tremaining: 2.67s\n",
      "575:\tlearn: 0.0918022\ttotal: 3.62s\tremaining: 2.66s\n",
      "576:\tlearn: 0.0917822\ttotal: 3.63s\tremaining: 2.66s\n",
      "577:\tlearn: 0.0917594\ttotal: 3.63s\tremaining: 2.65s\n",
      "578:\tlearn: 0.0917337\ttotal: 3.64s\tremaining: 2.65s\n",
      "579:\tlearn: 0.0917174\ttotal: 3.65s\tremaining: 2.64s\n",
      "580:\tlearn: 0.0916755\ttotal: 3.65s\tremaining: 2.63s\n",
      "581:\tlearn: 0.0916587\ttotal: 3.66s\tremaining: 2.63s\n",
      "582:\tlearn: 0.0916411\ttotal: 3.66s\tremaining: 2.62s\n",
      "583:\tlearn: 0.0916244\ttotal: 3.67s\tremaining: 2.61s\n",
      "584:\tlearn: 0.0915933\ttotal: 3.67s\tremaining: 2.61s\n",
      "585:\tlearn: 0.0915699\ttotal: 3.68s\tremaining: 2.6s\n",
      "586:\tlearn: 0.0915530\ttotal: 3.69s\tremaining: 2.59s\n",
      "587:\tlearn: 0.0915257\ttotal: 3.69s\tremaining: 2.59s\n",
      "588:\tlearn: 0.0915117\ttotal: 3.7s\tremaining: 2.58s\n",
      "589:\tlearn: 0.0914957\ttotal: 3.71s\tremaining: 2.58s\n",
      "590:\tlearn: 0.0914796\ttotal: 3.71s\tremaining: 2.57s\n",
      "591:\tlearn: 0.0914367\ttotal: 3.72s\tremaining: 2.56s\n",
      "592:\tlearn: 0.0914152\ttotal: 3.72s\tremaining: 2.55s\n",
      "593:\tlearn: 0.0913654\ttotal: 3.73s\tremaining: 2.55s\n",
      "594:\tlearn: 0.0913540\ttotal: 3.73s\tremaining: 2.54s\n",
      "595:\tlearn: 0.0913402\ttotal: 3.74s\tremaining: 2.53s\n",
      "596:\tlearn: 0.0913205\ttotal: 3.74s\tremaining: 2.53s\n",
      "597:\tlearn: 0.0913084\ttotal: 3.75s\tremaining: 2.52s\n",
      "598:\tlearn: 0.0912663\ttotal: 3.76s\tremaining: 2.51s\n",
      "599:\tlearn: 0.0912574\ttotal: 3.76s\tremaining: 2.51s\n",
      "600:\tlearn: 0.0912477\ttotal: 3.77s\tremaining: 2.5s\n",
      "601:\tlearn: 0.0912136\ttotal: 3.77s\tremaining: 2.49s\n",
      "602:\tlearn: 0.0911674\ttotal: 3.78s\tremaining: 2.49s\n",
      "603:\tlearn: 0.0911312\ttotal: 3.79s\tremaining: 2.48s\n",
      "604:\tlearn: 0.0911047\ttotal: 3.79s\tremaining: 2.47s\n",
      "605:\tlearn: 0.0910953\ttotal: 3.79s\tremaining: 2.47s\n",
      "606:\tlearn: 0.0910807\ttotal: 3.8s\tremaining: 2.46s\n",
      "607:\tlearn: 0.0910471\ttotal: 3.81s\tremaining: 2.45s\n",
      "608:\tlearn: 0.0910166\ttotal: 3.81s\tremaining: 2.45s\n",
      "609:\tlearn: 0.0910012\ttotal: 3.82s\tremaining: 2.44s\n",
      "610:\tlearn: 0.0909774\ttotal: 3.82s\tremaining: 2.43s\n",
      "611:\tlearn: 0.0909624\ttotal: 3.83s\tremaining: 2.43s\n",
      "612:\tlearn: 0.0909351\ttotal: 3.83s\tremaining: 2.42s\n",
      "613:\tlearn: 0.0908958\ttotal: 3.84s\tremaining: 2.41s\n",
      "614:\tlearn: 0.0908699\ttotal: 3.85s\tremaining: 2.41s\n",
      "615:\tlearn: 0.0908436\ttotal: 3.85s\tremaining: 2.4s\n",
      "616:\tlearn: 0.0908292\ttotal: 3.86s\tremaining: 2.4s\n",
      "617:\tlearn: 0.0908153\ttotal: 3.87s\tremaining: 2.4s\n",
      "618:\tlearn: 0.0907990\ttotal: 3.89s\tremaining: 2.4s\n",
      "619:\tlearn: 0.0907867\ttotal: 3.91s\tremaining: 2.39s\n",
      "620:\tlearn: 0.0907785\ttotal: 3.92s\tremaining: 2.39s\n",
      "621:\tlearn: 0.0907587\ttotal: 3.93s\tremaining: 2.39s\n",
      "622:\tlearn: 0.0907425\ttotal: 3.94s\tremaining: 2.38s\n",
      "623:\tlearn: 0.0907186\ttotal: 3.95s\tremaining: 2.38s\n",
      "624:\tlearn: 0.0907066\ttotal: 3.96s\tremaining: 2.37s\n",
      "625:\tlearn: 0.0906858\ttotal: 3.96s\tremaining: 2.37s\n",
      "626:\tlearn: 0.0906699\ttotal: 3.96s\tremaining: 2.36s\n",
      "627:\tlearn: 0.0906577\ttotal: 3.97s\tremaining: 2.35s\n",
      "628:\tlearn: 0.0906482\ttotal: 3.98s\tremaining: 2.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629:\tlearn: 0.0906094\ttotal: 3.99s\tremaining: 2.34s\n",
      "630:\tlearn: 0.0905952\ttotal: 3.99s\tremaining: 2.33s\n",
      "631:\tlearn: 0.0905749\ttotal: 4s\tremaining: 2.33s\n",
      "632:\tlearn: 0.0905552\ttotal: 4s\tremaining: 2.32s\n",
      "633:\tlearn: 0.0905365\ttotal: 4.01s\tremaining: 2.31s\n",
      "634:\tlearn: 0.0905192\ttotal: 4.01s\tremaining: 2.31s\n",
      "635:\tlearn: 0.0905057\ttotal: 4.02s\tremaining: 2.3s\n",
      "636:\tlearn: 0.0904956\ttotal: 4.02s\tremaining: 2.29s\n",
      "637:\tlearn: 0.0904720\ttotal: 4.03s\tremaining: 2.29s\n",
      "638:\tlearn: 0.0904428\ttotal: 4.04s\tremaining: 2.28s\n",
      "639:\tlearn: 0.0904354\ttotal: 4.04s\tremaining: 2.27s\n",
      "640:\tlearn: 0.0903969\ttotal: 4.05s\tremaining: 2.27s\n",
      "641:\tlearn: 0.0903672\ttotal: 4.05s\tremaining: 2.26s\n",
      "642:\tlearn: 0.0903582\ttotal: 4.06s\tremaining: 2.25s\n",
      "643:\tlearn: 0.0903410\ttotal: 4.06s\tremaining: 2.25s\n",
      "644:\tlearn: 0.0903029\ttotal: 4.07s\tremaining: 2.24s\n",
      "645:\tlearn: 0.0902697\ttotal: 4.07s\tremaining: 2.23s\n",
      "646:\tlearn: 0.0902406\ttotal: 4.08s\tremaining: 2.23s\n",
      "647:\tlearn: 0.0902333\ttotal: 4.08s\tremaining: 2.22s\n",
      "648:\tlearn: 0.0902018\ttotal: 4.09s\tremaining: 2.21s\n",
      "649:\tlearn: 0.0901765\ttotal: 4.1s\tremaining: 2.21s\n",
      "650:\tlearn: 0.0901608\ttotal: 4.1s\tremaining: 2.2s\n",
      "651:\tlearn: 0.0901513\ttotal: 4.11s\tremaining: 2.19s\n",
      "652:\tlearn: 0.0901219\ttotal: 4.11s\tremaining: 2.19s\n",
      "653:\tlearn: 0.0900913\ttotal: 4.12s\tremaining: 2.18s\n",
      "654:\tlearn: 0.0900732\ttotal: 4.13s\tremaining: 2.17s\n",
      "655:\tlearn: 0.0900620\ttotal: 4.13s\tremaining: 2.17s\n",
      "656:\tlearn: 0.0900298\ttotal: 4.13s\tremaining: 2.16s\n",
      "657:\tlearn: 0.0900141\ttotal: 4.14s\tremaining: 2.15s\n",
      "658:\tlearn: 0.0900019\ttotal: 4.15s\tremaining: 2.15s\n",
      "659:\tlearn: 0.0899822\ttotal: 4.15s\tremaining: 2.14s\n",
      "660:\tlearn: 0.0899421\ttotal: 4.16s\tremaining: 2.13s\n",
      "661:\tlearn: 0.0899338\ttotal: 4.17s\tremaining: 2.13s\n",
      "662:\tlearn: 0.0899143\ttotal: 4.17s\tremaining: 2.12s\n",
      "663:\tlearn: 0.0899053\ttotal: 4.18s\tremaining: 2.12s\n",
      "664:\tlearn: 0.0898769\ttotal: 4.19s\tremaining: 2.11s\n",
      "665:\tlearn: 0.0898457\ttotal: 4.19s\tremaining: 2.1s\n",
      "666:\tlearn: 0.0898204\ttotal: 4.2s\tremaining: 2.1s\n",
      "667:\tlearn: 0.0897950\ttotal: 4.2s\tremaining: 2.09s\n",
      "668:\tlearn: 0.0897727\ttotal: 4.21s\tremaining: 2.08s\n",
      "669:\tlearn: 0.0897535\ttotal: 4.21s\tremaining: 2.08s\n",
      "670:\tlearn: 0.0897262\ttotal: 4.22s\tremaining: 2.07s\n",
      "671:\tlearn: 0.0897101\ttotal: 4.22s\tremaining: 2.06s\n",
      "672:\tlearn: 0.0897024\ttotal: 4.23s\tremaining: 2.06s\n",
      "673:\tlearn: 0.0896956\ttotal: 4.24s\tremaining: 2.05s\n",
      "674:\tlearn: 0.0896830\ttotal: 4.24s\tremaining: 2.04s\n",
      "675:\tlearn: 0.0896685\ttotal: 4.25s\tremaining: 2.04s\n",
      "676:\tlearn: 0.0896602\ttotal: 4.25s\tremaining: 2.03s\n",
      "677:\tlearn: 0.0896472\ttotal: 4.26s\tremaining: 2.02s\n",
      "678:\tlearn: 0.0896140\ttotal: 4.26s\tremaining: 2.02s\n",
      "679:\tlearn: 0.0895963\ttotal: 4.27s\tremaining: 2.01s\n",
      "680:\tlearn: 0.0895725\ttotal: 4.28s\tremaining: 2s\n",
      "681:\tlearn: 0.0895546\ttotal: 4.28s\tremaining: 2s\n",
      "682:\tlearn: 0.0895376\ttotal: 4.29s\tremaining: 1.99s\n",
      "683:\tlearn: 0.0895324\ttotal: 4.29s\tremaining: 1.98s\n",
      "684:\tlearn: 0.0895205\ttotal: 4.3s\tremaining: 1.98s\n",
      "685:\tlearn: 0.0894992\ttotal: 4.3s\tremaining: 1.97s\n",
      "686:\tlearn: 0.0894849\ttotal: 4.31s\tremaining: 1.96s\n",
      "687:\tlearn: 0.0894690\ttotal: 4.32s\tremaining: 1.96s\n",
      "688:\tlearn: 0.0894615\ttotal: 4.32s\tremaining: 1.95s\n",
      "689:\tlearn: 0.0894545\ttotal: 4.33s\tremaining: 1.94s\n",
      "690:\tlearn: 0.0894442\ttotal: 4.33s\tremaining: 1.94s\n",
      "691:\tlearn: 0.0894246\ttotal: 4.34s\tremaining: 1.93s\n",
      "692:\tlearn: 0.0894094\ttotal: 4.34s\tremaining: 1.92s\n",
      "693:\tlearn: 0.0893965\ttotal: 4.35s\tremaining: 1.92s\n",
      "694:\tlearn: 0.0893842\ttotal: 4.36s\tremaining: 1.91s\n",
      "695:\tlearn: 0.0893524\ttotal: 4.36s\tremaining: 1.91s\n",
      "696:\tlearn: 0.0893397\ttotal: 4.37s\tremaining: 1.9s\n",
      "697:\tlearn: 0.0893277\ttotal: 4.37s\tremaining: 1.89s\n",
      "698:\tlearn: 0.0893152\ttotal: 4.38s\tremaining: 1.89s\n",
      "699:\tlearn: 0.0893033\ttotal: 4.38s\tremaining: 1.88s\n",
      "700:\tlearn: 0.0892938\ttotal: 4.39s\tremaining: 1.87s\n",
      "701:\tlearn: 0.0892765\ttotal: 4.39s\tremaining: 1.86s\n",
      "702:\tlearn: 0.0892480\ttotal: 4.4s\tremaining: 1.86s\n",
      "703:\tlearn: 0.0892421\ttotal: 4.41s\tremaining: 1.85s\n",
      "704:\tlearn: 0.0892210\ttotal: 4.41s\tremaining: 1.84s\n",
      "705:\tlearn: 0.0891924\ttotal: 4.42s\tremaining: 1.84s\n",
      "706:\tlearn: 0.0891666\ttotal: 4.42s\tremaining: 1.83s\n",
      "707:\tlearn: 0.0891520\ttotal: 4.43s\tremaining: 1.83s\n",
      "708:\tlearn: 0.0891388\ttotal: 4.43s\tremaining: 1.82s\n",
      "709:\tlearn: 0.0891251\ttotal: 4.44s\tremaining: 1.81s\n",
      "710:\tlearn: 0.0891111\ttotal: 4.45s\tremaining: 1.81s\n",
      "711:\tlearn: 0.0891007\ttotal: 4.45s\tremaining: 1.8s\n",
      "712:\tlearn: 0.0890914\ttotal: 4.46s\tremaining: 1.79s\n",
      "713:\tlearn: 0.0890643\ttotal: 4.47s\tremaining: 1.79s\n",
      "714:\tlearn: 0.0890559\ttotal: 4.47s\tremaining: 1.78s\n",
      "715:\tlearn: 0.0890428\ttotal: 4.48s\tremaining: 1.78s\n",
      "716:\tlearn: 0.0890298\ttotal: 4.48s\tremaining: 1.77s\n",
      "717:\tlearn: 0.0890108\ttotal: 4.49s\tremaining: 1.76s\n",
      "718:\tlearn: 0.0889853\ttotal: 4.49s\tremaining: 1.76s\n",
      "719:\tlearn: 0.0889666\ttotal: 4.5s\tremaining: 1.75s\n",
      "720:\tlearn: 0.0889604\ttotal: 4.51s\tremaining: 1.74s\n",
      "721:\tlearn: 0.0889459\ttotal: 4.51s\tremaining: 1.74s\n",
      "722:\tlearn: 0.0889222\ttotal: 4.52s\tremaining: 1.73s\n",
      "723:\tlearn: 0.0889065\ttotal: 4.52s\tremaining: 1.72s\n",
      "724:\tlearn: 0.0888912\ttotal: 4.53s\tremaining: 1.72s\n",
      "725:\tlearn: 0.0888800\ttotal: 4.53s\tremaining: 1.71s\n",
      "726:\tlearn: 0.0888673\ttotal: 4.54s\tremaining: 1.7s\n",
      "727:\tlearn: 0.0888541\ttotal: 4.54s\tremaining: 1.7s\n",
      "728:\tlearn: 0.0888428\ttotal: 4.55s\tremaining: 1.69s\n",
      "729:\tlearn: 0.0888314\ttotal: 4.55s\tremaining: 1.68s\n",
      "730:\tlearn: 0.0888192\ttotal: 4.56s\tremaining: 1.68s\n",
      "731:\tlearn: 0.0888040\ttotal: 4.57s\tremaining: 1.67s\n",
      "732:\tlearn: 0.0887910\ttotal: 4.57s\tremaining: 1.67s\n",
      "733:\tlearn: 0.0887725\ttotal: 4.58s\tremaining: 1.66s\n",
      "734:\tlearn: 0.0887498\ttotal: 4.58s\tremaining: 1.65s\n",
      "735:\tlearn: 0.0887371\ttotal: 4.59s\tremaining: 1.65s\n",
      "736:\tlearn: 0.0887273\ttotal: 4.59s\tremaining: 1.64s\n",
      "737:\tlearn: 0.0887161\ttotal: 4.61s\tremaining: 1.63s\n",
      "738:\tlearn: 0.0886944\ttotal: 4.61s\tremaining: 1.63s\n",
      "739:\tlearn: 0.0886857\ttotal: 4.62s\tremaining: 1.62s\n",
      "740:\tlearn: 0.0886768\ttotal: 4.63s\tremaining: 1.62s\n",
      "741:\tlearn: 0.0886530\ttotal: 4.63s\tremaining: 1.61s\n",
      "742:\tlearn: 0.0886423\ttotal: 4.64s\tremaining: 1.6s\n",
      "743:\tlearn: 0.0886353\ttotal: 4.64s\tremaining: 1.6s\n",
      "744:\tlearn: 0.0886167\ttotal: 4.65s\tremaining: 1.59s\n",
      "745:\tlearn: 0.0885958\ttotal: 4.65s\tremaining: 1.58s\n",
      "746:\tlearn: 0.0885825\ttotal: 4.66s\tremaining: 1.58s\n",
      "747:\tlearn: 0.0885711\ttotal: 4.67s\tremaining: 1.57s\n",
      "748:\tlearn: 0.0885588\ttotal: 4.67s\tremaining: 1.56s\n",
      "749:\tlearn: 0.0885499\ttotal: 4.68s\tremaining: 1.56s\n",
      "750:\tlearn: 0.0885379\ttotal: 4.68s\tremaining: 1.55s\n",
      "751:\tlearn: 0.0885273\ttotal: 4.69s\tremaining: 1.54s\n",
      "752:\tlearn: 0.0885130\ttotal: 4.69s\tremaining: 1.54s\n",
      "753:\tlearn: 0.0885058\ttotal: 4.7s\tremaining: 1.53s\n",
      "754:\tlearn: 0.0884936\ttotal: 4.7s\tremaining: 1.53s\n",
      "755:\tlearn: 0.0884813\ttotal: 4.71s\tremaining: 1.52s\n",
      "756:\tlearn: 0.0884679\ttotal: 4.72s\tremaining: 1.51s\n",
      "757:\tlearn: 0.0884418\ttotal: 4.72s\tremaining: 1.51s\n",
      "758:\tlearn: 0.0884290\ttotal: 4.73s\tremaining: 1.5s\n",
      "759:\tlearn: 0.0884187\ttotal: 4.74s\tremaining: 1.5s\n",
      "760:\tlearn: 0.0884062\ttotal: 4.74s\tremaining: 1.49s\n",
      "761:\tlearn: 0.0883966\ttotal: 4.75s\tremaining: 1.48s\n",
      "762:\tlearn: 0.0883844\ttotal: 4.75s\tremaining: 1.48s\n",
      "763:\tlearn: 0.0883586\ttotal: 4.76s\tremaining: 1.47s\n",
      "764:\tlearn: 0.0883445\ttotal: 4.77s\tremaining: 1.46s\n",
      "765:\tlearn: 0.0883244\ttotal: 4.77s\tremaining: 1.46s\n",
      "766:\tlearn: 0.0883069\ttotal: 4.78s\tremaining: 1.45s\n",
      "767:\tlearn: 0.0883013\ttotal: 4.78s\tremaining: 1.44s\n",
      "768:\tlearn: 0.0882724\ttotal: 4.79s\tremaining: 1.44s\n",
      "769:\tlearn: 0.0882602\ttotal: 4.79s\tremaining: 1.43s\n",
      "770:\tlearn: 0.0882514\ttotal: 4.8s\tremaining: 1.43s\n",
      "771:\tlearn: 0.0882313\ttotal: 4.81s\tremaining: 1.42s\n",
      "772:\tlearn: 0.0882086\ttotal: 4.82s\tremaining: 1.42s\n",
      "773:\tlearn: 0.0881787\ttotal: 4.83s\tremaining: 1.41s\n",
      "774:\tlearn: 0.0881574\ttotal: 4.85s\tremaining: 1.41s\n",
      "775:\tlearn: 0.0881455\ttotal: 4.86s\tremaining: 1.4s\n",
      "776:\tlearn: 0.0881123\ttotal: 4.88s\tremaining: 1.4s\n",
      "777:\tlearn: 0.0881033\ttotal: 4.89s\tremaining: 1.39s\n",
      "778:\tlearn: 0.0880909\ttotal: 4.9s\tremaining: 1.39s\n",
      "779:\tlearn: 0.0880788\ttotal: 4.9s\tremaining: 1.38s\n",
      "780:\tlearn: 0.0880662\ttotal: 4.91s\tremaining: 1.38s\n",
      "781:\tlearn: 0.0880567\ttotal: 4.91s\tremaining: 1.37s\n",
      "782:\tlearn: 0.0880512\ttotal: 4.92s\tremaining: 1.36s\n",
      "783:\tlearn: 0.0880110\ttotal: 4.92s\tremaining: 1.36s\n",
      "784:\tlearn: 0.0880013\ttotal: 4.93s\tremaining: 1.35s\n",
      "785:\tlearn: 0.0879876\ttotal: 4.94s\tremaining: 1.34s\n",
      "786:\tlearn: 0.0879772\ttotal: 4.94s\tremaining: 1.34s\n",
      "787:\tlearn: 0.0879667\ttotal: 4.95s\tremaining: 1.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788:\tlearn: 0.0879566\ttotal: 4.96s\tremaining: 1.33s\n",
      "789:\tlearn: 0.0879391\ttotal: 4.96s\tremaining: 1.32s\n",
      "790:\tlearn: 0.0879272\ttotal: 4.97s\tremaining: 1.31s\n",
      "791:\tlearn: 0.0879057\ttotal: 4.98s\tremaining: 1.31s\n",
      "792:\tlearn: 0.0878943\ttotal: 4.98s\tremaining: 1.3s\n",
      "793:\tlearn: 0.0878823\ttotal: 4.99s\tremaining: 1.29s\n",
      "794:\tlearn: 0.0878723\ttotal: 4.99s\tremaining: 1.29s\n",
      "795:\tlearn: 0.0878622\ttotal: 5s\tremaining: 1.28s\n",
      "796:\tlearn: 0.0878542\ttotal: 5.01s\tremaining: 1.27s\n",
      "797:\tlearn: 0.0878458\ttotal: 5.01s\tremaining: 1.27s\n",
      "798:\tlearn: 0.0878365\ttotal: 5.02s\tremaining: 1.26s\n",
      "799:\tlearn: 0.0878292\ttotal: 5.02s\tremaining: 1.26s\n",
      "800:\tlearn: 0.0878215\ttotal: 5.03s\tremaining: 1.25s\n",
      "801:\tlearn: 0.0878149\ttotal: 5.04s\tremaining: 1.24s\n",
      "802:\tlearn: 0.0878066\ttotal: 5.04s\tremaining: 1.24s\n",
      "803:\tlearn: 0.0877834\ttotal: 5.05s\tremaining: 1.23s\n",
      "804:\tlearn: 0.0877697\ttotal: 5.05s\tremaining: 1.22s\n",
      "805:\tlearn: 0.0877631\ttotal: 5.06s\tremaining: 1.22s\n",
      "806:\tlearn: 0.0877344\ttotal: 5.07s\tremaining: 1.21s\n",
      "807:\tlearn: 0.0876938\ttotal: 5.07s\tremaining: 1.21s\n",
      "808:\tlearn: 0.0876811\ttotal: 5.08s\tremaining: 1.2s\n",
      "809:\tlearn: 0.0876590\ttotal: 5.08s\tremaining: 1.19s\n",
      "810:\tlearn: 0.0876411\ttotal: 5.09s\tremaining: 1.19s\n",
      "811:\tlearn: 0.0876317\ttotal: 5.1s\tremaining: 1.18s\n",
      "812:\tlearn: 0.0876239\ttotal: 5.1s\tremaining: 1.17s\n",
      "813:\tlearn: 0.0876152\ttotal: 5.11s\tremaining: 1.17s\n",
      "814:\tlearn: 0.0876060\ttotal: 5.11s\tremaining: 1.16s\n",
      "815:\tlearn: 0.0875943\ttotal: 5.12s\tremaining: 1.15s\n",
      "816:\tlearn: 0.0875722\ttotal: 5.12s\tremaining: 1.15s\n",
      "817:\tlearn: 0.0875635\ttotal: 5.13s\tremaining: 1.14s\n",
      "818:\tlearn: 0.0875540\ttotal: 5.14s\tremaining: 1.14s\n",
      "819:\tlearn: 0.0875403\ttotal: 5.14s\tremaining: 1.13s\n",
      "820:\tlearn: 0.0875322\ttotal: 5.15s\tremaining: 1.12s\n",
      "821:\tlearn: 0.0875072\ttotal: 5.16s\tremaining: 1.12s\n",
      "822:\tlearn: 0.0874702\ttotal: 5.16s\tremaining: 1.11s\n",
      "823:\tlearn: 0.0874599\ttotal: 5.17s\tremaining: 1.1s\n",
      "824:\tlearn: 0.0874488\ttotal: 5.17s\tremaining: 1.1s\n",
      "825:\tlearn: 0.0874359\ttotal: 5.18s\tremaining: 1.09s\n",
      "826:\tlearn: 0.0874263\ttotal: 5.19s\tremaining: 1.08s\n",
      "827:\tlearn: 0.0874221\ttotal: 5.19s\tremaining: 1.08s\n",
      "828:\tlearn: 0.0874121\ttotal: 5.2s\tremaining: 1.07s\n",
      "829:\tlearn: 0.0874044\ttotal: 5.2s\tremaining: 1.06s\n",
      "830:\tlearn: 0.0873902\ttotal: 5.21s\tremaining: 1.06s\n",
      "831:\tlearn: 0.0873783\ttotal: 5.21s\tremaining: 1.05s\n",
      "832:\tlearn: 0.0873586\ttotal: 5.22s\tremaining: 1.05s\n",
      "833:\tlearn: 0.0873488\ttotal: 5.23s\tremaining: 1.04s\n",
      "834:\tlearn: 0.0873308\ttotal: 5.23s\tremaining: 1.03s\n",
      "835:\tlearn: 0.0873197\ttotal: 5.24s\tremaining: 1.03s\n",
      "836:\tlearn: 0.0872945\ttotal: 5.25s\tremaining: 1.02s\n",
      "837:\tlearn: 0.0872862\ttotal: 5.25s\tremaining: 1.01s\n",
      "838:\tlearn: 0.0872719\ttotal: 5.26s\tremaining: 1.01s\n",
      "839:\tlearn: 0.0872646\ttotal: 5.26s\tremaining: 1s\n",
      "840:\tlearn: 0.0872547\ttotal: 5.27s\tremaining: 996ms\n",
      "841:\tlearn: 0.0872464\ttotal: 5.28s\tremaining: 990ms\n",
      "842:\tlearn: 0.0872367\ttotal: 5.28s\tremaining: 984ms\n",
      "843:\tlearn: 0.0872287\ttotal: 5.29s\tremaining: 977ms\n",
      "844:\tlearn: 0.0872205\ttotal: 5.29s\tremaining: 971ms\n",
      "845:\tlearn: 0.0872118\ttotal: 5.3s\tremaining: 964ms\n",
      "846:\tlearn: 0.0872073\ttotal: 5.3s\tremaining: 958ms\n",
      "847:\tlearn: 0.0872014\ttotal: 5.31s\tremaining: 952ms\n",
      "848:\tlearn: 0.0871958\ttotal: 5.32s\tremaining: 945ms\n",
      "849:\tlearn: 0.0871847\ttotal: 5.32s\tremaining: 939ms\n",
      "850:\tlearn: 0.0871695\ttotal: 5.33s\tremaining: 933ms\n",
      "851:\tlearn: 0.0871547\ttotal: 5.33s\tremaining: 927ms\n",
      "852:\tlearn: 0.0871458\ttotal: 5.34s\tremaining: 920ms\n",
      "853:\tlearn: 0.0871200\ttotal: 5.35s\tremaining: 914ms\n",
      "854:\tlearn: 0.0870951\ttotal: 5.35s\tremaining: 908ms\n",
      "855:\tlearn: 0.0870688\ttotal: 5.36s\tremaining: 902ms\n",
      "856:\tlearn: 0.0870369\ttotal: 5.37s\tremaining: 895ms\n",
      "857:\tlearn: 0.0870264\ttotal: 5.37s\tremaining: 889ms\n",
      "858:\tlearn: 0.0869992\ttotal: 5.38s\tremaining: 883ms\n",
      "859:\tlearn: 0.0869950\ttotal: 5.38s\tremaining: 877ms\n",
      "860:\tlearn: 0.0869886\ttotal: 5.39s\tremaining: 870ms\n",
      "861:\tlearn: 0.0869800\ttotal: 5.4s\tremaining: 864ms\n",
      "862:\tlearn: 0.0869748\ttotal: 5.4s\tremaining: 858ms\n",
      "863:\tlearn: 0.0869659\ttotal: 5.41s\tremaining: 851ms\n",
      "864:\tlearn: 0.0869587\ttotal: 5.41s\tremaining: 845ms\n",
      "865:\tlearn: 0.0869505\ttotal: 5.42s\tremaining: 838ms\n",
      "866:\tlearn: 0.0869419\ttotal: 5.42s\tremaining: 832ms\n",
      "867:\tlearn: 0.0869339\ttotal: 5.43s\tremaining: 826ms\n",
      "868:\tlearn: 0.0869213\ttotal: 5.43s\tremaining: 819ms\n",
      "869:\tlearn: 0.0869127\ttotal: 5.44s\tremaining: 813ms\n",
      "870:\tlearn: 0.0869020\ttotal: 5.45s\tremaining: 807ms\n",
      "871:\tlearn: 0.0868924\ttotal: 5.45s\tremaining: 800ms\n",
      "872:\tlearn: 0.0868867\ttotal: 5.46s\tremaining: 794ms\n",
      "873:\tlearn: 0.0868654\ttotal: 5.46s\tremaining: 787ms\n",
      "874:\tlearn: 0.0868579\ttotal: 5.47s\tremaining: 781ms\n",
      "875:\tlearn: 0.0868521\ttotal: 5.47s\tremaining: 775ms\n",
      "876:\tlearn: 0.0868447\ttotal: 5.48s\tremaining: 768ms\n",
      "877:\tlearn: 0.0868348\ttotal: 5.48s\tremaining: 762ms\n",
      "878:\tlearn: 0.0868206\ttotal: 5.49s\tremaining: 756ms\n",
      "879:\tlearn: 0.0868088\ttotal: 5.5s\tremaining: 750ms\n",
      "880:\tlearn: 0.0867977\ttotal: 5.5s\tremaining: 743ms\n",
      "881:\tlearn: 0.0867833\ttotal: 5.51s\tremaining: 737ms\n",
      "882:\tlearn: 0.0867698\ttotal: 5.52s\tremaining: 731ms\n",
      "883:\tlearn: 0.0867603\ttotal: 5.52s\tremaining: 725ms\n",
      "884:\tlearn: 0.0867452\ttotal: 5.53s\tremaining: 718ms\n",
      "885:\tlearn: 0.0867155\ttotal: 5.53s\tremaining: 712ms\n",
      "886:\tlearn: 0.0866988\ttotal: 5.54s\tremaining: 706ms\n",
      "887:\tlearn: 0.0866931\ttotal: 5.54s\tremaining: 699ms\n",
      "888:\tlearn: 0.0866724\ttotal: 5.55s\tremaining: 693ms\n",
      "889:\tlearn: 0.0866645\ttotal: 5.56s\tremaining: 687ms\n",
      "890:\tlearn: 0.0866485\ttotal: 5.56s\tremaining: 681ms\n",
      "891:\tlearn: 0.0866281\ttotal: 5.57s\tremaining: 674ms\n",
      "892:\tlearn: 0.0866072\ttotal: 5.57s\tremaining: 668ms\n",
      "893:\tlearn: 0.0865966\ttotal: 5.58s\tremaining: 662ms\n",
      "894:\tlearn: 0.0865861\ttotal: 5.59s\tremaining: 655ms\n",
      "895:\tlearn: 0.0865809\ttotal: 5.59s\tremaining: 649ms\n",
      "896:\tlearn: 0.0865553\ttotal: 5.6s\tremaining: 643ms\n",
      "897:\tlearn: 0.0865438\ttotal: 5.6s\tremaining: 636ms\n",
      "898:\tlearn: 0.0865266\ttotal: 5.61s\tremaining: 630ms\n",
      "899:\tlearn: 0.0865203\ttotal: 5.62s\tremaining: 624ms\n",
      "900:\tlearn: 0.0865018\ttotal: 5.62s\tremaining: 618ms\n",
      "901:\tlearn: 0.0864947\ttotal: 5.63s\tremaining: 612ms\n",
      "902:\tlearn: 0.0864798\ttotal: 5.63s\tremaining: 605ms\n",
      "903:\tlearn: 0.0864517\ttotal: 5.64s\tremaining: 599ms\n",
      "904:\tlearn: 0.0864395\ttotal: 5.65s\tremaining: 593ms\n",
      "905:\tlearn: 0.0864302\ttotal: 5.65s\tremaining: 587ms\n",
      "906:\tlearn: 0.0864237\ttotal: 5.66s\tremaining: 580ms\n",
      "907:\tlearn: 0.0864098\ttotal: 5.67s\tremaining: 574ms\n",
      "908:\tlearn: 0.0864046\ttotal: 5.67s\tremaining: 568ms\n",
      "909:\tlearn: 0.0863921\ttotal: 5.68s\tremaining: 561ms\n",
      "910:\tlearn: 0.0863832\ttotal: 5.68s\tremaining: 555ms\n",
      "911:\tlearn: 0.0863581\ttotal: 5.7s\tremaining: 550ms\n",
      "912:\tlearn: 0.0863272\ttotal: 5.7s\tremaining: 544ms\n",
      "913:\tlearn: 0.0863200\ttotal: 5.71s\tremaining: 538ms\n",
      "914:\tlearn: 0.0863155\ttotal: 5.72s\tremaining: 531ms\n",
      "915:\tlearn: 0.0863057\ttotal: 5.72s\tremaining: 525ms\n",
      "916:\tlearn: 0.0862996\ttotal: 5.73s\tremaining: 519ms\n",
      "917:\tlearn: 0.0862927\ttotal: 5.74s\tremaining: 512ms\n",
      "918:\tlearn: 0.0862768\ttotal: 5.74s\tremaining: 506ms\n",
      "919:\tlearn: 0.0862707\ttotal: 5.76s\tremaining: 501ms\n",
      "920:\tlearn: 0.0862608\ttotal: 5.77s\tremaining: 495ms\n",
      "921:\tlearn: 0.0862524\ttotal: 5.78s\tremaining: 489ms\n",
      "922:\tlearn: 0.0862437\ttotal: 5.79s\tremaining: 483ms\n",
      "923:\tlearn: 0.0862386\ttotal: 5.81s\tremaining: 478ms\n",
      "924:\tlearn: 0.0862340\ttotal: 5.82s\tremaining: 472ms\n",
      "925:\tlearn: 0.0862201\ttotal: 5.84s\tremaining: 466ms\n",
      "926:\tlearn: 0.0862079\ttotal: 5.84s\tremaining: 460ms\n",
      "927:\tlearn: 0.0861868\ttotal: 5.85s\tremaining: 454ms\n",
      "928:\tlearn: 0.0861768\ttotal: 5.87s\tremaining: 448ms\n",
      "929:\tlearn: 0.0861696\ttotal: 5.87s\tremaining: 442ms\n",
      "930:\tlearn: 0.0861603\ttotal: 5.88s\tremaining: 436ms\n",
      "931:\tlearn: 0.0861496\ttotal: 5.88s\tremaining: 429ms\n",
      "932:\tlearn: 0.0861401\ttotal: 5.89s\tremaining: 423ms\n",
      "933:\tlearn: 0.0861298\ttotal: 5.9s\tremaining: 417ms\n",
      "934:\tlearn: 0.0861155\ttotal: 5.91s\tremaining: 411ms\n",
      "935:\tlearn: 0.0861067\ttotal: 5.91s\tremaining: 404ms\n",
      "936:\tlearn: 0.0861029\ttotal: 5.92s\tremaining: 398ms\n",
      "937:\tlearn: 0.0860826\ttotal: 5.92s\tremaining: 391ms\n",
      "938:\tlearn: 0.0860733\ttotal: 5.93s\tremaining: 385ms\n",
      "939:\tlearn: 0.0860640\ttotal: 5.93s\tremaining: 379ms\n",
      "940:\tlearn: 0.0860412\ttotal: 5.94s\tremaining: 372ms\n",
      "941:\tlearn: 0.0860290\ttotal: 5.94s\tremaining: 366ms\n",
      "942:\tlearn: 0.0860202\ttotal: 5.95s\tremaining: 360ms\n",
      "943:\tlearn: 0.0860089\ttotal: 5.96s\tremaining: 353ms\n",
      "944:\tlearn: 0.0860047\ttotal: 5.96s\tremaining: 347ms\n",
      "945:\tlearn: 0.0859846\ttotal: 5.97s\tremaining: 341ms\n",
      "946:\tlearn: 0.0859795\ttotal: 5.97s\tremaining: 334ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947:\tlearn: 0.0859685\ttotal: 5.98s\tremaining: 328ms\n",
      "948:\tlearn: 0.0859523\ttotal: 5.99s\tremaining: 322ms\n",
      "949:\tlearn: 0.0859447\ttotal: 5.99s\tremaining: 315ms\n",
      "950:\tlearn: 0.0859307\ttotal: 6s\tremaining: 309ms\n",
      "951:\tlearn: 0.0859121\ttotal: 6s\tremaining: 303ms\n",
      "952:\tlearn: 0.0858940\ttotal: 6.01s\tremaining: 296ms\n",
      "953:\tlearn: 0.0858879\ttotal: 6.01s\tremaining: 290ms\n",
      "954:\tlearn: 0.0858786\ttotal: 6.02s\tremaining: 284ms\n",
      "955:\tlearn: 0.0858653\ttotal: 6.03s\tremaining: 277ms\n",
      "956:\tlearn: 0.0858591\ttotal: 6.03s\tremaining: 271ms\n",
      "957:\tlearn: 0.0858502\ttotal: 6.04s\tremaining: 265ms\n",
      "958:\tlearn: 0.0858441\ttotal: 6.04s\tremaining: 258ms\n",
      "959:\tlearn: 0.0858362\ttotal: 6.05s\tremaining: 252ms\n",
      "960:\tlearn: 0.0858128\ttotal: 6.05s\tremaining: 246ms\n",
      "961:\tlearn: 0.0858062\ttotal: 6.06s\tremaining: 239ms\n",
      "962:\tlearn: 0.0858007\ttotal: 6.07s\tremaining: 233ms\n",
      "963:\tlearn: 0.0857941\ttotal: 6.07s\tremaining: 227ms\n",
      "964:\tlearn: 0.0857874\ttotal: 6.08s\tremaining: 220ms\n",
      "965:\tlearn: 0.0857810\ttotal: 6.08s\tremaining: 214ms\n",
      "966:\tlearn: 0.0857730\ttotal: 6.09s\tremaining: 208ms\n",
      "967:\tlearn: 0.0857584\ttotal: 6.09s\tremaining: 201ms\n",
      "968:\tlearn: 0.0857406\ttotal: 6.1s\tremaining: 195ms\n",
      "969:\tlearn: 0.0857350\ttotal: 6.1s\tremaining: 189ms\n",
      "970:\tlearn: 0.0857277\ttotal: 6.11s\tremaining: 182ms\n",
      "971:\tlearn: 0.0857175\ttotal: 6.11s\tremaining: 176ms\n",
      "972:\tlearn: 0.0857120\ttotal: 6.12s\tremaining: 170ms\n",
      "973:\tlearn: 0.0857045\ttotal: 6.13s\tremaining: 164ms\n",
      "974:\tlearn: 0.0856998\ttotal: 6.13s\tremaining: 157ms\n",
      "975:\tlearn: 0.0856954\ttotal: 6.13s\tremaining: 151ms\n",
      "976:\tlearn: 0.0856873\ttotal: 6.14s\tremaining: 145ms\n",
      "977:\tlearn: 0.0856819\ttotal: 6.15s\tremaining: 138ms\n",
      "978:\tlearn: 0.0856714\ttotal: 6.15s\tremaining: 132ms\n",
      "979:\tlearn: 0.0856582\ttotal: 6.16s\tremaining: 126ms\n",
      "980:\tlearn: 0.0856453\ttotal: 6.16s\tremaining: 119ms\n",
      "981:\tlearn: 0.0856393\ttotal: 6.17s\tremaining: 113ms\n",
      "982:\tlearn: 0.0856253\ttotal: 6.17s\tremaining: 107ms\n",
      "983:\tlearn: 0.0856023\ttotal: 6.18s\tremaining: 101ms\n",
      "984:\tlearn: 0.0855903\ttotal: 6.19s\tremaining: 94.3ms\n",
      "985:\tlearn: 0.0855755\ttotal: 6.2s\tremaining: 88ms\n",
      "986:\tlearn: 0.0855608\ttotal: 6.21s\tremaining: 81.7ms\n",
      "987:\tlearn: 0.0855525\ttotal: 6.21s\tremaining: 75.4ms\n",
      "988:\tlearn: 0.0855354\ttotal: 6.21s\tremaining: 69.1ms\n",
      "989:\tlearn: 0.0855192\ttotal: 6.22s\tremaining: 62.8ms\n",
      "990:\tlearn: 0.0854967\ttotal: 6.23s\tremaining: 56.6ms\n",
      "991:\tlearn: 0.0854885\ttotal: 6.23s\tremaining: 50.3ms\n",
      "992:\tlearn: 0.0854770\ttotal: 6.24s\tremaining: 44ms\n",
      "993:\tlearn: 0.0854678\ttotal: 6.24s\tremaining: 37.7ms\n",
      "994:\tlearn: 0.0854573\ttotal: 6.25s\tremaining: 31.4ms\n",
      "995:\tlearn: 0.0854470\ttotal: 6.25s\tremaining: 25.1ms\n",
      "996:\tlearn: 0.0854411\ttotal: 6.26s\tremaining: 18.8ms\n",
      "997:\tlearn: 0.0854374\ttotal: 6.26s\tremaining: 12.6ms\n",
      "998:\tlearn: 0.0854239\ttotal: 6.27s\tremaining: 6.28ms\n",
      "999:\tlearn: 0.0854197\ttotal: 6.28s\tremaining: 0us\n",
      "0:\tlearn: 0.4375681\ttotal: 4.6ms\tremaining: 4.6s\n",
      "1:\tlearn: 0.4273997\ttotal: 12.9ms\tremaining: 6.43s\n",
      "2:\tlearn: 0.4170340\ttotal: 22.4ms\tremaining: 7.45s\n",
      "3:\tlearn: 0.4071677\ttotal: 28.1ms\tremaining: 6.99s\n",
      "4:\tlearn: 0.3975075\ttotal: 35.8ms\tremaining: 7.13s\n",
      "5:\tlearn: 0.3882622\ttotal: 43ms\tremaining: 7.12s\n",
      "6:\tlearn: 0.3794578\ttotal: 49.7ms\tremaining: 7.05s\n",
      "7:\tlearn: 0.3711158\ttotal: 56.4ms\tremaining: 7s\n",
      "8:\tlearn: 0.3629338\ttotal: 64.4ms\tremaining: 7.09s\n",
      "9:\tlearn: 0.3548936\ttotal: 70.4ms\tremaining: 6.97s\n",
      "10:\tlearn: 0.3472370\ttotal: 76.3ms\tremaining: 6.86s\n",
      "11:\tlearn: 0.3392926\ttotal: 81.7ms\tremaining: 6.72s\n",
      "12:\tlearn: 0.3318008\ttotal: 87.4ms\tremaining: 6.63s\n",
      "13:\tlearn: 0.3246898\ttotal: 93.7ms\tremaining: 6.6s\n",
      "14:\tlearn: 0.3179857\ttotal: 100ms\tremaining: 6.57s\n",
      "15:\tlearn: 0.3110779\ttotal: 108ms\tremaining: 6.66s\n",
      "16:\tlearn: 0.3045444\ttotal: 115ms\tremaining: 6.62s\n",
      "17:\tlearn: 0.2983775\ttotal: 120ms\tremaining: 6.57s\n",
      "18:\tlearn: 0.2921985\ttotal: 126ms\tremaining: 6.5s\n",
      "19:\tlearn: 0.2863031\ttotal: 132ms\tremaining: 6.46s\n",
      "20:\tlearn: 0.2807274\ttotal: 138ms\tremaining: 6.41s\n",
      "21:\tlearn: 0.2753290\ttotal: 143ms\tremaining: 6.35s\n",
      "22:\tlearn: 0.2696846\ttotal: 150ms\tremaining: 6.38s\n",
      "23:\tlearn: 0.2647418\ttotal: 158ms\tremaining: 6.41s\n",
      "24:\tlearn: 0.2595709\ttotal: 163ms\tremaining: 6.37s\n",
      "25:\tlearn: 0.2547906\ttotal: 169ms\tremaining: 6.35s\n",
      "26:\tlearn: 0.2499724\ttotal: 175ms\tremaining: 6.31s\n",
      "27:\tlearn: 0.2457177\ttotal: 180ms\tremaining: 6.26s\n",
      "28:\tlearn: 0.2415947\ttotal: 186ms\tremaining: 6.22s\n",
      "29:\tlearn: 0.2372442\ttotal: 195ms\tremaining: 6.29s\n",
      "30:\tlearn: 0.2331626\ttotal: 200ms\tremaining: 6.26s\n",
      "31:\tlearn: 0.2291876\ttotal: 206ms\tremaining: 6.23s\n",
      "32:\tlearn: 0.2252001\ttotal: 214ms\tremaining: 6.28s\n",
      "33:\tlearn: 0.2215205\ttotal: 223ms\tremaining: 6.32s\n",
      "34:\tlearn: 0.2181971\ttotal: 232ms\tremaining: 6.4s\n",
      "35:\tlearn: 0.2147137\ttotal: 238ms\tremaining: 6.37s\n",
      "36:\tlearn: 0.2113232\ttotal: 244ms\tremaining: 6.34s\n",
      "37:\tlearn: 0.2080402\ttotal: 249ms\tremaining: 6.3s\n",
      "38:\tlearn: 0.2050479\ttotal: 255ms\tremaining: 6.28s\n",
      "39:\tlearn: 0.2022844\ttotal: 262ms\tremaining: 6.29s\n",
      "40:\tlearn: 0.1995068\ttotal: 269ms\tremaining: 6.28s\n",
      "41:\tlearn: 0.1968845\ttotal: 275ms\tremaining: 6.26s\n",
      "42:\tlearn: 0.1944824\ttotal: 282ms\tremaining: 6.27s\n",
      "43:\tlearn: 0.1917944\ttotal: 288ms\tremaining: 6.25s\n",
      "44:\tlearn: 0.1893666\ttotal: 293ms\tremaining: 6.22s\n",
      "45:\tlearn: 0.1871902\ttotal: 298ms\tremaining: 6.19s\n",
      "46:\tlearn: 0.1850897\ttotal: 304ms\tremaining: 6.17s\n",
      "47:\tlearn: 0.1830720\ttotal: 310ms\tremaining: 6.16s\n",
      "48:\tlearn: 0.1807917\ttotal: 316ms\tremaining: 6.13s\n",
      "49:\tlearn: 0.1785608\ttotal: 322ms\tremaining: 6.11s\n",
      "50:\tlearn: 0.1766737\ttotal: 328ms\tremaining: 6.1s\n",
      "51:\tlearn: 0.1747988\ttotal: 334ms\tremaining: 6.08s\n",
      "52:\tlearn: 0.1727687\ttotal: 340ms\tremaining: 6.07s\n",
      "53:\tlearn: 0.1710936\ttotal: 346ms\tremaining: 6.05s\n",
      "54:\tlearn: 0.1695567\ttotal: 358ms\tremaining: 6.15s\n",
      "55:\tlearn: 0.1680063\ttotal: 369ms\tremaining: 6.22s\n",
      "56:\tlearn: 0.1665242\ttotal: 385ms\tremaining: 6.37s\n",
      "57:\tlearn: 0.1649814\ttotal: 401ms\tremaining: 6.51s\n",
      "58:\tlearn: 0.1636418\ttotal: 415ms\tremaining: 6.61s\n",
      "59:\tlearn: 0.1622814\ttotal: 425ms\tremaining: 6.66s\n",
      "60:\tlearn: 0.1609781\ttotal: 441ms\tremaining: 6.79s\n",
      "61:\tlearn: 0.1595449\ttotal: 451ms\tremaining: 6.83s\n",
      "62:\tlearn: 0.1581926\ttotal: 458ms\tremaining: 6.8s\n",
      "63:\tlearn: 0.1570802\ttotal: 463ms\tremaining: 6.77s\n",
      "64:\tlearn: 0.1556814\ttotal: 469ms\tremaining: 6.74s\n",
      "65:\tlearn: 0.1546019\ttotal: 475ms\tremaining: 6.72s\n",
      "66:\tlearn: 0.1533243\ttotal: 482ms\tremaining: 6.71s\n",
      "67:\tlearn: 0.1522056\ttotal: 488ms\tremaining: 6.68s\n",
      "68:\tlearn: 0.1512724\ttotal: 496ms\tremaining: 6.69s\n",
      "69:\tlearn: 0.1503990\ttotal: 502ms\tremaining: 6.66s\n",
      "70:\tlearn: 0.1492425\ttotal: 509ms\tremaining: 6.66s\n",
      "71:\tlearn: 0.1483554\ttotal: 518ms\tremaining: 6.67s\n",
      "72:\tlearn: 0.1473263\ttotal: 524ms\tremaining: 6.65s\n",
      "73:\tlearn: 0.1463176\ttotal: 530ms\tremaining: 6.63s\n",
      "74:\tlearn: 0.1454580\ttotal: 536ms\tremaining: 6.61s\n",
      "75:\tlearn: 0.1444018\ttotal: 541ms\tremaining: 6.58s\n",
      "76:\tlearn: 0.1436510\ttotal: 548ms\tremaining: 6.57s\n",
      "77:\tlearn: 0.1429662\ttotal: 554ms\tremaining: 6.54s\n",
      "78:\tlearn: 0.1422522\ttotal: 559ms\tremaining: 6.52s\n",
      "79:\tlearn: 0.1415456\ttotal: 565ms\tremaining: 6.5s\n",
      "80:\tlearn: 0.1408893\ttotal: 573ms\tremaining: 6.5s\n",
      "81:\tlearn: 0.1403002\ttotal: 578ms\tremaining: 6.47s\n",
      "82:\tlearn: 0.1396759\ttotal: 583ms\tremaining: 6.45s\n",
      "83:\tlearn: 0.1389708\ttotal: 589ms\tremaining: 6.43s\n",
      "84:\tlearn: 0.1384311\ttotal: 595ms\tremaining: 6.4s\n",
      "85:\tlearn: 0.1379127\ttotal: 601ms\tremaining: 6.38s\n",
      "86:\tlearn: 0.1373994\ttotal: 606ms\tremaining: 6.36s\n",
      "87:\tlearn: 0.1369772\ttotal: 612ms\tremaining: 6.34s\n",
      "88:\tlearn: 0.1363158\ttotal: 618ms\tremaining: 6.32s\n",
      "89:\tlearn: 0.1358192\ttotal: 624ms\tremaining: 6.31s\n",
      "90:\tlearn: 0.1351551\ttotal: 632ms\tremaining: 6.31s\n",
      "91:\tlearn: 0.1345266\ttotal: 677ms\tremaining: 6.68s\n",
      "92:\tlearn: 0.1339838\ttotal: 688ms\tremaining: 6.71s\n",
      "93:\tlearn: 0.1335647\ttotal: 695ms\tremaining: 6.7s\n",
      "94:\tlearn: 0.1330586\ttotal: 702ms\tremaining: 6.69s\n",
      "95:\tlearn: 0.1326303\ttotal: 709ms\tremaining: 6.68s\n",
      "96:\tlearn: 0.1322671\ttotal: 716ms\tremaining: 6.66s\n",
      "97:\tlearn: 0.1319060\ttotal: 723ms\tremaining: 6.65s\n",
      "98:\tlearn: 0.1313530\ttotal: 729ms\tremaining: 6.63s\n",
      "99:\tlearn: 0.1308373\ttotal: 735ms\tremaining: 6.62s\n",
      "100:\tlearn: 0.1304477\ttotal: 740ms\tremaining: 6.59s\n",
      "101:\tlearn: 0.1300292\ttotal: 746ms\tremaining: 6.57s\n",
      "102:\tlearn: 0.1295804\ttotal: 752ms\tremaining: 6.55s\n",
      "103:\tlearn: 0.1292596\ttotal: 758ms\tremaining: 6.53s\n",
      "104:\tlearn: 0.1287509\ttotal: 764ms\tremaining: 6.51s\n",
      "105:\tlearn: 0.1282333\ttotal: 770ms\tremaining: 6.49s\n",
      "106:\tlearn: 0.1278001\ttotal: 777ms\tremaining: 6.48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107:\tlearn: 0.1275223\ttotal: 785ms\tremaining: 6.48s\n",
      "108:\tlearn: 0.1271520\ttotal: 791ms\tremaining: 6.47s\n",
      "109:\tlearn: 0.1268135\ttotal: 797ms\tremaining: 6.45s\n",
      "110:\tlearn: 0.1265366\ttotal: 806ms\tremaining: 6.45s\n",
      "111:\tlearn: 0.1261140\ttotal: 815ms\tremaining: 6.46s\n",
      "112:\tlearn: 0.1258029\ttotal: 823ms\tremaining: 6.46s\n",
      "113:\tlearn: 0.1255937\ttotal: 831ms\tremaining: 6.46s\n",
      "114:\tlearn: 0.1251622\ttotal: 840ms\tremaining: 6.46s\n",
      "115:\tlearn: 0.1248245\ttotal: 847ms\tremaining: 6.45s\n",
      "116:\tlearn: 0.1245952\ttotal: 852ms\tremaining: 6.43s\n",
      "117:\tlearn: 0.1243935\ttotal: 857ms\tremaining: 6.41s\n",
      "118:\tlearn: 0.1240225\ttotal: 863ms\tremaining: 6.39s\n",
      "119:\tlearn: 0.1238233\ttotal: 868ms\tremaining: 6.37s\n",
      "120:\tlearn: 0.1235284\ttotal: 874ms\tremaining: 6.35s\n",
      "121:\tlearn: 0.1233332\ttotal: 879ms\tremaining: 6.33s\n",
      "122:\tlearn: 0.1229611\ttotal: 888ms\tremaining: 6.33s\n",
      "123:\tlearn: 0.1226377\ttotal: 894ms\tremaining: 6.31s\n",
      "124:\tlearn: 0.1224269\ttotal: 899ms\tremaining: 6.29s\n",
      "125:\tlearn: 0.1222544\ttotal: 904ms\tremaining: 6.27s\n",
      "126:\tlearn: 0.1220891\ttotal: 911ms\tremaining: 6.26s\n",
      "127:\tlearn: 0.1216933\ttotal: 917ms\tremaining: 6.25s\n",
      "128:\tlearn: 0.1214093\ttotal: 923ms\tremaining: 6.23s\n",
      "129:\tlearn: 0.1210492\ttotal: 932ms\tremaining: 6.23s\n",
      "130:\tlearn: 0.1208952\ttotal: 938ms\tremaining: 6.22s\n",
      "131:\tlearn: 0.1207451\ttotal: 945ms\tremaining: 6.22s\n",
      "132:\tlearn: 0.1203686\ttotal: 951ms\tremaining: 6.2s\n",
      "133:\tlearn: 0.1200389\ttotal: 957ms\tremaining: 6.18s\n",
      "134:\tlearn: 0.1197920\ttotal: 962ms\tremaining: 6.16s\n",
      "135:\tlearn: 0.1196143\ttotal: 969ms\tremaining: 6.16s\n",
      "136:\tlearn: 0.1194700\ttotal: 975ms\tremaining: 6.14s\n",
      "137:\tlearn: 0.1193037\ttotal: 981ms\tremaining: 6.12s\n",
      "138:\tlearn: 0.1191546\ttotal: 986ms\tremaining: 6.11s\n",
      "139:\tlearn: 0.1190366\ttotal: 992ms\tremaining: 6.09s\n",
      "140:\tlearn: 0.1189086\ttotal: 997ms\tremaining: 6.07s\n",
      "141:\tlearn: 0.1187910\ttotal: 1s\tremaining: 6.06s\n",
      "142:\tlearn: 0.1185760\ttotal: 1.01s\tremaining: 6.05s\n",
      "143:\tlearn: 0.1184407\ttotal: 1.02s\tremaining: 6.04s\n",
      "144:\tlearn: 0.1181057\ttotal: 1.02s\tremaining: 6.03s\n",
      "145:\tlearn: 0.1179964\ttotal: 1.03s\tremaining: 6.01s\n",
      "146:\tlearn: 0.1178210\ttotal: 1.03s\tremaining: 6s\n",
      "147:\tlearn: 0.1175987\ttotal: 1.04s\tremaining: 5.98s\n",
      "148:\tlearn: 0.1174018\ttotal: 1.04s\tremaining: 5.97s\n",
      "149:\tlearn: 0.1170976\ttotal: 1.05s\tremaining: 5.97s\n",
      "150:\tlearn: 0.1168666\ttotal: 1.06s\tremaining: 5.98s\n",
      "151:\tlearn: 0.1166825\ttotal: 1.07s\tremaining: 5.96s\n",
      "152:\tlearn: 0.1164652\ttotal: 1.07s\tremaining: 5.95s\n",
      "153:\tlearn: 0.1163771\ttotal: 1.08s\tremaining: 5.93s\n",
      "154:\tlearn: 0.1161460\ttotal: 1.09s\tremaining: 5.92s\n",
      "155:\tlearn: 0.1159207\ttotal: 1.09s\tremaining: 5.91s\n",
      "156:\tlearn: 0.1158328\ttotal: 1.1s\tremaining: 5.91s\n",
      "157:\tlearn: 0.1157247\ttotal: 1.1s\tremaining: 5.89s\n",
      "158:\tlearn: 0.1155140\ttotal: 1.11s\tremaining: 5.88s\n",
      "159:\tlearn: 0.1153761\ttotal: 1.12s\tremaining: 5.86s\n",
      "160:\tlearn: 0.1152807\ttotal: 1.12s\tremaining: 5.85s\n",
      "161:\tlearn: 0.1151633\ttotal: 1.13s\tremaining: 5.84s\n",
      "162:\tlearn: 0.1150801\ttotal: 1.13s\tremaining: 5.82s\n",
      "163:\tlearn: 0.1149586\ttotal: 1.14s\tremaining: 5.81s\n",
      "164:\tlearn: 0.1147289\ttotal: 1.15s\tremaining: 5.79s\n",
      "165:\tlearn: 0.1144961\ttotal: 1.15s\tremaining: 5.78s\n",
      "166:\tlearn: 0.1142778\ttotal: 1.16s\tremaining: 5.76s\n",
      "167:\tlearn: 0.1140542\ttotal: 1.16s\tremaining: 5.75s\n",
      "168:\tlearn: 0.1138044\ttotal: 1.17s\tremaining: 5.74s\n",
      "169:\tlearn: 0.1135991\ttotal: 1.17s\tremaining: 5.72s\n",
      "170:\tlearn: 0.1134924\ttotal: 1.19s\tremaining: 5.75s\n",
      "171:\tlearn: 0.1133356\ttotal: 1.2s\tremaining: 5.76s\n",
      "172:\tlearn: 0.1132823\ttotal: 1.2s\tremaining: 5.76s\n",
      "173:\tlearn: 0.1132346\ttotal: 1.21s\tremaining: 5.76s\n",
      "174:\tlearn: 0.1131060\ttotal: 1.23s\tremaining: 5.79s\n",
      "175:\tlearn: 0.1130064\ttotal: 1.23s\tremaining: 5.78s\n",
      "176:\tlearn: 0.1128591\ttotal: 1.24s\tremaining: 5.77s\n",
      "177:\tlearn: 0.1127441\ttotal: 1.25s\tremaining: 5.75s\n",
      "178:\tlearn: 0.1125604\ttotal: 1.25s\tremaining: 5.74s\n",
      "179:\tlearn: 0.1124575\ttotal: 1.26s\tremaining: 5.73s\n",
      "180:\tlearn: 0.1122623\ttotal: 1.26s\tremaining: 5.71s\n",
      "181:\tlearn: 0.1120439\ttotal: 1.27s\tremaining: 5.71s\n",
      "182:\tlearn: 0.1119025\ttotal: 1.27s\tremaining: 5.7s\n",
      "183:\tlearn: 0.1118142\ttotal: 1.28s\tremaining: 5.68s\n",
      "184:\tlearn: 0.1116835\ttotal: 1.29s\tremaining: 5.68s\n",
      "185:\tlearn: 0.1115444\ttotal: 1.29s\tremaining: 5.67s\n",
      "186:\tlearn: 0.1114329\ttotal: 1.3s\tremaining: 5.66s\n",
      "187:\tlearn: 0.1112517\ttotal: 1.31s\tremaining: 5.66s\n",
      "188:\tlearn: 0.1111280\ttotal: 1.32s\tremaining: 5.68s\n",
      "189:\tlearn: 0.1109807\ttotal: 1.33s\tremaining: 5.69s\n",
      "190:\tlearn: 0.1108905\ttotal: 1.35s\tremaining: 5.73s\n",
      "191:\tlearn: 0.1106745\ttotal: 1.36s\tremaining: 5.74s\n",
      "192:\tlearn: 0.1106041\ttotal: 1.37s\tremaining: 5.75s\n",
      "193:\tlearn: 0.1104537\ttotal: 1.38s\tremaining: 5.75s\n",
      "194:\tlearn: 0.1102964\ttotal: 1.4s\tremaining: 5.76s\n",
      "195:\tlearn: 0.1101689\ttotal: 1.41s\tremaining: 5.77s\n",
      "196:\tlearn: 0.1100937\ttotal: 1.41s\tremaining: 5.76s\n",
      "197:\tlearn: 0.1099884\ttotal: 1.42s\tremaining: 5.75s\n",
      "198:\tlearn: 0.1098342\ttotal: 1.42s\tremaining: 5.73s\n",
      "199:\tlearn: 0.1097266\ttotal: 1.43s\tremaining: 5.72s\n",
      "200:\tlearn: 0.1096391\ttotal: 1.44s\tremaining: 5.7s\n",
      "201:\tlearn: 0.1095847\ttotal: 1.44s\tremaining: 5.69s\n",
      "202:\tlearn: 0.1094893\ttotal: 1.45s\tremaining: 5.68s\n",
      "203:\tlearn: 0.1094394\ttotal: 1.45s\tremaining: 5.67s\n",
      "204:\tlearn: 0.1092827\ttotal: 1.46s\tremaining: 5.65s\n",
      "205:\tlearn: 0.1092103\ttotal: 1.46s\tremaining: 5.64s\n",
      "206:\tlearn: 0.1090858\ttotal: 1.47s\tremaining: 5.63s\n",
      "207:\tlearn: 0.1090191\ttotal: 1.48s\tremaining: 5.62s\n",
      "208:\tlearn: 0.1089116\ttotal: 1.48s\tremaining: 5.62s\n",
      "209:\tlearn: 0.1087635\ttotal: 1.49s\tremaining: 5.61s\n",
      "210:\tlearn: 0.1086019\ttotal: 1.5s\tremaining: 5.59s\n",
      "211:\tlearn: 0.1085416\ttotal: 1.5s\tremaining: 5.58s\n",
      "212:\tlearn: 0.1084352\ttotal: 1.51s\tremaining: 5.57s\n",
      "213:\tlearn: 0.1083747\ttotal: 1.51s\tremaining: 5.55s\n",
      "214:\tlearn: 0.1083203\ttotal: 1.52s\tremaining: 5.54s\n",
      "215:\tlearn: 0.1082716\ttotal: 1.52s\tremaining: 5.53s\n",
      "216:\tlearn: 0.1081718\ttotal: 1.53s\tremaining: 5.51s\n",
      "217:\tlearn: 0.1081361\ttotal: 1.53s\tremaining: 5.5s\n",
      "218:\tlearn: 0.1080761\ttotal: 1.54s\tremaining: 5.49s\n",
      "219:\tlearn: 0.1080344\ttotal: 1.54s\tremaining: 5.47s\n",
      "220:\tlearn: 0.1079266\ttotal: 1.55s\tremaining: 5.46s\n",
      "221:\tlearn: 0.1078488\ttotal: 1.55s\tremaining: 5.45s\n",
      "222:\tlearn: 0.1077812\ttotal: 1.56s\tremaining: 5.44s\n",
      "223:\tlearn: 0.1076827\ttotal: 1.57s\tremaining: 5.43s\n",
      "224:\tlearn: 0.1076194\ttotal: 1.57s\tremaining: 5.42s\n",
      "225:\tlearn: 0.1075633\ttotal: 1.58s\tremaining: 5.42s\n",
      "226:\tlearn: 0.1074832\ttotal: 1.59s\tremaining: 5.4s\n",
      "227:\tlearn: 0.1073599\ttotal: 1.59s\tremaining: 5.39s\n",
      "228:\tlearn: 0.1072647\ttotal: 1.6s\tremaining: 5.38s\n",
      "229:\tlearn: 0.1071185\ttotal: 1.6s\tremaining: 5.37s\n",
      "230:\tlearn: 0.1069577\ttotal: 1.61s\tremaining: 5.36s\n",
      "231:\tlearn: 0.1068889\ttotal: 1.61s\tremaining: 5.34s\n",
      "232:\tlearn: 0.1068122\ttotal: 1.62s\tremaining: 5.33s\n",
      "233:\tlearn: 0.1066961\ttotal: 1.63s\tremaining: 5.32s\n",
      "234:\tlearn: 0.1065609\ttotal: 1.63s\tremaining: 5.31s\n",
      "235:\tlearn: 0.1064424\ttotal: 1.64s\tremaining: 5.3s\n",
      "236:\tlearn: 0.1063894\ttotal: 1.64s\tremaining: 5.29s\n",
      "237:\tlearn: 0.1062669\ttotal: 1.65s\tremaining: 5.28s\n",
      "238:\tlearn: 0.1061364\ttotal: 1.65s\tremaining: 5.26s\n",
      "239:\tlearn: 0.1060134\ttotal: 1.66s\tremaining: 5.25s\n",
      "240:\tlearn: 0.1058757\ttotal: 1.67s\tremaining: 5.24s\n",
      "241:\tlearn: 0.1058105\ttotal: 1.67s\tremaining: 5.23s\n",
      "242:\tlearn: 0.1057605\ttotal: 1.68s\tremaining: 5.22s\n",
      "243:\tlearn: 0.1057106\ttotal: 1.68s\tremaining: 5.21s\n",
      "244:\tlearn: 0.1056067\ttotal: 1.69s\tremaining: 5.2s\n",
      "245:\tlearn: 0.1055297\ttotal: 1.69s\tremaining: 5.19s\n",
      "246:\tlearn: 0.1054194\ttotal: 1.7s\tremaining: 5.18s\n",
      "247:\tlearn: 0.1053455\ttotal: 1.71s\tremaining: 5.17s\n",
      "248:\tlearn: 0.1052567\ttotal: 1.71s\tremaining: 5.16s\n",
      "249:\tlearn: 0.1051666\ttotal: 1.72s\tremaining: 5.15s\n",
      "250:\tlearn: 0.1051036\ttotal: 1.72s\tremaining: 5.14s\n",
      "251:\tlearn: 0.1050085\ttotal: 1.73s\tremaining: 5.13s\n",
      "252:\tlearn: 0.1049366\ttotal: 1.74s\tremaining: 5.12s\n",
      "253:\tlearn: 0.1048751\ttotal: 1.74s\tremaining: 5.11s\n",
      "254:\tlearn: 0.1048389\ttotal: 1.75s\tremaining: 5.1s\n",
      "255:\tlearn: 0.1047982\ttotal: 1.75s\tremaining: 5.09s\n",
      "256:\tlearn: 0.1047661\ttotal: 1.76s\tremaining: 5.08s\n",
      "257:\tlearn: 0.1047063\ttotal: 1.76s\tremaining: 5.08s\n",
      "258:\tlearn: 0.1046400\ttotal: 1.77s\tremaining: 5.07s\n",
      "259:\tlearn: 0.1045855\ttotal: 1.78s\tremaining: 5.06s\n",
      "260:\tlearn: 0.1045441\ttotal: 1.78s\tremaining: 5.05s\n",
      "261:\tlearn: 0.1044245\ttotal: 1.79s\tremaining: 5.04s\n",
      "262:\tlearn: 0.1043704\ttotal: 1.79s\tremaining: 5.03s\n",
      "263:\tlearn: 0.1043103\ttotal: 1.8s\tremaining: 5.02s\n",
      "264:\tlearn: 0.1042210\ttotal: 1.8s\tremaining: 5.01s\n",
      "265:\tlearn: 0.1041575\ttotal: 1.81s\tremaining: 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266:\tlearn: 0.1041339\ttotal: 1.82s\tremaining: 4.99s\n",
      "267:\tlearn: 0.1039895\ttotal: 1.82s\tremaining: 4.98s\n",
      "268:\tlearn: 0.1039277\ttotal: 1.83s\tremaining: 4.97s\n",
      "269:\tlearn: 0.1038997\ttotal: 1.84s\tremaining: 4.97s\n",
      "270:\tlearn: 0.1038228\ttotal: 1.84s\tremaining: 4.96s\n",
      "271:\tlearn: 0.1037696\ttotal: 1.85s\tremaining: 4.95s\n",
      "272:\tlearn: 0.1037496\ttotal: 1.85s\tremaining: 4.94s\n",
      "273:\tlearn: 0.1036703\ttotal: 1.86s\tremaining: 4.93s\n",
      "274:\tlearn: 0.1035776\ttotal: 1.86s\tremaining: 4.92s\n",
      "275:\tlearn: 0.1035198\ttotal: 1.87s\tremaining: 4.91s\n",
      "276:\tlearn: 0.1034572\ttotal: 1.88s\tremaining: 4.9s\n",
      "277:\tlearn: 0.1033905\ttotal: 1.88s\tremaining: 4.89s\n",
      "278:\tlearn: 0.1033210\ttotal: 1.89s\tremaining: 4.88s\n",
      "279:\tlearn: 0.1032425\ttotal: 1.89s\tremaining: 4.87s\n",
      "280:\tlearn: 0.1031813\ttotal: 1.9s\tremaining: 4.86s\n",
      "281:\tlearn: 0.1031369\ttotal: 1.91s\tremaining: 4.86s\n",
      "282:\tlearn: 0.1030395\ttotal: 1.91s\tremaining: 4.85s\n",
      "283:\tlearn: 0.1029730\ttotal: 1.92s\tremaining: 4.84s\n",
      "284:\tlearn: 0.1029374\ttotal: 1.93s\tremaining: 4.83s\n",
      "285:\tlearn: 0.1028960\ttotal: 1.93s\tremaining: 4.83s\n",
      "286:\tlearn: 0.1028202\ttotal: 1.94s\tremaining: 4.82s\n",
      "287:\tlearn: 0.1027360\ttotal: 1.95s\tremaining: 4.81s\n",
      "288:\tlearn: 0.1027120\ttotal: 1.96s\tremaining: 4.81s\n",
      "289:\tlearn: 0.1026479\ttotal: 1.97s\tremaining: 4.82s\n",
      "290:\tlearn: 0.1025784\ttotal: 1.98s\tremaining: 4.83s\n",
      "291:\tlearn: 0.1024857\ttotal: 2s\tremaining: 4.84s\n",
      "292:\tlearn: 0.1024194\ttotal: 2s\tremaining: 4.83s\n",
      "293:\tlearn: 0.1023682\ttotal: 2.01s\tremaining: 4.83s\n",
      "294:\tlearn: 0.1023297\ttotal: 2.02s\tremaining: 4.82s\n",
      "295:\tlearn: 0.1022792\ttotal: 2.02s\tremaining: 4.81s\n",
      "296:\tlearn: 0.1022107\ttotal: 2.03s\tremaining: 4.8s\n",
      "297:\tlearn: 0.1021185\ttotal: 2.04s\tremaining: 4.81s\n",
      "298:\tlearn: 0.1020403\ttotal: 2.05s\tremaining: 4.82s\n",
      "299:\tlearn: 0.1019519\ttotal: 2.07s\tremaining: 4.83s\n",
      "300:\tlearn: 0.1019119\ttotal: 2.08s\tremaining: 4.84s\n",
      "301:\tlearn: 0.1018556\ttotal: 2.09s\tremaining: 4.83s\n",
      "302:\tlearn: 0.1018035\ttotal: 2.1s\tremaining: 4.82s\n",
      "303:\tlearn: 0.1017590\ttotal: 2.1s\tremaining: 4.81s\n",
      "304:\tlearn: 0.1017298\ttotal: 2.1s\tremaining: 4.8s\n",
      "305:\tlearn: 0.1016859\ttotal: 2.11s\tremaining: 4.79s\n",
      "306:\tlearn: 0.1016208\ttotal: 2.12s\tremaining: 4.79s\n",
      "307:\tlearn: 0.1015152\ttotal: 2.13s\tremaining: 4.79s\n",
      "308:\tlearn: 0.1014831\ttotal: 2.13s\tremaining: 4.77s\n",
      "309:\tlearn: 0.1014399\ttotal: 2.14s\tremaining: 4.76s\n",
      "310:\tlearn: 0.1014001\ttotal: 2.15s\tremaining: 4.76s\n",
      "311:\tlearn: 0.1013106\ttotal: 2.16s\tremaining: 4.75s\n",
      "312:\tlearn: 0.1012647\ttotal: 2.16s\tremaining: 4.75s\n",
      "313:\tlearn: 0.1011787\ttotal: 2.17s\tremaining: 4.74s\n",
      "314:\tlearn: 0.1011186\ttotal: 2.18s\tremaining: 4.73s\n",
      "315:\tlearn: 0.1010592\ttotal: 2.18s\tremaining: 4.72s\n",
      "316:\tlearn: 0.1009948\ttotal: 2.19s\tremaining: 4.71s\n",
      "317:\tlearn: 0.1009666\ttotal: 2.19s\tremaining: 4.71s\n",
      "318:\tlearn: 0.1008868\ttotal: 2.2s\tremaining: 4.7s\n",
      "319:\tlearn: 0.1008052\ttotal: 2.21s\tremaining: 4.69s\n",
      "320:\tlearn: 0.1007502\ttotal: 2.22s\tremaining: 4.69s\n",
      "321:\tlearn: 0.1006896\ttotal: 2.22s\tremaining: 4.68s\n",
      "322:\tlearn: 0.1006077\ttotal: 2.23s\tremaining: 4.67s\n",
      "323:\tlearn: 0.1005445\ttotal: 2.24s\tremaining: 4.67s\n",
      "324:\tlearn: 0.1005253\ttotal: 2.24s\tremaining: 4.66s\n",
      "325:\tlearn: 0.1004550\ttotal: 2.25s\tremaining: 4.65s\n",
      "326:\tlearn: 0.1004008\ttotal: 2.25s\tremaining: 4.64s\n",
      "327:\tlearn: 0.1003553\ttotal: 2.27s\tremaining: 4.64s\n",
      "328:\tlearn: 0.1002994\ttotal: 2.28s\tremaining: 4.64s\n",
      "329:\tlearn: 0.1002193\ttotal: 2.29s\tremaining: 4.64s\n",
      "330:\tlearn: 0.1001861\ttotal: 2.3s\tremaining: 4.66s\n",
      "331:\tlearn: 0.1001304\ttotal: 2.31s\tremaining: 4.66s\n",
      "332:\tlearn: 0.1000466\ttotal: 2.33s\tremaining: 4.66s\n",
      "333:\tlearn: 0.0999849\ttotal: 2.35s\tremaining: 4.68s\n",
      "334:\tlearn: 0.0999550\ttotal: 2.35s\tremaining: 4.67s\n",
      "335:\tlearn: 0.0998656\ttotal: 2.36s\tremaining: 4.66s\n",
      "336:\tlearn: 0.0997965\ttotal: 2.36s\tremaining: 4.65s\n",
      "337:\tlearn: 0.0997456\ttotal: 2.37s\tremaining: 4.64s\n",
      "338:\tlearn: 0.0996987\ttotal: 2.37s\tremaining: 4.63s\n",
      "339:\tlearn: 0.0996378\ttotal: 2.38s\tremaining: 4.62s\n",
      "340:\tlearn: 0.0995886\ttotal: 2.39s\tremaining: 4.61s\n",
      "341:\tlearn: 0.0995468\ttotal: 2.39s\tremaining: 4.6s\n",
      "342:\tlearn: 0.0995086\ttotal: 2.4s\tremaining: 4.59s\n",
      "343:\tlearn: 0.0994679\ttotal: 2.4s\tremaining: 4.59s\n",
      "344:\tlearn: 0.0993876\ttotal: 2.41s\tremaining: 4.58s\n",
      "345:\tlearn: 0.0993149\ttotal: 2.42s\tremaining: 4.57s\n",
      "346:\tlearn: 0.0992802\ttotal: 2.43s\tremaining: 4.57s\n",
      "347:\tlearn: 0.0992519\ttotal: 2.43s\tremaining: 4.56s\n",
      "348:\tlearn: 0.0992167\ttotal: 2.44s\tremaining: 4.55s\n",
      "349:\tlearn: 0.0991740\ttotal: 2.44s\tremaining: 4.54s\n",
      "350:\tlearn: 0.0991179\ttotal: 2.45s\tremaining: 4.53s\n",
      "351:\tlearn: 0.0990931\ttotal: 2.46s\tremaining: 4.52s\n",
      "352:\tlearn: 0.0989952\ttotal: 2.46s\tremaining: 4.51s\n",
      "353:\tlearn: 0.0989558\ttotal: 2.47s\tremaining: 4.5s\n",
      "354:\tlearn: 0.0989301\ttotal: 2.47s\tremaining: 4.5s\n",
      "355:\tlearn: 0.0988692\ttotal: 2.48s\tremaining: 4.49s\n",
      "356:\tlearn: 0.0988482\ttotal: 2.49s\tremaining: 4.48s\n",
      "357:\tlearn: 0.0988131\ttotal: 2.49s\tremaining: 4.47s\n",
      "358:\tlearn: 0.0987863\ttotal: 2.5s\tremaining: 4.46s\n",
      "359:\tlearn: 0.0987529\ttotal: 2.5s\tremaining: 4.45s\n",
      "360:\tlearn: 0.0986612\ttotal: 2.51s\tremaining: 4.44s\n",
      "361:\tlearn: 0.0986121\ttotal: 2.52s\tremaining: 4.43s\n",
      "362:\tlearn: 0.0985812\ttotal: 2.52s\tremaining: 4.42s\n",
      "363:\tlearn: 0.0985421\ttotal: 2.53s\tremaining: 4.41s\n",
      "364:\tlearn: 0.0985097\ttotal: 2.53s\tremaining: 4.41s\n",
      "365:\tlearn: 0.0984627\ttotal: 2.54s\tremaining: 4.4s\n",
      "366:\tlearn: 0.0984252\ttotal: 2.55s\tremaining: 4.4s\n",
      "367:\tlearn: 0.0983852\ttotal: 2.56s\tremaining: 4.39s\n",
      "368:\tlearn: 0.0983319\ttotal: 2.56s\tremaining: 4.38s\n",
      "369:\tlearn: 0.0982912\ttotal: 2.57s\tremaining: 4.37s\n",
      "370:\tlearn: 0.0982521\ttotal: 2.57s\tremaining: 4.36s\n",
      "371:\tlearn: 0.0982341\ttotal: 2.58s\tremaining: 4.35s\n",
      "372:\tlearn: 0.0982017\ttotal: 2.58s\tremaining: 4.34s\n",
      "373:\tlearn: 0.0981595\ttotal: 2.59s\tremaining: 4.33s\n",
      "374:\tlearn: 0.0981174\ttotal: 2.6s\tremaining: 4.33s\n",
      "375:\tlearn: 0.0980862\ttotal: 2.6s\tremaining: 4.32s\n",
      "376:\tlearn: 0.0980636\ttotal: 2.61s\tremaining: 4.31s\n",
      "377:\tlearn: 0.0980340\ttotal: 2.61s\tremaining: 4.3s\n",
      "378:\tlearn: 0.0979971\ttotal: 2.62s\tremaining: 4.29s\n",
      "379:\tlearn: 0.0979639\ttotal: 2.63s\tremaining: 4.28s\n",
      "380:\tlearn: 0.0979283\ttotal: 2.63s\tremaining: 4.27s\n",
      "381:\tlearn: 0.0978870\ttotal: 2.64s\tremaining: 4.26s\n",
      "382:\tlearn: 0.0978518\ttotal: 2.64s\tremaining: 4.26s\n",
      "383:\tlearn: 0.0978218\ttotal: 2.65s\tremaining: 4.25s\n",
      "384:\tlearn: 0.0977751\ttotal: 2.65s\tremaining: 4.24s\n",
      "385:\tlearn: 0.0977147\ttotal: 2.66s\tremaining: 4.23s\n",
      "386:\tlearn: 0.0976894\ttotal: 2.67s\tremaining: 4.23s\n",
      "387:\tlearn: 0.0976527\ttotal: 2.67s\tremaining: 4.22s\n",
      "388:\tlearn: 0.0976333\ttotal: 2.68s\tremaining: 4.21s\n",
      "389:\tlearn: 0.0975510\ttotal: 2.68s\tremaining: 4.2s\n",
      "390:\tlearn: 0.0975036\ttotal: 2.69s\tremaining: 4.19s\n",
      "391:\tlearn: 0.0974767\ttotal: 2.69s\tremaining: 4.18s\n",
      "392:\tlearn: 0.0974564\ttotal: 2.7s\tremaining: 4.17s\n",
      "393:\tlearn: 0.0973774\ttotal: 2.71s\tremaining: 4.16s\n",
      "394:\tlearn: 0.0973000\ttotal: 2.71s\tremaining: 4.15s\n",
      "395:\tlearn: 0.0972532\ttotal: 2.72s\tremaining: 4.15s\n",
      "396:\tlearn: 0.0972206\ttotal: 2.72s\tremaining: 4.14s\n",
      "397:\tlearn: 0.0971908\ttotal: 2.73s\tremaining: 4.13s\n",
      "398:\tlearn: 0.0971516\ttotal: 2.74s\tremaining: 4.13s\n",
      "399:\tlearn: 0.0971325\ttotal: 2.75s\tremaining: 4.12s\n",
      "400:\tlearn: 0.0970882\ttotal: 2.75s\tremaining: 4.11s\n",
      "401:\tlearn: 0.0970369\ttotal: 2.76s\tremaining: 4.1s\n",
      "402:\tlearn: 0.0969485\ttotal: 2.77s\tremaining: 4.1s\n",
      "403:\tlearn: 0.0968959\ttotal: 2.78s\tremaining: 4.09s\n",
      "404:\tlearn: 0.0968738\ttotal: 2.78s\tremaining: 4.09s\n",
      "405:\tlearn: 0.0967901\ttotal: 2.79s\tremaining: 4.08s\n",
      "406:\tlearn: 0.0967584\ttotal: 2.8s\tremaining: 4.07s\n",
      "407:\tlearn: 0.0966897\ttotal: 2.8s\tremaining: 4.07s\n",
      "408:\tlearn: 0.0966642\ttotal: 2.81s\tremaining: 4.06s\n",
      "409:\tlearn: 0.0966448\ttotal: 2.81s\tremaining: 4.05s\n",
      "410:\tlearn: 0.0966305\ttotal: 2.82s\tremaining: 4.04s\n",
      "411:\tlearn: 0.0965781\ttotal: 2.83s\tremaining: 4.03s\n",
      "412:\tlearn: 0.0965481\ttotal: 2.83s\tremaining: 4.02s\n",
      "413:\tlearn: 0.0965156\ttotal: 2.84s\tremaining: 4.01s\n",
      "414:\tlearn: 0.0964913\ttotal: 2.84s\tremaining: 4.01s\n",
      "415:\tlearn: 0.0964684\ttotal: 2.85s\tremaining: 4s\n",
      "416:\tlearn: 0.0964442\ttotal: 2.86s\tremaining: 4s\n",
      "417:\tlearn: 0.0964014\ttotal: 2.86s\tremaining: 3.99s\n",
      "418:\tlearn: 0.0963716\ttotal: 2.87s\tremaining: 3.98s\n",
      "419:\tlearn: 0.0963442\ttotal: 2.88s\tremaining: 3.97s\n",
      "420:\tlearn: 0.0962850\ttotal: 2.88s\tremaining: 3.96s\n",
      "421:\tlearn: 0.0962467\ttotal: 2.89s\tremaining: 3.96s\n",
      "422:\tlearn: 0.0962306\ttotal: 2.89s\tremaining: 3.95s\n",
      "423:\tlearn: 0.0962136\ttotal: 2.9s\tremaining: 3.94s\n",
      "424:\tlearn: 0.0961893\ttotal: 2.9s\tremaining: 3.93s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425:\tlearn: 0.0961688\ttotal: 2.91s\tremaining: 3.93s\n",
      "426:\tlearn: 0.0961488\ttotal: 2.92s\tremaining: 3.92s\n",
      "427:\tlearn: 0.0961301\ttotal: 2.93s\tremaining: 3.91s\n",
      "428:\tlearn: 0.0960791\ttotal: 2.93s\tremaining: 3.91s\n",
      "429:\tlearn: 0.0960539\ttotal: 2.94s\tremaining: 3.9s\n",
      "430:\tlearn: 0.0960330\ttotal: 2.94s\tremaining: 3.89s\n",
      "431:\tlearn: 0.0959952\ttotal: 2.95s\tremaining: 3.88s\n",
      "432:\tlearn: 0.0959352\ttotal: 2.96s\tremaining: 3.87s\n",
      "433:\tlearn: 0.0958881\ttotal: 2.96s\tremaining: 3.86s\n",
      "434:\tlearn: 0.0958699\ttotal: 2.97s\tremaining: 3.85s\n",
      "435:\tlearn: 0.0958195\ttotal: 2.97s\tremaining: 3.85s\n",
      "436:\tlearn: 0.0957985\ttotal: 2.98s\tremaining: 3.84s\n",
      "437:\tlearn: 0.0957551\ttotal: 2.98s\tremaining: 3.83s\n",
      "438:\tlearn: 0.0957058\ttotal: 2.99s\tremaining: 3.82s\n",
      "439:\tlearn: 0.0956630\ttotal: 2.99s\tremaining: 3.81s\n",
      "440:\tlearn: 0.0956253\ttotal: 3s\tremaining: 3.8s\n",
      "441:\tlearn: 0.0955973\ttotal: 3.01s\tremaining: 3.8s\n",
      "442:\tlearn: 0.0955745\ttotal: 3.01s\tremaining: 3.79s\n",
      "443:\tlearn: 0.0955423\ttotal: 3.02s\tremaining: 3.78s\n",
      "444:\tlearn: 0.0954749\ttotal: 3.03s\tremaining: 3.77s\n",
      "445:\tlearn: 0.0954566\ttotal: 3.03s\tremaining: 3.77s\n",
      "446:\tlearn: 0.0954211\ttotal: 3.04s\tremaining: 3.76s\n",
      "447:\tlearn: 0.0953773\ttotal: 3.04s\tremaining: 3.75s\n",
      "448:\tlearn: 0.0953360\ttotal: 3.05s\tremaining: 3.75s\n",
      "449:\tlearn: 0.0953204\ttotal: 3.06s\tremaining: 3.74s\n",
      "450:\tlearn: 0.0953073\ttotal: 3.06s\tremaining: 3.73s\n",
      "451:\tlearn: 0.0952725\ttotal: 3.07s\tremaining: 3.72s\n",
      "452:\tlearn: 0.0952187\ttotal: 3.08s\tremaining: 3.71s\n",
      "453:\tlearn: 0.0952045\ttotal: 3.08s\tremaining: 3.71s\n",
      "454:\tlearn: 0.0951893\ttotal: 3.09s\tremaining: 3.7s\n",
      "455:\tlearn: 0.0951552\ttotal: 3.09s\tremaining: 3.69s\n",
      "456:\tlearn: 0.0951365\ttotal: 3.1s\tremaining: 3.68s\n",
      "457:\tlearn: 0.0951215\ttotal: 3.1s\tremaining: 3.67s\n",
      "458:\tlearn: 0.0951022\ttotal: 3.11s\tremaining: 3.66s\n",
      "459:\tlearn: 0.0950770\ttotal: 3.11s\tremaining: 3.66s\n",
      "460:\tlearn: 0.0950592\ttotal: 3.12s\tremaining: 3.65s\n",
      "461:\tlearn: 0.0950422\ttotal: 3.13s\tremaining: 3.64s\n",
      "462:\tlearn: 0.0950286\ttotal: 3.13s\tremaining: 3.63s\n",
      "463:\tlearn: 0.0950147\ttotal: 3.14s\tremaining: 3.63s\n",
      "464:\tlearn: 0.0950024\ttotal: 3.14s\tremaining: 3.62s\n",
      "465:\tlearn: 0.0949639\ttotal: 3.15s\tremaining: 3.61s\n",
      "466:\tlearn: 0.0949479\ttotal: 3.15s\tremaining: 3.6s\n",
      "467:\tlearn: 0.0949359\ttotal: 3.16s\tremaining: 3.59s\n",
      "468:\tlearn: 0.0949144\ttotal: 3.17s\tremaining: 3.58s\n",
      "469:\tlearn: 0.0948716\ttotal: 3.17s\tremaining: 3.58s\n",
      "470:\tlearn: 0.0948556\ttotal: 3.18s\tremaining: 3.57s\n",
      "471:\tlearn: 0.0947996\ttotal: 3.18s\tremaining: 3.56s\n",
      "472:\tlearn: 0.0947264\ttotal: 3.19s\tremaining: 3.55s\n",
      "473:\tlearn: 0.0947059\ttotal: 3.2s\tremaining: 3.55s\n",
      "474:\tlearn: 0.0946920\ttotal: 3.21s\tremaining: 3.55s\n",
      "475:\tlearn: 0.0946436\ttotal: 3.22s\tremaining: 3.55s\n",
      "476:\tlearn: 0.0946225\ttotal: 3.24s\tremaining: 3.55s\n",
      "477:\tlearn: 0.0945807\ttotal: 3.25s\tremaining: 3.55s\n",
      "478:\tlearn: 0.0945630\ttotal: 3.26s\tremaining: 3.55s\n",
      "479:\tlearn: 0.0945496\ttotal: 3.27s\tremaining: 3.55s\n",
      "480:\tlearn: 0.0945384\ttotal: 3.29s\tremaining: 3.55s\n",
      "481:\tlearn: 0.0945125\ttotal: 3.3s\tremaining: 3.54s\n",
      "482:\tlearn: 0.0944727\ttotal: 3.3s\tremaining: 3.53s\n",
      "483:\tlearn: 0.0944270\ttotal: 3.31s\tremaining: 3.53s\n",
      "484:\tlearn: 0.0943922\ttotal: 3.31s\tremaining: 3.52s\n",
      "485:\tlearn: 0.0943482\ttotal: 3.32s\tremaining: 3.51s\n",
      "486:\tlearn: 0.0942843\ttotal: 3.33s\tremaining: 3.5s\n",
      "487:\tlearn: 0.0942483\ttotal: 3.33s\tremaining: 3.49s\n",
      "488:\tlearn: 0.0942198\ttotal: 3.34s\tremaining: 3.49s\n",
      "489:\tlearn: 0.0941696\ttotal: 3.34s\tremaining: 3.48s\n",
      "490:\tlearn: 0.0941521\ttotal: 3.35s\tremaining: 3.47s\n",
      "491:\tlearn: 0.0940946\ttotal: 3.35s\tremaining: 3.46s\n",
      "492:\tlearn: 0.0940765\ttotal: 3.36s\tremaining: 3.46s\n",
      "493:\tlearn: 0.0940491\ttotal: 3.37s\tremaining: 3.45s\n",
      "494:\tlearn: 0.0940164\ttotal: 3.37s\tremaining: 3.44s\n",
      "495:\tlearn: 0.0939962\ttotal: 3.38s\tremaining: 3.43s\n",
      "496:\tlearn: 0.0939835\ttotal: 3.39s\tremaining: 3.43s\n",
      "497:\tlearn: 0.0939679\ttotal: 3.4s\tremaining: 3.42s\n",
      "498:\tlearn: 0.0939394\ttotal: 3.4s\tremaining: 3.42s\n",
      "499:\tlearn: 0.0938865\ttotal: 3.41s\tremaining: 3.41s\n",
      "500:\tlearn: 0.0938747\ttotal: 3.42s\tremaining: 3.4s\n",
      "501:\tlearn: 0.0938532\ttotal: 3.42s\tremaining: 3.39s\n",
      "502:\tlearn: 0.0938164\ttotal: 3.43s\tremaining: 3.39s\n",
      "503:\tlearn: 0.0938050\ttotal: 3.43s\tremaining: 3.38s\n",
      "504:\tlearn: 0.0937838\ttotal: 3.44s\tremaining: 3.37s\n",
      "505:\tlearn: 0.0937717\ttotal: 3.44s\tremaining: 3.36s\n",
      "506:\tlearn: 0.0937354\ttotal: 3.45s\tremaining: 3.35s\n",
      "507:\tlearn: 0.0937267\ttotal: 3.46s\tremaining: 3.35s\n",
      "508:\tlearn: 0.0936966\ttotal: 3.46s\tremaining: 3.34s\n",
      "509:\tlearn: 0.0936754\ttotal: 3.47s\tremaining: 3.33s\n",
      "510:\tlearn: 0.0936677\ttotal: 3.47s\tremaining: 3.32s\n",
      "511:\tlearn: 0.0936596\ttotal: 3.48s\tremaining: 3.31s\n",
      "512:\tlearn: 0.0936452\ttotal: 3.49s\tremaining: 3.31s\n",
      "513:\tlearn: 0.0936162\ttotal: 3.5s\tremaining: 3.31s\n",
      "514:\tlearn: 0.0935976\ttotal: 3.5s\tremaining: 3.3s\n",
      "515:\tlearn: 0.0935781\ttotal: 3.51s\tremaining: 3.29s\n",
      "516:\tlearn: 0.0935515\ttotal: 3.52s\tremaining: 3.28s\n",
      "517:\tlearn: 0.0935394\ttotal: 3.52s\tremaining: 3.28s\n",
      "518:\tlearn: 0.0935253\ttotal: 3.53s\tremaining: 3.27s\n",
      "519:\tlearn: 0.0934921\ttotal: 3.53s\tremaining: 3.26s\n",
      "520:\tlearn: 0.0934613\ttotal: 3.54s\tremaining: 3.25s\n",
      "521:\tlearn: 0.0934464\ttotal: 3.54s\tremaining: 3.24s\n",
      "522:\tlearn: 0.0934340\ttotal: 3.55s\tremaining: 3.24s\n",
      "523:\tlearn: 0.0933692\ttotal: 3.55s\tremaining: 3.23s\n",
      "524:\tlearn: 0.0933243\ttotal: 3.56s\tremaining: 3.22s\n",
      "525:\tlearn: 0.0933149\ttotal: 3.56s\tremaining: 3.21s\n",
      "526:\tlearn: 0.0932943\ttotal: 3.57s\tremaining: 3.2s\n",
      "527:\tlearn: 0.0932708\ttotal: 3.58s\tremaining: 3.2s\n",
      "528:\tlearn: 0.0932461\ttotal: 3.58s\tremaining: 3.19s\n",
      "529:\tlearn: 0.0932102\ttotal: 3.59s\tremaining: 3.18s\n",
      "530:\tlearn: 0.0931849\ttotal: 3.59s\tremaining: 3.17s\n",
      "531:\tlearn: 0.0931541\ttotal: 3.6s\tremaining: 3.17s\n",
      "532:\tlearn: 0.0931419\ttotal: 3.6s\tremaining: 3.16s\n",
      "533:\tlearn: 0.0931315\ttotal: 3.61s\tremaining: 3.15s\n",
      "534:\tlearn: 0.0930931\ttotal: 3.62s\tremaining: 3.14s\n",
      "535:\tlearn: 0.0930785\ttotal: 3.62s\tremaining: 3.14s\n",
      "536:\tlearn: 0.0930611\ttotal: 3.63s\tremaining: 3.13s\n",
      "537:\tlearn: 0.0930228\ttotal: 3.64s\tremaining: 3.12s\n",
      "538:\tlearn: 0.0929983\ttotal: 3.64s\tremaining: 3.12s\n",
      "539:\tlearn: 0.0929678\ttotal: 3.65s\tremaining: 3.11s\n",
      "540:\tlearn: 0.0929282\ttotal: 3.65s\tremaining: 3.1s\n",
      "541:\tlearn: 0.0929172\ttotal: 3.66s\tremaining: 3.09s\n",
      "542:\tlearn: 0.0928906\ttotal: 3.66s\tremaining: 3.08s\n",
      "543:\tlearn: 0.0928377\ttotal: 3.67s\tremaining: 3.08s\n",
      "544:\tlearn: 0.0928247\ttotal: 3.68s\tremaining: 3.07s\n",
      "545:\tlearn: 0.0927965\ttotal: 3.68s\tremaining: 3.06s\n",
      "546:\tlearn: 0.0927812\ttotal: 3.69s\tremaining: 3.05s\n",
      "547:\tlearn: 0.0927358\ttotal: 3.69s\tremaining: 3.04s\n",
      "548:\tlearn: 0.0927207\ttotal: 3.7s\tremaining: 3.04s\n",
      "549:\tlearn: 0.0927059\ttotal: 3.7s\tremaining: 3.03s\n",
      "550:\tlearn: 0.0926891\ttotal: 3.71s\tremaining: 3.02s\n",
      "551:\tlearn: 0.0926313\ttotal: 3.71s\tremaining: 3.01s\n",
      "552:\tlearn: 0.0926162\ttotal: 3.72s\tremaining: 3.01s\n",
      "553:\tlearn: 0.0925834\ttotal: 3.73s\tremaining: 3s\n",
      "554:\tlearn: 0.0925714\ttotal: 3.73s\tremaining: 2.99s\n",
      "555:\tlearn: 0.0925197\ttotal: 3.74s\tremaining: 2.98s\n",
      "556:\tlearn: 0.0925035\ttotal: 3.74s\tremaining: 2.98s\n",
      "557:\tlearn: 0.0924805\ttotal: 3.75s\tremaining: 2.97s\n",
      "558:\tlearn: 0.0924490\ttotal: 3.75s\tremaining: 2.96s\n",
      "559:\tlearn: 0.0924087\ttotal: 3.76s\tremaining: 2.95s\n",
      "560:\tlearn: 0.0923945\ttotal: 3.77s\tremaining: 2.95s\n",
      "561:\tlearn: 0.0923587\ttotal: 3.77s\tremaining: 2.94s\n",
      "562:\tlearn: 0.0923302\ttotal: 3.78s\tremaining: 2.93s\n",
      "563:\tlearn: 0.0923089\ttotal: 3.78s\tremaining: 2.92s\n",
      "564:\tlearn: 0.0922824\ttotal: 3.79s\tremaining: 2.92s\n",
      "565:\tlearn: 0.0922645\ttotal: 3.79s\tremaining: 2.91s\n",
      "566:\tlearn: 0.0922259\ttotal: 3.8s\tremaining: 2.9s\n",
      "567:\tlearn: 0.0921714\ttotal: 3.8s\tremaining: 2.89s\n",
      "568:\tlearn: 0.0921613\ttotal: 3.81s\tremaining: 2.88s\n",
      "569:\tlearn: 0.0921453\ttotal: 3.81s\tremaining: 2.88s\n",
      "570:\tlearn: 0.0921340\ttotal: 3.82s\tremaining: 2.87s\n",
      "571:\tlearn: 0.0921050\ttotal: 3.83s\tremaining: 2.86s\n",
      "572:\tlearn: 0.0920835\ttotal: 3.83s\tremaining: 2.85s\n",
      "573:\tlearn: 0.0920729\ttotal: 3.84s\tremaining: 2.85s\n",
      "574:\tlearn: 0.0920504\ttotal: 3.84s\tremaining: 2.84s\n",
      "575:\tlearn: 0.0920263\ttotal: 3.85s\tremaining: 2.83s\n",
      "576:\tlearn: 0.0920084\ttotal: 3.85s\tremaining: 2.83s\n",
      "577:\tlearn: 0.0919783\ttotal: 3.86s\tremaining: 2.82s\n",
      "578:\tlearn: 0.0919456\ttotal: 3.86s\tremaining: 2.81s\n",
      "579:\tlearn: 0.0919199\ttotal: 3.87s\tremaining: 2.8s\n",
      "580:\tlearn: 0.0919102\ttotal: 3.88s\tremaining: 2.8s\n",
      "581:\tlearn: 0.0918888\ttotal: 3.88s\tremaining: 2.79s\n",
      "582:\tlearn: 0.0918715\ttotal: 3.89s\tremaining: 2.78s\n",
      "583:\tlearn: 0.0918623\ttotal: 3.9s\tremaining: 2.78s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584:\tlearn: 0.0918403\ttotal: 3.9s\tremaining: 2.77s\n",
      "585:\tlearn: 0.0918318\ttotal: 3.91s\tremaining: 2.76s\n",
      "586:\tlearn: 0.0918223\ttotal: 3.91s\tremaining: 2.75s\n",
      "587:\tlearn: 0.0917999\ttotal: 3.92s\tremaining: 2.75s\n",
      "588:\tlearn: 0.0917896\ttotal: 3.92s\tremaining: 2.74s\n",
      "589:\tlearn: 0.0917686\ttotal: 3.93s\tremaining: 2.73s\n",
      "590:\tlearn: 0.0917447\ttotal: 3.94s\tremaining: 2.72s\n",
      "591:\tlearn: 0.0917294\ttotal: 3.94s\tremaining: 2.72s\n",
      "592:\tlearn: 0.0917209\ttotal: 3.95s\tremaining: 2.71s\n",
      "593:\tlearn: 0.0917106\ttotal: 3.95s\tremaining: 2.7s\n",
      "594:\tlearn: 0.0916853\ttotal: 3.96s\tremaining: 2.69s\n",
      "595:\tlearn: 0.0916733\ttotal: 3.96s\tremaining: 2.69s\n",
      "596:\tlearn: 0.0916634\ttotal: 3.97s\tremaining: 2.68s\n",
      "597:\tlearn: 0.0916415\ttotal: 3.98s\tremaining: 2.67s\n",
      "598:\tlearn: 0.0916117\ttotal: 3.98s\tremaining: 2.67s\n",
      "599:\tlearn: 0.0915959\ttotal: 3.99s\tremaining: 2.66s\n",
      "600:\tlearn: 0.0915856\ttotal: 3.99s\tremaining: 2.65s\n",
      "601:\tlearn: 0.0915357\ttotal: 4s\tremaining: 2.64s\n",
      "602:\tlearn: 0.0915184\ttotal: 4s\tremaining: 2.63s\n",
      "603:\tlearn: 0.0914932\ttotal: 4.01s\tremaining: 2.63s\n",
      "604:\tlearn: 0.0914608\ttotal: 4.01s\tremaining: 2.62s\n",
      "605:\tlearn: 0.0914526\ttotal: 4.02s\tremaining: 2.62s\n",
      "606:\tlearn: 0.0914427\ttotal: 4.03s\tremaining: 2.61s\n",
      "607:\tlearn: 0.0914010\ttotal: 4.03s\tremaining: 2.6s\n",
      "608:\tlearn: 0.0913877\ttotal: 4.04s\tremaining: 2.59s\n",
      "609:\tlearn: 0.0913614\ttotal: 4.04s\tremaining: 2.58s\n",
      "610:\tlearn: 0.0913330\ttotal: 4.05s\tremaining: 2.58s\n",
      "611:\tlearn: 0.0913108\ttotal: 4.05s\tremaining: 2.57s\n",
      "612:\tlearn: 0.0912892\ttotal: 4.06s\tremaining: 2.56s\n",
      "613:\tlearn: 0.0912695\ttotal: 4.07s\tremaining: 2.56s\n",
      "614:\tlearn: 0.0912627\ttotal: 4.07s\tremaining: 2.55s\n",
      "615:\tlearn: 0.0912433\ttotal: 4.08s\tremaining: 2.54s\n",
      "616:\tlearn: 0.0912342\ttotal: 4.09s\tremaining: 2.54s\n",
      "617:\tlearn: 0.0912227\ttotal: 4.09s\tremaining: 2.53s\n",
      "618:\tlearn: 0.0912146\ttotal: 4.1s\tremaining: 2.52s\n",
      "619:\tlearn: 0.0911896\ttotal: 4.1s\tremaining: 2.51s\n",
      "620:\tlearn: 0.0911715\ttotal: 4.11s\tremaining: 2.51s\n",
      "621:\tlearn: 0.0911195\ttotal: 4.11s\tremaining: 2.5s\n",
      "622:\tlearn: 0.0910870\ttotal: 4.12s\tremaining: 2.49s\n",
      "623:\tlearn: 0.0910494\ttotal: 4.13s\tremaining: 2.49s\n",
      "624:\tlearn: 0.0910319\ttotal: 4.13s\tremaining: 2.48s\n",
      "625:\tlearn: 0.0910231\ttotal: 4.14s\tremaining: 2.47s\n",
      "626:\tlearn: 0.0909934\ttotal: 4.14s\tremaining: 2.46s\n",
      "627:\tlearn: 0.0909599\ttotal: 4.16s\tremaining: 2.46s\n",
      "628:\tlearn: 0.0909353\ttotal: 4.17s\tremaining: 2.46s\n",
      "629:\tlearn: 0.0909192\ttotal: 4.18s\tremaining: 2.46s\n",
      "630:\tlearn: 0.0909073\ttotal: 4.19s\tremaining: 2.45s\n",
      "631:\tlearn: 0.0908917\ttotal: 4.2s\tremaining: 2.45s\n",
      "632:\tlearn: 0.0908572\ttotal: 4.21s\tremaining: 2.44s\n",
      "633:\tlearn: 0.0908380\ttotal: 4.22s\tremaining: 2.44s\n",
      "634:\tlearn: 0.0908162\ttotal: 4.24s\tremaining: 2.43s\n",
      "635:\tlearn: 0.0908017\ttotal: 4.24s\tremaining: 2.43s\n",
      "636:\tlearn: 0.0907965\ttotal: 4.25s\tremaining: 2.42s\n",
      "637:\tlearn: 0.0907901\ttotal: 4.25s\tremaining: 2.41s\n",
      "638:\tlearn: 0.0907812\ttotal: 4.26s\tremaining: 2.4s\n",
      "639:\tlearn: 0.0907372\ttotal: 4.26s\tremaining: 2.4s\n",
      "640:\tlearn: 0.0907221\ttotal: 4.27s\tremaining: 2.39s\n",
      "641:\tlearn: 0.0906982\ttotal: 4.28s\tremaining: 2.39s\n",
      "642:\tlearn: 0.0906795\ttotal: 4.29s\tremaining: 2.38s\n",
      "643:\tlearn: 0.0906551\ttotal: 4.29s\tremaining: 2.37s\n",
      "644:\tlearn: 0.0906461\ttotal: 4.3s\tremaining: 2.36s\n",
      "645:\tlearn: 0.0906374\ttotal: 4.3s\tremaining: 2.36s\n",
      "646:\tlearn: 0.0905891\ttotal: 4.31s\tremaining: 2.35s\n",
      "647:\tlearn: 0.0905643\ttotal: 4.31s\tremaining: 2.34s\n",
      "648:\tlearn: 0.0905478\ttotal: 4.32s\tremaining: 2.33s\n",
      "649:\tlearn: 0.0905389\ttotal: 4.32s\tremaining: 2.33s\n",
      "650:\tlearn: 0.0905155\ttotal: 4.33s\tremaining: 2.32s\n",
      "651:\tlearn: 0.0904902\ttotal: 4.33s\tremaining: 2.31s\n",
      "652:\tlearn: 0.0904774\ttotal: 4.34s\tremaining: 2.31s\n",
      "653:\tlearn: 0.0904726\ttotal: 4.35s\tremaining: 2.3s\n",
      "654:\tlearn: 0.0904483\ttotal: 4.35s\tremaining: 2.29s\n",
      "655:\tlearn: 0.0904359\ttotal: 4.36s\tremaining: 2.29s\n",
      "656:\tlearn: 0.0904255\ttotal: 4.36s\tremaining: 2.28s\n",
      "657:\tlearn: 0.0903937\ttotal: 4.37s\tremaining: 2.27s\n",
      "658:\tlearn: 0.0903781\ttotal: 4.37s\tremaining: 2.26s\n",
      "659:\tlearn: 0.0903722\ttotal: 4.38s\tremaining: 2.26s\n",
      "660:\tlearn: 0.0903256\ttotal: 4.38s\tremaining: 2.25s\n",
      "661:\tlearn: 0.0903196\ttotal: 4.39s\tremaining: 2.24s\n",
      "662:\tlearn: 0.0902882\ttotal: 4.4s\tremaining: 2.23s\n",
      "663:\tlearn: 0.0902721\ttotal: 4.4s\tremaining: 2.23s\n",
      "664:\tlearn: 0.0902543\ttotal: 4.41s\tremaining: 2.22s\n",
      "665:\tlearn: 0.0902246\ttotal: 4.41s\tremaining: 2.21s\n",
      "666:\tlearn: 0.0902163\ttotal: 4.42s\tremaining: 2.21s\n",
      "667:\tlearn: 0.0901963\ttotal: 4.42s\tremaining: 2.2s\n",
      "668:\tlearn: 0.0901749\ttotal: 4.43s\tremaining: 2.19s\n",
      "669:\tlearn: 0.0901329\ttotal: 4.43s\tremaining: 2.18s\n",
      "670:\tlearn: 0.0901211\ttotal: 4.44s\tremaining: 2.18s\n",
      "671:\tlearn: 0.0900801\ttotal: 4.45s\tremaining: 2.17s\n",
      "672:\tlearn: 0.0900575\ttotal: 4.45s\tremaining: 2.16s\n",
      "673:\tlearn: 0.0900489\ttotal: 4.46s\tremaining: 2.15s\n",
      "674:\tlearn: 0.0900305\ttotal: 4.46s\tremaining: 2.15s\n",
      "675:\tlearn: 0.0900139\ttotal: 4.47s\tremaining: 2.14s\n",
      "676:\tlearn: 0.0900002\ttotal: 4.47s\tremaining: 2.13s\n",
      "677:\tlearn: 0.0899731\ttotal: 4.48s\tremaining: 2.13s\n",
      "678:\tlearn: 0.0899430\ttotal: 4.49s\tremaining: 2.12s\n",
      "679:\tlearn: 0.0899096\ttotal: 4.5s\tremaining: 2.12s\n",
      "680:\tlearn: 0.0899014\ttotal: 4.5s\tremaining: 2.11s\n",
      "681:\tlearn: 0.0898867\ttotal: 4.51s\tremaining: 2.1s\n",
      "682:\tlearn: 0.0898708\ttotal: 4.52s\tremaining: 2.1s\n",
      "683:\tlearn: 0.0898599\ttotal: 4.52s\tremaining: 2.09s\n",
      "684:\tlearn: 0.0898485\ttotal: 4.53s\tremaining: 2.08s\n",
      "685:\tlearn: 0.0898270\ttotal: 4.53s\tremaining: 2.08s\n",
      "686:\tlearn: 0.0898066\ttotal: 4.54s\tremaining: 2.07s\n",
      "687:\tlearn: 0.0897835\ttotal: 4.54s\tremaining: 2.06s\n",
      "688:\tlearn: 0.0897593\ttotal: 4.55s\tremaining: 2.05s\n",
      "689:\tlearn: 0.0897331\ttotal: 4.55s\tremaining: 2.05s\n",
      "690:\tlearn: 0.0897045\ttotal: 4.56s\tremaining: 2.04s\n",
      "691:\tlearn: 0.0896965\ttotal: 4.57s\tremaining: 2.03s\n",
      "692:\tlearn: 0.0896844\ttotal: 4.57s\tremaining: 2.02s\n",
      "693:\tlearn: 0.0896729\ttotal: 4.58s\tremaining: 2.02s\n",
      "694:\tlearn: 0.0896665\ttotal: 4.58s\tremaining: 2.01s\n",
      "695:\tlearn: 0.0896441\ttotal: 4.59s\tremaining: 2s\n",
      "696:\tlearn: 0.0896164\ttotal: 4.59s\tremaining: 2s\n",
      "697:\tlearn: 0.0895745\ttotal: 4.6s\tremaining: 1.99s\n",
      "698:\tlearn: 0.0895510\ttotal: 4.6s\tremaining: 1.98s\n",
      "699:\tlearn: 0.0895170\ttotal: 4.61s\tremaining: 1.98s\n",
      "700:\tlearn: 0.0894917\ttotal: 4.61s\tremaining: 1.97s\n",
      "701:\tlearn: 0.0894726\ttotal: 4.62s\tremaining: 1.96s\n",
      "702:\tlearn: 0.0894417\ttotal: 4.63s\tremaining: 1.95s\n",
      "703:\tlearn: 0.0894271\ttotal: 4.63s\tremaining: 1.95s\n",
      "704:\tlearn: 0.0893902\ttotal: 4.64s\tremaining: 1.94s\n",
      "705:\tlearn: 0.0893801\ttotal: 4.64s\tremaining: 1.93s\n",
      "706:\tlearn: 0.0893702\ttotal: 4.65s\tremaining: 1.93s\n",
      "707:\tlearn: 0.0893582\ttotal: 4.66s\tremaining: 1.92s\n",
      "708:\tlearn: 0.0893470\ttotal: 4.67s\tremaining: 1.92s\n",
      "709:\tlearn: 0.0893391\ttotal: 4.68s\tremaining: 1.91s\n",
      "710:\tlearn: 0.0893278\ttotal: 4.69s\tremaining: 1.91s\n",
      "711:\tlearn: 0.0892809\ttotal: 4.7s\tremaining: 1.9s\n",
      "712:\tlearn: 0.0892643\ttotal: 4.7s\tremaining: 1.89s\n",
      "713:\tlearn: 0.0892559\ttotal: 4.71s\tremaining: 1.89s\n",
      "714:\tlearn: 0.0892278\ttotal: 4.71s\tremaining: 1.88s\n",
      "715:\tlearn: 0.0892142\ttotal: 4.72s\tremaining: 1.87s\n",
      "716:\tlearn: 0.0891965\ttotal: 4.73s\tremaining: 1.86s\n",
      "717:\tlearn: 0.0891835\ttotal: 4.73s\tremaining: 1.86s\n",
      "718:\tlearn: 0.0891570\ttotal: 4.74s\tremaining: 1.85s\n",
      "719:\tlearn: 0.0891436\ttotal: 4.74s\tremaining: 1.84s\n",
      "720:\tlearn: 0.0891384\ttotal: 4.75s\tremaining: 1.84s\n",
      "721:\tlearn: 0.0891122\ttotal: 4.75s\tremaining: 1.83s\n",
      "722:\tlearn: 0.0890921\ttotal: 4.76s\tremaining: 1.82s\n",
      "723:\tlearn: 0.0890835\ttotal: 4.77s\tremaining: 1.82s\n",
      "724:\tlearn: 0.0890634\ttotal: 4.77s\tremaining: 1.81s\n",
      "725:\tlearn: 0.0890551\ttotal: 4.78s\tremaining: 1.8s\n",
      "726:\tlearn: 0.0890447\ttotal: 4.79s\tremaining: 1.8s\n",
      "727:\tlearn: 0.0890155\ttotal: 4.79s\tremaining: 1.79s\n",
      "728:\tlearn: 0.0889759\ttotal: 4.79s\tremaining: 1.78s\n",
      "729:\tlearn: 0.0889585\ttotal: 4.8s\tremaining: 1.77s\n",
      "730:\tlearn: 0.0889473\ttotal: 4.81s\tremaining: 1.77s\n",
      "731:\tlearn: 0.0889328\ttotal: 4.81s\tremaining: 1.76s\n",
      "732:\tlearn: 0.0889252\ttotal: 4.82s\tremaining: 1.75s\n",
      "733:\tlearn: 0.0889062\ttotal: 4.82s\tremaining: 1.75s\n",
      "734:\tlearn: 0.0888847\ttotal: 4.83s\tremaining: 1.74s\n",
      "735:\tlearn: 0.0888775\ttotal: 4.83s\tremaining: 1.73s\n",
      "736:\tlearn: 0.0888577\ttotal: 4.84s\tremaining: 1.73s\n",
      "737:\tlearn: 0.0888483\ttotal: 4.85s\tremaining: 1.72s\n",
      "738:\tlearn: 0.0888395\ttotal: 4.85s\tremaining: 1.71s\n",
      "739:\tlearn: 0.0888317\ttotal: 4.86s\tremaining: 1.71s\n",
      "740:\tlearn: 0.0888041\ttotal: 4.87s\tremaining: 1.7s\n",
      "741:\tlearn: 0.0887957\ttotal: 4.87s\tremaining: 1.69s\n",
      "742:\tlearn: 0.0887882\ttotal: 4.88s\tremaining: 1.69s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743:\tlearn: 0.0887617\ttotal: 4.88s\tremaining: 1.68s\n",
      "744:\tlearn: 0.0887530\ttotal: 4.89s\tremaining: 1.67s\n",
      "745:\tlearn: 0.0887391\ttotal: 4.9s\tremaining: 1.67s\n",
      "746:\tlearn: 0.0887237\ttotal: 4.91s\tremaining: 1.66s\n",
      "747:\tlearn: 0.0887124\ttotal: 4.92s\tremaining: 1.66s\n",
      "748:\tlearn: 0.0886779\ttotal: 4.93s\tremaining: 1.65s\n",
      "749:\tlearn: 0.0886626\ttotal: 4.94s\tremaining: 1.65s\n",
      "750:\tlearn: 0.0886426\ttotal: 4.95s\tremaining: 1.64s\n",
      "751:\tlearn: 0.0886291\ttotal: 4.96s\tremaining: 1.64s\n",
      "752:\tlearn: 0.0886034\ttotal: 4.97s\tremaining: 1.63s\n",
      "753:\tlearn: 0.0885885\ttotal: 4.99s\tremaining: 1.63s\n",
      "754:\tlearn: 0.0885735\ttotal: 5s\tremaining: 1.62s\n",
      "755:\tlearn: 0.0885656\ttotal: 5.01s\tremaining: 1.62s\n",
      "756:\tlearn: 0.0885286\ttotal: 5.01s\tremaining: 1.61s\n",
      "757:\tlearn: 0.0885150\ttotal: 5.02s\tremaining: 1.6s\n",
      "758:\tlearn: 0.0885060\ttotal: 5.03s\tremaining: 1.6s\n",
      "759:\tlearn: 0.0884954\ttotal: 5.04s\tremaining: 1.59s\n",
      "760:\tlearn: 0.0884807\ttotal: 5.04s\tremaining: 1.58s\n",
      "761:\tlearn: 0.0884695\ttotal: 5.05s\tremaining: 1.58s\n",
      "762:\tlearn: 0.0884568\ttotal: 5.06s\tremaining: 1.57s\n",
      "763:\tlearn: 0.0884461\ttotal: 5.07s\tremaining: 1.57s\n",
      "764:\tlearn: 0.0884359\ttotal: 5.09s\tremaining: 1.56s\n",
      "765:\tlearn: 0.0884243\ttotal: 5.11s\tremaining: 1.56s\n",
      "766:\tlearn: 0.0884064\ttotal: 5.13s\tremaining: 1.56s\n",
      "767:\tlearn: 0.0883942\ttotal: 5.14s\tremaining: 1.55s\n",
      "768:\tlearn: 0.0883722\ttotal: 5.15s\tremaining: 1.55s\n",
      "769:\tlearn: 0.0883522\ttotal: 5.16s\tremaining: 1.54s\n",
      "770:\tlearn: 0.0883391\ttotal: 5.17s\tremaining: 1.53s\n",
      "771:\tlearn: 0.0883210\ttotal: 5.18s\tremaining: 1.53s\n",
      "772:\tlearn: 0.0883130\ttotal: 5.19s\tremaining: 1.52s\n",
      "773:\tlearn: 0.0883047\ttotal: 5.2s\tremaining: 1.52s\n",
      "774:\tlearn: 0.0882847\ttotal: 5.2s\tremaining: 1.51s\n",
      "775:\tlearn: 0.0882769\ttotal: 5.21s\tremaining: 1.5s\n",
      "776:\tlearn: 0.0882688\ttotal: 5.21s\tremaining: 1.5s\n",
      "777:\tlearn: 0.0882516\ttotal: 5.22s\tremaining: 1.49s\n",
      "778:\tlearn: 0.0882353\ttotal: 5.22s\tremaining: 1.48s\n",
      "779:\tlearn: 0.0882246\ttotal: 5.23s\tremaining: 1.48s\n",
      "780:\tlearn: 0.0882067\ttotal: 5.24s\tremaining: 1.47s\n",
      "781:\tlearn: 0.0881907\ttotal: 5.24s\tremaining: 1.46s\n",
      "782:\tlearn: 0.0881815\ttotal: 5.25s\tremaining: 1.45s\n",
      "783:\tlearn: 0.0881678\ttotal: 5.25s\tremaining: 1.45s\n",
      "784:\tlearn: 0.0881589\ttotal: 5.26s\tremaining: 1.44s\n",
      "785:\tlearn: 0.0881451\ttotal: 5.27s\tremaining: 1.43s\n",
      "786:\tlearn: 0.0881347\ttotal: 5.27s\tremaining: 1.43s\n",
      "787:\tlearn: 0.0880961\ttotal: 5.28s\tremaining: 1.42s\n",
      "788:\tlearn: 0.0880813\ttotal: 5.28s\tremaining: 1.41s\n",
      "789:\tlearn: 0.0880657\ttotal: 5.29s\tremaining: 1.41s\n",
      "790:\tlearn: 0.0880440\ttotal: 5.29s\tremaining: 1.4s\n",
      "791:\tlearn: 0.0880335\ttotal: 5.3s\tremaining: 1.39s\n",
      "792:\tlearn: 0.0880108\ttotal: 5.3s\tremaining: 1.38s\n",
      "793:\tlearn: 0.0879904\ttotal: 5.31s\tremaining: 1.38s\n",
      "794:\tlearn: 0.0879840\ttotal: 5.32s\tremaining: 1.37s\n",
      "795:\tlearn: 0.0879549\ttotal: 5.32s\tremaining: 1.36s\n",
      "796:\tlearn: 0.0879494\ttotal: 5.33s\tremaining: 1.36s\n",
      "797:\tlearn: 0.0879242\ttotal: 5.34s\tremaining: 1.35s\n",
      "798:\tlearn: 0.0879149\ttotal: 5.34s\tremaining: 1.34s\n",
      "799:\tlearn: 0.0878881\ttotal: 5.35s\tremaining: 1.34s\n",
      "800:\tlearn: 0.0878829\ttotal: 5.36s\tremaining: 1.33s\n",
      "801:\tlearn: 0.0878672\ttotal: 5.36s\tremaining: 1.32s\n",
      "802:\tlearn: 0.0878559\ttotal: 5.37s\tremaining: 1.32s\n",
      "803:\tlearn: 0.0878466\ttotal: 5.37s\tremaining: 1.31s\n",
      "804:\tlearn: 0.0878356\ttotal: 5.38s\tremaining: 1.3s\n",
      "805:\tlearn: 0.0878265\ttotal: 5.38s\tremaining: 1.29s\n",
      "806:\tlearn: 0.0878121\ttotal: 5.39s\tremaining: 1.29s\n",
      "807:\tlearn: 0.0877830\ttotal: 5.39s\tremaining: 1.28s\n",
      "808:\tlearn: 0.0877691\ttotal: 5.4s\tremaining: 1.27s\n",
      "809:\tlearn: 0.0877612\ttotal: 5.41s\tremaining: 1.27s\n",
      "810:\tlearn: 0.0877409\ttotal: 5.41s\tremaining: 1.26s\n",
      "811:\tlearn: 0.0877215\ttotal: 5.42s\tremaining: 1.25s\n",
      "812:\tlearn: 0.0877082\ttotal: 5.42s\tremaining: 1.25s\n",
      "813:\tlearn: 0.0876932\ttotal: 5.43s\tremaining: 1.24s\n",
      "814:\tlearn: 0.0876767\ttotal: 5.44s\tremaining: 1.23s\n",
      "815:\tlearn: 0.0876692\ttotal: 5.44s\tremaining: 1.23s\n",
      "816:\tlearn: 0.0876583\ttotal: 5.45s\tremaining: 1.22s\n",
      "817:\tlearn: 0.0876464\ttotal: 5.45s\tremaining: 1.21s\n",
      "818:\tlearn: 0.0876328\ttotal: 5.46s\tremaining: 1.21s\n",
      "819:\tlearn: 0.0876116\ttotal: 5.46s\tremaining: 1.2s\n",
      "820:\tlearn: 0.0875971\ttotal: 5.47s\tremaining: 1.19s\n",
      "821:\tlearn: 0.0875833\ttotal: 5.47s\tremaining: 1.19s\n",
      "822:\tlearn: 0.0875795\ttotal: 5.48s\tremaining: 1.18s\n",
      "823:\tlearn: 0.0875646\ttotal: 5.49s\tremaining: 1.17s\n",
      "824:\tlearn: 0.0875517\ttotal: 5.49s\tremaining: 1.16s\n",
      "825:\tlearn: 0.0875451\ttotal: 5.5s\tremaining: 1.16s\n",
      "826:\tlearn: 0.0875246\ttotal: 5.5s\tremaining: 1.15s\n",
      "827:\tlearn: 0.0875042\ttotal: 5.51s\tremaining: 1.14s\n",
      "828:\tlearn: 0.0874859\ttotal: 5.51s\tremaining: 1.14s\n",
      "829:\tlearn: 0.0874786\ttotal: 5.52s\tremaining: 1.13s\n",
      "830:\tlearn: 0.0874733\ttotal: 5.52s\tremaining: 1.12s\n",
      "831:\tlearn: 0.0874638\ttotal: 5.53s\tremaining: 1.12s\n",
      "832:\tlearn: 0.0874463\ttotal: 5.53s\tremaining: 1.11s\n",
      "833:\tlearn: 0.0874219\ttotal: 5.54s\tremaining: 1.1s\n",
      "834:\tlearn: 0.0874111\ttotal: 5.54s\tremaining: 1.09s\n",
      "835:\tlearn: 0.0874018\ttotal: 5.55s\tremaining: 1.09s\n",
      "836:\tlearn: 0.0873898\ttotal: 5.56s\tremaining: 1.08s\n",
      "837:\tlearn: 0.0873802\ttotal: 5.56s\tremaining: 1.07s\n",
      "838:\tlearn: 0.0873720\ttotal: 5.57s\tremaining: 1.07s\n",
      "839:\tlearn: 0.0873623\ttotal: 5.57s\tremaining: 1.06s\n",
      "840:\tlearn: 0.0873568\ttotal: 5.58s\tremaining: 1.05s\n",
      "841:\tlearn: 0.0873490\ttotal: 5.58s\tremaining: 1.05s\n",
      "842:\tlearn: 0.0873433\ttotal: 5.59s\tremaining: 1.04s\n",
      "843:\tlearn: 0.0873321\ttotal: 5.6s\tremaining: 1.03s\n",
      "844:\tlearn: 0.0873255\ttotal: 5.6s\tremaining: 1.03s\n",
      "845:\tlearn: 0.0873158\ttotal: 5.61s\tremaining: 1.02s\n",
      "846:\tlearn: 0.0873014\ttotal: 5.61s\tremaining: 1.01s\n",
      "847:\tlearn: 0.0872876\ttotal: 5.62s\tremaining: 1.01s\n",
      "848:\tlearn: 0.0872696\ttotal: 5.63s\tremaining: 1s\n",
      "849:\tlearn: 0.0872580\ttotal: 5.63s\tremaining: 995ms\n",
      "850:\tlearn: 0.0872484\ttotal: 5.64s\tremaining: 988ms\n",
      "851:\tlearn: 0.0872367\ttotal: 5.65s\tremaining: 981ms\n",
      "852:\tlearn: 0.0872246\ttotal: 5.65s\tremaining: 974ms\n",
      "853:\tlearn: 0.0872006\ttotal: 5.66s\tremaining: 967ms\n",
      "854:\tlearn: 0.0871815\ttotal: 5.66s\tremaining: 960ms\n",
      "855:\tlearn: 0.0871674\ttotal: 5.67s\tremaining: 954ms\n",
      "856:\tlearn: 0.0871564\ttotal: 5.67s\tremaining: 947ms\n",
      "857:\tlearn: 0.0871444\ttotal: 5.68s\tremaining: 940ms\n",
      "858:\tlearn: 0.0871313\ttotal: 5.69s\tremaining: 934ms\n",
      "859:\tlearn: 0.0871215\ttotal: 5.69s\tremaining: 927ms\n",
      "860:\tlearn: 0.0871112\ttotal: 5.7s\tremaining: 920ms\n",
      "861:\tlearn: 0.0871040\ttotal: 5.7s\tremaining: 913ms\n",
      "862:\tlearn: 0.0870963\ttotal: 5.71s\tremaining: 906ms\n",
      "863:\tlearn: 0.0870865\ttotal: 5.71s\tremaining: 900ms\n",
      "864:\tlearn: 0.0870762\ttotal: 5.72s\tremaining: 893ms\n",
      "865:\tlearn: 0.0870680\ttotal: 5.73s\tremaining: 886ms\n",
      "866:\tlearn: 0.0870603\ttotal: 5.73s\tremaining: 879ms\n",
      "867:\tlearn: 0.0870398\ttotal: 5.74s\tremaining: 873ms\n",
      "868:\tlearn: 0.0870297\ttotal: 5.74s\tremaining: 866ms\n",
      "869:\tlearn: 0.0870186\ttotal: 5.75s\tremaining: 859ms\n",
      "870:\tlearn: 0.0870121\ttotal: 5.75s\tremaining: 852ms\n",
      "871:\tlearn: 0.0870032\ttotal: 5.76s\tremaining: 845ms\n",
      "872:\tlearn: 0.0869945\ttotal: 5.76s\tremaining: 839ms\n",
      "873:\tlearn: 0.0869748\ttotal: 5.77s\tremaining: 832ms\n",
      "874:\tlearn: 0.0869473\ttotal: 5.78s\tremaining: 825ms\n",
      "875:\tlearn: 0.0869398\ttotal: 5.78s\tremaining: 818ms\n",
      "876:\tlearn: 0.0869236\ttotal: 5.79s\tremaining: 812ms\n",
      "877:\tlearn: 0.0869117\ttotal: 5.79s\tremaining: 805ms\n",
      "878:\tlearn: 0.0869030\ttotal: 5.8s\tremaining: 798ms\n",
      "879:\tlearn: 0.0868811\ttotal: 5.8s\tremaining: 791ms\n",
      "880:\tlearn: 0.0868742\ttotal: 5.81s\tremaining: 785ms\n",
      "881:\tlearn: 0.0868587\ttotal: 5.82s\tremaining: 778ms\n",
      "882:\tlearn: 0.0868450\ttotal: 5.82s\tremaining: 771ms\n",
      "883:\tlearn: 0.0868292\ttotal: 5.83s\tremaining: 765ms\n",
      "884:\tlearn: 0.0868249\ttotal: 5.83s\tremaining: 758ms\n",
      "885:\tlearn: 0.0867899\ttotal: 5.84s\tremaining: 751ms\n",
      "886:\tlearn: 0.0867816\ttotal: 5.84s\tremaining: 745ms\n",
      "887:\tlearn: 0.0867757\ttotal: 5.85s\tremaining: 738ms\n",
      "888:\tlearn: 0.0867671\ttotal: 5.86s\tremaining: 732ms\n",
      "889:\tlearn: 0.0867534\ttotal: 5.86s\tremaining: 725ms\n",
      "890:\tlearn: 0.0867382\ttotal: 5.87s\tremaining: 718ms\n",
      "891:\tlearn: 0.0867203\ttotal: 5.88s\tremaining: 711ms\n",
      "892:\tlearn: 0.0867117\ttotal: 5.88s\tremaining: 705ms\n",
      "893:\tlearn: 0.0866945\ttotal: 5.89s\tremaining: 698ms\n",
      "894:\tlearn: 0.0866902\ttotal: 5.89s\tremaining: 692ms\n",
      "895:\tlearn: 0.0866813\ttotal: 5.9s\tremaining: 685ms\n",
      "896:\tlearn: 0.0866768\ttotal: 5.91s\tremaining: 679ms\n",
      "897:\tlearn: 0.0866558\ttotal: 5.92s\tremaining: 672ms\n",
      "898:\tlearn: 0.0866320\ttotal: 5.92s\tremaining: 665ms\n",
      "899:\tlearn: 0.0866244\ttotal: 5.93s\tremaining: 659ms\n",
      "900:\tlearn: 0.0866129\ttotal: 5.93s\tremaining: 652ms\n",
      "901:\tlearn: 0.0866040\ttotal: 5.94s\tremaining: 645ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902:\tlearn: 0.0865716\ttotal: 5.94s\tremaining: 638ms\n",
      "903:\tlearn: 0.0865635\ttotal: 5.95s\tremaining: 632ms\n",
      "904:\tlearn: 0.0865511\ttotal: 5.96s\tremaining: 625ms\n",
      "905:\tlearn: 0.0865368\ttotal: 5.96s\tremaining: 619ms\n",
      "906:\tlearn: 0.0865295\ttotal: 5.97s\tremaining: 612ms\n",
      "907:\tlearn: 0.0865206\ttotal: 5.98s\tremaining: 606ms\n",
      "908:\tlearn: 0.0865102\ttotal: 5.99s\tremaining: 599ms\n",
      "909:\tlearn: 0.0865018\ttotal: 5.99s\tremaining: 593ms\n",
      "910:\tlearn: 0.0864896\ttotal: 6s\tremaining: 586ms\n",
      "911:\tlearn: 0.0864798\ttotal: 6s\tremaining: 579ms\n",
      "912:\tlearn: 0.0864669\ttotal: 6.01s\tremaining: 573ms\n",
      "913:\tlearn: 0.0864323\ttotal: 6.01s\tremaining: 566ms\n",
      "914:\tlearn: 0.0864176\ttotal: 6.02s\tremaining: 559ms\n",
      "915:\tlearn: 0.0864083\ttotal: 6.02s\tremaining: 552ms\n",
      "916:\tlearn: 0.0863868\ttotal: 6.03s\tremaining: 546ms\n",
      "917:\tlearn: 0.0863481\ttotal: 6.03s\tremaining: 539ms\n",
      "918:\tlearn: 0.0863265\ttotal: 6.04s\tremaining: 532ms\n",
      "919:\tlearn: 0.0863126\ttotal: 6.05s\tremaining: 526ms\n",
      "920:\tlearn: 0.0862972\ttotal: 6.06s\tremaining: 520ms\n",
      "921:\tlearn: 0.0862876\ttotal: 6.07s\tremaining: 514ms\n",
      "922:\tlearn: 0.0862753\ttotal: 6.08s\tremaining: 508ms\n",
      "923:\tlearn: 0.0862688\ttotal: 6.1s\tremaining: 501ms\n",
      "924:\tlearn: 0.0862614\ttotal: 6.11s\tremaining: 495ms\n",
      "925:\tlearn: 0.0862538\ttotal: 6.12s\tremaining: 489ms\n",
      "926:\tlearn: 0.0862279\ttotal: 6.13s\tremaining: 483ms\n",
      "927:\tlearn: 0.0862201\ttotal: 6.13s\tremaining: 476ms\n",
      "928:\tlearn: 0.0862110\ttotal: 6.14s\tremaining: 469ms\n",
      "929:\tlearn: 0.0861888\ttotal: 6.14s\tremaining: 463ms\n",
      "930:\tlearn: 0.0861720\ttotal: 6.15s\tremaining: 456ms\n",
      "931:\tlearn: 0.0861652\ttotal: 6.16s\tremaining: 449ms\n",
      "932:\tlearn: 0.0861571\ttotal: 6.16s\tremaining: 442ms\n",
      "933:\tlearn: 0.0861371\ttotal: 6.17s\tremaining: 436ms\n",
      "934:\tlearn: 0.0861296\ttotal: 6.17s\tremaining: 429ms\n",
      "935:\tlearn: 0.0861211\ttotal: 6.18s\tremaining: 422ms\n",
      "936:\tlearn: 0.0861116\ttotal: 6.18s\tremaining: 416ms\n",
      "937:\tlearn: 0.0861043\ttotal: 6.19s\tremaining: 409ms\n",
      "938:\tlearn: 0.0860984\ttotal: 6.2s\tremaining: 403ms\n",
      "939:\tlearn: 0.0860814\ttotal: 6.21s\tremaining: 396ms\n",
      "940:\tlearn: 0.0860731\ttotal: 6.21s\tremaining: 390ms\n",
      "941:\tlearn: 0.0860543\ttotal: 6.22s\tremaining: 383ms\n",
      "942:\tlearn: 0.0860478\ttotal: 6.23s\tremaining: 377ms\n",
      "943:\tlearn: 0.0860381\ttotal: 6.24s\tremaining: 370ms\n",
      "944:\tlearn: 0.0860285\ttotal: 6.24s\tremaining: 363ms\n",
      "945:\tlearn: 0.0860211\ttotal: 6.25s\tremaining: 357ms\n",
      "946:\tlearn: 0.0860153\ttotal: 6.25s\tremaining: 350ms\n",
      "947:\tlearn: 0.0859907\ttotal: 6.26s\tremaining: 343ms\n",
      "948:\tlearn: 0.0859727\ttotal: 6.26s\tremaining: 337ms\n",
      "949:\tlearn: 0.0859695\ttotal: 6.27s\tremaining: 330ms\n",
      "950:\tlearn: 0.0859628\ttotal: 6.27s\tremaining: 323ms\n",
      "951:\tlearn: 0.0859478\ttotal: 6.28s\tremaining: 317ms\n",
      "952:\tlearn: 0.0859188\ttotal: 6.29s\tremaining: 310ms\n",
      "953:\tlearn: 0.0859024\ttotal: 6.29s\tremaining: 304ms\n",
      "954:\tlearn: 0.0858923\ttotal: 6.3s\tremaining: 297ms\n",
      "955:\tlearn: 0.0858876\ttotal: 6.31s\tremaining: 290ms\n",
      "956:\tlearn: 0.0858773\ttotal: 6.31s\tremaining: 284ms\n",
      "957:\tlearn: 0.0858695\ttotal: 6.32s\tremaining: 277ms\n",
      "958:\tlearn: 0.0858605\ttotal: 6.32s\tremaining: 270ms\n",
      "959:\tlearn: 0.0858500\ttotal: 6.33s\tremaining: 264ms\n",
      "960:\tlearn: 0.0858396\ttotal: 6.34s\tremaining: 257ms\n",
      "961:\tlearn: 0.0858204\ttotal: 6.34s\tremaining: 251ms\n",
      "962:\tlearn: 0.0858145\ttotal: 6.35s\tremaining: 244ms\n",
      "963:\tlearn: 0.0858042\ttotal: 6.35s\tremaining: 237ms\n",
      "964:\tlearn: 0.0857977\ttotal: 6.36s\tremaining: 231ms\n",
      "965:\tlearn: 0.0857820\ttotal: 6.36s\tremaining: 224ms\n",
      "966:\tlearn: 0.0857704\ttotal: 6.37s\tremaining: 217ms\n",
      "967:\tlearn: 0.0857524\ttotal: 6.37s\tremaining: 211ms\n",
      "968:\tlearn: 0.0857428\ttotal: 6.38s\tremaining: 204ms\n",
      "969:\tlearn: 0.0857393\ttotal: 6.38s\tremaining: 197ms\n",
      "970:\tlearn: 0.0857311\ttotal: 6.39s\tremaining: 191ms\n",
      "971:\tlearn: 0.0857270\ttotal: 6.39s\tremaining: 184ms\n",
      "972:\tlearn: 0.0857174\ttotal: 6.4s\tremaining: 178ms\n",
      "973:\tlearn: 0.0857063\ttotal: 6.41s\tremaining: 171ms\n",
      "974:\tlearn: 0.0856818\ttotal: 6.41s\tremaining: 164ms\n",
      "975:\tlearn: 0.0856564\ttotal: 6.42s\tremaining: 158ms\n",
      "976:\tlearn: 0.0856490\ttotal: 6.42s\tremaining: 151ms\n",
      "977:\tlearn: 0.0856400\ttotal: 6.43s\tremaining: 145ms\n",
      "978:\tlearn: 0.0856352\ttotal: 6.44s\tremaining: 138ms\n",
      "979:\tlearn: 0.0856214\ttotal: 6.45s\tremaining: 132ms\n",
      "980:\tlearn: 0.0856058\ttotal: 6.45s\tremaining: 125ms\n",
      "981:\tlearn: 0.0855994\ttotal: 6.46s\tremaining: 118ms\n",
      "982:\tlearn: 0.0855941\ttotal: 6.47s\tremaining: 112ms\n",
      "983:\tlearn: 0.0855789\ttotal: 6.48s\tremaining: 105ms\n",
      "984:\tlearn: 0.0855692\ttotal: 6.48s\tremaining: 98.8ms\n",
      "985:\tlearn: 0.0855612\ttotal: 6.49s\tremaining: 92.2ms\n",
      "986:\tlearn: 0.0855306\ttotal: 6.5s\tremaining: 85.6ms\n",
      "987:\tlearn: 0.0855245\ttotal: 6.5s\tremaining: 79ms\n",
      "988:\tlearn: 0.0855101\ttotal: 6.51s\tremaining: 72.4ms\n",
      "989:\tlearn: 0.0855013\ttotal: 6.51s\tremaining: 65.8ms\n",
      "990:\tlearn: 0.0854900\ttotal: 6.52s\tremaining: 59.2ms\n",
      "991:\tlearn: 0.0854833\ttotal: 6.53s\tremaining: 52.6ms\n",
      "992:\tlearn: 0.0854788\ttotal: 6.53s\tremaining: 46ms\n",
      "993:\tlearn: 0.0854639\ttotal: 6.54s\tremaining: 39.5ms\n",
      "994:\tlearn: 0.0854525\ttotal: 6.54s\tremaining: 32.9ms\n",
      "995:\tlearn: 0.0854367\ttotal: 6.55s\tremaining: 26.3ms\n",
      "996:\tlearn: 0.0854310\ttotal: 6.55s\tremaining: 19.7ms\n",
      "997:\tlearn: 0.0854230\ttotal: 6.56s\tremaining: 13.1ms\n",
      "998:\tlearn: 0.0854035\ttotal: 6.56s\tremaining: 6.57ms\n",
      "999:\tlearn: 0.0853989\ttotal: 6.57s\tremaining: 0us\n",
      "0:\tlearn: 0.4372226\ttotal: 5.31ms\tremaining: 5.3s\n",
      "1:\tlearn: 0.4264907\ttotal: 14.1ms\tremaining: 7.01s\n",
      "2:\tlearn: 0.4161370\ttotal: 24ms\tremaining: 7.98s\n",
      "3:\tlearn: 0.4062674\ttotal: 31.4ms\tremaining: 7.82s\n",
      "4:\tlearn: 0.3966883\ttotal: 39.1ms\tremaining: 7.78s\n",
      "5:\tlearn: 0.3874756\ttotal: 44.8ms\tremaining: 7.42s\n",
      "6:\tlearn: 0.3783537\ttotal: 50.4ms\tremaining: 7.14s\n",
      "7:\tlearn: 0.3699848\ttotal: 56.2ms\tremaining: 6.97s\n",
      "8:\tlearn: 0.3617828\ttotal: 65.8ms\tremaining: 7.25s\n",
      "9:\tlearn: 0.3538104\ttotal: 71.8ms\tremaining: 7.11s\n",
      "10:\tlearn: 0.3462601\ttotal: 78.8ms\tremaining: 7.08s\n",
      "11:\tlearn: 0.3385202\ttotal: 86ms\tremaining: 7.08s\n",
      "12:\tlearn: 0.3310348\ttotal: 91.4ms\tremaining: 6.94s\n",
      "13:\tlearn: 0.3238807\ttotal: 96.9ms\tremaining: 6.83s\n",
      "14:\tlearn: 0.3171998\ttotal: 103ms\tremaining: 6.75s\n",
      "15:\tlearn: 0.3102765\ttotal: 111ms\tremaining: 6.81s\n",
      "16:\tlearn: 0.3037226\ttotal: 116ms\tremaining: 6.73s\n",
      "17:\tlearn: 0.2976639\ttotal: 122ms\tremaining: 6.66s\n",
      "18:\tlearn: 0.2917654\ttotal: 128ms\tremaining: 6.61s\n",
      "19:\tlearn: 0.2858532\ttotal: 134ms\tremaining: 6.55s\n",
      "20:\tlearn: 0.2802451\ttotal: 140ms\tremaining: 6.54s\n",
      "21:\tlearn: 0.2747645\ttotal: 147ms\tremaining: 6.54s\n",
      "22:\tlearn: 0.2691889\ttotal: 153ms\tremaining: 6.49s\n",
      "23:\tlearn: 0.2642810\ttotal: 158ms\tremaining: 6.44s\n",
      "24:\tlearn: 0.2594196\ttotal: 164ms\tremaining: 6.4s\n",
      "25:\tlearn: 0.2548537\ttotal: 170ms\tremaining: 6.36s\n",
      "26:\tlearn: 0.2501182\ttotal: 175ms\tremaining: 6.32s\n",
      "27:\tlearn: 0.2455135\ttotal: 181ms\tremaining: 6.3s\n",
      "28:\tlearn: 0.2414000\ttotal: 187ms\tremaining: 6.27s\n",
      "29:\tlearn: 0.2370226\ttotal: 193ms\tremaining: 6.23s\n",
      "30:\tlearn: 0.2331064\ttotal: 199ms\tremaining: 6.22s\n",
      "31:\tlearn: 0.2290382\ttotal: 204ms\tremaining: 6.18s\n",
      "32:\tlearn: 0.2250573\ttotal: 210ms\tremaining: 6.15s\n",
      "33:\tlearn: 0.2214228\ttotal: 215ms\tremaining: 6.12s\n",
      "34:\tlearn: 0.2179541\ttotal: 222ms\tremaining: 6.12s\n",
      "35:\tlearn: 0.2146408\ttotal: 229ms\tremaining: 6.13s\n",
      "36:\tlearn: 0.2112132\ttotal: 234ms\tremaining: 6.1s\n",
      "37:\tlearn: 0.2078805\ttotal: 240ms\tremaining: 6.07s\n",
      "38:\tlearn: 0.2049040\ttotal: 245ms\tremaining: 6.04s\n",
      "39:\tlearn: 0.2020456\ttotal: 251ms\tremaining: 6.02s\n",
      "40:\tlearn: 0.1993194\ttotal: 257ms\tremaining: 6s\n",
      "41:\tlearn: 0.1966850\ttotal: 265ms\tremaining: 6.03s\n",
      "42:\tlearn: 0.1939097\ttotal: 271ms\tremaining: 6.02s\n",
      "43:\tlearn: 0.1914820\ttotal: 277ms\tremaining: 6.02s\n",
      "44:\tlearn: 0.1890148\ttotal: 283ms\tremaining: 6.01s\n",
      "45:\tlearn: 0.1868338\ttotal: 289ms\tremaining: 5.98s\n",
      "46:\tlearn: 0.1846685\ttotal: 294ms\tremaining: 5.96s\n",
      "47:\tlearn: 0.1825807\ttotal: 300ms\tremaining: 5.94s\n",
      "48:\tlearn: 0.1803910\ttotal: 305ms\tremaining: 5.93s\n",
      "49:\tlearn: 0.1781917\ttotal: 312ms\tremaining: 5.92s\n",
      "50:\tlearn: 0.1761856\ttotal: 319ms\tremaining: 5.93s\n",
      "51:\tlearn: 0.1744159\ttotal: 327ms\tremaining: 5.95s\n",
      "52:\tlearn: 0.1725239\ttotal: 334ms\tremaining: 5.96s\n",
      "53:\tlearn: 0.1708429\ttotal: 339ms\tremaining: 5.94s\n",
      "54:\tlearn: 0.1693006\ttotal: 345ms\tremaining: 5.92s\n",
      "55:\tlearn: 0.1674145\ttotal: 360ms\tremaining: 6.06s\n",
      "56:\tlearn: 0.1657132\ttotal: 374ms\tremaining: 6.18s\n",
      "57:\tlearn: 0.1640279\ttotal: 397ms\tremaining: 6.44s\n",
      "58:\tlearn: 0.1625285\ttotal: 420ms\tremaining: 6.7s\n",
      "59:\tlearn: 0.1611633\ttotal: 438ms\tremaining: 6.86s\n",
      "60:\tlearn: 0.1598950\ttotal: 447ms\tremaining: 6.89s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61:\tlearn: 0.1586390\ttotal: 454ms\tremaining: 6.86s\n",
      "62:\tlearn: 0.1573388\ttotal: 460ms\tremaining: 6.83s\n",
      "63:\tlearn: 0.1561976\ttotal: 465ms\tremaining: 6.8s\n",
      "64:\tlearn: 0.1548353\ttotal: 470ms\tremaining: 6.76s\n",
      "65:\tlearn: 0.1536223\ttotal: 475ms\tremaining: 6.72s\n",
      "66:\tlearn: 0.1523601\ttotal: 481ms\tremaining: 6.7s\n",
      "67:\tlearn: 0.1513194\ttotal: 486ms\tremaining: 6.67s\n",
      "68:\tlearn: 0.1502959\ttotal: 492ms\tremaining: 6.63s\n",
      "69:\tlearn: 0.1494071\ttotal: 500ms\tremaining: 6.65s\n",
      "70:\tlearn: 0.1484282\ttotal: 506ms\tremaining: 6.62s\n",
      "71:\tlearn: 0.1475423\ttotal: 512ms\tremaining: 6.6s\n",
      "72:\tlearn: 0.1465284\ttotal: 518ms\tremaining: 6.57s\n",
      "73:\tlearn: 0.1456124\ttotal: 523ms\tremaining: 6.54s\n",
      "74:\tlearn: 0.1447561\ttotal: 528ms\tremaining: 6.51s\n",
      "75:\tlearn: 0.1437109\ttotal: 540ms\tremaining: 6.57s\n",
      "76:\tlearn: 0.1430038\ttotal: 546ms\tremaining: 6.54s\n",
      "77:\tlearn: 0.1422957\ttotal: 551ms\tremaining: 6.52s\n",
      "78:\tlearn: 0.1416402\ttotal: 557ms\tremaining: 6.49s\n",
      "79:\tlearn: 0.1409915\ttotal: 563ms\tremaining: 6.47s\n",
      "80:\tlearn: 0.1403429\ttotal: 568ms\tremaining: 6.44s\n",
      "81:\tlearn: 0.1397362\ttotal: 575ms\tremaining: 6.44s\n",
      "82:\tlearn: 0.1391207\ttotal: 586ms\tremaining: 6.48s\n",
      "83:\tlearn: 0.1385146\ttotal: 594ms\tremaining: 6.47s\n",
      "84:\tlearn: 0.1379705\ttotal: 599ms\tremaining: 6.45s\n",
      "85:\tlearn: 0.1373477\ttotal: 604ms\tremaining: 6.42s\n",
      "86:\tlearn: 0.1367052\ttotal: 610ms\tremaining: 6.4s\n",
      "87:\tlearn: 0.1359913\ttotal: 617ms\tremaining: 6.4s\n",
      "88:\tlearn: 0.1353672\ttotal: 625ms\tremaining: 6.4s\n",
      "89:\tlearn: 0.1348197\ttotal: 631ms\tremaining: 6.38s\n",
      "90:\tlearn: 0.1341381\ttotal: 636ms\tremaining: 6.36s\n",
      "91:\tlearn: 0.1336451\ttotal: 641ms\tremaining: 6.33s\n",
      "92:\tlearn: 0.1331895\ttotal: 647ms\tremaining: 6.31s\n",
      "93:\tlearn: 0.1327642\ttotal: 653ms\tremaining: 6.29s\n",
      "94:\tlearn: 0.1322136\ttotal: 659ms\tremaining: 6.28s\n",
      "95:\tlearn: 0.1316942\ttotal: 673ms\tremaining: 6.34s\n",
      "96:\tlearn: 0.1313400\ttotal: 678ms\tremaining: 6.31s\n",
      "97:\tlearn: 0.1309755\ttotal: 684ms\tremaining: 6.29s\n",
      "98:\tlearn: 0.1305372\ttotal: 689ms\tremaining: 6.27s\n",
      "99:\tlearn: 0.1299990\ttotal: 696ms\tremaining: 6.26s\n",
      "100:\tlearn: 0.1296327\ttotal: 705ms\tremaining: 6.28s\n",
      "101:\tlearn: 0.1292046\ttotal: 720ms\tremaining: 6.34s\n",
      "102:\tlearn: 0.1288753\ttotal: 728ms\tremaining: 6.34s\n",
      "103:\tlearn: 0.1283724\ttotal: 734ms\tremaining: 6.33s\n",
      "104:\tlearn: 0.1280572\ttotal: 740ms\tremaining: 6.3s\n",
      "105:\tlearn: 0.1275502\ttotal: 752ms\tremaining: 6.34s\n",
      "106:\tlearn: 0.1271439\ttotal: 762ms\tremaining: 6.36s\n",
      "107:\tlearn: 0.1268162\ttotal: 767ms\tremaining: 6.34s\n",
      "108:\tlearn: 0.1264581\ttotal: 774ms\tremaining: 6.32s\n",
      "109:\tlearn: 0.1259315\ttotal: 779ms\tremaining: 6.3s\n",
      "110:\tlearn: 0.1256673\ttotal: 784ms\tremaining: 6.28s\n",
      "111:\tlearn: 0.1253099\ttotal: 794ms\tremaining: 6.29s\n",
      "112:\tlearn: 0.1249710\ttotal: 805ms\tremaining: 6.32s\n",
      "113:\tlearn: 0.1247241\ttotal: 811ms\tremaining: 6.3s\n",
      "114:\tlearn: 0.1244583\ttotal: 816ms\tremaining: 6.28s\n",
      "115:\tlearn: 0.1242245\ttotal: 822ms\tremaining: 6.26s\n",
      "116:\tlearn: 0.1239960\ttotal: 838ms\tremaining: 6.33s\n",
      "117:\tlearn: 0.1236418\ttotal: 848ms\tremaining: 6.34s\n",
      "118:\tlearn: 0.1232692\ttotal: 854ms\tremaining: 6.32s\n",
      "119:\tlearn: 0.1230759\ttotal: 860ms\tremaining: 6.3s\n",
      "120:\tlearn: 0.1228318\ttotal: 865ms\tremaining: 6.28s\n",
      "121:\tlearn: 0.1226514\ttotal: 870ms\tremaining: 6.26s\n",
      "122:\tlearn: 0.1223717\ttotal: 877ms\tremaining: 6.25s\n",
      "123:\tlearn: 0.1219093\ttotal: 893ms\tremaining: 6.31s\n",
      "124:\tlearn: 0.1216293\ttotal: 898ms\tremaining: 6.29s\n",
      "125:\tlearn: 0.1214427\ttotal: 904ms\tremaining: 6.27s\n",
      "126:\tlearn: 0.1211739\ttotal: 910ms\tremaining: 6.25s\n",
      "127:\tlearn: 0.1208200\ttotal: 916ms\tremaining: 6.24s\n",
      "128:\tlearn: 0.1205958\ttotal: 927ms\tremaining: 6.26s\n",
      "129:\tlearn: 0.1204067\ttotal: 938ms\tremaining: 6.28s\n",
      "130:\tlearn: 0.1201678\ttotal: 944ms\tremaining: 6.26s\n",
      "131:\tlearn: 0.1200095\ttotal: 950ms\tremaining: 6.25s\n",
      "132:\tlearn: 0.1196686\ttotal: 957ms\tremaining: 6.24s\n",
      "133:\tlearn: 0.1193516\ttotal: 963ms\tremaining: 6.22s\n",
      "134:\tlearn: 0.1191181\ttotal: 981ms\tremaining: 6.28s\n",
      "135:\tlearn: 0.1189302\ttotal: 986ms\tremaining: 6.26s\n",
      "136:\tlearn: 0.1187209\ttotal: 994ms\tremaining: 6.26s\n",
      "137:\tlearn: 0.1185793\ttotal: 1000ms\tremaining: 6.24s\n",
      "138:\tlearn: 0.1184482\ttotal: 1s\tremaining: 6.22s\n",
      "139:\tlearn: 0.1183300\ttotal: 1.02s\tremaining: 6.28s\n",
      "140:\tlearn: 0.1181967\ttotal: 1.03s\tremaining: 6.26s\n",
      "141:\tlearn: 0.1178413\ttotal: 1.03s\tremaining: 6.25s\n",
      "142:\tlearn: 0.1175782\ttotal: 1.04s\tremaining: 6.23s\n",
      "143:\tlearn: 0.1174537\ttotal: 1.04s\tremaining: 6.21s\n",
      "144:\tlearn: 0.1171827\ttotal: 1.06s\tremaining: 6.23s\n",
      "145:\tlearn: 0.1170672\ttotal: 1.07s\tremaining: 6.25s\n",
      "146:\tlearn: 0.1168961\ttotal: 1.07s\tremaining: 6.24s\n",
      "147:\tlearn: 0.1166844\ttotal: 1.08s\tremaining: 6.23s\n",
      "148:\tlearn: 0.1165203\ttotal: 1.09s\tremaining: 6.21s\n",
      "149:\tlearn: 0.1162407\ttotal: 1.09s\tremaining: 6.2s\n",
      "150:\tlearn: 0.1160258\ttotal: 1.11s\tremaining: 6.23s\n",
      "151:\tlearn: 0.1158452\ttotal: 1.11s\tremaining: 6.22s\n",
      "152:\tlearn: 0.1156543\ttotal: 1.12s\tremaining: 6.2s\n",
      "153:\tlearn: 0.1155066\ttotal: 1.13s\tremaining: 6.18s\n",
      "154:\tlearn: 0.1153791\ttotal: 1.13s\tremaining: 6.17s\n",
      "155:\tlearn: 0.1151384\ttotal: 1.14s\tremaining: 6.17s\n",
      "156:\tlearn: 0.1149807\ttotal: 1.15s\tremaining: 6.2s\n",
      "157:\tlearn: 0.1148753\ttotal: 1.16s\tremaining: 6.18s\n",
      "158:\tlearn: 0.1147349\ttotal: 1.17s\tremaining: 6.17s\n",
      "159:\tlearn: 0.1146131\ttotal: 1.17s\tremaining: 6.15s\n",
      "160:\tlearn: 0.1145079\ttotal: 1.18s\tremaining: 6.13s\n",
      "161:\tlearn: 0.1142882\ttotal: 1.19s\tremaining: 6.13s\n",
      "162:\tlearn: 0.1142163\ttotal: 1.2s\tremaining: 6.15s\n",
      "163:\tlearn: 0.1140096\ttotal: 1.2s\tremaining: 6.14s\n",
      "164:\tlearn: 0.1138182\ttotal: 1.21s\tremaining: 6.12s\n",
      "165:\tlearn: 0.1137053\ttotal: 1.22s\tremaining: 6.11s\n",
      "166:\tlearn: 0.1135088\ttotal: 1.22s\tremaining: 6.09s\n",
      "167:\tlearn: 0.1134199\ttotal: 1.23s\tremaining: 6.09s\n",
      "168:\tlearn: 0.1131845\ttotal: 1.24s\tremaining: 6.11s\n",
      "169:\tlearn: 0.1131013\ttotal: 1.25s\tremaining: 6.09s\n",
      "170:\tlearn: 0.1128546\ttotal: 1.25s\tremaining: 6.08s\n",
      "171:\tlearn: 0.1127071\ttotal: 1.26s\tremaining: 6.07s\n",
      "172:\tlearn: 0.1125214\ttotal: 1.27s\tremaining: 6.05s\n",
      "173:\tlearn: 0.1124468\ttotal: 1.28s\tremaining: 6.08s\n",
      "174:\tlearn: 0.1122578\ttotal: 1.29s\tremaining: 6.06s\n",
      "175:\tlearn: 0.1120832\ttotal: 1.29s\tremaining: 6.05s\n",
      "176:\tlearn: 0.1119203\ttotal: 1.3s\tremaining: 6.04s\n",
      "177:\tlearn: 0.1117225\ttotal: 1.31s\tremaining: 6.03s\n",
      "178:\tlearn: 0.1116019\ttotal: 1.33s\tremaining: 6.1s\n",
      "179:\tlearn: 0.1115030\ttotal: 1.34s\tremaining: 6.11s\n",
      "180:\tlearn: 0.1113860\ttotal: 1.35s\tremaining: 6.13s\n",
      "181:\tlearn: 0.1112113\ttotal: 1.38s\tremaining: 6.19s\n",
      "182:\tlearn: 0.1110805\ttotal: 1.39s\tremaining: 6.2s\n",
      "183:\tlearn: 0.1109977\ttotal: 1.4s\tremaining: 6.21s\n",
      "184:\tlearn: 0.1108447\ttotal: 1.42s\tremaining: 6.27s\n",
      "185:\tlearn: 0.1107335\ttotal: 1.43s\tremaining: 6.25s\n",
      "186:\tlearn: 0.1106194\ttotal: 1.44s\tremaining: 6.24s\n",
      "187:\tlearn: 0.1104855\ttotal: 1.44s\tremaining: 6.22s\n",
      "188:\tlearn: 0.1103237\ttotal: 1.45s\tremaining: 6.2s\n",
      "189:\tlearn: 0.1102112\ttotal: 1.46s\tremaining: 6.21s\n",
      "190:\tlearn: 0.1100964\ttotal: 1.47s\tremaining: 6.21s\n",
      "191:\tlearn: 0.1099485\ttotal: 1.47s\tremaining: 6.19s\n",
      "192:\tlearn: 0.1097589\ttotal: 1.48s\tremaining: 6.17s\n",
      "193:\tlearn: 0.1096576\ttotal: 1.48s\tremaining: 6.16s\n",
      "194:\tlearn: 0.1095093\ttotal: 1.49s\tremaining: 6.14s\n",
      "195:\tlearn: 0.1093869\ttotal: 1.49s\tremaining: 6.13s\n",
      "196:\tlearn: 0.1093403\ttotal: 1.5s\tremaining: 6.13s\n",
      "197:\tlearn: 0.1092442\ttotal: 1.51s\tremaining: 6.13s\n",
      "198:\tlearn: 0.1090542\ttotal: 1.52s\tremaining: 6.11s\n",
      "199:\tlearn: 0.1089893\ttotal: 1.52s\tremaining: 6.1s\n",
      "200:\tlearn: 0.1088825\ttotal: 1.53s\tremaining: 6.08s\n",
      "201:\tlearn: 0.1088255\ttotal: 1.54s\tremaining: 6.07s\n",
      "202:\tlearn: 0.1087778\ttotal: 1.55s\tremaining: 6.08s\n",
      "203:\tlearn: 0.1087217\ttotal: 1.55s\tremaining: 6.06s\n",
      "204:\tlearn: 0.1085760\ttotal: 1.56s\tremaining: 6.05s\n",
      "205:\tlearn: 0.1085089\ttotal: 1.56s\tremaining: 6.03s\n",
      "206:\tlearn: 0.1083728\ttotal: 1.57s\tremaining: 6.03s\n",
      "207:\tlearn: 0.1083085\ttotal: 1.58s\tremaining: 6.02s\n",
      "208:\tlearn: 0.1082471\ttotal: 1.59s\tremaining: 6.03s\n",
      "209:\tlearn: 0.1081218\ttotal: 1.6s\tremaining: 6.01s\n",
      "210:\tlearn: 0.1080688\ttotal: 1.6s\tremaining: 6s\n",
      "211:\tlearn: 0.1080101\ttotal: 1.61s\tremaining: 5.99s\n",
      "212:\tlearn: 0.1079478\ttotal: 1.61s\tremaining: 5.97s\n",
      "213:\tlearn: 0.1079071\ttotal: 1.63s\tremaining: 5.97s\n",
      "214:\tlearn: 0.1078618\ttotal: 1.64s\tremaining: 5.99s\n",
      "215:\tlearn: 0.1078066\ttotal: 1.65s\tremaining: 5.98s\n",
      "216:\tlearn: 0.1077421\ttotal: 1.65s\tremaining: 5.96s\n",
      "217:\tlearn: 0.1076512\ttotal: 1.66s\tremaining: 5.95s\n",
      "218:\tlearn: 0.1075887\ttotal: 1.66s\tremaining: 5.93s\n",
      "219:\tlearn: 0.1074621\ttotal: 1.67s\tremaining: 5.94s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220:\tlearn: 0.1073831\ttotal: 1.68s\tremaining: 5.92s\n",
      "221:\tlearn: 0.1073009\ttotal: 1.69s\tremaining: 5.91s\n",
      "222:\tlearn: 0.1072381\ttotal: 1.69s\tremaining: 5.9s\n",
      "223:\tlearn: 0.1071274\ttotal: 1.7s\tremaining: 5.89s\n",
      "224:\tlearn: 0.1070472\ttotal: 1.71s\tremaining: 5.89s\n",
      "225:\tlearn: 0.1070025\ttotal: 1.72s\tremaining: 5.89s\n",
      "226:\tlearn: 0.1069752\ttotal: 1.73s\tremaining: 5.88s\n",
      "227:\tlearn: 0.1068564\ttotal: 1.73s\tremaining: 5.87s\n",
      "228:\tlearn: 0.1067345\ttotal: 1.74s\tremaining: 5.85s\n",
      "229:\tlearn: 0.1066702\ttotal: 1.74s\tremaining: 5.84s\n",
      "230:\tlearn: 0.1065649\ttotal: 1.75s\tremaining: 5.83s\n",
      "231:\tlearn: 0.1065024\ttotal: 1.76s\tremaining: 5.82s\n",
      "232:\tlearn: 0.1064266\ttotal: 1.77s\tremaining: 5.83s\n",
      "233:\tlearn: 0.1063001\ttotal: 1.78s\tremaining: 5.82s\n",
      "234:\tlearn: 0.1062594\ttotal: 1.78s\tremaining: 5.8s\n",
      "235:\tlearn: 0.1061615\ttotal: 1.79s\tremaining: 5.79s\n",
      "236:\tlearn: 0.1061127\ttotal: 1.79s\tremaining: 5.78s\n",
      "237:\tlearn: 0.1059420\ttotal: 1.81s\tremaining: 5.79s\n",
      "238:\tlearn: 0.1058340\ttotal: 1.81s\tremaining: 5.78s\n",
      "239:\tlearn: 0.1057607\ttotal: 1.82s\tremaining: 5.76s\n",
      "240:\tlearn: 0.1056815\ttotal: 1.82s\tremaining: 5.75s\n",
      "241:\tlearn: 0.1056217\ttotal: 1.83s\tremaining: 5.74s\n",
      "242:\tlearn: 0.1055879\ttotal: 1.84s\tremaining: 5.72s\n",
      "243:\tlearn: 0.1055402\ttotal: 1.85s\tremaining: 5.74s\n",
      "244:\tlearn: 0.1054455\ttotal: 1.86s\tremaining: 5.73s\n",
      "245:\tlearn: 0.1053978\ttotal: 1.86s\tremaining: 5.71s\n",
      "246:\tlearn: 0.1053572\ttotal: 1.87s\tremaining: 5.7s\n",
      "247:\tlearn: 0.1052422\ttotal: 1.87s\tremaining: 5.68s\n",
      "248:\tlearn: 0.1051965\ttotal: 1.88s\tremaining: 5.67s\n",
      "249:\tlearn: 0.1051295\ttotal: 1.9s\tremaining: 5.69s\n",
      "250:\tlearn: 0.1050147\ttotal: 1.91s\tremaining: 5.71s\n",
      "251:\tlearn: 0.1049524\ttotal: 1.92s\tremaining: 5.71s\n",
      "252:\tlearn: 0.1048146\ttotal: 1.95s\tremaining: 5.75s\n",
      "253:\tlearn: 0.1047583\ttotal: 1.95s\tremaining: 5.74s\n",
      "254:\tlearn: 0.1047088\ttotal: 1.96s\tremaining: 5.72s\n",
      "255:\tlearn: 0.1046697\ttotal: 1.96s\tremaining: 5.71s\n",
      "256:\tlearn: 0.1045017\ttotal: 1.97s\tremaining: 5.7s\n",
      "257:\tlearn: 0.1044666\ttotal: 1.99s\tremaining: 5.72s\n",
      "258:\tlearn: 0.1043846\ttotal: 2s\tremaining: 5.71s\n",
      "259:\tlearn: 0.1042841\ttotal: 2s\tremaining: 5.7s\n",
      "260:\tlearn: 0.1041719\ttotal: 2.01s\tremaining: 5.69s\n",
      "261:\tlearn: 0.1040470\ttotal: 2.02s\tremaining: 5.7s\n",
      "262:\tlearn: 0.1040223\ttotal: 2.03s\tremaining: 5.7s\n",
      "263:\tlearn: 0.1039611\ttotal: 2.04s\tremaining: 5.68s\n",
      "264:\tlearn: 0.1038782\ttotal: 2.04s\tremaining: 5.67s\n",
      "265:\tlearn: 0.1037910\ttotal: 2.05s\tremaining: 5.66s\n",
      "266:\tlearn: 0.1037545\ttotal: 2.06s\tremaining: 5.64s\n",
      "267:\tlearn: 0.1037088\ttotal: 2.07s\tremaining: 5.64s\n",
      "268:\tlearn: 0.1036295\ttotal: 2.08s\tremaining: 5.65s\n",
      "269:\tlearn: 0.1035959\ttotal: 2.08s\tremaining: 5.63s\n",
      "270:\tlearn: 0.1035276\ttotal: 2.09s\tremaining: 5.62s\n",
      "271:\tlearn: 0.1034675\ttotal: 2.1s\tremaining: 5.61s\n",
      "272:\tlearn: 0.1034402\ttotal: 2.11s\tremaining: 5.61s\n",
      "273:\tlearn: 0.1033919\ttotal: 2.12s\tremaining: 5.62s\n",
      "274:\tlearn: 0.1033406\ttotal: 2.13s\tremaining: 5.62s\n",
      "275:\tlearn: 0.1032706\ttotal: 2.14s\tremaining: 5.61s\n",
      "276:\tlearn: 0.1031893\ttotal: 2.14s\tremaining: 5.6s\n",
      "277:\tlearn: 0.1030999\ttotal: 2.15s\tremaining: 5.59s\n",
      "278:\tlearn: 0.1029619\ttotal: 2.17s\tremaining: 5.6s\n",
      "279:\tlearn: 0.1029195\ttotal: 2.17s\tremaining: 5.58s\n",
      "280:\tlearn: 0.1028412\ttotal: 2.18s\tremaining: 5.59s\n",
      "281:\tlearn: 0.1027655\ttotal: 2.19s\tremaining: 5.58s\n",
      "282:\tlearn: 0.1027403\ttotal: 2.21s\tremaining: 5.59s\n",
      "283:\tlearn: 0.1026989\ttotal: 2.21s\tremaining: 5.58s\n",
      "284:\tlearn: 0.1025875\ttotal: 2.22s\tremaining: 5.57s\n",
      "285:\tlearn: 0.1025394\ttotal: 2.22s\tremaining: 5.55s\n",
      "286:\tlearn: 0.1024635\ttotal: 2.23s\tremaining: 5.54s\n",
      "287:\tlearn: 0.1024034\ttotal: 2.23s\tremaining: 5.53s\n",
      "288:\tlearn: 0.1023504\ttotal: 2.25s\tremaining: 5.53s\n",
      "289:\tlearn: 0.1022994\ttotal: 2.26s\tremaining: 5.53s\n",
      "290:\tlearn: 0.1022050\ttotal: 2.27s\tremaining: 5.53s\n",
      "291:\tlearn: 0.1021088\ttotal: 2.27s\tremaining: 5.52s\n",
      "292:\tlearn: 0.1020395\ttotal: 2.29s\tremaining: 5.52s\n",
      "293:\tlearn: 0.1019567\ttotal: 2.3s\tremaining: 5.53s\n",
      "294:\tlearn: 0.1019045\ttotal: 2.32s\tremaining: 5.54s\n",
      "295:\tlearn: 0.1018146\ttotal: 2.37s\tremaining: 5.63s\n",
      "296:\tlearn: 0.1017352\ttotal: 2.39s\tremaining: 5.66s\n",
      "297:\tlearn: 0.1016848\ttotal: 2.4s\tremaining: 5.66s\n",
      "298:\tlearn: 0.1016533\ttotal: 2.41s\tremaining: 5.66s\n",
      "299:\tlearn: 0.1016176\ttotal: 2.42s\tremaining: 5.65s\n",
      "300:\tlearn: 0.1015717\ttotal: 2.44s\tremaining: 5.66s\n",
      "301:\tlearn: 0.1014997\ttotal: 2.44s\tremaining: 5.65s\n",
      "302:\tlearn: 0.1014668\ttotal: 2.45s\tremaining: 5.63s\n",
      "303:\tlearn: 0.1014368\ttotal: 2.45s\tremaining: 5.62s\n",
      "304:\tlearn: 0.1013778\ttotal: 2.46s\tremaining: 5.61s\n",
      "305:\tlearn: 0.1013137\ttotal: 2.47s\tremaining: 5.6s\n",
      "306:\tlearn: 0.1012127\ttotal: 2.48s\tremaining: 5.59s\n",
      "307:\tlearn: 0.1011434\ttotal: 2.48s\tremaining: 5.58s\n",
      "308:\tlearn: 0.1011094\ttotal: 2.49s\tremaining: 5.57s\n",
      "309:\tlearn: 0.1010670\ttotal: 2.5s\tremaining: 5.56s\n",
      "310:\tlearn: 0.1010422\ttotal: 2.5s\tremaining: 5.54s\n",
      "311:\tlearn: 0.1010168\ttotal: 2.51s\tremaining: 5.54s\n",
      "312:\tlearn: 0.1009021\ttotal: 2.52s\tremaining: 5.54s\n",
      "313:\tlearn: 0.1008104\ttotal: 2.53s\tremaining: 5.52s\n",
      "314:\tlearn: 0.1007789\ttotal: 2.53s\tremaining: 5.51s\n",
      "315:\tlearn: 0.1007019\ttotal: 2.54s\tremaining: 5.5s\n",
      "316:\tlearn: 0.1006403\ttotal: 2.54s\tremaining: 5.48s\n",
      "317:\tlearn: 0.1005831\ttotal: 2.56s\tremaining: 5.49s\n",
      "318:\tlearn: 0.1005635\ttotal: 2.56s\tremaining: 5.48s\n",
      "319:\tlearn: 0.1004593\ttotal: 2.57s\tremaining: 5.46s\n",
      "320:\tlearn: 0.1003978\ttotal: 2.58s\tremaining: 5.45s\n",
      "321:\tlearn: 0.1003169\ttotal: 2.58s\tremaining: 5.44s\n",
      "322:\tlearn: 0.1002446\ttotal: 2.59s\tremaining: 5.43s\n",
      "323:\tlearn: 0.1002134\ttotal: 2.6s\tremaining: 5.44s\n",
      "324:\tlearn: 0.1001747\ttotal: 2.61s\tremaining: 5.42s\n",
      "325:\tlearn: 0.1000897\ttotal: 2.62s\tremaining: 5.41s\n",
      "326:\tlearn: 0.1000443\ttotal: 2.62s\tremaining: 5.4s\n",
      "327:\tlearn: 0.0999987\ttotal: 2.63s\tremaining: 5.38s\n",
      "328:\tlearn: 0.0999451\ttotal: 2.63s\tremaining: 5.37s\n",
      "329:\tlearn: 0.0998687\ttotal: 2.65s\tremaining: 5.38s\n",
      "330:\tlearn: 0.0998108\ttotal: 2.65s\tremaining: 5.37s\n",
      "331:\tlearn: 0.0997180\ttotal: 2.66s\tremaining: 5.35s\n",
      "332:\tlearn: 0.0996557\ttotal: 2.67s\tremaining: 5.34s\n",
      "333:\tlearn: 0.0996113\ttotal: 2.67s\tremaining: 5.33s\n",
      "334:\tlearn: 0.0995734\ttotal: 2.68s\tremaining: 5.32s\n",
      "335:\tlearn: 0.0994629\ttotal: 2.69s\tremaining: 5.33s\n",
      "336:\tlearn: 0.0994447\ttotal: 2.7s\tremaining: 5.32s\n",
      "337:\tlearn: 0.0993580\ttotal: 2.71s\tremaining: 5.31s\n",
      "338:\tlearn: 0.0993030\ttotal: 2.71s\tremaining: 5.29s\n",
      "339:\tlearn: 0.0992503\ttotal: 2.72s\tremaining: 5.28s\n",
      "340:\tlearn: 0.0991814\ttotal: 2.73s\tremaining: 5.28s\n",
      "341:\tlearn: 0.0991350\ttotal: 2.74s\tremaining: 5.27s\n",
      "342:\tlearn: 0.0990596\ttotal: 2.74s\tremaining: 5.25s\n",
      "343:\tlearn: 0.0990318\ttotal: 2.75s\tremaining: 5.24s\n",
      "344:\tlearn: 0.0989640\ttotal: 2.75s\tremaining: 5.23s\n",
      "345:\tlearn: 0.0988879\ttotal: 2.76s\tremaining: 5.22s\n",
      "346:\tlearn: 0.0988447\ttotal: 2.78s\tremaining: 5.23s\n",
      "347:\tlearn: 0.0987748\ttotal: 2.79s\tremaining: 5.22s\n",
      "348:\tlearn: 0.0987238\ttotal: 2.79s\tremaining: 5.21s\n",
      "349:\tlearn: 0.0986701\ttotal: 2.8s\tremaining: 5.2s\n",
      "350:\tlearn: 0.0986344\ttotal: 2.81s\tremaining: 5.19s\n",
      "351:\tlearn: 0.0985312\ttotal: 2.82s\tremaining: 5.19s\n",
      "352:\tlearn: 0.0984927\ttotal: 2.83s\tremaining: 5.18s\n",
      "353:\tlearn: 0.0984371\ttotal: 2.83s\tremaining: 5.17s\n",
      "354:\tlearn: 0.0983831\ttotal: 2.84s\tremaining: 5.16s\n",
      "355:\tlearn: 0.0983507\ttotal: 2.85s\tremaining: 5.15s\n",
      "356:\tlearn: 0.0982926\ttotal: 2.85s\tremaining: 5.13s\n",
      "357:\tlearn: 0.0982339\ttotal: 2.86s\tremaining: 5.14s\n",
      "358:\tlearn: 0.0982106\ttotal: 2.87s\tremaining: 5.13s\n",
      "359:\tlearn: 0.0981568\ttotal: 2.88s\tremaining: 5.11s\n",
      "360:\tlearn: 0.0980818\ttotal: 2.88s\tremaining: 5.1s\n",
      "361:\tlearn: 0.0980138\ttotal: 2.89s\tremaining: 5.09s\n",
      "362:\tlearn: 0.0979536\ttotal: 2.9s\tremaining: 5.08s\n",
      "363:\tlearn: 0.0979234\ttotal: 2.91s\tremaining: 5.08s\n",
      "364:\tlearn: 0.0978864\ttotal: 2.92s\tremaining: 5.07s\n",
      "365:\tlearn: 0.0978627\ttotal: 2.92s\tremaining: 5.06s\n",
      "366:\tlearn: 0.0978295\ttotal: 2.93s\tremaining: 5.05s\n",
      "367:\tlearn: 0.0977774\ttotal: 2.93s\tremaining: 5.04s\n",
      "368:\tlearn: 0.0976924\ttotal: 2.94s\tremaining: 5.02s\n",
      "369:\tlearn: 0.0976191\ttotal: 2.95s\tremaining: 5.03s\n",
      "370:\tlearn: 0.0975809\ttotal: 2.96s\tremaining: 5.01s\n",
      "371:\tlearn: 0.0975396\ttotal: 2.96s\tremaining: 5s\n",
      "372:\tlearn: 0.0974804\ttotal: 2.97s\tremaining: 4.99s\n",
      "373:\tlearn: 0.0974469\ttotal: 2.98s\tremaining: 4.98s\n",
      "374:\tlearn: 0.0973993\ttotal: 2.98s\tremaining: 4.97s\n",
      "375:\tlearn: 0.0973794\ttotal: 3s\tremaining: 4.97s\n",
      "376:\tlearn: 0.0973294\ttotal: 3s\tremaining: 4.96s\n",
      "377:\tlearn: 0.0972769\ttotal: 3.01s\tremaining: 4.95s\n",
      "378:\tlearn: 0.0972373\ttotal: 3.02s\tremaining: 4.94s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379:\tlearn: 0.0972005\ttotal: 3.02s\tremaining: 4.93s\n",
      "380:\tlearn: 0.0971830\ttotal: 3.03s\tremaining: 4.92s\n",
      "381:\tlearn: 0.0971541\ttotal: 3.05s\tremaining: 4.93s\n",
      "382:\tlearn: 0.0971039\ttotal: 3.05s\tremaining: 4.92s\n",
      "383:\tlearn: 0.0970669\ttotal: 3.06s\tremaining: 4.91s\n",
      "384:\tlearn: 0.0970041\ttotal: 3.08s\tremaining: 4.91s\n",
      "385:\tlearn: 0.0969482\ttotal: 3.09s\tremaining: 4.91s\n",
      "386:\tlearn: 0.0969236\ttotal: 3.1s\tremaining: 4.91s\n",
      "387:\tlearn: 0.0968948\ttotal: 3.1s\tremaining: 4.89s\n",
      "388:\tlearn: 0.0968461\ttotal: 3.11s\tremaining: 4.88s\n",
      "389:\tlearn: 0.0968055\ttotal: 3.11s\tremaining: 4.87s\n",
      "390:\tlearn: 0.0967746\ttotal: 3.13s\tremaining: 4.87s\n",
      "391:\tlearn: 0.0966982\ttotal: 3.13s\tremaining: 4.86s\n",
      "392:\tlearn: 0.0966740\ttotal: 3.14s\tremaining: 4.85s\n",
      "393:\tlearn: 0.0966542\ttotal: 3.15s\tremaining: 4.84s\n",
      "394:\tlearn: 0.0966231\ttotal: 3.15s\tremaining: 4.83s\n",
      "395:\tlearn: 0.0965778\ttotal: 3.16s\tremaining: 4.82s\n",
      "396:\tlearn: 0.0965421\ttotal: 3.17s\tremaining: 4.82s\n",
      "397:\tlearn: 0.0964697\ttotal: 3.19s\tremaining: 4.82s\n",
      "398:\tlearn: 0.0964044\ttotal: 3.19s\tremaining: 4.81s\n",
      "399:\tlearn: 0.0963647\ttotal: 3.2s\tremaining: 4.8s\n",
      "400:\tlearn: 0.0963319\ttotal: 3.22s\tremaining: 4.81s\n",
      "401:\tlearn: 0.0962968\ttotal: 3.23s\tremaining: 4.8s\n",
      "402:\tlearn: 0.0962670\ttotal: 3.23s\tremaining: 4.79s\n",
      "403:\tlearn: 0.0962417\ttotal: 3.24s\tremaining: 4.78s\n",
      "404:\tlearn: 0.0962078\ttotal: 3.24s\tremaining: 4.76s\n",
      "405:\tlearn: 0.0961185\ttotal: 3.25s\tremaining: 4.76s\n",
      "406:\tlearn: 0.0960848\ttotal: 3.26s\tremaining: 4.75s\n",
      "407:\tlearn: 0.0960163\ttotal: 3.27s\tremaining: 4.74s\n",
      "408:\tlearn: 0.0959942\ttotal: 3.27s\tremaining: 4.73s\n",
      "409:\tlearn: 0.0959616\ttotal: 3.28s\tremaining: 4.72s\n",
      "410:\tlearn: 0.0958978\ttotal: 3.29s\tremaining: 4.71s\n",
      "411:\tlearn: 0.0958483\ttotal: 3.31s\tremaining: 4.72s\n",
      "412:\tlearn: 0.0958189\ttotal: 3.32s\tremaining: 4.72s\n",
      "413:\tlearn: 0.0957605\ttotal: 3.36s\tremaining: 4.75s\n",
      "414:\tlearn: 0.0957360\ttotal: 3.37s\tremaining: 4.75s\n",
      "415:\tlearn: 0.0957040\ttotal: 3.38s\tremaining: 4.75s\n",
      "416:\tlearn: 0.0956802\ttotal: 3.4s\tremaining: 4.75s\n",
      "417:\tlearn: 0.0956386\ttotal: 3.4s\tremaining: 4.74s\n",
      "418:\tlearn: 0.0956022\ttotal: 3.41s\tremaining: 4.73s\n",
      "419:\tlearn: 0.0955881\ttotal: 3.41s\tremaining: 4.71s\n",
      "420:\tlearn: 0.0955696\ttotal: 3.42s\tremaining: 4.7s\n",
      "421:\tlearn: 0.0955482\ttotal: 3.42s\tremaining: 4.69s\n",
      "422:\tlearn: 0.0955098\ttotal: 3.44s\tremaining: 4.69s\n",
      "423:\tlearn: 0.0954838\ttotal: 3.45s\tremaining: 4.68s\n",
      "424:\tlearn: 0.0954511\ttotal: 3.45s\tremaining: 4.67s\n",
      "425:\tlearn: 0.0954159\ttotal: 3.46s\tremaining: 4.66s\n",
      "426:\tlearn: 0.0953602\ttotal: 3.46s\tremaining: 4.65s\n",
      "427:\tlearn: 0.0953209\ttotal: 3.48s\tremaining: 4.66s\n",
      "428:\tlearn: 0.0952969\ttotal: 3.49s\tremaining: 4.65s\n",
      "429:\tlearn: 0.0952616\ttotal: 3.5s\tremaining: 4.64s\n",
      "430:\tlearn: 0.0952324\ttotal: 3.5s\tremaining: 4.63s\n",
      "431:\tlearn: 0.0952055\ttotal: 3.51s\tremaining: 4.61s\n",
      "432:\tlearn: 0.0951874\ttotal: 3.52s\tremaining: 4.61s\n",
      "433:\tlearn: 0.0951677\ttotal: 3.53s\tremaining: 4.6s\n",
      "434:\tlearn: 0.0951540\ttotal: 3.53s\tremaining: 4.59s\n",
      "435:\tlearn: 0.0951140\ttotal: 3.54s\tremaining: 4.58s\n",
      "436:\tlearn: 0.0950975\ttotal: 3.54s\tremaining: 4.57s\n",
      "437:\tlearn: 0.0950680\ttotal: 3.55s\tremaining: 4.55s\n",
      "438:\tlearn: 0.0950525\ttotal: 3.56s\tremaining: 4.54s\n",
      "439:\tlearn: 0.0950006\ttotal: 3.57s\tremaining: 4.54s\n",
      "440:\tlearn: 0.0949392\ttotal: 3.58s\tremaining: 4.54s\n",
      "441:\tlearn: 0.0949178\ttotal: 3.58s\tremaining: 4.53s\n",
      "442:\tlearn: 0.0948914\ttotal: 3.59s\tremaining: 4.51s\n",
      "443:\tlearn: 0.0948749\ttotal: 3.6s\tremaining: 4.5s\n",
      "444:\tlearn: 0.0948210\ttotal: 3.61s\tremaining: 4.51s\n",
      "445:\tlearn: 0.0947796\ttotal: 3.62s\tremaining: 4.5s\n",
      "446:\tlearn: 0.0947469\ttotal: 3.63s\tremaining: 4.49s\n",
      "447:\tlearn: 0.0947315\ttotal: 3.63s\tremaining: 4.47s\n",
      "448:\tlearn: 0.0946966\ttotal: 3.64s\tremaining: 4.46s\n",
      "449:\tlearn: 0.0946437\ttotal: 3.64s\tremaining: 4.45s\n",
      "450:\tlearn: 0.0946250\ttotal: 3.66s\tremaining: 4.45s\n",
      "451:\tlearn: 0.0945769\ttotal: 3.66s\tremaining: 4.44s\n",
      "452:\tlearn: 0.0945623\ttotal: 3.67s\tremaining: 4.43s\n",
      "453:\tlearn: 0.0945282\ttotal: 3.67s\tremaining: 4.42s\n",
      "454:\tlearn: 0.0945020\ttotal: 3.68s\tremaining: 4.41s\n",
      "455:\tlearn: 0.0944639\ttotal: 3.69s\tremaining: 4.4s\n",
      "456:\tlearn: 0.0944101\ttotal: 3.7s\tremaining: 4.4s\n",
      "457:\tlearn: 0.0943767\ttotal: 3.71s\tremaining: 4.39s\n",
      "458:\tlearn: 0.0943440\ttotal: 3.72s\tremaining: 4.38s\n",
      "459:\tlearn: 0.0943082\ttotal: 3.73s\tremaining: 4.37s\n",
      "460:\tlearn: 0.0942804\ttotal: 3.74s\tremaining: 4.38s\n",
      "461:\tlearn: 0.0942616\ttotal: 3.75s\tremaining: 4.37s\n",
      "462:\tlearn: 0.0942470\ttotal: 3.76s\tremaining: 4.36s\n",
      "463:\tlearn: 0.0941944\ttotal: 3.76s\tremaining: 4.35s\n",
      "464:\tlearn: 0.0941595\ttotal: 3.77s\tremaining: 4.34s\n",
      "465:\tlearn: 0.0941365\ttotal: 3.79s\tremaining: 4.34s\n",
      "466:\tlearn: 0.0941124\ttotal: 3.8s\tremaining: 4.33s\n",
      "467:\tlearn: 0.0940895\ttotal: 3.8s\tremaining: 4.32s\n",
      "468:\tlearn: 0.0940720\ttotal: 3.81s\tremaining: 4.31s\n",
      "469:\tlearn: 0.0940511\ttotal: 3.81s\tremaining: 4.3s\n",
      "470:\tlearn: 0.0940262\ttotal: 3.82s\tremaining: 4.29s\n",
      "471:\tlearn: 0.0940063\ttotal: 3.84s\tremaining: 4.29s\n",
      "472:\tlearn: 0.0939833\ttotal: 3.84s\tremaining: 4.28s\n",
      "473:\tlearn: 0.0939690\ttotal: 3.85s\tremaining: 4.27s\n",
      "474:\tlearn: 0.0939483\ttotal: 3.85s\tremaining: 4.26s\n",
      "475:\tlearn: 0.0939205\ttotal: 3.86s\tremaining: 4.25s\n",
      "476:\tlearn: 0.0938698\ttotal: 3.87s\tremaining: 4.24s\n",
      "477:\tlearn: 0.0938536\ttotal: 3.88s\tremaining: 4.24s\n",
      "478:\tlearn: 0.0938271\ttotal: 3.89s\tremaining: 4.23s\n",
      "479:\tlearn: 0.0938073\ttotal: 3.89s\tremaining: 4.22s\n",
      "480:\tlearn: 0.0937619\ttotal: 3.9s\tremaining: 4.21s\n",
      "481:\tlearn: 0.0937120\ttotal: 3.9s\tremaining: 4.2s\n",
      "482:\tlearn: 0.0936843\ttotal: 3.92s\tremaining: 4.2s\n",
      "483:\tlearn: 0.0936419\ttotal: 3.93s\tremaining: 4.19s\n",
      "484:\tlearn: 0.0936207\ttotal: 3.93s\tremaining: 4.18s\n",
      "485:\tlearn: 0.0935763\ttotal: 3.94s\tremaining: 4.17s\n",
      "486:\tlearn: 0.0935523\ttotal: 3.94s\tremaining: 4.15s\n",
      "487:\tlearn: 0.0935303\ttotal: 3.97s\tremaining: 4.16s\n",
      "488:\tlearn: 0.0935024\ttotal: 3.98s\tremaining: 4.16s\n",
      "489:\tlearn: 0.0934713\ttotal: 3.98s\tremaining: 4.14s\n",
      "490:\tlearn: 0.0934618\ttotal: 3.99s\tremaining: 4.13s\n",
      "491:\tlearn: 0.0934292\ttotal: 3.99s\tremaining: 4.12s\n",
      "492:\tlearn: 0.0934054\ttotal: 4.01s\tremaining: 4.12s\n",
      "493:\tlearn: 0.0933959\ttotal: 4.01s\tremaining: 4.11s\n",
      "494:\tlearn: 0.0933758\ttotal: 4.02s\tremaining: 4.1s\n",
      "495:\tlearn: 0.0933388\ttotal: 4.03s\tremaining: 4.09s\n",
      "496:\tlearn: 0.0933038\ttotal: 4.03s\tremaining: 4.08s\n",
      "497:\tlearn: 0.0932779\ttotal: 4.04s\tremaining: 4.07s\n",
      "498:\tlearn: 0.0932595\ttotal: 4.05s\tremaining: 4.07s\n",
      "499:\tlearn: 0.0932348\ttotal: 4.06s\tremaining: 4.06s\n",
      "500:\tlearn: 0.0932175\ttotal: 4.06s\tremaining: 4.05s\n",
      "501:\tlearn: 0.0931792\ttotal: 4.07s\tremaining: 4.04s\n",
      "502:\tlearn: 0.0931307\ttotal: 4.08s\tremaining: 4.03s\n",
      "503:\tlearn: 0.0931138\ttotal: 4.08s\tremaining: 4.02s\n",
      "504:\tlearn: 0.0930795\ttotal: 4.1s\tremaining: 4.02s\n",
      "505:\tlearn: 0.0930604\ttotal: 4.11s\tremaining: 4.01s\n",
      "506:\tlearn: 0.0930219\ttotal: 4.11s\tremaining: 4s\n",
      "507:\tlearn: 0.0930039\ttotal: 4.12s\tremaining: 3.99s\n",
      "508:\tlearn: 0.0929747\ttotal: 4.12s\tremaining: 3.98s\n",
      "509:\tlearn: 0.0929421\ttotal: 4.13s\tremaining: 3.97s\n",
      "510:\tlearn: 0.0929166\ttotal: 4.14s\tremaining: 3.96s\n",
      "511:\tlearn: 0.0928962\ttotal: 4.14s\tremaining: 3.95s\n",
      "512:\tlearn: 0.0928477\ttotal: 4.15s\tremaining: 3.94s\n",
      "513:\tlearn: 0.0927894\ttotal: 4.16s\tremaining: 3.93s\n",
      "514:\tlearn: 0.0927744\ttotal: 4.16s\tremaining: 3.92s\n",
      "515:\tlearn: 0.0927451\ttotal: 4.17s\tremaining: 3.91s\n",
      "516:\tlearn: 0.0927065\ttotal: 4.19s\tremaining: 3.91s\n",
      "517:\tlearn: 0.0926686\ttotal: 4.19s\tremaining: 3.9s\n",
      "518:\tlearn: 0.0926570\ttotal: 4.2s\tremaining: 3.89s\n",
      "519:\tlearn: 0.0926445\ttotal: 4.2s\tremaining: 3.88s\n",
      "520:\tlearn: 0.0925843\ttotal: 4.21s\tremaining: 3.87s\n",
      "521:\tlearn: 0.0925665\ttotal: 4.22s\tremaining: 3.87s\n",
      "522:\tlearn: 0.0925470\ttotal: 4.23s\tremaining: 3.86s\n",
      "523:\tlearn: 0.0925147\ttotal: 4.24s\tremaining: 3.85s\n",
      "524:\tlearn: 0.0924919\ttotal: 4.24s\tremaining: 3.84s\n",
      "525:\tlearn: 0.0924679\ttotal: 4.25s\tremaining: 3.83s\n",
      "526:\tlearn: 0.0924539\ttotal: 4.25s\tremaining: 3.82s\n",
      "527:\tlearn: 0.0923934\ttotal: 4.26s\tremaining: 3.81s\n",
      "528:\tlearn: 0.0923659\ttotal: 4.28s\tremaining: 3.81s\n",
      "529:\tlearn: 0.0923357\ttotal: 4.29s\tremaining: 3.8s\n",
      "530:\tlearn: 0.0923052\ttotal: 4.3s\tremaining: 3.8s\n",
      "531:\tlearn: 0.0922879\ttotal: 4.32s\tremaining: 3.8s\n",
      "532:\tlearn: 0.0922406\ttotal: 4.33s\tremaining: 3.79s\n",
      "533:\tlearn: 0.0922007\ttotal: 4.34s\tremaining: 3.79s\n",
      "534:\tlearn: 0.0921824\ttotal: 4.36s\tremaining: 3.79s\n",
      "535:\tlearn: 0.0921449\ttotal: 4.36s\tremaining: 3.77s\n",
      "536:\tlearn: 0.0921287\ttotal: 4.37s\tremaining: 3.77s\n",
      "537:\tlearn: 0.0921039\ttotal: 4.37s\tremaining: 3.75s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538:\tlearn: 0.0920862\ttotal: 4.38s\tremaining: 3.75s\n",
      "539:\tlearn: 0.0920608\ttotal: 4.39s\tremaining: 3.74s\n",
      "540:\tlearn: 0.0920489\ttotal: 4.4s\tremaining: 3.73s\n",
      "541:\tlearn: 0.0920230\ttotal: 4.4s\tremaining: 3.72s\n",
      "542:\tlearn: 0.0919876\ttotal: 4.41s\tremaining: 3.71s\n",
      "543:\tlearn: 0.0919459\ttotal: 4.41s\tremaining: 3.7s\n",
      "544:\tlearn: 0.0919166\ttotal: 4.42s\tremaining: 3.69s\n",
      "545:\tlearn: 0.0918930\ttotal: 4.42s\tremaining: 3.68s\n",
      "546:\tlearn: 0.0918525\ttotal: 4.43s\tremaining: 3.67s\n",
      "547:\tlearn: 0.0918297\ttotal: 4.44s\tremaining: 3.66s\n",
      "548:\tlearn: 0.0918101\ttotal: 4.45s\tremaining: 3.65s\n",
      "549:\tlearn: 0.0917818\ttotal: 4.45s\tremaining: 3.64s\n",
      "550:\tlearn: 0.0917624\ttotal: 4.46s\tremaining: 3.64s\n",
      "551:\tlearn: 0.0917396\ttotal: 4.47s\tremaining: 3.63s\n",
      "552:\tlearn: 0.0916759\ttotal: 4.48s\tremaining: 3.62s\n",
      "553:\tlearn: 0.0916391\ttotal: 4.49s\tremaining: 3.61s\n",
      "554:\tlearn: 0.0916272\ttotal: 4.49s\tremaining: 3.6s\n",
      "555:\tlearn: 0.0916141\ttotal: 4.5s\tremaining: 3.59s\n",
      "556:\tlearn: 0.0916015\ttotal: 4.5s\tremaining: 3.58s\n",
      "557:\tlearn: 0.0915901\ttotal: 4.51s\tremaining: 3.57s\n",
      "558:\tlearn: 0.0915646\ttotal: 4.52s\tremaining: 3.56s\n",
      "559:\tlearn: 0.0915521\ttotal: 4.52s\tremaining: 3.55s\n",
      "560:\tlearn: 0.0915405\ttotal: 4.53s\tremaining: 3.54s\n",
      "561:\tlearn: 0.0915293\ttotal: 4.54s\tremaining: 3.54s\n",
      "562:\tlearn: 0.0915017\ttotal: 4.54s\tremaining: 3.53s\n",
      "563:\tlearn: 0.0914921\ttotal: 4.57s\tremaining: 3.53s\n",
      "564:\tlearn: 0.0914709\ttotal: 4.58s\tremaining: 3.52s\n",
      "565:\tlearn: 0.0914397\ttotal: 4.58s\tremaining: 3.52s\n",
      "566:\tlearn: 0.0914283\ttotal: 4.59s\tremaining: 3.5s\n",
      "567:\tlearn: 0.0914124\ttotal: 4.59s\tremaining: 3.5s\n",
      "568:\tlearn: 0.0913992\ttotal: 4.61s\tremaining: 3.49s\n",
      "569:\tlearn: 0.0913776\ttotal: 4.62s\tremaining: 3.48s\n",
      "570:\tlearn: 0.0913625\ttotal: 4.63s\tremaining: 3.47s\n",
      "571:\tlearn: 0.0913458\ttotal: 4.63s\tremaining: 3.46s\n",
      "572:\tlearn: 0.0913020\ttotal: 4.64s\tremaining: 3.46s\n",
      "573:\tlearn: 0.0912831\ttotal: 4.65s\tremaining: 3.45s\n",
      "574:\tlearn: 0.0912511\ttotal: 4.66s\tremaining: 3.44s\n",
      "575:\tlearn: 0.0912042\ttotal: 4.67s\tremaining: 3.44s\n",
      "576:\tlearn: 0.0911902\ttotal: 4.67s\tremaining: 3.42s\n",
      "577:\tlearn: 0.0911827\ttotal: 4.68s\tremaining: 3.42s\n",
      "578:\tlearn: 0.0911739\ttotal: 4.68s\tremaining: 3.4s\n",
      "579:\tlearn: 0.0911383\ttotal: 4.7s\tremaining: 3.4s\n",
      "580:\tlearn: 0.0911138\ttotal: 4.7s\tremaining: 3.39s\n",
      "581:\tlearn: 0.0910885\ttotal: 4.71s\tremaining: 3.38s\n",
      "582:\tlearn: 0.0910606\ttotal: 4.71s\tremaining: 3.37s\n",
      "583:\tlearn: 0.0910476\ttotal: 4.72s\tremaining: 3.36s\n",
      "584:\tlearn: 0.0910371\ttotal: 4.73s\tremaining: 3.35s\n",
      "585:\tlearn: 0.0909990\ttotal: 4.74s\tremaining: 3.35s\n",
      "586:\tlearn: 0.0909754\ttotal: 4.74s\tremaining: 3.34s\n",
      "587:\tlearn: 0.0909612\ttotal: 4.75s\tremaining: 3.33s\n",
      "588:\tlearn: 0.0909489\ttotal: 4.75s\tremaining: 3.32s\n",
      "589:\tlearn: 0.0909449\ttotal: 4.76s\tremaining: 3.31s\n",
      "590:\tlearn: 0.0909132\ttotal: 4.76s\tremaining: 3.3s\n",
      "591:\tlearn: 0.0908877\ttotal: 4.77s\tremaining: 3.29s\n",
      "592:\tlearn: 0.0908746\ttotal: 4.78s\tremaining: 3.28s\n",
      "593:\tlearn: 0.0908379\ttotal: 4.79s\tremaining: 3.27s\n",
      "594:\tlearn: 0.0908191\ttotal: 4.79s\tremaining: 3.26s\n",
      "595:\tlearn: 0.0907896\ttotal: 4.8s\tremaining: 3.25s\n",
      "596:\tlearn: 0.0907306\ttotal: 4.8s\tremaining: 3.24s\n",
      "597:\tlearn: 0.0907056\ttotal: 4.81s\tremaining: 3.23s\n",
      "598:\tlearn: 0.0906733\ttotal: 4.82s\tremaining: 3.23s\n",
      "599:\tlearn: 0.0906474\ttotal: 4.83s\tremaining: 3.22s\n",
      "600:\tlearn: 0.0906359\ttotal: 4.83s\tremaining: 3.21s\n",
      "601:\tlearn: 0.0906192\ttotal: 4.84s\tremaining: 3.2s\n",
      "602:\tlearn: 0.0905973\ttotal: 4.84s\tremaining: 3.19s\n",
      "603:\tlearn: 0.0905859\ttotal: 4.85s\tremaining: 3.18s\n",
      "604:\tlearn: 0.0905670\ttotal: 4.86s\tremaining: 3.17s\n",
      "605:\tlearn: 0.0905328\ttotal: 4.87s\tremaining: 3.17s\n",
      "606:\tlearn: 0.0905070\ttotal: 4.88s\tremaining: 3.16s\n",
      "607:\tlearn: 0.0904971\ttotal: 4.88s\tremaining: 3.15s\n",
      "608:\tlearn: 0.0904898\ttotal: 4.89s\tremaining: 3.14s\n",
      "609:\tlearn: 0.0904802\ttotal: 4.89s\tremaining: 3.13s\n",
      "610:\tlearn: 0.0904598\ttotal: 4.9s\tremaining: 3.12s\n",
      "611:\tlearn: 0.0904042\ttotal: 4.91s\tremaining: 3.11s\n",
      "612:\tlearn: 0.0903838\ttotal: 4.92s\tremaining: 3.1s\n",
      "613:\tlearn: 0.0903737\ttotal: 4.92s\tremaining: 3.09s\n",
      "614:\tlearn: 0.0903270\ttotal: 4.93s\tremaining: 3.09s\n",
      "615:\tlearn: 0.0903033\ttotal: 4.93s\tremaining: 3.08s\n",
      "616:\tlearn: 0.0902512\ttotal: 4.94s\tremaining: 3.07s\n",
      "617:\tlearn: 0.0902343\ttotal: 4.96s\tremaining: 3.07s\n",
      "618:\tlearn: 0.0902005\ttotal: 4.97s\tremaining: 3.06s\n",
      "619:\tlearn: 0.0901828\ttotal: 4.97s\tremaining: 3.05s\n",
      "620:\tlearn: 0.0901425\ttotal: 4.98s\tremaining: 3.04s\n",
      "621:\tlearn: 0.0901318\ttotal: 4.98s\tremaining: 3.03s\n",
      "622:\tlearn: 0.0901221\ttotal: 4.99s\tremaining: 3.02s\n",
      "623:\tlearn: 0.0900892\ttotal: 5s\tremaining: 3.01s\n",
      "624:\tlearn: 0.0900599\ttotal: 5.01s\tremaining: 3s\n",
      "625:\tlearn: 0.0900238\ttotal: 5.01s\tremaining: 2.99s\n",
      "626:\tlearn: 0.0899974\ttotal: 5.02s\tremaining: 2.98s\n",
      "627:\tlearn: 0.0899831\ttotal: 5.02s\tremaining: 2.98s\n",
      "628:\tlearn: 0.0899662\ttotal: 5.03s\tremaining: 2.96s\n",
      "629:\tlearn: 0.0899329\ttotal: 5.04s\tremaining: 2.96s\n",
      "630:\tlearn: 0.0899142\ttotal: 5.04s\tremaining: 2.95s\n",
      "631:\tlearn: 0.0898813\ttotal: 5.05s\tremaining: 2.94s\n",
      "632:\tlearn: 0.0898332\ttotal: 5.06s\tremaining: 2.93s\n",
      "633:\tlearn: 0.0898257\ttotal: 5.06s\tremaining: 2.92s\n",
      "634:\tlearn: 0.0897896\ttotal: 5.07s\tremaining: 2.91s\n",
      "635:\tlearn: 0.0897661\ttotal: 5.08s\tremaining: 2.9s\n",
      "636:\tlearn: 0.0897583\ttotal: 5.08s\tremaining: 2.9s\n",
      "637:\tlearn: 0.0897278\ttotal: 5.09s\tremaining: 2.89s\n",
      "638:\tlearn: 0.0897088\ttotal: 5.1s\tremaining: 2.88s\n",
      "639:\tlearn: 0.0896842\ttotal: 5.1s\tremaining: 2.87s\n",
      "640:\tlearn: 0.0896463\ttotal: 5.11s\tremaining: 2.86s\n",
      "641:\tlearn: 0.0896349\ttotal: 5.12s\tremaining: 2.85s\n",
      "642:\tlearn: 0.0896085\ttotal: 5.12s\tremaining: 2.84s\n",
      "643:\tlearn: 0.0895876\ttotal: 5.13s\tremaining: 2.84s\n",
      "644:\tlearn: 0.0895676\ttotal: 5.14s\tremaining: 2.83s\n",
      "645:\tlearn: 0.0895345\ttotal: 5.14s\tremaining: 2.82s\n",
      "646:\tlearn: 0.0895245\ttotal: 5.15s\tremaining: 2.81s\n",
      "647:\tlearn: 0.0894893\ttotal: 5.15s\tremaining: 2.8s\n",
      "648:\tlearn: 0.0894593\ttotal: 5.16s\tremaining: 2.79s\n",
      "649:\tlearn: 0.0894276\ttotal: 5.17s\tremaining: 2.78s\n",
      "650:\tlearn: 0.0894110\ttotal: 5.18s\tremaining: 2.77s\n",
      "651:\tlearn: 0.0893876\ttotal: 5.18s\tremaining: 2.77s\n",
      "652:\tlearn: 0.0893823\ttotal: 5.19s\tremaining: 2.76s\n",
      "653:\tlearn: 0.0893633\ttotal: 5.19s\tremaining: 2.75s\n",
      "654:\tlearn: 0.0893455\ttotal: 5.2s\tremaining: 2.74s\n",
      "655:\tlearn: 0.0893336\ttotal: 5.21s\tremaining: 2.73s\n",
      "656:\tlearn: 0.0893253\ttotal: 5.23s\tremaining: 2.73s\n",
      "657:\tlearn: 0.0892976\ttotal: 5.24s\tremaining: 2.72s\n",
      "658:\tlearn: 0.0892517\ttotal: 5.26s\tremaining: 2.72s\n",
      "659:\tlearn: 0.0892366\ttotal: 5.27s\tremaining: 2.72s\n",
      "660:\tlearn: 0.0891924\ttotal: 5.28s\tremaining: 2.71s\n",
      "661:\tlearn: 0.0891633\ttotal: 5.31s\tremaining: 2.71s\n",
      "662:\tlearn: 0.0891347\ttotal: 5.32s\tremaining: 2.7s\n",
      "663:\tlearn: 0.0891145\ttotal: 5.32s\tremaining: 2.69s\n",
      "664:\tlearn: 0.0891018\ttotal: 5.33s\tremaining: 2.68s\n",
      "665:\tlearn: 0.0890879\ttotal: 5.33s\tremaining: 2.67s\n",
      "666:\tlearn: 0.0890797\ttotal: 5.34s\tremaining: 2.67s\n",
      "667:\tlearn: 0.0890530\ttotal: 5.35s\tremaining: 2.66s\n",
      "668:\tlearn: 0.0890358\ttotal: 5.36s\tremaining: 2.65s\n",
      "669:\tlearn: 0.0890160\ttotal: 5.36s\tremaining: 2.64s\n",
      "670:\tlearn: 0.0890002\ttotal: 5.37s\tremaining: 2.63s\n",
      "671:\tlearn: 0.0889874\ttotal: 5.37s\tremaining: 2.62s\n",
      "672:\tlearn: 0.0889727\ttotal: 5.38s\tremaining: 2.61s\n",
      "673:\tlearn: 0.0889456\ttotal: 5.39s\tremaining: 2.61s\n",
      "674:\tlearn: 0.0889364\ttotal: 5.4s\tremaining: 2.6s\n",
      "675:\tlearn: 0.0888961\ttotal: 5.4s\tremaining: 2.59s\n",
      "676:\tlearn: 0.0888727\ttotal: 5.41s\tremaining: 2.58s\n",
      "677:\tlearn: 0.0888599\ttotal: 5.41s\tremaining: 2.57s\n",
      "678:\tlearn: 0.0888489\ttotal: 5.42s\tremaining: 2.56s\n",
      "679:\tlearn: 0.0888269\ttotal: 5.43s\tremaining: 2.56s\n",
      "680:\tlearn: 0.0887967\ttotal: 5.44s\tremaining: 2.55s\n",
      "681:\tlearn: 0.0887687\ttotal: 5.45s\tremaining: 2.54s\n",
      "682:\tlearn: 0.0887543\ttotal: 5.45s\tremaining: 2.53s\n",
      "683:\tlearn: 0.0887354\ttotal: 5.46s\tremaining: 2.52s\n",
      "684:\tlearn: 0.0887213\ttotal: 5.46s\tremaining: 2.51s\n",
      "685:\tlearn: 0.0887065\ttotal: 5.47s\tremaining: 2.51s\n",
      "686:\tlearn: 0.0886903\ttotal: 5.48s\tremaining: 2.5s\n",
      "687:\tlearn: 0.0886789\ttotal: 5.49s\tremaining: 2.49s\n",
      "688:\tlearn: 0.0886525\ttotal: 5.49s\tremaining: 2.48s\n",
      "689:\tlearn: 0.0886216\ttotal: 5.5s\tremaining: 2.47s\n",
      "690:\tlearn: 0.0885801\ttotal: 5.5s\tremaining: 2.46s\n",
      "691:\tlearn: 0.0885725\ttotal: 5.51s\tremaining: 2.45s\n",
      "692:\tlearn: 0.0885509\ttotal: 5.52s\tremaining: 2.45s\n",
      "693:\tlearn: 0.0885397\ttotal: 5.53s\tremaining: 2.44s\n",
      "694:\tlearn: 0.0885219\ttotal: 5.54s\tremaining: 2.43s\n",
      "695:\tlearn: 0.0884978\ttotal: 5.54s\tremaining: 2.42s\n",
      "696:\tlearn: 0.0884950\ttotal: 5.55s\tremaining: 2.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697:\tlearn: 0.0884801\ttotal: 5.56s\tremaining: 2.41s\n",
      "698:\tlearn: 0.0884596\ttotal: 5.57s\tremaining: 2.4s\n",
      "699:\tlearn: 0.0884388\ttotal: 5.58s\tremaining: 2.39s\n",
      "700:\tlearn: 0.0884109\ttotal: 5.58s\tremaining: 2.38s\n",
      "701:\tlearn: 0.0883938\ttotal: 5.59s\tremaining: 2.37s\n",
      "702:\tlearn: 0.0883788\ttotal: 5.59s\tremaining: 2.36s\n",
      "703:\tlearn: 0.0883662\ttotal: 5.61s\tremaining: 2.36s\n",
      "704:\tlearn: 0.0883415\ttotal: 5.61s\tremaining: 2.35s\n",
      "705:\tlearn: 0.0883141\ttotal: 5.62s\tremaining: 2.34s\n",
      "706:\tlearn: 0.0882798\ttotal: 5.63s\tremaining: 2.33s\n",
      "707:\tlearn: 0.0882666\ttotal: 5.63s\tremaining: 2.32s\n",
      "708:\tlearn: 0.0882457\ttotal: 5.65s\tremaining: 2.32s\n",
      "709:\tlearn: 0.0882156\ttotal: 5.66s\tremaining: 2.31s\n",
      "710:\tlearn: 0.0881930\ttotal: 5.66s\tremaining: 2.3s\n",
      "711:\tlearn: 0.0881777\ttotal: 5.67s\tremaining: 2.29s\n",
      "712:\tlearn: 0.0881679\ttotal: 5.67s\tremaining: 2.28s\n",
      "713:\tlearn: 0.0881498\ttotal: 5.69s\tremaining: 2.28s\n",
      "714:\tlearn: 0.0881372\ttotal: 5.7s\tremaining: 2.27s\n",
      "715:\tlearn: 0.0881215\ttotal: 5.71s\tremaining: 2.26s\n",
      "716:\tlearn: 0.0881032\ttotal: 5.71s\tremaining: 2.25s\n",
      "717:\tlearn: 0.0880849\ttotal: 5.72s\tremaining: 2.25s\n",
      "718:\tlearn: 0.0880748\ttotal: 5.72s\tremaining: 2.24s\n",
      "719:\tlearn: 0.0880617\ttotal: 5.74s\tremaining: 2.23s\n",
      "720:\tlearn: 0.0880458\ttotal: 5.74s\tremaining: 2.22s\n",
      "721:\tlearn: 0.0880349\ttotal: 5.75s\tremaining: 2.21s\n",
      "722:\tlearn: 0.0880322\ttotal: 5.76s\tremaining: 2.21s\n",
      "723:\tlearn: 0.0880261\ttotal: 5.76s\tremaining: 2.2s\n",
      "724:\tlearn: 0.0879959\ttotal: 5.77s\tremaining: 2.19s\n",
      "725:\tlearn: 0.0879870\ttotal: 5.78s\tremaining: 2.18s\n",
      "726:\tlearn: 0.0879592\ttotal: 5.79s\tremaining: 2.17s\n",
      "727:\tlearn: 0.0879449\ttotal: 5.79s\tremaining: 2.17s\n",
      "728:\tlearn: 0.0879328\ttotal: 5.8s\tremaining: 2.16s\n",
      "729:\tlearn: 0.0879099\ttotal: 5.81s\tremaining: 2.15s\n",
      "730:\tlearn: 0.0878792\ttotal: 5.81s\tremaining: 2.14s\n",
      "731:\tlearn: 0.0878617\ttotal: 5.83s\tremaining: 2.13s\n",
      "732:\tlearn: 0.0878416\ttotal: 5.84s\tremaining: 2.13s\n",
      "733:\tlearn: 0.0878264\ttotal: 5.84s\tremaining: 2.12s\n",
      "734:\tlearn: 0.0878111\ttotal: 5.85s\tremaining: 2.11s\n",
      "735:\tlearn: 0.0877957\ttotal: 5.85s\tremaining: 2.1s\n",
      "736:\tlearn: 0.0877875\ttotal: 5.87s\tremaining: 2.09s\n",
      "737:\tlearn: 0.0877684\ttotal: 5.88s\tremaining: 2.09s\n",
      "738:\tlearn: 0.0877601\ttotal: 5.88s\tremaining: 2.08s\n",
      "739:\tlearn: 0.0877512\ttotal: 5.89s\tremaining: 2.07s\n",
      "740:\tlearn: 0.0877265\ttotal: 5.89s\tremaining: 2.06s\n",
      "741:\tlearn: 0.0877186\ttotal: 5.9s\tremaining: 2.05s\n",
      "742:\tlearn: 0.0877070\ttotal: 5.91s\tremaining: 2.04s\n",
      "743:\tlearn: 0.0876903\ttotal: 5.92s\tremaining: 2.04s\n",
      "744:\tlearn: 0.0876846\ttotal: 5.93s\tremaining: 2.03s\n",
      "745:\tlearn: 0.0876694\ttotal: 5.93s\tremaining: 2.02s\n",
      "746:\tlearn: 0.0876420\ttotal: 5.94s\tremaining: 2.01s\n",
      "747:\tlearn: 0.0876394\ttotal: 5.95s\tremaining: 2s\n",
      "748:\tlearn: 0.0876201\ttotal: 5.96s\tremaining: 2s\n",
      "749:\tlearn: 0.0876011\ttotal: 5.96s\tremaining: 1.99s\n",
      "750:\tlearn: 0.0875933\ttotal: 5.97s\tremaining: 1.98s\n",
      "751:\tlearn: 0.0875763\ttotal: 5.98s\tremaining: 1.97s\n",
      "752:\tlearn: 0.0875681\ttotal: 5.98s\tremaining: 1.96s\n",
      "753:\tlearn: 0.0875405\ttotal: 5.99s\tremaining: 1.95s\n",
      "754:\tlearn: 0.0875234\ttotal: 6s\tremaining: 1.95s\n",
      "755:\tlearn: 0.0875092\ttotal: 6s\tremaining: 1.94s\n",
      "756:\tlearn: 0.0874954\ttotal: 6.01s\tremaining: 1.93s\n",
      "757:\tlearn: 0.0874780\ttotal: 6.01s\tremaining: 1.92s\n",
      "758:\tlearn: 0.0874563\ttotal: 6.02s\tremaining: 1.91s\n",
      "759:\tlearn: 0.0874406\ttotal: 6.02s\tremaining: 1.9s\n",
      "760:\tlearn: 0.0874322\ttotal: 6.03s\tremaining: 1.89s\n",
      "761:\tlearn: 0.0874196\ttotal: 6.04s\tremaining: 1.89s\n",
      "762:\tlearn: 0.0873962\ttotal: 6.04s\tremaining: 1.88s\n",
      "763:\tlearn: 0.0873886\ttotal: 6.05s\tremaining: 1.87s\n",
      "764:\tlearn: 0.0873808\ttotal: 6.05s\tremaining: 1.86s\n",
      "765:\tlearn: 0.0873655\ttotal: 6.06s\tremaining: 1.85s\n",
      "766:\tlearn: 0.0873500\ttotal: 6.07s\tremaining: 1.84s\n",
      "767:\tlearn: 0.0873303\ttotal: 6.07s\tremaining: 1.83s\n",
      "768:\tlearn: 0.0873148\ttotal: 6.08s\tremaining: 1.83s\n",
      "769:\tlearn: 0.0872959\ttotal: 6.09s\tremaining: 1.82s\n",
      "770:\tlearn: 0.0872883\ttotal: 6.09s\tremaining: 1.81s\n",
      "771:\tlearn: 0.0872802\ttotal: 6.1s\tremaining: 1.8s\n",
      "772:\tlearn: 0.0872712\ttotal: 6.1s\tremaining: 1.79s\n",
      "773:\tlearn: 0.0872636\ttotal: 6.11s\tremaining: 1.78s\n",
      "774:\tlearn: 0.0872543\ttotal: 6.11s\tremaining: 1.77s\n",
      "775:\tlearn: 0.0872472\ttotal: 6.13s\tremaining: 1.77s\n",
      "776:\tlearn: 0.0872247\ttotal: 6.13s\tremaining: 1.76s\n",
      "777:\tlearn: 0.0872024\ttotal: 6.14s\tremaining: 1.75s\n",
      "778:\tlearn: 0.0871857\ttotal: 6.14s\tremaining: 1.74s\n",
      "779:\tlearn: 0.0871436\ttotal: 6.15s\tremaining: 1.73s\n",
      "780:\tlearn: 0.0871216\ttotal: 6.15s\tremaining: 1.73s\n",
      "781:\tlearn: 0.0871063\ttotal: 6.16s\tremaining: 1.72s\n",
      "782:\tlearn: 0.0870807\ttotal: 6.17s\tremaining: 1.71s\n",
      "783:\tlearn: 0.0870733\ttotal: 6.18s\tremaining: 1.7s\n",
      "784:\tlearn: 0.0870534\ttotal: 6.19s\tremaining: 1.7s\n",
      "785:\tlearn: 0.0870491\ttotal: 6.2s\tremaining: 1.69s\n",
      "786:\tlearn: 0.0870467\ttotal: 6.22s\tremaining: 1.68s\n",
      "787:\tlearn: 0.0870245\ttotal: 6.24s\tremaining: 1.68s\n",
      "788:\tlearn: 0.0870205\ttotal: 6.25s\tremaining: 1.67s\n",
      "789:\tlearn: 0.0869942\ttotal: 6.27s\tremaining: 1.67s\n",
      "790:\tlearn: 0.0869823\ttotal: 6.28s\tremaining: 1.66s\n",
      "791:\tlearn: 0.0869807\ttotal: 6.28s\tremaining: 1.65s\n",
      "792:\tlearn: 0.0869674\ttotal: 6.29s\tremaining: 1.64s\n",
      "793:\tlearn: 0.0869318\ttotal: 6.29s\tremaining: 1.63s\n",
      "794:\tlearn: 0.0869229\ttotal: 6.3s\tremaining: 1.62s\n",
      "795:\tlearn: 0.0869095\ttotal: 6.3s\tremaining: 1.61s\n",
      "796:\tlearn: 0.0868973\ttotal: 6.31s\tremaining: 1.61s\n",
      "797:\tlearn: 0.0868886\ttotal: 6.32s\tremaining: 1.6s\n",
      "798:\tlearn: 0.0868759\ttotal: 6.32s\tremaining: 1.59s\n",
      "799:\tlearn: 0.0868688\ttotal: 6.33s\tremaining: 1.58s\n",
      "800:\tlearn: 0.0868613\ttotal: 6.33s\tremaining: 1.57s\n",
      "801:\tlearn: 0.0868443\ttotal: 6.34s\tremaining: 1.56s\n",
      "802:\tlearn: 0.0868247\ttotal: 6.35s\tremaining: 1.56s\n",
      "803:\tlearn: 0.0868106\ttotal: 6.35s\tremaining: 1.55s\n",
      "804:\tlearn: 0.0867928\ttotal: 6.36s\tremaining: 1.54s\n",
      "805:\tlearn: 0.0867735\ttotal: 6.36s\tremaining: 1.53s\n",
      "806:\tlearn: 0.0867483\ttotal: 6.37s\tremaining: 1.52s\n",
      "807:\tlearn: 0.0867336\ttotal: 6.38s\tremaining: 1.51s\n",
      "808:\tlearn: 0.0867270\ttotal: 6.38s\tremaining: 1.51s\n",
      "809:\tlearn: 0.0867246\ttotal: 6.39s\tremaining: 1.5s\n",
      "810:\tlearn: 0.0867126\ttotal: 6.4s\tremaining: 1.49s\n",
      "811:\tlearn: 0.0866901\ttotal: 6.41s\tremaining: 1.48s\n",
      "812:\tlearn: 0.0866775\ttotal: 6.41s\tremaining: 1.47s\n",
      "813:\tlearn: 0.0866563\ttotal: 6.42s\tremaining: 1.47s\n",
      "814:\tlearn: 0.0866504\ttotal: 6.42s\tremaining: 1.46s\n",
      "815:\tlearn: 0.0866206\ttotal: 6.43s\tremaining: 1.45s\n",
      "816:\tlearn: 0.0866127\ttotal: 6.44s\tremaining: 1.44s\n",
      "817:\tlearn: 0.0866062\ttotal: 6.44s\tremaining: 1.43s\n",
      "818:\tlearn: 0.0865899\ttotal: 6.45s\tremaining: 1.43s\n",
      "819:\tlearn: 0.0865679\ttotal: 6.45s\tremaining: 1.42s\n",
      "820:\tlearn: 0.0865493\ttotal: 6.46s\tremaining: 1.41s\n",
      "821:\tlearn: 0.0865391\ttotal: 6.46s\tremaining: 1.4s\n",
      "822:\tlearn: 0.0865237\ttotal: 6.47s\tremaining: 1.39s\n",
      "823:\tlearn: 0.0865129\ttotal: 6.48s\tremaining: 1.38s\n",
      "824:\tlearn: 0.0865082\ttotal: 6.49s\tremaining: 1.38s\n",
      "825:\tlearn: 0.0864937\ttotal: 6.49s\tremaining: 1.37s\n",
      "826:\tlearn: 0.0864816\ttotal: 6.5s\tremaining: 1.36s\n",
      "827:\tlearn: 0.0864697\ttotal: 6.5s\tremaining: 1.35s\n",
      "828:\tlearn: 0.0864624\ttotal: 6.51s\tremaining: 1.34s\n",
      "829:\tlearn: 0.0864541\ttotal: 6.52s\tremaining: 1.33s\n",
      "830:\tlearn: 0.0864266\ttotal: 6.53s\tremaining: 1.33s\n",
      "831:\tlearn: 0.0863988\ttotal: 6.53s\tremaining: 1.32s\n",
      "832:\tlearn: 0.0863784\ttotal: 6.54s\tremaining: 1.31s\n",
      "833:\tlearn: 0.0863579\ttotal: 6.55s\tremaining: 1.3s\n",
      "834:\tlearn: 0.0863444\ttotal: 6.57s\tremaining: 1.3s\n",
      "835:\tlearn: 0.0863419\ttotal: 6.58s\tremaining: 1.29s\n",
      "836:\tlearn: 0.0863169\ttotal: 6.58s\tremaining: 1.28s\n",
      "837:\tlearn: 0.0862763\ttotal: 6.59s\tremaining: 1.27s\n",
      "838:\tlearn: 0.0862714\ttotal: 6.59s\tremaining: 1.26s\n",
      "839:\tlearn: 0.0862413\ttotal: 6.6s\tremaining: 1.26s\n",
      "840:\tlearn: 0.0862356\ttotal: 6.61s\tremaining: 1.25s\n",
      "841:\tlearn: 0.0862096\ttotal: 6.62s\tremaining: 1.24s\n",
      "842:\tlearn: 0.0862022\ttotal: 6.62s\tremaining: 1.23s\n",
      "843:\tlearn: 0.0861828\ttotal: 6.63s\tremaining: 1.23s\n",
      "844:\tlearn: 0.0861661\ttotal: 6.63s\tremaining: 1.22s\n",
      "845:\tlearn: 0.0861529\ttotal: 6.64s\tremaining: 1.21s\n",
      "846:\tlearn: 0.0861331\ttotal: 6.64s\tremaining: 1.2s\n",
      "847:\tlearn: 0.0861213\ttotal: 6.65s\tremaining: 1.19s\n",
      "848:\tlearn: 0.0861143\ttotal: 6.66s\tremaining: 1.18s\n",
      "849:\tlearn: 0.0861050\ttotal: 6.66s\tremaining: 1.18s\n",
      "850:\tlearn: 0.0860861\ttotal: 6.67s\tremaining: 1.17s\n",
      "851:\tlearn: 0.0860742\ttotal: 6.67s\tremaining: 1.16s\n",
      "852:\tlearn: 0.0860679\ttotal: 6.67s\tremaining: 1.15s\n",
      "853:\tlearn: 0.0860464\ttotal: 6.68s\tremaining: 1.14s\n",
      "854:\tlearn: 0.0860297\ttotal: 6.69s\tremaining: 1.13s\n",
      "855:\tlearn: 0.0860113\ttotal: 6.7s\tremaining: 1.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856:\tlearn: 0.0860001\ttotal: 6.71s\tremaining: 1.12s\n",
      "857:\tlearn: 0.0859665\ttotal: 6.72s\tremaining: 1.11s\n",
      "858:\tlearn: 0.0859529\ttotal: 6.73s\tremaining: 1.1s\n",
      "859:\tlearn: 0.0859434\ttotal: 6.74s\tremaining: 1.1s\n",
      "860:\tlearn: 0.0859322\ttotal: 6.75s\tremaining: 1.09s\n",
      "861:\tlearn: 0.0859305\ttotal: 6.75s\tremaining: 1.08s\n",
      "862:\tlearn: 0.0859268\ttotal: 6.76s\tremaining: 1.07s\n",
      "863:\tlearn: 0.0859046\ttotal: 6.77s\tremaining: 1.06s\n",
      "864:\tlearn: 0.0858927\ttotal: 6.77s\tremaining: 1.06s\n",
      "865:\tlearn: 0.0858718\ttotal: 6.78s\tremaining: 1.05s\n",
      "866:\tlearn: 0.0858628\ttotal: 6.78s\tremaining: 1.04s\n",
      "867:\tlearn: 0.0858478\ttotal: 6.79s\tremaining: 1.03s\n",
      "868:\tlearn: 0.0858384\ttotal: 6.79s\tremaining: 1.02s\n",
      "869:\tlearn: 0.0858344\ttotal: 6.8s\tremaining: 1.01s\n",
      "870:\tlearn: 0.0858229\ttotal: 6.8s\tremaining: 1.01s\n",
      "871:\tlearn: 0.0858116\ttotal: 6.81s\tremaining: 1000ms\n",
      "872:\tlearn: 0.0857990\ttotal: 6.82s\tremaining: 992ms\n",
      "873:\tlearn: 0.0857907\ttotal: 6.82s\tremaining: 984ms\n",
      "874:\tlearn: 0.0857769\ttotal: 6.83s\tremaining: 976ms\n",
      "875:\tlearn: 0.0857482\ttotal: 6.83s\tremaining: 967ms\n",
      "876:\tlearn: 0.0857405\ttotal: 6.84s\tremaining: 960ms\n",
      "877:\tlearn: 0.0857222\ttotal: 6.85s\tremaining: 951ms\n",
      "878:\tlearn: 0.0857104\ttotal: 6.85s\tremaining: 943ms\n",
      "879:\tlearn: 0.0856987\ttotal: 6.86s\tremaining: 935ms\n",
      "880:\tlearn: 0.0856969\ttotal: 6.86s\tremaining: 927ms\n",
      "881:\tlearn: 0.0856902\ttotal: 6.87s\tremaining: 919ms\n",
      "882:\tlearn: 0.0856824\ttotal: 6.87s\tremaining: 911ms\n",
      "883:\tlearn: 0.0856744\ttotal: 6.88s\tremaining: 903ms\n",
      "884:\tlearn: 0.0856608\ttotal: 6.88s\tremaining: 895ms\n",
      "885:\tlearn: 0.0856463\ttotal: 6.89s\tremaining: 887ms\n",
      "886:\tlearn: 0.0856270\ttotal: 6.9s\tremaining: 879ms\n",
      "887:\tlearn: 0.0856162\ttotal: 6.91s\tremaining: 871ms\n",
      "888:\tlearn: 0.0856008\ttotal: 6.91s\tremaining: 863ms\n",
      "889:\tlearn: 0.0855892\ttotal: 6.92s\tremaining: 855ms\n",
      "890:\tlearn: 0.0855751\ttotal: 6.92s\tremaining: 847ms\n",
      "891:\tlearn: 0.0855603\ttotal: 6.93s\tremaining: 839ms\n",
      "892:\tlearn: 0.0855484\ttotal: 6.93s\tremaining: 831ms\n",
      "893:\tlearn: 0.0855473\ttotal: 6.94s\tremaining: 823ms\n",
      "894:\tlearn: 0.0855366\ttotal: 6.94s\tremaining: 815ms\n",
      "895:\tlearn: 0.0855278\ttotal: 6.95s\tremaining: 807ms\n",
      "896:\tlearn: 0.0855014\ttotal: 6.95s\tremaining: 799ms\n",
      "897:\tlearn: 0.0854849\ttotal: 6.96s\tremaining: 791ms\n",
      "898:\tlearn: 0.0854659\ttotal: 6.96s\tremaining: 783ms\n",
      "899:\tlearn: 0.0854505\ttotal: 6.97s\tremaining: 774ms\n",
      "900:\tlearn: 0.0854340\ttotal: 6.98s\tremaining: 767ms\n",
      "901:\tlearn: 0.0854231\ttotal: 6.98s\tremaining: 759ms\n",
      "902:\tlearn: 0.0854084\ttotal: 6.99s\tremaining: 751ms\n",
      "903:\tlearn: 0.0853774\ttotal: 7s\tremaining: 743ms\n",
      "904:\tlearn: 0.0853587\ttotal: 7s\tremaining: 735ms\n",
      "905:\tlearn: 0.0853465\ttotal: 7s\tremaining: 727ms\n",
      "906:\tlearn: 0.0853365\ttotal: 7.01s\tremaining: 719ms\n",
      "907:\tlearn: 0.0853295\ttotal: 7.02s\tremaining: 711ms\n",
      "908:\tlearn: 0.0853151\ttotal: 7.02s\tremaining: 703ms\n",
      "909:\tlearn: 0.0853046\ttotal: 7.03s\tremaining: 695ms\n",
      "910:\tlearn: 0.0852988\ttotal: 7.03s\tremaining: 687ms\n",
      "911:\tlearn: 0.0852821\ttotal: 7.04s\tremaining: 679ms\n",
      "912:\tlearn: 0.0852691\ttotal: 7.04s\tremaining: 671ms\n",
      "913:\tlearn: 0.0852601\ttotal: 7.05s\tremaining: 663ms\n",
      "914:\tlearn: 0.0852538\ttotal: 7.06s\tremaining: 655ms\n",
      "915:\tlearn: 0.0852414\ttotal: 7.06s\tremaining: 648ms\n",
      "916:\tlearn: 0.0852306\ttotal: 7.07s\tremaining: 640ms\n",
      "917:\tlearn: 0.0852288\ttotal: 7.08s\tremaining: 632ms\n",
      "918:\tlearn: 0.0852231\ttotal: 7.08s\tremaining: 624ms\n",
      "919:\tlearn: 0.0852098\ttotal: 7.09s\tremaining: 616ms\n",
      "920:\tlearn: 0.0852014\ttotal: 7.09s\tremaining: 609ms\n",
      "921:\tlearn: 0.0851979\ttotal: 7.1s\tremaining: 601ms\n",
      "922:\tlearn: 0.0851900\ttotal: 7.11s\tremaining: 593ms\n",
      "923:\tlearn: 0.0851770\ttotal: 7.11s\tremaining: 585ms\n",
      "924:\tlearn: 0.0851728\ttotal: 7.12s\tremaining: 577ms\n",
      "925:\tlearn: 0.0851628\ttotal: 7.12s\tremaining: 569ms\n",
      "926:\tlearn: 0.0851551\ttotal: 7.13s\tremaining: 562ms\n",
      "927:\tlearn: 0.0851453\ttotal: 7.14s\tremaining: 554ms\n",
      "928:\tlearn: 0.0851356\ttotal: 7.16s\tremaining: 547ms\n",
      "929:\tlearn: 0.0851063\ttotal: 7.18s\tremaining: 540ms\n",
      "930:\tlearn: 0.0851015\ttotal: 7.19s\tremaining: 533ms\n",
      "931:\tlearn: 0.0850943\ttotal: 7.2s\tremaining: 526ms\n",
      "932:\tlearn: 0.0850764\ttotal: 7.21s\tremaining: 518ms\n",
      "933:\tlearn: 0.0850537\ttotal: 7.22s\tremaining: 511ms\n",
      "934:\tlearn: 0.0850452\ttotal: 7.23s\tremaining: 503ms\n",
      "935:\tlearn: 0.0850299\ttotal: 7.24s\tremaining: 495ms\n",
      "936:\tlearn: 0.0850191\ttotal: 7.24s\tremaining: 487ms\n",
      "937:\tlearn: 0.0850082\ttotal: 7.25s\tremaining: 479ms\n",
      "938:\tlearn: 0.0850052\ttotal: 7.25s\tremaining: 471ms\n",
      "939:\tlearn: 0.0849943\ttotal: 7.26s\tremaining: 463ms\n",
      "940:\tlearn: 0.0849775\ttotal: 7.26s\tremaining: 456ms\n",
      "941:\tlearn: 0.0849479\ttotal: 7.27s\tremaining: 448ms\n",
      "942:\tlearn: 0.0849318\ttotal: 7.28s\tremaining: 440ms\n",
      "943:\tlearn: 0.0849205\ttotal: 7.28s\tremaining: 432ms\n",
      "944:\tlearn: 0.0849118\ttotal: 7.29s\tremaining: 424ms\n",
      "945:\tlearn: 0.0848810\ttotal: 7.3s\tremaining: 417ms\n",
      "946:\tlearn: 0.0848721\ttotal: 7.31s\tremaining: 409ms\n",
      "947:\tlearn: 0.0848555\ttotal: 7.31s\tremaining: 401ms\n",
      "948:\tlearn: 0.0848472\ttotal: 7.32s\tremaining: 393ms\n",
      "949:\tlearn: 0.0848254\ttotal: 7.32s\tremaining: 386ms\n",
      "950:\tlearn: 0.0848144\ttotal: 7.33s\tremaining: 378ms\n",
      "951:\tlearn: 0.0848036\ttotal: 7.34s\tremaining: 370ms\n",
      "952:\tlearn: 0.0847721\ttotal: 7.34s\tremaining: 362ms\n",
      "953:\tlearn: 0.0847634\ttotal: 7.35s\tremaining: 354ms\n",
      "954:\tlearn: 0.0847552\ttotal: 7.35s\tremaining: 346ms\n",
      "955:\tlearn: 0.0847253\ttotal: 7.36s\tremaining: 339ms\n",
      "956:\tlearn: 0.0847144\ttotal: 7.36s\tremaining: 331ms\n",
      "957:\tlearn: 0.0847042\ttotal: 7.37s\tremaining: 323ms\n",
      "958:\tlearn: 0.0846957\ttotal: 7.38s\tremaining: 315ms\n",
      "959:\tlearn: 0.0846865\ttotal: 7.38s\tremaining: 308ms\n",
      "960:\tlearn: 0.0846711\ttotal: 7.39s\tremaining: 300ms\n",
      "961:\tlearn: 0.0846609\ttotal: 7.39s\tremaining: 292ms\n",
      "962:\tlearn: 0.0846460\ttotal: 7.4s\tremaining: 284ms\n",
      "963:\tlearn: 0.0846290\ttotal: 7.41s\tremaining: 277ms\n",
      "964:\tlearn: 0.0846232\ttotal: 7.41s\tremaining: 269ms\n",
      "965:\tlearn: 0.0846174\ttotal: 7.42s\tremaining: 261ms\n",
      "966:\tlearn: 0.0846055\ttotal: 7.42s\tremaining: 253ms\n",
      "967:\tlearn: 0.0845937\ttotal: 7.43s\tremaining: 246ms\n",
      "968:\tlearn: 0.0845800\ttotal: 7.43s\tremaining: 238ms\n",
      "969:\tlearn: 0.0845690\ttotal: 7.44s\tremaining: 230ms\n",
      "970:\tlearn: 0.0845589\ttotal: 7.45s\tremaining: 222ms\n",
      "971:\tlearn: 0.0845448\ttotal: 7.45s\tremaining: 215ms\n",
      "972:\tlearn: 0.0845352\ttotal: 7.46s\tremaining: 207ms\n",
      "973:\tlearn: 0.0845270\ttotal: 7.46s\tremaining: 199ms\n",
      "974:\tlearn: 0.0845234\ttotal: 7.47s\tremaining: 192ms\n",
      "975:\tlearn: 0.0845168\ttotal: 7.48s\tremaining: 184ms\n",
      "976:\tlearn: 0.0845047\ttotal: 7.48s\tremaining: 176ms\n",
      "977:\tlearn: 0.0844955\ttotal: 7.49s\tremaining: 168ms\n",
      "978:\tlearn: 0.0844879\ttotal: 7.49s\tremaining: 161ms\n",
      "979:\tlearn: 0.0844756\ttotal: 7.5s\tremaining: 153ms\n",
      "980:\tlearn: 0.0844684\ttotal: 7.5s\tremaining: 145ms\n",
      "981:\tlearn: 0.0844629\ttotal: 7.51s\tremaining: 138ms\n",
      "982:\tlearn: 0.0844576\ttotal: 7.52s\tremaining: 130ms\n",
      "983:\tlearn: 0.0844490\ttotal: 7.53s\tremaining: 122ms\n",
      "984:\tlearn: 0.0844409\ttotal: 7.53s\tremaining: 115ms\n",
      "985:\tlearn: 0.0844270\ttotal: 7.54s\tremaining: 107ms\n",
      "986:\tlearn: 0.0844152\ttotal: 7.54s\tremaining: 99.3ms\n",
      "987:\tlearn: 0.0844094\ttotal: 7.55s\tremaining: 91.7ms\n",
      "988:\tlearn: 0.0843998\ttotal: 7.56s\tremaining: 84.1ms\n",
      "989:\tlearn: 0.0843728\ttotal: 7.57s\tremaining: 76.4ms\n",
      "990:\tlearn: 0.0843660\ttotal: 7.57s\tremaining: 68.8ms\n",
      "991:\tlearn: 0.0843589\ttotal: 7.58s\tremaining: 61.1ms\n",
      "992:\tlearn: 0.0843511\ttotal: 7.58s\tremaining: 53.5ms\n",
      "993:\tlearn: 0.0843415\ttotal: 7.59s\tremaining: 45.8ms\n",
      "994:\tlearn: 0.0843333\ttotal: 7.6s\tremaining: 38.2ms\n",
      "995:\tlearn: 0.0843283\ttotal: 7.61s\tremaining: 30.5ms\n",
      "996:\tlearn: 0.0843151\ttotal: 7.61s\tremaining: 22.9ms\n",
      "997:\tlearn: 0.0842989\ttotal: 7.62s\tremaining: 15.3ms\n",
      "998:\tlearn: 0.0842896\ttotal: 7.62s\tremaining: 7.63ms\n",
      "999:\tlearn: 0.0842825\ttotal: 7.63s\tremaining: 0us\n",
      "0:\tlearn: 0.4384688\ttotal: 4.15ms\tremaining: 4.14s\n",
      "1:\tlearn: 0.4278376\ttotal: 12.4ms\tremaining: 6.19s\n",
      "2:\tlearn: 0.4175165\ttotal: 22.2ms\tremaining: 7.39s\n",
      "3:\tlearn: 0.4076102\ttotal: 27.5ms\tremaining: 6.85s\n",
      "4:\tlearn: 0.3980056\ttotal: 33ms\tremaining: 6.58s\n",
      "5:\tlearn: 0.3887279\ttotal: 38.5ms\tremaining: 6.38s\n",
      "6:\tlearn: 0.3795599\ttotal: 44.4ms\tremaining: 6.29s\n",
      "7:\tlearn: 0.3710484\ttotal: 49.8ms\tremaining: 6.18s\n",
      "8:\tlearn: 0.3626121\ttotal: 59ms\tremaining: 6.5s\n",
      "9:\tlearn: 0.3546003\ttotal: 66.6ms\tremaining: 6.6s\n",
      "10:\tlearn: 0.3468823\ttotal: 72.7ms\tremaining: 6.53s\n",
      "11:\tlearn: 0.3393193\ttotal: 79.9ms\tremaining: 6.58s\n",
      "12:\tlearn: 0.3318386\ttotal: 85.5ms\tremaining: 6.49s\n",
      "13:\tlearn: 0.3248020\ttotal: 90.9ms\tremaining: 6.4s\n",
      "14:\tlearn: 0.3181690\ttotal: 97.6ms\tremaining: 6.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\tlearn: 0.3112177\ttotal: 107ms\tremaining: 6.57s\n",
      "16:\tlearn: 0.3048908\ttotal: 113ms\tremaining: 6.51s\n",
      "17:\tlearn: 0.2985039\ttotal: 117ms\tremaining: 6.4s\n",
      "18:\tlearn: 0.2925707\ttotal: 123ms\tremaining: 6.33s\n",
      "19:\tlearn: 0.2868378\ttotal: 128ms\tremaining: 6.29s\n",
      "20:\tlearn: 0.2812698\ttotal: 134ms\tremaining: 6.24s\n",
      "21:\tlearn: 0.2759224\ttotal: 143ms\tremaining: 6.37s\n",
      "22:\tlearn: 0.2702797\ttotal: 152ms\tremaining: 6.46s\n",
      "23:\tlearn: 0.2650901\ttotal: 157ms\tremaining: 6.4s\n",
      "24:\tlearn: 0.2600141\ttotal: 164ms\tremaining: 6.39s\n",
      "25:\tlearn: 0.2554404\ttotal: 169ms\tremaining: 6.33s\n",
      "26:\tlearn: 0.2508557\ttotal: 175ms\tremaining: 6.3s\n",
      "27:\tlearn: 0.2462296\ttotal: 182ms\tremaining: 6.31s\n",
      "28:\tlearn: 0.2420949\ttotal: 196ms\tremaining: 6.58s\n",
      "29:\tlearn: 0.2380778\ttotal: 202ms\tremaining: 6.53s\n",
      "30:\tlearn: 0.2337253\ttotal: 208ms\tremaining: 6.49s\n",
      "31:\tlearn: 0.2296541\ttotal: 214ms\tremaining: 6.47s\n",
      "32:\tlearn: 0.2256442\ttotal: 220ms\tremaining: 6.43s\n",
      "33:\tlearn: 0.2220030\ttotal: 229ms\tremaining: 6.5s\n",
      "34:\tlearn: 0.2186555\ttotal: 238ms\tremaining: 6.55s\n",
      "35:\tlearn: 0.2151325\ttotal: 243ms\tremaining: 6.5s\n",
      "36:\tlearn: 0.2117166\ttotal: 249ms\tremaining: 6.48s\n",
      "37:\tlearn: 0.2083830\ttotal: 254ms\tremaining: 6.43s\n",
      "38:\tlearn: 0.2053727\ttotal: 260ms\tremaining: 6.42s\n",
      "39:\tlearn: 0.2023710\ttotal: 267ms\tremaining: 6.42s\n",
      "40:\tlearn: 0.1996753\ttotal: 289ms\tremaining: 6.77s\n",
      "41:\tlearn: 0.1970411\ttotal: 331ms\tremaining: 7.54s\n",
      "42:\tlearn: 0.1946303\ttotal: 336ms\tremaining: 7.48s\n",
      "43:\tlearn: 0.1919767\ttotal: 342ms\tremaining: 7.42s\n",
      "44:\tlearn: 0.1895635\ttotal: 347ms\tremaining: 7.36s\n",
      "45:\tlearn: 0.1873740\ttotal: 352ms\tremaining: 7.3s\n",
      "46:\tlearn: 0.1851049\ttotal: 357ms\tremaining: 7.25s\n",
      "47:\tlearn: 0.1829735\ttotal: 370ms\tremaining: 7.34s\n",
      "48:\tlearn: 0.1809776\ttotal: 378ms\tremaining: 7.33s\n",
      "49:\tlearn: 0.1787470\ttotal: 388ms\tremaining: 7.36s\n",
      "50:\tlearn: 0.1767914\ttotal: 400ms\tremaining: 7.45s\n",
      "51:\tlearn: 0.1749951\ttotal: 411ms\tremaining: 7.5s\n",
      "52:\tlearn: 0.1732926\ttotal: 424ms\tremaining: 7.58s\n",
      "53:\tlearn: 0.1713354\ttotal: 435ms\tremaining: 7.62s\n",
      "54:\tlearn: 0.1696931\ttotal: 445ms\tremaining: 7.64s\n",
      "55:\tlearn: 0.1682156\ttotal: 464ms\tremaining: 7.81s\n",
      "56:\tlearn: 0.1667381\ttotal: 475ms\tremaining: 7.86s\n",
      "57:\tlearn: 0.1651201\ttotal: 485ms\tremaining: 7.87s\n",
      "58:\tlearn: 0.1636277\ttotal: 491ms\tremaining: 7.83s\n",
      "59:\tlearn: 0.1622027\ttotal: 499ms\tremaining: 7.81s\n",
      "60:\tlearn: 0.1609023\ttotal: 506ms\tremaining: 7.79s\n",
      "61:\tlearn: 0.1596372\ttotal: 511ms\tremaining: 7.74s\n",
      "62:\tlearn: 0.1583726\ttotal: 517ms\tremaining: 7.68s\n",
      "63:\tlearn: 0.1571023\ttotal: 522ms\tremaining: 7.64s\n",
      "64:\tlearn: 0.1557121\ttotal: 527ms\tremaining: 7.59s\n",
      "65:\tlearn: 0.1546105\ttotal: 533ms\tremaining: 7.54s\n",
      "66:\tlearn: 0.1531850\ttotal: 539ms\tremaining: 7.51s\n",
      "67:\tlearn: 0.1521240\ttotal: 545ms\tremaining: 7.47s\n",
      "68:\tlearn: 0.1511215\ttotal: 550ms\tremaining: 7.42s\n",
      "69:\tlearn: 0.1501826\ttotal: 555ms\tremaining: 7.38s\n",
      "70:\tlearn: 0.1490719\ttotal: 560ms\tremaining: 7.33s\n",
      "71:\tlearn: 0.1482218\ttotal: 566ms\tremaining: 7.29s\n",
      "72:\tlearn: 0.1473600\ttotal: 573ms\tremaining: 7.27s\n",
      "73:\tlearn: 0.1463674\ttotal: 578ms\tremaining: 7.23s\n",
      "74:\tlearn: 0.1455458\ttotal: 584ms\tremaining: 7.2s\n",
      "75:\tlearn: 0.1446694\ttotal: 594ms\tremaining: 7.23s\n",
      "76:\tlearn: 0.1439793\ttotal: 603ms\tremaining: 7.23s\n",
      "77:\tlearn: 0.1431408\ttotal: 611ms\tremaining: 7.22s\n",
      "78:\tlearn: 0.1423606\ttotal: 618ms\tremaining: 7.2s\n",
      "79:\tlearn: 0.1417035\ttotal: 624ms\tremaining: 7.17s\n",
      "80:\tlearn: 0.1410508\ttotal: 630ms\tremaining: 7.15s\n",
      "81:\tlearn: 0.1404654\ttotal: 637ms\tremaining: 7.13s\n",
      "82:\tlearn: 0.1398454\ttotal: 642ms\tremaining: 7.09s\n",
      "83:\tlearn: 0.1391525\ttotal: 647ms\tremaining: 7.05s\n",
      "84:\tlearn: 0.1385920\ttotal: 653ms\tremaining: 7.03s\n",
      "85:\tlearn: 0.1380561\ttotal: 664ms\tremaining: 7.05s\n",
      "86:\tlearn: 0.1374621\ttotal: 685ms\tremaining: 7.19s\n",
      "87:\tlearn: 0.1370200\ttotal: 695ms\tremaining: 7.21s\n",
      "88:\tlearn: 0.1363009\ttotal: 703ms\tremaining: 7.2s\n",
      "89:\tlearn: 0.1356482\ttotal: 709ms\tremaining: 7.17s\n",
      "90:\tlearn: 0.1349529\ttotal: 716ms\tremaining: 7.15s\n",
      "91:\tlearn: 0.1342413\ttotal: 726ms\tremaining: 7.16s\n",
      "92:\tlearn: 0.1336908\ttotal: 731ms\tremaining: 7.13s\n",
      "93:\tlearn: 0.1332572\ttotal: 737ms\tremaining: 7.11s\n",
      "94:\tlearn: 0.1326968\ttotal: 742ms\tremaining: 7.07s\n",
      "95:\tlearn: 0.1323150\ttotal: 748ms\tremaining: 7.05s\n",
      "96:\tlearn: 0.1319645\ttotal: 753ms\tremaining: 7.01s\n",
      "97:\tlearn: 0.1315591\ttotal: 758ms\tremaining: 6.98s\n",
      "98:\tlearn: 0.1310375\ttotal: 763ms\tremaining: 6.95s\n",
      "99:\tlearn: 0.1305396\ttotal: 769ms\tremaining: 6.92s\n",
      "100:\tlearn: 0.1301772\ttotal: 775ms\tremaining: 6.89s\n",
      "101:\tlearn: 0.1298290\ttotal: 780ms\tremaining: 6.87s\n",
      "102:\tlearn: 0.1293436\ttotal: 785ms\tremaining: 6.84s\n",
      "103:\tlearn: 0.1290035\ttotal: 790ms\tremaining: 6.81s\n",
      "104:\tlearn: 0.1286663\ttotal: 799ms\tremaining: 6.81s\n",
      "105:\tlearn: 0.1282634\ttotal: 805ms\tremaining: 6.79s\n",
      "106:\tlearn: 0.1277982\ttotal: 810ms\tremaining: 6.76s\n",
      "107:\tlearn: 0.1275096\ttotal: 815ms\tremaining: 6.73s\n",
      "108:\tlearn: 0.1271405\ttotal: 821ms\tremaining: 6.71s\n",
      "109:\tlearn: 0.1267248\ttotal: 826ms\tremaining: 6.68s\n",
      "110:\tlearn: 0.1264439\ttotal: 832ms\tremaining: 6.66s\n",
      "111:\tlearn: 0.1260181\ttotal: 838ms\tremaining: 6.64s\n",
      "112:\tlearn: 0.1257073\ttotal: 843ms\tremaining: 6.61s\n",
      "113:\tlearn: 0.1253692\ttotal: 848ms\tremaining: 6.59s\n",
      "114:\tlearn: 0.1250321\ttotal: 859ms\tremaining: 6.61s\n",
      "115:\tlearn: 0.1247832\ttotal: 865ms\tremaining: 6.59s\n",
      "116:\tlearn: 0.1245590\ttotal: 872ms\tremaining: 6.58s\n",
      "117:\tlearn: 0.1239867\ttotal: 877ms\tremaining: 6.55s\n",
      "118:\tlearn: 0.1235915\ttotal: 883ms\tremaining: 6.54s\n",
      "119:\tlearn: 0.1233848\ttotal: 888ms\tremaining: 6.51s\n",
      "120:\tlearn: 0.1230319\ttotal: 893ms\tremaining: 6.49s\n",
      "121:\tlearn: 0.1228329\ttotal: 899ms\tremaining: 6.47s\n",
      "122:\tlearn: 0.1224713\ttotal: 904ms\tremaining: 6.45s\n",
      "123:\tlearn: 0.1221037\ttotal: 910ms\tremaining: 6.43s\n",
      "124:\tlearn: 0.1218200\ttotal: 915ms\tremaining: 6.41s\n",
      "125:\tlearn: 0.1216575\ttotal: 921ms\tremaining: 6.39s\n",
      "126:\tlearn: 0.1214752\ttotal: 926ms\tremaining: 6.37s\n",
      "127:\tlearn: 0.1211823\ttotal: 932ms\tremaining: 6.35s\n",
      "128:\tlearn: 0.1209182\ttotal: 937ms\tremaining: 6.33s\n",
      "129:\tlearn: 0.1206706\ttotal: 943ms\tremaining: 6.31s\n",
      "130:\tlearn: 0.1203184\ttotal: 948ms\tremaining: 6.29s\n",
      "131:\tlearn: 0.1199866\ttotal: 954ms\tremaining: 6.27s\n",
      "132:\tlearn: 0.1195624\ttotal: 960ms\tremaining: 6.26s\n",
      "133:\tlearn: 0.1192421\ttotal: 969ms\tremaining: 6.26s\n",
      "134:\tlearn: 0.1190261\ttotal: 975ms\tremaining: 6.25s\n",
      "135:\tlearn: 0.1188477\ttotal: 980ms\tremaining: 6.22s\n",
      "136:\tlearn: 0.1186774\ttotal: 985ms\tremaining: 6.21s\n",
      "137:\tlearn: 0.1185311\ttotal: 990ms\tremaining: 6.19s\n",
      "138:\tlearn: 0.1183982\ttotal: 996ms\tremaining: 6.17s\n",
      "139:\tlearn: 0.1182710\ttotal: 1s\tremaining: 6.15s\n",
      "140:\tlearn: 0.1181565\ttotal: 1.01s\tremaining: 6.13s\n",
      "141:\tlearn: 0.1177991\ttotal: 1.01s\tremaining: 6.12s\n",
      "142:\tlearn: 0.1176490\ttotal: 1.02s\tremaining: 6.1s\n",
      "143:\tlearn: 0.1175422\ttotal: 1.02s\tremaining: 6.08s\n",
      "144:\tlearn: 0.1172633\ttotal: 1.03s\tremaining: 6.06s\n",
      "145:\tlearn: 0.1171505\ttotal: 1.03s\tremaining: 6.04s\n",
      "146:\tlearn: 0.1169753\ttotal: 1.04s\tremaining: 6.03s\n",
      "147:\tlearn: 0.1167420\ttotal: 1.04s\tremaining: 6.01s\n",
      "148:\tlearn: 0.1164264\ttotal: 1.05s\tremaining: 6.01s\n",
      "149:\tlearn: 0.1161547\ttotal: 1.06s\tremaining: 5.99s\n",
      "150:\tlearn: 0.1159294\ttotal: 1.06s\tremaining: 5.98s\n",
      "151:\tlearn: 0.1157364\ttotal: 1.07s\tremaining: 5.96s\n",
      "152:\tlearn: 0.1155257\ttotal: 1.07s\tremaining: 5.94s\n",
      "153:\tlearn: 0.1153813\ttotal: 1.08s\tremaining: 5.93s\n",
      "154:\tlearn: 0.1152092\ttotal: 1.08s\tremaining: 5.91s\n",
      "155:\tlearn: 0.1149899\ttotal: 1.09s\tremaining: 5.9s\n",
      "156:\tlearn: 0.1148676\ttotal: 1.1s\tremaining: 5.88s\n",
      "157:\tlearn: 0.1146028\ttotal: 1.1s\tremaining: 5.89s\n",
      "158:\tlearn: 0.1144819\ttotal: 1.11s\tremaining: 5.88s\n",
      "159:\tlearn: 0.1143553\ttotal: 1.12s\tremaining: 5.86s\n",
      "160:\tlearn: 0.1142642\ttotal: 1.12s\tremaining: 5.85s\n",
      "161:\tlearn: 0.1141441\ttotal: 1.13s\tremaining: 5.83s\n",
      "162:\tlearn: 0.1140627\ttotal: 1.13s\tremaining: 5.82s\n",
      "163:\tlearn: 0.1138653\ttotal: 1.14s\tremaining: 5.81s\n",
      "164:\tlearn: 0.1135126\ttotal: 1.15s\tremaining: 5.81s\n",
      "165:\tlearn: 0.1133317\ttotal: 1.15s\tremaining: 5.8s\n",
      "166:\tlearn: 0.1131147\ttotal: 1.16s\tremaining: 5.79s\n",
      "167:\tlearn: 0.1130317\ttotal: 1.17s\tremaining: 5.77s\n",
      "168:\tlearn: 0.1127945\ttotal: 1.17s\tremaining: 5.75s\n",
      "169:\tlearn: 0.1127056\ttotal: 1.18s\tremaining: 5.74s\n",
      "170:\tlearn: 0.1124690\ttotal: 1.18s\tremaining: 5.74s\n",
      "171:\tlearn: 0.1123056\ttotal: 1.19s\tremaining: 5.73s\n",
      "172:\tlearn: 0.1121491\ttotal: 1.2s\tremaining: 5.72s\n",
      "173:\tlearn: 0.1120599\ttotal: 1.2s\tremaining: 5.71s\n",
      "174:\tlearn: 0.1119694\ttotal: 1.21s\tremaining: 5.69s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175:\tlearn: 0.1117704\ttotal: 1.21s\tremaining: 5.68s\n",
      "176:\tlearn: 0.1116433\ttotal: 1.22s\tremaining: 5.67s\n",
      "177:\tlearn: 0.1113862\ttotal: 1.23s\tremaining: 5.67s\n",
      "178:\tlearn: 0.1112647\ttotal: 1.25s\tremaining: 5.71s\n",
      "179:\tlearn: 0.1111812\ttotal: 1.25s\tremaining: 5.72s\n",
      "180:\tlearn: 0.1110142\ttotal: 1.26s\tremaining: 5.71s\n",
      "181:\tlearn: 0.1109155\ttotal: 1.27s\tremaining: 5.7s\n",
      "182:\tlearn: 0.1108165\ttotal: 1.28s\tremaining: 5.72s\n",
      "183:\tlearn: 0.1107307\ttotal: 1.29s\tremaining: 5.71s\n",
      "184:\tlearn: 0.1106509\ttotal: 1.29s\tremaining: 5.7s\n",
      "185:\tlearn: 0.1105320\ttotal: 1.3s\tremaining: 5.68s\n",
      "186:\tlearn: 0.1104164\ttotal: 1.3s\tremaining: 5.67s\n",
      "187:\tlearn: 0.1102445\ttotal: 1.31s\tremaining: 5.65s\n",
      "188:\tlearn: 0.1101062\ttotal: 1.32s\tremaining: 5.65s\n",
      "189:\tlearn: 0.1099832\ttotal: 1.32s\tremaining: 5.65s\n",
      "190:\tlearn: 0.1099230\ttotal: 1.33s\tremaining: 5.63s\n",
      "191:\tlearn: 0.1097329\ttotal: 1.33s\tremaining: 5.62s\n",
      "192:\tlearn: 0.1095611\ttotal: 1.34s\tremaining: 5.61s\n",
      "193:\tlearn: 0.1094391\ttotal: 1.35s\tremaining: 5.62s\n",
      "194:\tlearn: 0.1093670\ttotal: 1.37s\tremaining: 5.67s\n",
      "195:\tlearn: 0.1092356\ttotal: 1.38s\tremaining: 5.67s\n",
      "196:\tlearn: 0.1091761\ttotal: 1.4s\tremaining: 5.69s\n",
      "197:\tlearn: 0.1090658\ttotal: 1.42s\tremaining: 5.73s\n",
      "198:\tlearn: 0.1089853\ttotal: 1.42s\tremaining: 5.73s\n",
      "199:\tlearn: 0.1089220\ttotal: 1.44s\tremaining: 5.76s\n",
      "200:\tlearn: 0.1087844\ttotal: 1.45s\tremaining: 5.77s\n",
      "201:\tlearn: 0.1086839\ttotal: 1.46s\tremaining: 5.78s\n",
      "202:\tlearn: 0.1086147\ttotal: 1.47s\tremaining: 5.77s\n",
      "203:\tlearn: 0.1085621\ttotal: 1.47s\tremaining: 5.75s\n",
      "204:\tlearn: 0.1084081\ttotal: 1.48s\tremaining: 5.74s\n",
      "205:\tlearn: 0.1083314\ttotal: 1.49s\tremaining: 5.72s\n",
      "206:\tlearn: 0.1082516\ttotal: 1.49s\tremaining: 5.72s\n",
      "207:\tlearn: 0.1081851\ttotal: 1.5s\tremaining: 5.72s\n",
      "208:\tlearn: 0.1081276\ttotal: 1.51s\tremaining: 5.71s\n",
      "209:\tlearn: 0.1080557\ttotal: 1.51s\tremaining: 5.7s\n",
      "210:\tlearn: 0.1078828\ttotal: 1.52s\tremaining: 5.69s\n",
      "211:\tlearn: 0.1077149\ttotal: 1.52s\tremaining: 5.67s\n",
      "212:\tlearn: 0.1075915\ttotal: 1.53s\tremaining: 5.66s\n",
      "213:\tlearn: 0.1075508\ttotal: 1.54s\tremaining: 5.66s\n",
      "214:\tlearn: 0.1075018\ttotal: 1.55s\tremaining: 5.65s\n",
      "215:\tlearn: 0.1074370\ttotal: 1.55s\tremaining: 5.63s\n",
      "216:\tlearn: 0.1073707\ttotal: 1.56s\tremaining: 5.62s\n",
      "217:\tlearn: 0.1073055\ttotal: 1.56s\tremaining: 5.61s\n",
      "218:\tlearn: 0.1071699\ttotal: 1.57s\tremaining: 5.6s\n",
      "219:\tlearn: 0.1070461\ttotal: 1.57s\tremaining: 5.59s\n",
      "220:\tlearn: 0.1069588\ttotal: 1.58s\tremaining: 5.58s\n",
      "221:\tlearn: 0.1068991\ttotal: 1.59s\tremaining: 5.58s\n",
      "222:\tlearn: 0.1068361\ttotal: 1.6s\tremaining: 5.56s\n",
      "223:\tlearn: 0.1067021\ttotal: 1.6s\tremaining: 5.55s\n",
      "224:\tlearn: 0.1066174\ttotal: 1.61s\tremaining: 5.54s\n",
      "225:\tlearn: 0.1064810\ttotal: 1.61s\tremaining: 5.52s\n",
      "226:\tlearn: 0.1063704\ttotal: 1.62s\tremaining: 5.51s\n",
      "227:\tlearn: 0.1062647\ttotal: 1.63s\tremaining: 5.51s\n",
      "228:\tlearn: 0.1061678\ttotal: 1.63s\tremaining: 5.5s\n",
      "229:\tlearn: 0.1061322\ttotal: 1.64s\tremaining: 5.48s\n",
      "230:\tlearn: 0.1060149\ttotal: 1.64s\tremaining: 5.48s\n",
      "231:\tlearn: 0.1058834\ttotal: 1.65s\tremaining: 5.47s\n",
      "232:\tlearn: 0.1058006\ttotal: 1.66s\tremaining: 5.46s\n",
      "233:\tlearn: 0.1056979\ttotal: 1.67s\tremaining: 5.46s\n",
      "234:\tlearn: 0.1055330\ttotal: 1.67s\tremaining: 5.45s\n",
      "235:\tlearn: 0.1054199\ttotal: 1.68s\tremaining: 5.44s\n",
      "236:\tlearn: 0.1053570\ttotal: 1.69s\tremaining: 5.43s\n",
      "237:\tlearn: 0.1052035\ttotal: 1.69s\tremaining: 5.42s\n",
      "238:\tlearn: 0.1051132\ttotal: 1.7s\tremaining: 5.4s\n",
      "239:\tlearn: 0.1049897\ttotal: 1.7s\tremaining: 5.39s\n",
      "240:\tlearn: 0.1048971\ttotal: 1.71s\tremaining: 5.39s\n",
      "241:\tlearn: 0.1048314\ttotal: 1.72s\tremaining: 5.38s\n",
      "242:\tlearn: 0.1047935\ttotal: 1.72s\tremaining: 5.37s\n",
      "243:\tlearn: 0.1046429\ttotal: 1.73s\tremaining: 5.36s\n",
      "244:\tlearn: 0.1045858\ttotal: 1.74s\tremaining: 5.35s\n",
      "245:\tlearn: 0.1044717\ttotal: 1.74s\tremaining: 5.34s\n",
      "246:\tlearn: 0.1044309\ttotal: 1.75s\tremaining: 5.34s\n",
      "247:\tlearn: 0.1043630\ttotal: 1.76s\tremaining: 5.34s\n",
      "248:\tlearn: 0.1043052\ttotal: 1.77s\tremaining: 5.33s\n",
      "249:\tlearn: 0.1042827\ttotal: 1.77s\tremaining: 5.32s\n",
      "250:\tlearn: 0.1042587\ttotal: 1.78s\tremaining: 5.3s\n",
      "251:\tlearn: 0.1041374\ttotal: 1.78s\tremaining: 5.29s\n",
      "252:\tlearn: 0.1040327\ttotal: 1.79s\tremaining: 5.28s\n",
      "253:\tlearn: 0.1039663\ttotal: 1.8s\tremaining: 5.28s\n",
      "254:\tlearn: 0.1039220\ttotal: 1.8s\tremaining: 5.27s\n",
      "255:\tlearn: 0.1038863\ttotal: 1.81s\tremaining: 5.26s\n",
      "256:\tlearn: 0.1038568\ttotal: 1.81s\tremaining: 5.25s\n",
      "257:\tlearn: 0.1038094\ttotal: 1.82s\tremaining: 5.24s\n",
      "258:\tlearn: 0.1037047\ttotal: 1.82s\tremaining: 5.22s\n",
      "259:\tlearn: 0.1036583\ttotal: 1.83s\tremaining: 5.21s\n",
      "260:\tlearn: 0.1035589\ttotal: 1.84s\tremaining: 5.2s\n",
      "261:\tlearn: 0.1034491\ttotal: 1.84s\tremaining: 5.2s\n",
      "262:\tlearn: 0.1034163\ttotal: 1.85s\tremaining: 5.19s\n",
      "263:\tlearn: 0.1033480\ttotal: 1.86s\tremaining: 5.18s\n",
      "264:\tlearn: 0.1033067\ttotal: 1.86s\tremaining: 5.16s\n",
      "265:\tlearn: 0.1032188\ttotal: 1.87s\tremaining: 5.15s\n",
      "266:\tlearn: 0.1031336\ttotal: 1.87s\tremaining: 5.14s\n",
      "267:\tlearn: 0.1030445\ttotal: 1.88s\tremaining: 5.13s\n",
      "268:\tlearn: 0.1029194\ttotal: 1.89s\tremaining: 5.13s\n",
      "269:\tlearn: 0.1028893\ttotal: 1.89s\tremaining: 5.12s\n",
      "270:\tlearn: 0.1028488\ttotal: 1.9s\tremaining: 5.11s\n",
      "271:\tlearn: 0.1027947\ttotal: 1.9s\tremaining: 5.09s\n",
      "272:\tlearn: 0.1027585\ttotal: 1.91s\tremaining: 5.09s\n",
      "273:\tlearn: 0.1027343\ttotal: 1.92s\tremaining: 5.08s\n",
      "274:\tlearn: 0.1027058\ttotal: 1.92s\tremaining: 5.07s\n",
      "275:\tlearn: 0.1026323\ttotal: 1.93s\tremaining: 5.06s\n",
      "276:\tlearn: 0.1025661\ttotal: 1.93s\tremaining: 5.05s\n",
      "277:\tlearn: 0.1025171\ttotal: 1.94s\tremaining: 5.04s\n",
      "278:\tlearn: 0.1024668\ttotal: 1.95s\tremaining: 5.03s\n",
      "279:\tlearn: 0.1023411\ttotal: 1.95s\tremaining: 5.02s\n",
      "280:\tlearn: 0.1022565\ttotal: 1.96s\tremaining: 5.01s\n",
      "281:\tlearn: 0.1022147\ttotal: 1.96s\tremaining: 5s\n",
      "282:\tlearn: 0.1021761\ttotal: 1.97s\tremaining: 4.99s\n",
      "283:\tlearn: 0.1020794\ttotal: 1.97s\tremaining: 4.98s\n",
      "284:\tlearn: 0.1019545\ttotal: 1.98s\tremaining: 4.97s\n",
      "285:\tlearn: 0.1019087\ttotal: 1.99s\tremaining: 4.96s\n",
      "286:\tlearn: 0.1018502\ttotal: 1.99s\tremaining: 4.95s\n",
      "287:\tlearn: 0.1017798\ttotal: 2s\tremaining: 4.94s\n",
      "288:\tlearn: 0.1017538\ttotal: 2s\tremaining: 4.92s\n",
      "289:\tlearn: 0.1016855\ttotal: 2.01s\tremaining: 4.92s\n",
      "290:\tlearn: 0.1016274\ttotal: 2.02s\tremaining: 4.91s\n",
      "291:\tlearn: 0.1015320\ttotal: 2.02s\tremaining: 4.9s\n",
      "292:\tlearn: 0.1014766\ttotal: 2.03s\tremaining: 4.89s\n",
      "293:\tlearn: 0.1013880\ttotal: 2.03s\tremaining: 4.88s\n",
      "294:\tlearn: 0.1013422\ttotal: 2.04s\tremaining: 4.88s\n",
      "295:\tlearn: 0.1012542\ttotal: 2.05s\tremaining: 4.87s\n",
      "296:\tlearn: 0.1011986\ttotal: 2.06s\tremaining: 4.87s\n",
      "297:\tlearn: 0.1011536\ttotal: 2.06s\tremaining: 4.86s\n",
      "298:\tlearn: 0.1010879\ttotal: 2.07s\tremaining: 4.86s\n",
      "299:\tlearn: 0.1009751\ttotal: 2.08s\tremaining: 4.85s\n",
      "300:\tlearn: 0.1009117\ttotal: 2.08s\tremaining: 4.83s\n",
      "301:\tlearn: 0.1008355\ttotal: 2.09s\tremaining: 4.82s\n",
      "302:\tlearn: 0.1007337\ttotal: 2.09s\tremaining: 4.81s\n",
      "303:\tlearn: 0.1006724\ttotal: 2.1s\tremaining: 4.8s\n",
      "304:\tlearn: 0.1006427\ttotal: 2.1s\tremaining: 4.79s\n",
      "305:\tlearn: 0.1006019\ttotal: 2.11s\tremaining: 4.78s\n",
      "306:\tlearn: 0.1005371\ttotal: 2.12s\tremaining: 4.77s\n",
      "307:\tlearn: 0.1004559\ttotal: 2.12s\tremaining: 4.76s\n",
      "308:\tlearn: 0.1004114\ttotal: 2.13s\tremaining: 4.76s\n",
      "309:\tlearn: 0.1003510\ttotal: 2.13s\tremaining: 4.75s\n",
      "310:\tlearn: 0.1003162\ttotal: 2.14s\tremaining: 4.74s\n",
      "311:\tlearn: 0.1002285\ttotal: 2.15s\tremaining: 4.74s\n",
      "312:\tlearn: 0.1001137\ttotal: 2.15s\tremaining: 4.72s\n",
      "313:\tlearn: 0.1000086\ttotal: 2.16s\tremaining: 4.71s\n",
      "314:\tlearn: 0.0999658\ttotal: 2.16s\tremaining: 4.7s\n",
      "315:\tlearn: 0.0999253\ttotal: 2.17s\tremaining: 4.69s\n",
      "316:\tlearn: 0.0998219\ttotal: 2.17s\tremaining: 4.68s\n",
      "317:\tlearn: 0.0997493\ttotal: 2.18s\tremaining: 4.68s\n",
      "318:\tlearn: 0.0997291\ttotal: 2.19s\tremaining: 4.67s\n",
      "319:\tlearn: 0.0996290\ttotal: 2.19s\tremaining: 4.66s\n",
      "320:\tlearn: 0.0995753\ttotal: 2.2s\tremaining: 4.65s\n",
      "321:\tlearn: 0.0995209\ttotal: 2.21s\tremaining: 4.64s\n",
      "322:\tlearn: 0.0994691\ttotal: 2.21s\tremaining: 4.63s\n",
      "323:\tlearn: 0.0994059\ttotal: 2.22s\tremaining: 4.63s\n",
      "324:\tlearn: 0.0993602\ttotal: 2.22s\tremaining: 4.62s\n",
      "325:\tlearn: 0.0993352\ttotal: 2.23s\tremaining: 4.61s\n",
      "326:\tlearn: 0.0992652\ttotal: 2.24s\tremaining: 4.6s\n",
      "327:\tlearn: 0.0992229\ttotal: 2.24s\tremaining: 4.59s\n",
      "328:\tlearn: 0.0991546\ttotal: 2.25s\tremaining: 4.58s\n",
      "329:\tlearn: 0.0991180\ttotal: 2.25s\tremaining: 4.57s\n",
      "330:\tlearn: 0.0990859\ttotal: 2.26s\tremaining: 4.56s\n",
      "331:\tlearn: 0.0990039\ttotal: 2.26s\tremaining: 4.55s\n",
      "332:\tlearn: 0.0989473\ttotal: 2.27s\tremaining: 4.54s\n",
      "333:\tlearn: 0.0989020\ttotal: 2.27s\tremaining: 4.54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334:\tlearn: 0.0988590\ttotal: 2.28s\tremaining: 4.53s\n",
      "335:\tlearn: 0.0988429\ttotal: 2.29s\tremaining: 4.51s\n",
      "336:\tlearn: 0.0988037\ttotal: 2.29s\tremaining: 4.51s\n",
      "337:\tlearn: 0.0987447\ttotal: 2.3s\tremaining: 4.51s\n",
      "338:\tlearn: 0.0986879\ttotal: 2.31s\tremaining: 4.5s\n",
      "339:\tlearn: 0.0985970\ttotal: 2.34s\tremaining: 4.54s\n",
      "340:\tlearn: 0.0985367\ttotal: 2.35s\tremaining: 4.53s\n",
      "341:\tlearn: 0.0984806\ttotal: 2.36s\tremaining: 4.55s\n",
      "342:\tlearn: 0.0983881\ttotal: 2.37s\tremaining: 4.55s\n",
      "343:\tlearn: 0.0983579\ttotal: 2.38s\tremaining: 4.55s\n",
      "344:\tlearn: 0.0983200\ttotal: 2.39s\tremaining: 4.54s\n",
      "345:\tlearn: 0.0982573\ttotal: 2.41s\tremaining: 4.55s\n",
      "346:\tlearn: 0.0982182\ttotal: 2.42s\tremaining: 4.55s\n",
      "347:\tlearn: 0.0981728\ttotal: 2.42s\tremaining: 4.54s\n",
      "348:\tlearn: 0.0981526\ttotal: 2.43s\tremaining: 4.53s\n",
      "349:\tlearn: 0.0981091\ttotal: 2.43s\tremaining: 4.52s\n",
      "350:\tlearn: 0.0980711\ttotal: 2.44s\tremaining: 4.51s\n",
      "351:\tlearn: 0.0980284\ttotal: 2.44s\tremaining: 4.5s\n",
      "352:\tlearn: 0.0979711\ttotal: 2.45s\tremaining: 4.49s\n",
      "353:\tlearn: 0.0979297\ttotal: 2.45s\tremaining: 4.48s\n",
      "354:\tlearn: 0.0979079\ttotal: 2.46s\tremaining: 4.47s\n",
      "355:\tlearn: 0.0978496\ttotal: 2.46s\tremaining: 4.46s\n",
      "356:\tlearn: 0.0977933\ttotal: 2.47s\tremaining: 4.45s\n",
      "357:\tlearn: 0.0977293\ttotal: 2.49s\tremaining: 4.46s\n",
      "358:\tlearn: 0.0976990\ttotal: 2.49s\tremaining: 4.45s\n",
      "359:\tlearn: 0.0976469\ttotal: 2.5s\tremaining: 4.44s\n",
      "360:\tlearn: 0.0976013\ttotal: 2.5s\tremaining: 4.43s\n",
      "361:\tlearn: 0.0975675\ttotal: 2.51s\tremaining: 4.42s\n",
      "362:\tlearn: 0.0975199\ttotal: 2.52s\tremaining: 4.41s\n",
      "363:\tlearn: 0.0974820\ttotal: 2.52s\tremaining: 4.4s\n",
      "364:\tlearn: 0.0974430\ttotal: 2.53s\tremaining: 4.39s\n",
      "365:\tlearn: 0.0973934\ttotal: 2.53s\tremaining: 4.38s\n",
      "366:\tlearn: 0.0972921\ttotal: 2.54s\tremaining: 4.37s\n",
      "367:\tlearn: 0.0972559\ttotal: 2.54s\tremaining: 4.37s\n",
      "368:\tlearn: 0.0972158\ttotal: 2.55s\tremaining: 4.36s\n",
      "369:\tlearn: 0.0971816\ttotal: 2.55s\tremaining: 4.34s\n",
      "370:\tlearn: 0.0971575\ttotal: 2.56s\tremaining: 4.34s\n",
      "371:\tlearn: 0.0970939\ttotal: 2.56s\tremaining: 4.33s\n",
      "372:\tlearn: 0.0970572\ttotal: 2.57s\tremaining: 4.32s\n",
      "373:\tlearn: 0.0969724\ttotal: 2.58s\tremaining: 4.31s\n",
      "374:\tlearn: 0.0969332\ttotal: 2.58s\tremaining: 4.3s\n",
      "375:\tlearn: 0.0968772\ttotal: 2.59s\tremaining: 4.3s\n",
      "376:\tlearn: 0.0968439\ttotal: 2.59s\tremaining: 4.29s\n",
      "377:\tlearn: 0.0968067\ttotal: 2.6s\tremaining: 4.28s\n",
      "378:\tlearn: 0.0967544\ttotal: 2.61s\tremaining: 4.27s\n",
      "379:\tlearn: 0.0966887\ttotal: 2.62s\tremaining: 4.27s\n",
      "380:\tlearn: 0.0966647\ttotal: 2.63s\tremaining: 4.27s\n",
      "381:\tlearn: 0.0966357\ttotal: 2.64s\tremaining: 4.27s\n",
      "382:\tlearn: 0.0965700\ttotal: 2.65s\tremaining: 4.27s\n",
      "383:\tlearn: 0.0965304\ttotal: 2.66s\tremaining: 4.27s\n",
      "384:\tlearn: 0.0965099\ttotal: 2.67s\tremaining: 4.27s\n",
      "385:\tlearn: 0.0964597\ttotal: 2.68s\tremaining: 4.27s\n",
      "386:\tlearn: 0.0964325\ttotal: 2.69s\tremaining: 4.26s\n",
      "387:\tlearn: 0.0963879\ttotal: 2.7s\tremaining: 4.25s\n",
      "388:\tlearn: 0.0963456\ttotal: 2.71s\tremaining: 4.25s\n",
      "389:\tlearn: 0.0963102\ttotal: 2.71s\tremaining: 4.24s\n",
      "390:\tlearn: 0.0962477\ttotal: 2.72s\tremaining: 4.24s\n",
      "391:\tlearn: 0.0962123\ttotal: 2.73s\tremaining: 4.24s\n",
      "392:\tlearn: 0.0961860\ttotal: 2.74s\tremaining: 4.23s\n",
      "393:\tlearn: 0.0961640\ttotal: 2.74s\tremaining: 4.22s\n",
      "394:\tlearn: 0.0961046\ttotal: 2.75s\tremaining: 4.21s\n",
      "395:\tlearn: 0.0960765\ttotal: 2.75s\tremaining: 4.2s\n",
      "396:\tlearn: 0.0960449\ttotal: 2.77s\tremaining: 4.2s\n",
      "397:\tlearn: 0.0960186\ttotal: 2.77s\tremaining: 4.19s\n",
      "398:\tlearn: 0.0959618\ttotal: 2.78s\tremaining: 4.19s\n",
      "399:\tlearn: 0.0959007\ttotal: 2.79s\tremaining: 4.18s\n",
      "400:\tlearn: 0.0958793\ttotal: 2.79s\tremaining: 4.17s\n",
      "401:\tlearn: 0.0958569\ttotal: 2.8s\tremaining: 4.17s\n",
      "402:\tlearn: 0.0958285\ttotal: 2.81s\tremaining: 4.16s\n",
      "403:\tlearn: 0.0958005\ttotal: 2.81s\tremaining: 4.15s\n",
      "404:\tlearn: 0.0957688\ttotal: 2.82s\tremaining: 4.14s\n",
      "405:\tlearn: 0.0957413\ttotal: 2.83s\tremaining: 4.14s\n",
      "406:\tlearn: 0.0957042\ttotal: 2.84s\tremaining: 4.13s\n",
      "407:\tlearn: 0.0956670\ttotal: 2.84s\tremaining: 4.12s\n",
      "408:\tlearn: 0.0956168\ttotal: 2.85s\tremaining: 4.12s\n",
      "409:\tlearn: 0.0955847\ttotal: 2.85s\tremaining: 4.11s\n",
      "410:\tlearn: 0.0955390\ttotal: 2.86s\tremaining: 4.1s\n",
      "411:\tlearn: 0.0955072\ttotal: 2.87s\tremaining: 4.09s\n",
      "412:\tlearn: 0.0954777\ttotal: 2.88s\tremaining: 4.09s\n",
      "413:\tlearn: 0.0954320\ttotal: 2.88s\tremaining: 4.08s\n",
      "414:\tlearn: 0.0953781\ttotal: 2.89s\tremaining: 4.07s\n",
      "415:\tlearn: 0.0953251\ttotal: 2.9s\tremaining: 4.06s\n",
      "416:\tlearn: 0.0952991\ttotal: 2.9s\tremaining: 4.06s\n",
      "417:\tlearn: 0.0952586\ttotal: 2.91s\tremaining: 4.05s\n",
      "418:\tlearn: 0.0952289\ttotal: 2.92s\tremaining: 4.04s\n",
      "419:\tlearn: 0.0951630\ttotal: 2.92s\tremaining: 4.04s\n",
      "420:\tlearn: 0.0951303\ttotal: 2.93s\tremaining: 4.03s\n",
      "421:\tlearn: 0.0951115\ttotal: 2.94s\tremaining: 4.03s\n",
      "422:\tlearn: 0.0950653\ttotal: 2.95s\tremaining: 4.02s\n",
      "423:\tlearn: 0.0950317\ttotal: 2.96s\tremaining: 4.02s\n",
      "424:\tlearn: 0.0949926\ttotal: 2.97s\tremaining: 4.02s\n",
      "425:\tlearn: 0.0949610\ttotal: 2.98s\tremaining: 4.01s\n",
      "426:\tlearn: 0.0949214\ttotal: 2.99s\tremaining: 4.01s\n",
      "427:\tlearn: 0.0948885\ttotal: 3.01s\tremaining: 4.02s\n",
      "428:\tlearn: 0.0948567\ttotal: 3.03s\tremaining: 4.03s\n",
      "429:\tlearn: 0.0948165\ttotal: 3.05s\tremaining: 4.04s\n",
      "430:\tlearn: 0.0947741\ttotal: 3.07s\tremaining: 4.05s\n",
      "431:\tlearn: 0.0947555\ttotal: 3.08s\tremaining: 4.06s\n",
      "432:\tlearn: 0.0947266\ttotal: 3.11s\tremaining: 4.08s\n",
      "433:\tlearn: 0.0946893\ttotal: 3.13s\tremaining: 4.08s\n",
      "434:\tlearn: 0.0946641\ttotal: 3.21s\tremaining: 4.17s\n",
      "435:\tlearn: 0.0946394\ttotal: 3.24s\tremaining: 4.2s\n",
      "436:\tlearn: 0.0946070\ttotal: 3.25s\tremaining: 4.19s\n",
      "437:\tlearn: 0.0945831\ttotal: 3.26s\tremaining: 4.19s\n",
      "438:\tlearn: 0.0945520\ttotal: 3.27s\tremaining: 4.18s\n",
      "439:\tlearn: 0.0945196\ttotal: 3.42s\tremaining: 4.35s\n",
      "440:\tlearn: 0.0944833\ttotal: 3.45s\tremaining: 4.37s\n",
      "441:\tlearn: 0.0944460\ttotal: 3.46s\tremaining: 4.37s\n",
      "442:\tlearn: 0.0944139\ttotal: 3.48s\tremaining: 4.38s\n",
      "443:\tlearn: 0.0943935\ttotal: 3.49s\tremaining: 4.37s\n",
      "444:\tlearn: 0.0943678\ttotal: 3.5s\tremaining: 4.36s\n",
      "445:\tlearn: 0.0943510\ttotal: 3.51s\tremaining: 4.36s\n",
      "446:\tlearn: 0.0943217\ttotal: 3.52s\tremaining: 4.35s\n",
      "447:\tlearn: 0.0942958\ttotal: 3.53s\tremaining: 4.35s\n",
      "448:\tlearn: 0.0942595\ttotal: 3.54s\tremaining: 4.34s\n",
      "449:\tlearn: 0.0942402\ttotal: 3.55s\tremaining: 4.33s\n",
      "450:\tlearn: 0.0942053\ttotal: 3.55s\tremaining: 4.33s\n",
      "451:\tlearn: 0.0941671\ttotal: 3.56s\tremaining: 4.32s\n",
      "452:\tlearn: 0.0941427\ttotal: 3.57s\tremaining: 4.31s\n",
      "453:\tlearn: 0.0941072\ttotal: 3.58s\tremaining: 4.3s\n",
      "454:\tlearn: 0.0940827\ttotal: 3.59s\tremaining: 4.29s\n",
      "455:\tlearn: 0.0940616\ttotal: 3.59s\tremaining: 4.28s\n",
      "456:\tlearn: 0.0940320\ttotal: 3.6s\tremaining: 4.27s\n",
      "457:\tlearn: 0.0939869\ttotal: 3.62s\tremaining: 4.28s\n",
      "458:\tlearn: 0.0939623\ttotal: 3.62s\tremaining: 4.27s\n",
      "459:\tlearn: 0.0939296\ttotal: 3.63s\tremaining: 4.26s\n",
      "460:\tlearn: 0.0938958\ttotal: 3.64s\tremaining: 4.26s\n",
      "461:\tlearn: 0.0938709\ttotal: 3.66s\tremaining: 4.26s\n",
      "462:\tlearn: 0.0938462\ttotal: 3.67s\tremaining: 4.26s\n",
      "463:\tlearn: 0.0938201\ttotal: 3.68s\tremaining: 4.25s\n",
      "464:\tlearn: 0.0937843\ttotal: 3.69s\tremaining: 4.24s\n",
      "465:\tlearn: 0.0937653\ttotal: 3.7s\tremaining: 4.25s\n",
      "466:\tlearn: 0.0937450\ttotal: 3.71s\tremaining: 4.24s\n",
      "467:\tlearn: 0.0937168\ttotal: 3.72s\tremaining: 4.23s\n",
      "468:\tlearn: 0.0936946\ttotal: 3.73s\tremaining: 4.22s\n",
      "469:\tlearn: 0.0936758\ttotal: 3.74s\tremaining: 4.22s\n",
      "470:\tlearn: 0.0936575\ttotal: 3.75s\tremaining: 4.22s\n",
      "471:\tlearn: 0.0936350\ttotal: 3.76s\tremaining: 4.21s\n",
      "472:\tlearn: 0.0936037\ttotal: 3.77s\tremaining: 4.2s\n",
      "473:\tlearn: 0.0935917\ttotal: 3.78s\tremaining: 4.19s\n",
      "474:\tlearn: 0.0935742\ttotal: 3.79s\tremaining: 4.19s\n",
      "475:\tlearn: 0.0935510\ttotal: 3.81s\tremaining: 4.19s\n",
      "476:\tlearn: 0.0935250\ttotal: 3.81s\tremaining: 4.18s\n",
      "477:\tlearn: 0.0934844\ttotal: 3.82s\tremaining: 4.17s\n",
      "478:\tlearn: 0.0934369\ttotal: 3.83s\tremaining: 4.17s\n",
      "479:\tlearn: 0.0934092\ttotal: 3.84s\tremaining: 4.17s\n",
      "480:\tlearn: 0.0933975\ttotal: 3.85s\tremaining: 4.16s\n",
      "481:\tlearn: 0.0933262\ttotal: 3.86s\tremaining: 4.15s\n",
      "482:\tlearn: 0.0932968\ttotal: 3.86s\tremaining: 4.14s\n",
      "483:\tlearn: 0.0932775\ttotal: 3.88s\tremaining: 4.14s\n",
      "484:\tlearn: 0.0932390\ttotal: 3.89s\tremaining: 4.13s\n",
      "485:\tlearn: 0.0932118\ttotal: 3.9s\tremaining: 4.12s\n",
      "486:\tlearn: 0.0931647\ttotal: 3.9s\tremaining: 4.11s\n",
      "487:\tlearn: 0.0931371\ttotal: 3.91s\tremaining: 4.11s\n",
      "488:\tlearn: 0.0931143\ttotal: 3.93s\tremaining: 4.1s\n",
      "489:\tlearn: 0.0930856\ttotal: 3.94s\tremaining: 4.09s\n",
      "490:\tlearn: 0.0930468\ttotal: 3.94s\tremaining: 4.08s\n",
      "491:\tlearn: 0.0930274\ttotal: 3.95s\tremaining: 4.08s\n",
      "492:\tlearn: 0.0930009\ttotal: 3.95s\tremaining: 4.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493:\tlearn: 0.0929824\ttotal: 3.97s\tremaining: 4.07s\n",
      "494:\tlearn: 0.0929616\ttotal: 3.98s\tremaining: 4.06s\n",
      "495:\tlearn: 0.0929375\ttotal: 3.99s\tremaining: 4.05s\n",
      "496:\tlearn: 0.0928699\ttotal: 4s\tremaining: 4.04s\n",
      "497:\tlearn: 0.0928480\ttotal: 4.01s\tremaining: 4.04s\n",
      "498:\tlearn: 0.0928193\ttotal: 4.02s\tremaining: 4.03s\n",
      "499:\tlearn: 0.0928039\ttotal: 4.03s\tremaining: 4.03s\n",
      "500:\tlearn: 0.0927673\ttotal: 4.05s\tremaining: 4.03s\n",
      "501:\tlearn: 0.0927281\ttotal: 4.06s\tremaining: 4.03s\n",
      "502:\tlearn: 0.0927113\ttotal: 4.07s\tremaining: 4.02s\n",
      "503:\tlearn: 0.0926724\ttotal: 4.08s\tremaining: 4.02s\n",
      "504:\tlearn: 0.0926593\ttotal: 4.11s\tremaining: 4.03s\n",
      "505:\tlearn: 0.0926293\ttotal: 4.12s\tremaining: 4.02s\n",
      "506:\tlearn: 0.0925716\ttotal: 4.14s\tremaining: 4.03s\n",
      "507:\tlearn: 0.0925307\ttotal: 4.16s\tremaining: 4.03s\n",
      "508:\tlearn: 0.0924997\ttotal: 4.17s\tremaining: 4.02s\n",
      "509:\tlearn: 0.0924760\ttotal: 4.18s\tremaining: 4.02s\n",
      "510:\tlearn: 0.0924425\ttotal: 4.2s\tremaining: 4.02s\n",
      "511:\tlearn: 0.0924234\ttotal: 4.21s\tremaining: 4.01s\n",
      "512:\tlearn: 0.0923779\ttotal: 4.22s\tremaining: 4s\n",
      "513:\tlearn: 0.0923561\ttotal: 4.22s\tremaining: 3.99s\n",
      "514:\tlearn: 0.0923351\ttotal: 4.24s\tremaining: 4s\n",
      "515:\tlearn: 0.0923139\ttotal: 4.25s\tremaining: 3.99s\n",
      "516:\tlearn: 0.0922971\ttotal: 4.26s\tremaining: 3.98s\n",
      "517:\tlearn: 0.0922373\ttotal: 4.27s\tremaining: 3.97s\n",
      "518:\tlearn: 0.0921984\ttotal: 4.29s\tremaining: 3.97s\n",
      "519:\tlearn: 0.0921566\ttotal: 4.3s\tremaining: 3.97s\n",
      "520:\tlearn: 0.0921308\ttotal: 4.33s\tremaining: 3.98s\n",
      "521:\tlearn: 0.0920963\ttotal: 4.34s\tremaining: 3.97s\n",
      "522:\tlearn: 0.0920797\ttotal: 4.46s\tremaining: 4.07s\n",
      "523:\tlearn: 0.0920352\ttotal: 4.65s\tremaining: 4.22s\n",
      "524:\tlearn: 0.0920089\ttotal: 4.68s\tremaining: 4.23s\n",
      "525:\tlearn: 0.0919818\ttotal: 4.71s\tremaining: 4.25s\n",
      "526:\tlearn: 0.0919652\ttotal: 4.76s\tremaining: 4.28s\n",
      "527:\tlearn: 0.0919380\ttotal: 4.79s\tremaining: 4.28s\n",
      "528:\tlearn: 0.0918930\ttotal: 4.8s\tremaining: 4.27s\n",
      "529:\tlearn: 0.0918732\ttotal: 4.82s\tremaining: 4.27s\n",
      "530:\tlearn: 0.0918578\ttotal: 4.83s\tremaining: 4.26s\n",
      "531:\tlearn: 0.0918011\ttotal: 4.84s\tremaining: 4.26s\n",
      "532:\tlearn: 0.0917851\ttotal: 4.86s\tremaining: 4.25s\n",
      "533:\tlearn: 0.0917457\ttotal: 4.87s\tremaining: 4.25s\n",
      "534:\tlearn: 0.0917180\ttotal: 4.88s\tremaining: 4.24s\n",
      "535:\tlearn: 0.0917039\ttotal: 4.89s\tremaining: 4.23s\n",
      "536:\tlearn: 0.0916548\ttotal: 4.9s\tremaining: 4.23s\n",
      "537:\tlearn: 0.0916441\ttotal: 4.92s\tremaining: 4.22s\n",
      "538:\tlearn: 0.0916307\ttotal: 4.93s\tremaining: 4.22s\n",
      "539:\tlearn: 0.0915972\ttotal: 4.95s\tremaining: 4.21s\n",
      "540:\tlearn: 0.0915651\ttotal: 4.97s\tremaining: 4.22s\n",
      "541:\tlearn: 0.0915179\ttotal: 4.99s\tremaining: 4.22s\n",
      "542:\tlearn: 0.0914911\ttotal: 5s\tremaining: 4.21s\n",
      "543:\tlearn: 0.0914676\ttotal: 5.02s\tremaining: 4.21s\n",
      "544:\tlearn: 0.0914450\ttotal: 5.03s\tremaining: 4.2s\n",
      "545:\tlearn: 0.0914147\ttotal: 5.05s\tremaining: 4.2s\n",
      "546:\tlearn: 0.0914044\ttotal: 5.06s\tremaining: 4.19s\n",
      "547:\tlearn: 0.0913803\ttotal: 5.07s\tremaining: 4.18s\n",
      "548:\tlearn: 0.0913681\ttotal: 5.09s\tremaining: 4.18s\n",
      "549:\tlearn: 0.0913369\ttotal: 5.1s\tremaining: 4.17s\n",
      "550:\tlearn: 0.0913102\ttotal: 5.11s\tremaining: 4.17s\n",
      "551:\tlearn: 0.0912939\ttotal: 5.12s\tremaining: 4.16s\n",
      "552:\tlearn: 0.0912732\ttotal: 5.14s\tremaining: 4.15s\n",
      "553:\tlearn: 0.0912482\ttotal: 5.15s\tremaining: 4.14s\n",
      "554:\tlearn: 0.0912239\ttotal: 5.16s\tremaining: 4.13s\n",
      "555:\tlearn: 0.0911957\ttotal: 5.17s\tremaining: 4.13s\n",
      "556:\tlearn: 0.0911745\ttotal: 5.18s\tremaining: 4.12s\n",
      "557:\tlearn: 0.0911529\ttotal: 5.2s\tremaining: 4.12s\n",
      "558:\tlearn: 0.0910995\ttotal: 5.21s\tremaining: 4.11s\n",
      "559:\tlearn: 0.0910809\ttotal: 5.21s\tremaining: 4.1s\n",
      "560:\tlearn: 0.0910361\ttotal: 5.23s\tremaining: 4.09s\n",
      "561:\tlearn: 0.0910154\ttotal: 5.24s\tremaining: 4.08s\n",
      "562:\tlearn: 0.0910000\ttotal: 5.26s\tremaining: 4.08s\n",
      "563:\tlearn: 0.0909850\ttotal: 5.27s\tremaining: 4.07s\n",
      "564:\tlearn: 0.0909492\ttotal: 5.28s\tremaining: 4.06s\n",
      "565:\tlearn: 0.0909193\ttotal: 5.29s\tremaining: 4.06s\n",
      "566:\tlearn: 0.0909059\ttotal: 5.3s\tremaining: 4.05s\n",
      "567:\tlearn: 0.0908922\ttotal: 5.32s\tremaining: 4.04s\n",
      "568:\tlearn: 0.0908415\ttotal: 5.33s\tremaining: 4.04s\n",
      "569:\tlearn: 0.0908189\ttotal: 5.34s\tremaining: 4.03s\n",
      "570:\tlearn: 0.0907950\ttotal: 5.35s\tremaining: 4.02s\n",
      "571:\tlearn: 0.0907436\ttotal: 5.36s\tremaining: 4.01s\n",
      "572:\tlearn: 0.0907288\ttotal: 5.37s\tremaining: 4s\n",
      "573:\tlearn: 0.0907150\ttotal: 5.38s\tremaining: 4s\n",
      "574:\tlearn: 0.0907051\ttotal: 5.39s\tremaining: 3.99s\n",
      "575:\tlearn: 0.0906777\ttotal: 5.41s\tremaining: 3.98s\n",
      "576:\tlearn: 0.0906467\ttotal: 5.43s\tremaining: 3.98s\n",
      "577:\tlearn: 0.0906188\ttotal: 5.44s\tremaining: 3.97s\n",
      "578:\tlearn: 0.0906040\ttotal: 5.45s\tremaining: 3.96s\n",
      "579:\tlearn: 0.0905918\ttotal: 5.46s\tremaining: 3.95s\n",
      "580:\tlearn: 0.0905768\ttotal: 5.47s\tremaining: 3.94s\n",
      "581:\tlearn: 0.0905538\ttotal: 5.48s\tremaining: 3.94s\n",
      "582:\tlearn: 0.0905349\ttotal: 5.5s\tremaining: 3.93s\n",
      "583:\tlearn: 0.0905069\ttotal: 5.51s\tremaining: 3.92s\n",
      "584:\tlearn: 0.0904844\ttotal: 5.52s\tremaining: 3.92s\n",
      "585:\tlearn: 0.0904638\ttotal: 5.54s\tremaining: 3.91s\n",
      "586:\tlearn: 0.0904401\ttotal: 5.57s\tremaining: 3.92s\n",
      "587:\tlearn: 0.0904155\ttotal: 5.59s\tremaining: 3.92s\n",
      "588:\tlearn: 0.0904015\ttotal: 5.63s\tremaining: 3.92s\n",
      "589:\tlearn: 0.0903844\ttotal: 5.65s\tremaining: 3.93s\n",
      "590:\tlearn: 0.0903655\ttotal: 5.66s\tremaining: 3.92s\n",
      "591:\tlearn: 0.0903493\ttotal: 5.68s\tremaining: 3.91s\n",
      "592:\tlearn: 0.0903222\ttotal: 5.69s\tremaining: 3.9s\n",
      "593:\tlearn: 0.0902967\ttotal: 5.7s\tremaining: 3.9s\n",
      "594:\tlearn: 0.0902588\ttotal: 5.71s\tremaining: 3.89s\n",
      "595:\tlearn: 0.0902433\ttotal: 5.72s\tremaining: 3.88s\n",
      "596:\tlearn: 0.0902182\ttotal: 5.73s\tremaining: 3.87s\n",
      "597:\tlearn: 0.0902062\ttotal: 5.74s\tremaining: 3.86s\n",
      "598:\tlearn: 0.0901908\ttotal: 5.75s\tremaining: 3.85s\n",
      "599:\tlearn: 0.0901709\ttotal: 5.76s\tremaining: 3.84s\n",
      "600:\tlearn: 0.0901351\ttotal: 5.78s\tremaining: 3.84s\n",
      "601:\tlearn: 0.0901178\ttotal: 5.79s\tremaining: 3.83s\n",
      "602:\tlearn: 0.0901033\ttotal: 5.8s\tremaining: 3.82s\n",
      "603:\tlearn: 0.0900922\ttotal: 5.81s\tremaining: 3.81s\n",
      "604:\tlearn: 0.0900776\ttotal: 5.82s\tremaining: 3.8s\n",
      "605:\tlearn: 0.0900600\ttotal: 5.83s\tremaining: 3.79s\n",
      "606:\tlearn: 0.0900307\ttotal: 5.84s\tremaining: 3.78s\n",
      "607:\tlearn: 0.0900080\ttotal: 5.85s\tremaining: 3.77s\n",
      "608:\tlearn: 0.0899955\ttotal: 5.86s\tremaining: 3.76s\n",
      "609:\tlearn: 0.0899776\ttotal: 5.87s\tremaining: 3.75s\n",
      "610:\tlearn: 0.0899642\ttotal: 5.89s\tremaining: 3.75s\n",
      "611:\tlearn: 0.0899546\ttotal: 5.9s\tremaining: 3.74s\n",
      "612:\tlearn: 0.0899318\ttotal: 5.92s\tremaining: 3.73s\n",
      "613:\tlearn: 0.0899070\ttotal: 5.93s\tremaining: 3.73s\n",
      "614:\tlearn: 0.0898974\ttotal: 5.94s\tremaining: 3.72s\n",
      "615:\tlearn: 0.0898661\ttotal: 5.95s\tremaining: 3.71s\n",
      "616:\tlearn: 0.0898406\ttotal: 5.96s\tremaining: 3.7s\n",
      "617:\tlearn: 0.0898260\ttotal: 5.98s\tremaining: 3.69s\n",
      "618:\tlearn: 0.0898152\ttotal: 5.99s\tremaining: 3.69s\n",
      "619:\tlearn: 0.0897836\ttotal: 6s\tremaining: 3.68s\n",
      "620:\tlearn: 0.0897669\ttotal: 6.01s\tremaining: 3.67s\n",
      "621:\tlearn: 0.0897471\ttotal: 6.03s\tremaining: 3.66s\n",
      "622:\tlearn: 0.0897247\ttotal: 6.04s\tremaining: 3.66s\n",
      "623:\tlearn: 0.0897040\ttotal: 6.05s\tremaining: 3.65s\n",
      "624:\tlearn: 0.0896918\ttotal: 6.07s\tremaining: 3.64s\n",
      "625:\tlearn: 0.0896551\ttotal: 6.08s\tremaining: 3.63s\n",
      "626:\tlearn: 0.0896440\ttotal: 6.1s\tremaining: 3.63s\n",
      "627:\tlearn: 0.0896262\ttotal: 6.11s\tremaining: 3.62s\n",
      "628:\tlearn: 0.0896142\ttotal: 6.12s\tremaining: 3.61s\n",
      "629:\tlearn: 0.0895978\ttotal: 6.13s\tremaining: 3.6s\n",
      "630:\tlearn: 0.0895789\ttotal: 6.14s\tremaining: 3.59s\n",
      "631:\tlearn: 0.0895493\ttotal: 6.16s\tremaining: 3.58s\n",
      "632:\tlearn: 0.0895193\ttotal: 6.17s\tremaining: 3.58s\n",
      "633:\tlearn: 0.0895073\ttotal: 6.19s\tremaining: 3.57s\n",
      "634:\tlearn: 0.0894899\ttotal: 6.2s\tremaining: 3.56s\n",
      "635:\tlearn: 0.0894479\ttotal: 6.21s\tremaining: 3.56s\n",
      "636:\tlearn: 0.0894328\ttotal: 6.23s\tremaining: 3.55s\n",
      "637:\tlearn: 0.0894198\ttotal: 6.24s\tremaining: 3.54s\n",
      "638:\tlearn: 0.0894098\ttotal: 6.25s\tremaining: 3.53s\n",
      "639:\tlearn: 0.0893645\ttotal: 6.27s\tremaining: 3.52s\n",
      "640:\tlearn: 0.0893536\ttotal: 6.28s\tremaining: 3.52s\n",
      "641:\tlearn: 0.0893353\ttotal: 6.29s\tremaining: 3.51s\n",
      "642:\tlearn: 0.0893235\ttotal: 6.3s\tremaining: 3.5s\n",
      "643:\tlearn: 0.0893110\ttotal: 6.32s\tremaining: 3.49s\n",
      "644:\tlearn: 0.0892796\ttotal: 6.33s\tremaining: 3.48s\n",
      "645:\tlearn: 0.0892489\ttotal: 6.35s\tremaining: 3.48s\n",
      "646:\tlearn: 0.0892378\ttotal: 6.36s\tremaining: 3.47s\n",
      "647:\tlearn: 0.0892061\ttotal: 6.38s\tremaining: 3.47s\n",
      "648:\tlearn: 0.0891839\ttotal: 6.39s\tremaining: 3.46s\n",
      "649:\tlearn: 0.0891662\ttotal: 6.41s\tremaining: 3.45s\n",
      "650:\tlearn: 0.0891505\ttotal: 6.42s\tremaining: 3.44s\n",
      "651:\tlearn: 0.0891372\ttotal: 6.44s\tremaining: 3.44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652:\tlearn: 0.0890908\ttotal: 6.45s\tremaining: 3.43s\n",
      "653:\tlearn: 0.0890816\ttotal: 6.46s\tremaining: 3.42s\n",
      "654:\tlearn: 0.0890673\ttotal: 6.48s\tremaining: 3.41s\n",
      "655:\tlearn: 0.0890356\ttotal: 6.49s\tremaining: 3.4s\n",
      "656:\tlearn: 0.0889922\ttotal: 6.5s\tremaining: 3.39s\n",
      "657:\tlearn: 0.0889634\ttotal: 6.51s\tremaining: 3.38s\n",
      "658:\tlearn: 0.0889487\ttotal: 6.53s\tremaining: 3.38s\n",
      "659:\tlearn: 0.0889210\ttotal: 6.54s\tremaining: 3.37s\n",
      "660:\tlearn: 0.0889064\ttotal: 6.56s\tremaining: 3.36s\n",
      "661:\tlearn: 0.0888904\ttotal: 6.58s\tremaining: 3.36s\n",
      "662:\tlearn: 0.0888787\ttotal: 6.61s\tremaining: 3.36s\n",
      "663:\tlearn: 0.0888647\ttotal: 6.64s\tremaining: 3.36s\n",
      "664:\tlearn: 0.0888509\ttotal: 6.68s\tremaining: 3.37s\n",
      "665:\tlearn: 0.0888441\ttotal: 6.7s\tremaining: 3.36s\n",
      "666:\tlearn: 0.0888126\ttotal: 6.71s\tremaining: 3.35s\n",
      "667:\tlearn: 0.0887815\ttotal: 6.73s\tremaining: 3.34s\n",
      "668:\tlearn: 0.0887754\ttotal: 6.74s\tremaining: 3.33s\n",
      "669:\tlearn: 0.0887683\ttotal: 6.75s\tremaining: 3.32s\n",
      "670:\tlearn: 0.0887554\ttotal: 6.76s\tremaining: 3.31s\n",
      "671:\tlearn: 0.0887291\ttotal: 6.78s\tremaining: 3.31s\n",
      "672:\tlearn: 0.0887136\ttotal: 6.79s\tremaining: 3.3s\n",
      "673:\tlearn: 0.0887007\ttotal: 6.8s\tremaining: 3.29s\n",
      "674:\tlearn: 0.0886842\ttotal: 6.81s\tremaining: 3.28s\n",
      "675:\tlearn: 0.0886611\ttotal: 6.84s\tremaining: 3.28s\n",
      "676:\tlearn: 0.0886532\ttotal: 6.85s\tremaining: 3.27s\n",
      "677:\tlearn: 0.0886294\ttotal: 6.86s\tremaining: 3.26s\n",
      "678:\tlearn: 0.0885890\ttotal: 6.87s\tremaining: 3.25s\n",
      "679:\tlearn: 0.0885762\ttotal: 6.88s\tremaining: 3.24s\n",
      "680:\tlearn: 0.0885604\ttotal: 6.9s\tremaining: 3.23s\n",
      "681:\tlearn: 0.0885449\ttotal: 6.92s\tremaining: 3.22s\n",
      "682:\tlearn: 0.0885327\ttotal: 6.93s\tremaining: 3.22s\n",
      "683:\tlearn: 0.0885236\ttotal: 6.95s\tremaining: 3.21s\n",
      "684:\tlearn: 0.0885071\ttotal: 6.96s\tremaining: 3.2s\n",
      "685:\tlearn: 0.0884991\ttotal: 6.97s\tremaining: 3.19s\n",
      "686:\tlearn: 0.0884661\ttotal: 6.99s\tremaining: 3.18s\n",
      "687:\tlearn: 0.0884531\ttotal: 7s\tremaining: 3.17s\n",
      "688:\tlearn: 0.0884242\ttotal: 7.01s\tremaining: 3.16s\n",
      "689:\tlearn: 0.0884144\ttotal: 7.03s\tremaining: 3.16s\n",
      "690:\tlearn: 0.0884012\ttotal: 7.04s\tremaining: 3.15s\n",
      "691:\tlearn: 0.0883930\ttotal: 7.05s\tremaining: 3.14s\n",
      "692:\tlearn: 0.0883757\ttotal: 7.06s\tremaining: 3.13s\n",
      "693:\tlearn: 0.0883572\ttotal: 7.08s\tremaining: 3.12s\n",
      "694:\tlearn: 0.0883389\ttotal: 7.08s\tremaining: 3.11s\n",
      "695:\tlearn: 0.0883197\ttotal: 7.09s\tremaining: 3.1s\n",
      "696:\tlearn: 0.0883021\ttotal: 7.11s\tremaining: 3.09s\n",
      "697:\tlearn: 0.0882896\ttotal: 7.12s\tremaining: 3.08s\n",
      "698:\tlearn: 0.0882780\ttotal: 7.13s\tremaining: 3.07s\n",
      "699:\tlearn: 0.0882618\ttotal: 7.15s\tremaining: 3.06s\n",
      "700:\tlearn: 0.0882413\ttotal: 7.16s\tremaining: 3.05s\n",
      "701:\tlearn: 0.0882330\ttotal: 7.17s\tremaining: 3.04s\n",
      "702:\tlearn: 0.0881949\ttotal: 7.18s\tremaining: 3.04s\n",
      "703:\tlearn: 0.0881848\ttotal: 7.2s\tremaining: 3.02s\n",
      "704:\tlearn: 0.0881667\ttotal: 7.2s\tremaining: 3.01s\n",
      "705:\tlearn: 0.0881465\ttotal: 7.21s\tremaining: 3s\n",
      "706:\tlearn: 0.0881270\ttotal: 7.22s\tremaining: 2.99s\n",
      "707:\tlearn: 0.0881192\ttotal: 7.24s\tremaining: 2.98s\n",
      "708:\tlearn: 0.0881033\ttotal: 7.25s\tremaining: 2.97s\n",
      "709:\tlearn: 0.0880757\ttotal: 7.26s\tremaining: 2.96s\n",
      "710:\tlearn: 0.0880535\ttotal: 7.27s\tremaining: 2.96s\n",
      "711:\tlearn: 0.0880490\ttotal: 7.29s\tremaining: 2.95s\n",
      "712:\tlearn: 0.0880359\ttotal: 7.3s\tremaining: 2.94s\n",
      "713:\tlearn: 0.0880021\ttotal: 7.31s\tremaining: 2.93s\n",
      "714:\tlearn: 0.0879941\ttotal: 7.33s\tremaining: 2.92s\n",
      "715:\tlearn: 0.0879831\ttotal: 7.34s\tremaining: 2.91s\n",
      "716:\tlearn: 0.0879655\ttotal: 7.35s\tremaining: 2.9s\n",
      "717:\tlearn: 0.0879583\ttotal: 7.37s\tremaining: 2.9s\n",
      "718:\tlearn: 0.0879488\ttotal: 7.38s\tremaining: 2.88s\n",
      "719:\tlearn: 0.0879420\ttotal: 7.39s\tremaining: 2.87s\n",
      "720:\tlearn: 0.0879221\ttotal: 7.41s\tremaining: 2.87s\n",
      "721:\tlearn: 0.0878976\ttotal: 7.44s\tremaining: 2.86s\n",
      "722:\tlearn: 0.0878883\ttotal: 7.45s\tremaining: 2.85s\n",
      "723:\tlearn: 0.0878700\ttotal: 7.46s\tremaining: 2.85s\n",
      "724:\tlearn: 0.0878565\ttotal: 7.48s\tremaining: 2.84s\n",
      "725:\tlearn: 0.0878235\ttotal: 7.49s\tremaining: 2.83s\n",
      "726:\tlearn: 0.0877970\ttotal: 7.5s\tremaining: 2.82s\n",
      "727:\tlearn: 0.0877921\ttotal: 7.51s\tremaining: 2.81s\n",
      "728:\tlearn: 0.0877725\ttotal: 7.53s\tremaining: 2.8s\n",
      "729:\tlearn: 0.0877602\ttotal: 7.54s\tremaining: 2.79s\n",
      "730:\tlearn: 0.0877487\ttotal: 7.56s\tremaining: 2.78s\n",
      "731:\tlearn: 0.0877313\ttotal: 7.58s\tremaining: 2.77s\n",
      "732:\tlearn: 0.0876911\ttotal: 7.62s\tremaining: 2.78s\n",
      "733:\tlearn: 0.0876786\ttotal: 7.67s\tremaining: 2.78s\n",
      "734:\tlearn: 0.0876473\ttotal: 7.7s\tremaining: 2.78s\n",
      "735:\tlearn: 0.0876319\ttotal: 7.73s\tremaining: 2.77s\n",
      "736:\tlearn: 0.0876210\ttotal: 7.75s\tremaining: 2.77s\n",
      "737:\tlearn: 0.0876058\ttotal: 7.76s\tremaining: 2.76s\n",
      "738:\tlearn: 0.0875835\ttotal: 7.79s\tremaining: 2.75s\n",
      "739:\tlearn: 0.0875523\ttotal: 7.8s\tremaining: 2.74s\n",
      "740:\tlearn: 0.0875294\ttotal: 7.81s\tremaining: 2.73s\n",
      "741:\tlearn: 0.0875090\ttotal: 7.82s\tremaining: 2.72s\n",
      "742:\tlearn: 0.0874884\ttotal: 7.83s\tremaining: 2.71s\n",
      "743:\tlearn: 0.0874770\ttotal: 7.84s\tremaining: 2.7s\n",
      "744:\tlearn: 0.0874589\ttotal: 7.85s\tremaining: 2.69s\n",
      "745:\tlearn: 0.0874423\ttotal: 7.86s\tremaining: 2.68s\n",
      "746:\tlearn: 0.0874289\ttotal: 7.87s\tremaining: 2.67s\n",
      "747:\tlearn: 0.0874145\ttotal: 7.88s\tremaining: 2.66s\n",
      "748:\tlearn: 0.0873991\ttotal: 7.89s\tremaining: 2.65s\n",
      "749:\tlearn: 0.0873928\ttotal: 7.91s\tremaining: 2.64s\n",
      "750:\tlearn: 0.0873655\ttotal: 7.92s\tremaining: 2.63s\n",
      "751:\tlearn: 0.0873453\ttotal: 7.94s\tremaining: 2.62s\n",
      "752:\tlearn: 0.0873340\ttotal: 7.96s\tremaining: 2.61s\n",
      "753:\tlearn: 0.0873145\ttotal: 7.97s\tremaining: 2.6s\n",
      "754:\tlearn: 0.0873093\ttotal: 7.98s\tremaining: 2.59s\n",
      "755:\tlearn: 0.0872977\ttotal: 8s\tremaining: 2.58s\n",
      "756:\tlearn: 0.0872586\ttotal: 8.01s\tremaining: 2.57s\n",
      "757:\tlearn: 0.0872330\ttotal: 8.02s\tremaining: 2.56s\n",
      "758:\tlearn: 0.0872191\ttotal: 8.03s\tremaining: 2.55s\n",
      "759:\tlearn: 0.0872037\ttotal: 8.04s\tremaining: 2.54s\n",
      "760:\tlearn: 0.0871795\ttotal: 8.06s\tremaining: 2.53s\n",
      "761:\tlearn: 0.0871661\ttotal: 8.07s\tremaining: 2.52s\n",
      "762:\tlearn: 0.0871567\ttotal: 8.08s\tremaining: 2.51s\n",
      "763:\tlearn: 0.0871233\ttotal: 8.1s\tremaining: 2.5s\n",
      "764:\tlearn: 0.0871151\ttotal: 8.11s\tremaining: 2.49s\n",
      "765:\tlearn: 0.0871047\ttotal: 8.12s\tremaining: 2.48s\n",
      "766:\tlearn: 0.0870875\ttotal: 8.13s\tremaining: 2.47s\n",
      "767:\tlearn: 0.0870753\ttotal: 8.14s\tremaining: 2.46s\n",
      "768:\tlearn: 0.0870657\ttotal: 8.15s\tremaining: 2.45s\n",
      "769:\tlearn: 0.0870478\ttotal: 8.16s\tremaining: 2.44s\n",
      "770:\tlearn: 0.0870374\ttotal: 8.17s\tremaining: 2.43s\n",
      "771:\tlearn: 0.0870275\ttotal: 8.18s\tremaining: 2.42s\n",
      "772:\tlearn: 0.0870107\ttotal: 8.2s\tremaining: 2.41s\n",
      "773:\tlearn: 0.0869940\ttotal: 8.21s\tremaining: 2.4s\n",
      "774:\tlearn: 0.0869727\ttotal: 8.22s\tremaining: 2.39s\n",
      "775:\tlearn: 0.0869483\ttotal: 8.24s\tremaining: 2.38s\n",
      "776:\tlearn: 0.0869392\ttotal: 8.25s\tremaining: 2.37s\n",
      "777:\tlearn: 0.0869339\ttotal: 8.26s\tremaining: 2.36s\n",
      "778:\tlearn: 0.0869209\ttotal: 8.27s\tremaining: 2.35s\n",
      "779:\tlearn: 0.0869100\ttotal: 8.28s\tremaining: 2.34s\n",
      "780:\tlearn: 0.0868975\ttotal: 8.29s\tremaining: 2.33s\n",
      "781:\tlearn: 0.0868857\ttotal: 8.31s\tremaining: 2.32s\n",
      "782:\tlearn: 0.0868721\ttotal: 8.32s\tremaining: 2.31s\n",
      "783:\tlearn: 0.0868589\ttotal: 8.34s\tremaining: 2.3s\n",
      "784:\tlearn: 0.0868517\ttotal: 8.35s\tremaining: 2.29s\n",
      "785:\tlearn: 0.0868385\ttotal: 8.37s\tremaining: 2.28s\n",
      "786:\tlearn: 0.0868122\ttotal: 8.38s\tremaining: 2.27s\n",
      "787:\tlearn: 0.0867846\ttotal: 8.4s\tremaining: 2.26s\n",
      "788:\tlearn: 0.0867686\ttotal: 8.41s\tremaining: 2.25s\n",
      "789:\tlearn: 0.0867500\ttotal: 8.42s\tremaining: 2.24s\n",
      "790:\tlearn: 0.0867456\ttotal: 8.46s\tremaining: 2.23s\n",
      "791:\tlearn: 0.0867391\ttotal: 8.48s\tremaining: 2.23s\n",
      "792:\tlearn: 0.0867101\ttotal: 8.5s\tremaining: 2.22s\n",
      "793:\tlearn: 0.0866983\ttotal: 8.51s\tremaining: 2.21s\n",
      "794:\tlearn: 0.0866727\ttotal: 8.53s\tremaining: 2.2s\n",
      "795:\tlearn: 0.0866477\ttotal: 8.54s\tremaining: 2.19s\n",
      "796:\tlearn: 0.0866307\ttotal: 8.55s\tremaining: 2.18s\n",
      "797:\tlearn: 0.0866082\ttotal: 8.56s\tremaining: 2.17s\n",
      "798:\tlearn: 0.0865972\ttotal: 8.58s\tremaining: 2.16s\n",
      "799:\tlearn: 0.0865736\ttotal: 8.6s\tremaining: 2.15s\n",
      "800:\tlearn: 0.0865669\ttotal: 8.61s\tremaining: 2.14s\n",
      "801:\tlearn: 0.0865440\ttotal: 8.62s\tremaining: 2.13s\n",
      "802:\tlearn: 0.0865202\ttotal: 8.64s\tremaining: 2.12s\n",
      "803:\tlearn: 0.0865110\ttotal: 8.68s\tremaining: 2.12s\n",
      "804:\tlearn: 0.0864762\ttotal: 8.7s\tremaining: 2.11s\n",
      "805:\tlearn: 0.0864545\ttotal: 8.77s\tremaining: 2.11s\n",
      "806:\tlearn: 0.0864442\ttotal: 8.81s\tremaining: 2.11s\n",
      "807:\tlearn: 0.0864323\ttotal: 8.83s\tremaining: 2.1s\n",
      "808:\tlearn: 0.0864189\ttotal: 8.84s\tremaining: 2.09s\n",
      "809:\tlearn: 0.0864087\ttotal: 8.85s\tremaining: 2.08s\n",
      "810:\tlearn: 0.0864005\ttotal: 8.87s\tremaining: 2.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811:\tlearn: 0.0863957\ttotal: 8.88s\tremaining: 2.06s\n",
      "812:\tlearn: 0.0863867\ttotal: 8.89s\tremaining: 2.04s\n",
      "813:\tlearn: 0.0863650\ttotal: 8.91s\tremaining: 2.04s\n",
      "814:\tlearn: 0.0863533\ttotal: 8.92s\tremaining: 2.02s\n",
      "815:\tlearn: 0.0863320\ttotal: 8.93s\tremaining: 2.01s\n",
      "816:\tlearn: 0.0863215\ttotal: 8.95s\tremaining: 2s\n",
      "817:\tlearn: 0.0863112\ttotal: 8.96s\tremaining: 1.99s\n",
      "818:\tlearn: 0.0863065\ttotal: 8.98s\tremaining: 1.98s\n",
      "819:\tlearn: 0.0862956\ttotal: 8.99s\tremaining: 1.97s\n",
      "820:\tlearn: 0.0862746\ttotal: 9s\tremaining: 1.96s\n",
      "821:\tlearn: 0.0862490\ttotal: 9.01s\tremaining: 1.95s\n",
      "822:\tlearn: 0.0862388\ttotal: 9.03s\tremaining: 1.94s\n",
      "823:\tlearn: 0.0862254\ttotal: 9.04s\tremaining: 1.93s\n",
      "824:\tlearn: 0.0861830\ttotal: 9.05s\tremaining: 1.92s\n",
      "825:\tlearn: 0.0861783\ttotal: 9.07s\tremaining: 1.91s\n",
      "826:\tlearn: 0.0861645\ttotal: 9.08s\tremaining: 1.9s\n",
      "827:\tlearn: 0.0861563\ttotal: 9.1s\tremaining: 1.89s\n",
      "828:\tlearn: 0.0861366\ttotal: 9.11s\tremaining: 1.88s\n",
      "829:\tlearn: 0.0861217\ttotal: 9.13s\tremaining: 1.87s\n",
      "830:\tlearn: 0.0861136\ttotal: 9.14s\tremaining: 1.86s\n",
      "831:\tlearn: 0.0861073\ttotal: 9.15s\tremaining: 1.85s\n",
      "832:\tlearn: 0.0860976\ttotal: 9.16s\tremaining: 1.84s\n",
      "833:\tlearn: 0.0860919\ttotal: 9.17s\tremaining: 1.82s\n",
      "834:\tlearn: 0.0860836\ttotal: 9.19s\tremaining: 1.81s\n",
      "835:\tlearn: 0.0860790\ttotal: 9.2s\tremaining: 1.8s\n",
      "836:\tlearn: 0.0860701\ttotal: 9.21s\tremaining: 1.79s\n",
      "837:\tlearn: 0.0860555\ttotal: 9.23s\tremaining: 1.78s\n",
      "838:\tlearn: 0.0860374\ttotal: 9.24s\tremaining: 1.77s\n",
      "839:\tlearn: 0.0860089\ttotal: 9.25s\tremaining: 1.76s\n",
      "840:\tlearn: 0.0859943\ttotal: 9.26s\tremaining: 1.75s\n",
      "841:\tlearn: 0.0859792\ttotal: 9.28s\tremaining: 1.74s\n",
      "842:\tlearn: 0.0859613\ttotal: 9.29s\tremaining: 1.73s\n",
      "843:\tlearn: 0.0859507\ttotal: 9.31s\tremaining: 1.72s\n",
      "844:\tlearn: 0.0859393\ttotal: 9.32s\tremaining: 1.71s\n",
      "845:\tlearn: 0.0859266\ttotal: 9.34s\tremaining: 1.7s\n",
      "846:\tlearn: 0.0859152\ttotal: 9.36s\tremaining: 1.69s\n",
      "847:\tlearn: 0.0859066\ttotal: 9.38s\tremaining: 1.68s\n",
      "848:\tlearn: 0.0858895\ttotal: 9.39s\tremaining: 1.67s\n",
      "849:\tlearn: 0.0858797\ttotal: 9.4s\tremaining: 1.66s\n",
      "850:\tlearn: 0.0858728\ttotal: 9.41s\tremaining: 1.65s\n",
      "851:\tlearn: 0.0858642\ttotal: 9.44s\tremaining: 1.64s\n",
      "852:\tlearn: 0.0858513\ttotal: 9.45s\tremaining: 1.63s\n",
      "853:\tlearn: 0.0858385\ttotal: 9.47s\tremaining: 1.62s\n",
      "854:\tlearn: 0.0858296\ttotal: 9.48s\tremaining: 1.61s\n",
      "855:\tlearn: 0.0858206\ttotal: 9.49s\tremaining: 1.6s\n",
      "856:\tlearn: 0.0858118\ttotal: 9.51s\tremaining: 1.59s\n",
      "857:\tlearn: 0.0858029\ttotal: 9.53s\tremaining: 1.58s\n",
      "858:\tlearn: 0.0857951\ttotal: 9.54s\tremaining: 1.57s\n",
      "859:\tlearn: 0.0857852\ttotal: 9.55s\tremaining: 1.55s\n",
      "860:\tlearn: 0.0857761\ttotal: 9.56s\tremaining: 1.54s\n",
      "861:\tlearn: 0.0857577\ttotal: 9.57s\tremaining: 1.53s\n",
      "862:\tlearn: 0.0857483\ttotal: 9.59s\tremaining: 1.52s\n",
      "863:\tlearn: 0.0857425\ttotal: 9.6s\tremaining: 1.51s\n",
      "864:\tlearn: 0.0857265\ttotal: 9.61s\tremaining: 1.5s\n",
      "865:\tlearn: 0.0857230\ttotal: 9.62s\tremaining: 1.49s\n",
      "866:\tlearn: 0.0857039\ttotal: 9.63s\tremaining: 1.48s\n",
      "867:\tlearn: 0.0856955\ttotal: 9.64s\tremaining: 1.47s\n",
      "868:\tlearn: 0.0856791\ttotal: 9.66s\tremaining: 1.46s\n",
      "869:\tlearn: 0.0856642\ttotal: 9.68s\tremaining: 1.45s\n",
      "870:\tlearn: 0.0856397\ttotal: 9.69s\tremaining: 1.43s\n",
      "871:\tlearn: 0.0856299\ttotal: 9.7s\tremaining: 1.42s\n",
      "872:\tlearn: 0.0856072\ttotal: 9.77s\tremaining: 1.42s\n",
      "873:\tlearn: 0.0855967\ttotal: 9.82s\tremaining: 1.42s\n",
      "874:\tlearn: 0.0855823\ttotal: 9.86s\tremaining: 1.41s\n",
      "875:\tlearn: 0.0855712\ttotal: 9.88s\tremaining: 1.4s\n",
      "876:\tlearn: 0.0855584\ttotal: 9.89s\tremaining: 1.39s\n",
      "877:\tlearn: 0.0855500\ttotal: 9.91s\tremaining: 1.38s\n",
      "878:\tlearn: 0.0855395\ttotal: 9.92s\tremaining: 1.36s\n",
      "879:\tlearn: 0.0855315\ttotal: 9.93s\tremaining: 1.35s\n",
      "880:\tlearn: 0.0855048\ttotal: 9.95s\tremaining: 1.34s\n",
      "881:\tlearn: 0.0854897\ttotal: 9.96s\tremaining: 1.33s\n",
      "882:\tlearn: 0.0854607\ttotal: 9.97s\tremaining: 1.32s\n",
      "883:\tlearn: 0.0854452\ttotal: 9.98s\tremaining: 1.31s\n",
      "884:\tlearn: 0.0854356\ttotal: 10s\tremaining: 1.3s\n",
      "885:\tlearn: 0.0854302\ttotal: 10s\tremaining: 1.29s\n",
      "886:\tlearn: 0.0853973\ttotal: 10s\tremaining: 1.28s\n",
      "887:\tlearn: 0.0853832\ttotal: 10.1s\tremaining: 1.27s\n",
      "888:\tlearn: 0.0853752\ttotal: 10.1s\tremaining: 1.26s\n",
      "889:\tlearn: 0.0853668\ttotal: 10.1s\tremaining: 1.25s\n",
      "890:\tlearn: 0.0853519\ttotal: 10.1s\tremaining: 1.23s\n",
      "891:\tlearn: 0.0853461\ttotal: 10.1s\tremaining: 1.22s\n",
      "892:\tlearn: 0.0853344\ttotal: 10.1s\tremaining: 1.21s\n",
      "893:\tlearn: 0.0853283\ttotal: 10.1s\tremaining: 1.2s\n",
      "894:\tlearn: 0.0853174\ttotal: 10.2s\tremaining: 1.19s\n",
      "895:\tlearn: 0.0852998\ttotal: 10.2s\tremaining: 1.18s\n",
      "896:\tlearn: 0.0852894\ttotal: 10.2s\tremaining: 1.17s\n",
      "897:\tlearn: 0.0852842\ttotal: 10.2s\tremaining: 1.16s\n",
      "898:\tlearn: 0.0852757\ttotal: 10.2s\tremaining: 1.15s\n",
      "899:\tlearn: 0.0852663\ttotal: 10.2s\tremaining: 1.14s\n",
      "900:\tlearn: 0.0852590\ttotal: 10.2s\tremaining: 1.13s\n",
      "901:\tlearn: 0.0852506\ttotal: 10.3s\tremaining: 1.11s\n",
      "902:\tlearn: 0.0852275\ttotal: 10.3s\tremaining: 1.1s\n",
      "903:\tlearn: 0.0852134\ttotal: 10.3s\tremaining: 1.09s\n",
      "904:\tlearn: 0.0852052\ttotal: 10.3s\tremaining: 1.08s\n",
      "905:\tlearn: 0.0851977\ttotal: 10.3s\tremaining: 1.07s\n",
      "906:\tlearn: 0.0851879\ttotal: 10.3s\tremaining: 1.06s\n",
      "907:\tlearn: 0.0851608\ttotal: 10.4s\tremaining: 1.05s\n",
      "908:\tlearn: 0.0851508\ttotal: 10.4s\tremaining: 1.04s\n",
      "909:\tlearn: 0.0851335\ttotal: 10.4s\tremaining: 1.03s\n",
      "910:\tlearn: 0.0851225\ttotal: 10.4s\tremaining: 1.02s\n",
      "911:\tlearn: 0.0851169\ttotal: 10.4s\tremaining: 1s\n",
      "912:\tlearn: 0.0851088\ttotal: 10.4s\tremaining: 994ms\n",
      "913:\tlearn: 0.0850972\ttotal: 10.4s\tremaining: 983ms\n",
      "914:\tlearn: 0.0850886\ttotal: 10.5s\tremaining: 972ms\n",
      "915:\tlearn: 0.0850814\ttotal: 10.5s\tremaining: 961ms\n",
      "916:\tlearn: 0.0850712\ttotal: 10.5s\tremaining: 949ms\n",
      "917:\tlearn: 0.0850653\ttotal: 10.5s\tremaining: 938ms\n",
      "918:\tlearn: 0.0850412\ttotal: 10.5s\tremaining: 927ms\n",
      "919:\tlearn: 0.0850275\ttotal: 10.5s\tremaining: 916ms\n",
      "920:\tlearn: 0.0850046\ttotal: 10.5s\tremaining: 905ms\n",
      "921:\tlearn: 0.0849973\ttotal: 10.6s\tremaining: 893ms\n",
      "922:\tlearn: 0.0849846\ttotal: 10.6s\tremaining: 882ms\n",
      "923:\tlearn: 0.0849715\ttotal: 10.6s\tremaining: 871ms\n",
      "924:\tlearn: 0.0849560\ttotal: 10.6s\tremaining: 861ms\n",
      "925:\tlearn: 0.0849371\ttotal: 10.6s\tremaining: 851ms\n",
      "926:\tlearn: 0.0849129\ttotal: 10.7s\tremaining: 839ms\n",
      "927:\tlearn: 0.0849024\ttotal: 10.7s\tremaining: 828ms\n",
      "928:\tlearn: 0.0848908\ttotal: 10.7s\tremaining: 816ms\n",
      "929:\tlearn: 0.0848675\ttotal: 10.7s\tremaining: 805ms\n",
      "930:\tlearn: 0.0848587\ttotal: 10.7s\tremaining: 793ms\n",
      "931:\tlearn: 0.0848535\ttotal: 10.7s\tremaining: 782ms\n",
      "932:\tlearn: 0.0848458\ttotal: 10.7s\tremaining: 771ms\n",
      "933:\tlearn: 0.0848367\ttotal: 10.7s\tremaining: 759ms\n",
      "934:\tlearn: 0.0848289\ttotal: 10.8s\tremaining: 750ms\n",
      "935:\tlearn: 0.0848229\ttotal: 10.8s\tremaining: 740ms\n",
      "936:\tlearn: 0.0848163\ttotal: 10.9s\tremaining: 731ms\n",
      "937:\tlearn: 0.0848085\ttotal: 10.9s\tremaining: 723ms\n",
      "938:\tlearn: 0.0848031\ttotal: 11s\tremaining: 712ms\n",
      "939:\tlearn: 0.0847908\ttotal: 11s\tremaining: 700ms\n",
      "940:\tlearn: 0.0847840\ttotal: 11s\tremaining: 689ms\n",
      "941:\tlearn: 0.0847623\ttotal: 11s\tremaining: 677ms\n",
      "942:\tlearn: 0.0847535\ttotal: 11s\tremaining: 666ms\n",
      "943:\tlearn: 0.0847423\ttotal: 11s\tremaining: 654ms\n",
      "944:\tlearn: 0.0847278\ttotal: 11s\tremaining: 643ms\n",
      "945:\tlearn: 0.0847221\ttotal: 11.1s\tremaining: 631ms\n",
      "946:\tlearn: 0.0847110\ttotal: 11.1s\tremaining: 619ms\n",
      "947:\tlearn: 0.0847017\ttotal: 11.1s\tremaining: 608ms\n",
      "948:\tlearn: 0.0846864\ttotal: 11.1s\tremaining: 596ms\n",
      "949:\tlearn: 0.0846803\ttotal: 11.1s\tremaining: 585ms\n",
      "950:\tlearn: 0.0846669\ttotal: 11.1s\tremaining: 573ms\n",
      "951:\tlearn: 0.0846551\ttotal: 11.1s\tremaining: 562ms\n",
      "952:\tlearn: 0.0846464\ttotal: 11.2s\tremaining: 550ms\n",
      "953:\tlearn: 0.0846381\ttotal: 11.2s\tremaining: 538ms\n",
      "954:\tlearn: 0.0846259\ttotal: 11.2s\tremaining: 527ms\n",
      "955:\tlearn: 0.0846078\ttotal: 11.2s\tremaining: 515ms\n",
      "956:\tlearn: 0.0845844\ttotal: 11.2s\tremaining: 504ms\n",
      "957:\tlearn: 0.0845744\ttotal: 11.2s\tremaining: 492ms\n",
      "958:\tlearn: 0.0845594\ttotal: 11.2s\tremaining: 480ms\n",
      "959:\tlearn: 0.0845541\ttotal: 11.2s\tremaining: 468ms\n",
      "960:\tlearn: 0.0845390\ttotal: 11.3s\tremaining: 457ms\n",
      "961:\tlearn: 0.0845338\ttotal: 11.3s\tremaining: 445ms\n",
      "962:\tlearn: 0.0845226\ttotal: 11.3s\tremaining: 434ms\n",
      "963:\tlearn: 0.0845098\ttotal: 11.3s\tremaining: 422ms\n",
      "964:\tlearn: 0.0844984\ttotal: 11.3s\tremaining: 410ms\n",
      "965:\tlearn: 0.0844857\ttotal: 11.3s\tremaining: 399ms\n",
      "966:\tlearn: 0.0844826\ttotal: 11.3s\tremaining: 387ms\n",
      "967:\tlearn: 0.0844742\ttotal: 11.3s\tremaining: 375ms\n",
      "968:\tlearn: 0.0844682\ttotal: 11.4s\tremaining: 363ms\n",
      "969:\tlearn: 0.0844527\ttotal: 11.4s\tremaining: 352ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970:\tlearn: 0.0844338\ttotal: 11.4s\tremaining: 340ms\n",
      "971:\tlearn: 0.0844218\ttotal: 11.4s\tremaining: 328ms\n",
      "972:\tlearn: 0.0844067\ttotal: 11.4s\tremaining: 317ms\n",
      "973:\tlearn: 0.0843946\ttotal: 11.4s\tremaining: 305ms\n",
      "974:\tlearn: 0.0843714\ttotal: 11.4s\tremaining: 293ms\n",
      "975:\tlearn: 0.0843628\ttotal: 11.5s\tremaining: 282ms\n",
      "976:\tlearn: 0.0843471\ttotal: 11.5s\tremaining: 270ms\n",
      "977:\tlearn: 0.0843373\ttotal: 11.5s\tremaining: 258ms\n",
      "978:\tlearn: 0.0843258\ttotal: 11.5s\tremaining: 247ms\n",
      "979:\tlearn: 0.0843210\ttotal: 11.5s\tremaining: 235ms\n",
      "980:\tlearn: 0.0843108\ttotal: 11.5s\tremaining: 223ms\n",
      "981:\tlearn: 0.0843042\ttotal: 11.5s\tremaining: 212ms\n",
      "982:\tlearn: 0.0842975\ttotal: 11.6s\tremaining: 200ms\n",
      "983:\tlearn: 0.0842913\ttotal: 11.6s\tremaining: 188ms\n",
      "984:\tlearn: 0.0842795\ttotal: 11.6s\tremaining: 176ms\n",
      "985:\tlearn: 0.0842683\ttotal: 11.6s\tremaining: 165ms\n",
      "986:\tlearn: 0.0842630\ttotal: 11.6s\tremaining: 153ms\n",
      "987:\tlearn: 0.0842468\ttotal: 11.6s\tremaining: 141ms\n",
      "988:\tlearn: 0.0842343\ttotal: 11.6s\tremaining: 129ms\n",
      "989:\tlearn: 0.0842271\ttotal: 11.7s\tremaining: 118ms\n",
      "990:\tlearn: 0.0842137\ttotal: 11.7s\tremaining: 106ms\n",
      "991:\tlearn: 0.0842070\ttotal: 11.7s\tremaining: 94.2ms\n",
      "992:\tlearn: 0.0842028\ttotal: 11.7s\tremaining: 82.4ms\n",
      "993:\tlearn: 0.0841941\ttotal: 11.7s\tremaining: 70.6ms\n",
      "994:\tlearn: 0.0841862\ttotal: 11.7s\tremaining: 58.9ms\n",
      "995:\tlearn: 0.0841820\ttotal: 11.7s\tremaining: 47.1ms\n",
      "996:\tlearn: 0.0841753\ttotal: 11.7s\tremaining: 35.3ms\n",
      "997:\tlearn: 0.0841625\ttotal: 11.8s\tremaining: 23.6ms\n",
      "998:\tlearn: 0.0841422\ttotal: 11.8s\tremaining: 11.8ms\n",
      "999:\tlearn: 0.0841312\ttotal: 11.8s\tremaining: 0us\n",
      "logloss: 22.456608816630993\n",
      "logloss: 23.01920830552185\n",
      "logloss: 23.647632785783642\n",
      "logloss: 24.08705755688655\n",
      "logloss: 23.613945292672994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:21:01,782] Finished trial#0 resulted in value: 0.9596821253257586. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:17,306] Finished trial#1 resulted in value: 0.9357427256009416. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:26,867] Finished trial#2 resulted in value: 0.9584777410088359. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:36,894] Finished trial#3 resulted in value: 0.9579537621927047. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:50,571] Finished trial#4 resulted in value: 0.648516401617903. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:52,677] Finished trial#5 resulted in value: 0.9573641737156503. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:56,305] Finished trial#6 resulted in value: 0.9596423823642546. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:14,110] Finished trial#7 resulted in value: 0.5948357544565132. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:20,142] Finished trial#8 resulted in value: 0.9594828081272023. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:26,472] Finished trial#9 resulted in value: 0.9595579230989273. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:33,423] Finished trial#10 resulted in value: 0.9587793263170191. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:35,268] Finished trial#11 resulted in value: 0.9571652262772199. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:44,136] Finished trial#12 resulted in value: 0.9589549376439737. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:05,424] Finished trial#13 resulted in value: 0.9576818855597375. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:07,272] Finished trial#14 resulted in value: 0.957329107444334. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:14,126] Finished trial#15 resulted in value: 0.9583387282800757. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:17,801] Finished trial#16 resulted in value: 0.9595798259525491. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:10,070] Finished trial#17 resulted in value: 0.9585421293099341. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:17,927] Finished trial#18 resulted in value: 0.9595336878897737. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:23,968] Finished trial#19 resulted in value: 0.9581816747483132. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[14:24:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:24:34,855] Finished trial#0 resulted in value: 0.9567937390387996. Current best value is 0.9567937390387996 with parameters: {'booster': 'gbtree', 'alpha': 0.00035533331680522024, 'max_depth': 5, 'eta': 1.718165234948709e-08, 'gamma': 0.9504890453596655, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:24:40,818] Finished trial#1 resulted in value: 0.9603484325624834. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:03,970] Finished trial#2 resulted in value: 0.9579566222829371. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:07,898] Finished trial#3 resulted in value: 0.9600643449239799. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:11,039] Finished trial#4 resulted in value: 0.9602398517336781. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:13,361] Finished trial#5 resulted in value: 0.9575723867962497. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:35,873] Finished trial#6 resulted in value: 0.9594287117089143. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:26:22,345] Finished trial#7 resulted in value: 0.9579335244043504. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:06,132] Finished trial#8 resulted in value: 0.957584134689377. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:09,424] Finished trial#9 resulted in value: 0.9599922028468754. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:28,104] Finished trial#10 resulted in value: 0.9588864120350189. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:41,191] Finished trial#11 resulted in value: 0.9596057990536393. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:43,660] Finished trial#12 resulted in value: 0.9577773932093221. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:48,644] Finished trial#13 resulted in value: 0.9599650886099005. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:56,247] Finished trial#14 resulted in value: 0.9588844162143957. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:02,494] Finished trial#15 resulted in value: 0.9594017727552282. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:05,619] Finished trial#16 resulted in value: 0.9580422124524294. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:21,031] Finished trial#17 resulted in value: 0.9599158399005996. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:22,825] Finished trial#18 resulted in value: 0.9577605341245341. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:31,134] Finished trial#19 resulted in value: 0.9599540919871477. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[14:28:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:28:36,487] Finished trial#0 resulted in value: -5281.996101440747. Current best value is -5281.996101440747 with parameters: {'booster': 'dart', 'alpha': 0.21584386307181835, 'max_depth': 1, 'eta': 8.364620654813273e-06, 'gamma': 5.1354720884141934e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.6861711435019764, 'skip_drop': 9.218609379057158e-05}.\n",
      "[I 2019-11-20 14:28:44,457] Finished trial#1 resulted in value: 0.9596666807955507. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:29,147] Finished trial#2 resulted in value: 0.9586916824849825. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:36,082] Finished trial#3 resulted in value: 0.9591160819994915. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:54,434] Finished trial#4 resulted in value: 0.9603118871310308. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n",
      "[I 2019-11-20 14:30:22,839] Finished trial#5 resulted in value: 0.960225519714872. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n",
      "[I 2019-11-20 14:31:00,639] Finished trial#6 resulted in value: 0.9590115921780233. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-410-4df09d1b0fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# pred_test_2は、2層目のモデルのテストデータの予測値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressorCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mpred_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2))}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-410-4df09d1b0fb0>\u001b[0m in \u001b[0;36mpredict_cv\u001b[0;34m(model, train_x, train_y, test_x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                    rate_drop=rate_drop, skip_drop=skip_drop)\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoost\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "# tensorflowの警告抑制\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# スタッキング\n",
    "# ----------------------------------\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# models.pyにModel1Xgb, Model1NN, Model2Linearを定義しているものとする\n",
    "# 各クラスは、fitで学習し、predictで予測値の確率を出力する\n",
    "\n",
    "\n",
    "# 学習データに対する「目的変数を知らない」予測値と、テストデータに対する予測値を返す関数\n",
    "def predict_cv(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    # クロスバリデーションで学習・予測を行い、予測値とインデックスを保存する\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # テストデータに対する予測値の平均をとる\n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test\n",
    "\n",
    "\n",
    "# 1層目のモデル\n",
    "\n",
    "model_1a =xgb.XGBRegressor(**{'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06})\n",
    "pred_train_1a, pred_test_1a = predict_cv(model_1a, X, y, test)\n",
    "\n",
    "model_1b = lgb.LGBMRegressor(**{'iterations': 55, 'depth': 22, 'learning_rate': 0.15022735769525583, 'random_strength': 40, 'bagging_temperature': 0.03207087240546606, 'od_type': 'Iter', 'od_wait': 41})\n",
    "pred_train_1b, pred_test_1b = predict_cv(model_1b, X, y, test)\n",
    "\n",
    "model_1c = LGBRegressorCV(n_trials=10)\n",
    "pred_train_1c, pred_test_1c = predict_cv(model_1c, X, y, test)\n",
    "\n",
    "model_1d = RFR(n_jobs=-1, random_state=2525)\n",
    "pred_train_1d, pred_test_1d = predict_cv(model_1d, X, y, test)\n",
    "\n",
    "model_1e =  CatBoost()\n",
    "pred_train_1e, pred_test_1e = predict_cv(model_1e, X, y, test)\n",
    "\n",
    "# model_1f=\n",
    "\n",
    "# 1層目のモデルの評価\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1a))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1b))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1c))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1d))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1e))}')\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成\n",
    "train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b,'pred_1c': pred_train_1c,'pred_1d': pred_train_1d,'pred_1e': pred_train_1e})\n",
    "test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b, 'pred_1c': pred_test_1c, 'pred_1d': pred_test_1d, 'pred_1e': pred_test_1e})\n",
    "\n",
    "\n",
    "# 2層目のモデル\n",
    "# pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n",
    "# pred_test_2は、2層目のモデルのテストデータの予測値\n",
    "model_2 = XGBRegressorCV(n_trials=20)\n",
    "pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, y, test_x_2)\n",
    "\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2))}')\n",
    "model_2b = lgb.LGBMRegressor()\n",
    "pred_train_2b, pred_test_2b = predict_cv(model_2b, train_x_2, y, test_x_2)\n",
    "train_x_3 = pd.DataFrame({'pred_2a': pred_train_2a, 'pred_2b': pred_train_2b})\n",
    "test_x_3 = pd.DataFrame({'pred_2a': pred_test_2a, 'pred_2b': pred_test_2b})\n",
    "\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2a))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2b))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.exp(pred_test_2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:23:37,643] Finished trial#0 resulted in value: 0.9305498191433168. Current best value is 0.9305498191433168 with parameters: {'booster': 'gbtree', 'alpha': 0.05398288589157677, 'max_depth': 3, 'eta': 0.7140800447741135, 'gamma': 0.00024190393122103457, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-19 14:24:54,582] Finished trial#1 resulted in value: 0.9603226552134976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:17,469] Finished trial#2 resulted in value: 0.9596122329185025. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:38,640] Finished trial#3 resulted in value: 0.9598963266814737. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:01,822] Finished trial#4 resulted in value: 0.9445592847731454. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:25,405] Finished trial#5 resulted in value: 0.9587629061169235. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:41,400] Finished trial#6 resulted in value: -387.9041239755456. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:54,677] Finished trial#7 resulted in value: 0.9592436087634768. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:28:56,906] Finished trial#8 resulted in value: 0.9566239935926616. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:07,426] Finished trial#9 resulted in value: 0.9567988998760226. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:42,840] Finished trial#10 resulted in value: 0.9476482257292294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:32:27,596] Finished trial#11 resulted in value: 0.956698401395849. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:33:16,474] Finished trial#12 resulted in value: -4.055874265306564. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:14,085] Finished trial#13 resulted in value: 0.9601138026638152. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:59,910] Finished trial#14 resulted in value: 0.9565527450078622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:33,531] Finished trial#15 resulted in value: 0.9446207766477638. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:48,702] Finished trial#16 resulted in value: -1399.7011359154567. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:00,125] Finished trial#17 resulted in value: 0.958760245390202. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:30,110] Finished trial#18 resulted in value: 0.9574032702359846. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:37:53,151] Finished trial#19 resulted in value: 0.9581539820151755. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:00,500] Finished trial#20 resulted in value: 0.956370606897214. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:18,114] Finished trial#21 resulted in value: 0.9586995742275745. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:41,289] Finished trial#22 resulted in value: 0.9594710457867158. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:06,406] Finished trial#23 resulted in value: 0.9545935103825889. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:24,873] Finished trial#24 resulted in value: 0.9594131128640582. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:52,900] Finished trial#25 resulted in value: 0.9568289978095595. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:40:18,111] Finished trial#26 resulted in value: 0.9568401840594071. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:44:38,410] Finished trial#27 resulted in value: 0.37976798637976894. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:09,769] Finished trial#28 resulted in value: 0.9569202272257933. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:31,764] Finished trial#29 resulted in value: 0.9530933288937604. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:46:20,190] Finished trial#30 resulted in value: 0.9596207162459487. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:47:21,110] Finished trial#31 resulted in value: 0.9587299746964092. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:22,964] Finished trial#32 resulted in value: 0.37901072492524795. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:59,562] Finished trial#33 resulted in value: 0.9557504185473518. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:50:19,887] Finished trial#34 resulted in value: 0.6920560246130634. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:07,764] Finished trial#35 resulted in value: 0.9563269444075339. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:15,479] Finished trial#36 resulted in value: 0.8104693935211186. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:44,993] Finished trial#37 resulted in value: 0.9596979309846085. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:52:08,960] Finished trial#38 resulted in value: 0.9594897252942587. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:34,318] Finished trial#39 resulted in value: 0.9560476206065488. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:59,365] Finished trial#40 resulted in value: 0.9589511941685105. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:35,311] Finished trial#41 resulted in value: 0.9557815871347504. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:57,821] Finished trial#42 resulted in value: 0.9596402683393249. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:22,898] Finished trial#43 resulted in value: 0.9596506393831744. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:49,563] Finished trial#44 resulted in value: 0.9583432840218722. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:12,913] Finished trial#45 resulted in value: 0.9503219866901705. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:44,323] Finished trial#46 resulted in value: 0.9439230284396045. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:07,679] Finished trial#47 resulted in value: 0.9577484164792291. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:33,616] Finished trial#48 resulted in value: 0.9588307979464294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:54,266] Finished trial#49 resulted in value: 0.9595208600484575. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:13,349] Finished trial#50 resulted in value: 0.958859235560175. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:34,458] Finished trial#51 resulted in value: 0.959801537313227. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:01,690] Finished trial#52 resulted in value: 0.9586469259847702. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:37,602] Finished trial#53 resulted in value: 0.9567041406226083. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:03,668] Finished trial#54 resulted in value: 0.9503185290450906. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:33,333] Finished trial#55 resulted in value: 0.9596033634027707. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:55,627] Finished trial#56 resulted in value: 0.9579607729400174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:30,560] Finished trial#57 resulted in value: 0.9559231059905114. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:56,760] Finished trial#58 resulted in value: 0.9597679573180903. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:23,527] Finished trial#59 resulted in value: 0.9580893053324463. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:52,486] Finished trial#60 resulted in value: 0.9557485563030305. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:11,578] Finished trial#61 resulted in value: 0.9594123430012976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:34,128] Finished trial#62 resulted in value: 0.9598668026754119. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:58,324] Finished trial#63 resulted in value: 0.9596332658609208. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:19,827] Finished trial#64 resulted in value: 0.9590550944936261. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:51,268] Finished trial#65 resulted in value: 0.9564344930902546. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:17,973] Finished trial#66 resulted in value: 0.9529373023400145. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:36,048] Finished trial#67 resulted in value: 0.9574774739825571. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:12,644] Finished trial#68 resulted in value: 0.9426763980484459. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:31,281] Finished trial#69 resulted in value: 0.9593833804168816. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:00,170] Finished trial#70 resulted in value: 0.9591893243636337. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:22,291] Finished trial#71 resulted in value: 0.9595051111829601. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:44,957] Finished trial#72 resulted in value: 0.9593363815773163. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:11,092] Finished trial#73 resulted in value: 0.958386606182769. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:38,646] Finished trial#74 resulted in value: 0.9576461057484386. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:08:02,243] Finished trial#75 resulted in value: 0.9592386374748388. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 15:09:04,771] Finished trial#76 resulted in value: 0.9597318738254854. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:06,931] Finished trial#77 resulted in value: 0.95899052150256. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:50,430] Finished trial#78 resulted in value: 0.9497551679431391. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:11:37,312] Finished trial#79 resulted in value: 0.9535350937350803. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:13:11,867] Finished trial#80 resulted in value: 0.37795736793164736. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:14:17,509] Finished trial#81 resulted in value: 0.9600409927854597. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:15:17,231] Finished trial#82 resulted in value: 0.9601820948004374. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:16:51,760] Finished trial#83 resulted in value: 0.9599088646926296. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:17:55,563] Finished trial#84 resulted in value: 0.9582942766014618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:19:16,561] Finished trial#85 resulted in value: 0.9556363922125138. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:20:50,623] Finished trial#86 resulted in value: 0.9598415829020268. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:21:56,978] Finished trial#87 resulted in value: 0.9591565948983902. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:23:02,511] Finished trial#88 resulted in value: 0.9592573940724174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:24:21,418] Finished trial#89 resulted in value: 0.9583596724830622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:25:05,185] Finished trial#90 resulted in value: 0.9579814074019618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:26:03,650] Finished trial#91 resulted in value: 0.9596818974789345. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:27:14,538] Finished trial#92 resulted in value: 0.9597103277990394. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:28:23,695] Finished trial#93 resulted in value: 0.9499916301977823. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:29:22,181] Finished trial#94 resulted in value: 0.9596422136661624. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:15,336] Finished trial#95 resulted in value: 0.9590829201283609. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:33,575] Finished trial#96 resulted in value: -498.1587888134253. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:19,094] Finished trial#97 resulted in value: 0.9524442506788089. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:49,887] Finished trial#98 resulted in value: 0.9479648390482927. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:33:28,437] Finished trial#99 resulted in value: 0.9563083963830048. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}\n",
      "\n",
      "[15:33:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# xgbr = XGBRegressorCV(n_trials=40)\n",
    "\n",
    "xgbr.fit(X, y)\n",
    "\n",
    "pred=np.exp(xgbr.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(pd.read_csv(\"test_data.csv\")['id'])\n",
    "sub[\"y\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
