{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML \n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train=pd.read_csv(\"train_data.csv\")\n",
    "test=pd.read_csv(\"test_data.csv\")\n",
    "import pandas_profiling as pdp\n",
    "df=pd.concat([train,test],sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pdp\n",
    "def preprocess():\n",
    "    train=pd.read_csv(\"train_data.csv\")\n",
    "    test=pd.read_csv(\"test_data.csv\")\n",
    "    df=pd.concat([train,test],sort=False)\n",
    "    df.age=df.age.map(lambda x:57 if x>58 else x)\n",
    "    df.num_child=df.num_child.map(lambda x:7 if x>7 else x)\n",
    "    df.study_time=df.study_time.map(lambda x:17 if x>17 else x)\n",
    "    df[\"familiy_num\"]=1+df.partner+df.num_child\n",
    "    arealist=list(train.groupby(\"area\").mean().salary.sort_values().index)\n",
    "    areadic={}\n",
    "    for i,area in enumerate(arealist):\n",
    "        areadic[area]=i+1\n",
    "    df.area=df.area.map(areadic)\n",
    "    df.position=df.position+1\n",
    "    df.sex=df.sex-1\n",
    "    df.salary=np.log(df.salary)\n",
    "    df[\"agexposition\"]=df.age*df.position.map(lambda x:1.5 if x==1 else x)\n",
    "    df.education=df.education+1\n",
    "#     df.drop([\"sex\"],axis=1)\n",
    "    train=df.dropna().drop([\"id\"],axis=1)\n",
    "    test=df[df.salary.isnull()].drop([\"id\"],axis=1)\n",
    "    test=test.drop([\"salary\",],axis=1)\n",
    "    X = train.drop([\"salary\"],axis=1)\n",
    "    y = train.salary\n",
    "    return X,y,test,df\n",
    "X,y,test,df=preprocess()\n",
    "#original 23.625521293118492\n",
    "train=pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.577470914009222, 22.33568473300726, 22.96702015021395, 22.86462929458672, 21.959873464946703]\n",
      "22.540935711352773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor(**{'booster': 'gblinear',\n",
    "                           'iterations': 309, 'depth': 16, 'learning_rate': 0.1,\n",
    "                           'random_strength': 48, 'bagging_temperature': 19.715729096205934, \n",
    "                           'od_type': 'Iter', 'od_wait': 26, 'lambda_l1': 0.726486176355415, \n",
    "                           'lambda_l2': 0.00044177449020498015, 'num_leaves': 188,\n",
    "                           'feature_fraction': 0.9443254919883529, 'bagging_fraction': 0.9271673814820428, \n",
    "                           'bagging_freq': 2, 'min_child_samples': 17})\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "train_X, valid_X,train_y, valid_y = train_test_split(X,y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdp.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9550358215611071\n",
      "MAE:23.610391\n",
      "Feature Importances:\n",
      "\tposition   : 0.010766\n",
      "\tage        : 0.003914\n",
      "\tarea       : 0.014661\n",
      "\tsex        : 0.001050\n",
      "\tpartner    : 0.013310\n",
      "\tnum_child  : 0.012098\n",
      "\teducation  : 0.045353\n",
      "\tservice_length : 0.004315\n",
      "\tstudy_time : 0.004520\n",
      "\tcommute    : 0.209512\n",
      "\tovertime   : 0.025041\n",
      "\tfamiliy_num : 0.024246\n",
      "\tagexposition : 0.631216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['agexposition', 'commute', 'education', 'overtime', 'familiy_num',\n",
       "       'area', 'partner', 'num_child', 'position', 'study_time',\n",
       "       'service_length', 'age', 'sex'], dtype=object)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rg = RFR(n_jobs=-1, random_state=2525)\n",
    " \n",
    "rg.fit(train_X,train_y)\n",
    "print(rg.score(valid_X,valid_y))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(rg.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "fti = rg.feature_importances_\n",
    "\n",
    "print('Feature Importances:')\n",
    "for i,feat in enumerate(valid_X.columns):\n",
    "    print('\\t{0:10s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "col_names = test.columns.values\n",
    "col_names_ = col_names[np.argsort(rg.feature_importances_)[::-1]]\n",
    "col_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.764094288724273, 22.792014845341466, 23.487210364564227, 23.352820410841307, 22.616116417087895]\n",
      "23.00245126531183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor()\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:55<00:08,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:25:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.87s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 特徴量を選択して、複数のモデルで精度を調査する\n",
    "from scipy.stats import mstats\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from sklearn import metrics,  feature_selection, ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\n",
    "# from imblearn import under_sampling, over_sampling\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "base_color = 'darkturquoise'\n",
    "base_color2 = 'gray'\n",
    "\n",
    "def generate_cmap(colors):\n",
    "    \n",
    "    values = range(len(colors))\n",
    "    vmax = np.ceil(np.max(values))\n",
    "    color_list = []\n",
    "    for v, c in zip(values, colors):\n",
    "        color_list.append((v/vmax, c))\n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list('custom_cmap', color_list)\n",
    "cm = generate_cmap([base_color2, 'white', base_color])\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    \"\"\"RMSE (Root Mean Square Error: 平均二乗誤差平方根) を計算する関数\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "params={'booster': 'dart', \n",
    "        'alpha': 0.009385870161072372, \n",
    "        'max_depth': 9, 'eta': 2.3698818355249718e-07,\n",
    "        'gamma': 3.167530221746867e-05, \n",
    "        'grow_policy': 'lossguide',\n",
    "        'sample_type': 'weighted',\n",
    "        'normalize_type': 'forest',\n",
    "        'rate_drop': 3.1207262366715483e-08,\n",
    "        'skip_drop': 1.2650261386504368e-05}\n",
    "\n",
    "\n",
    "models = [\n",
    " \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    " \n",
    "    #Gaussian Processes\n",
    "#     gaussian_process.GaussianProcessRegressor(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.Ridge(),\n",
    "\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeRegressor(),\n",
    "    tree.ExtraTreeRegressor(),\n",
    " \n",
    "    #xgboost\n",
    "    xgb. XGBRegressor(),\n",
    "    lgb.LGBMRegressor(),\n",
    "#     cat.CatBoostRegressor(),\n",
    "    \n",
    "]\n",
    " \n",
    "df_compare = pd.DataFrame(columns=['name', 'train_rmse', 'valid_rmse', 'time'])\n",
    "score_funcs = {\n",
    "    'rmse': make_scorer(rmse_score),\n",
    "}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    \n",
    "    cv_rlts = model_selection.cross_validate(model,X,y, scoring=score_funcs, cv=10, return_train_score=True)\n",
    " \n",
    "    for i in range(10):\n",
    "        s = pd.Series([name, cv_rlts['train_rmse'][i], cv_rlts['test_rmse'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name=name+str(i))\n",
    "        df_compare = df_compare.append(s)\n",
    "        \n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=df_compare, y='name', x='valid_rmse', orient='h', color=base_color, linewidth=0.5, width=0.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:42<00:16,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:26:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:56] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:56<00:00,  8.04s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = X.columns.tolist()\n",
    " \n",
    "# positive_cnt = int(df['salary'].sum())\n",
    "\n",
    "feature_importance_models = [\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    tree.DecisionTreeRegressor(),\n",
    "    xgb.XGBRegressor(),\n",
    "    lgb.LGBMRegressor()\n",
    "]\n",
    " \n",
    "scoring = ['rsme']\n",
    "df_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\n",
    "df_rfe_cols_cnt['cnt'] = 0\n",
    " \n",
    "for i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n",
    "    \n",
    "    rfe = feature_selection.RFECV(model, step=3)\n",
    "    rfe.fit(X, y)\n",
    "#     print(rfe.get_support())\n",
    "    rfe_cols = X[cols].columns.values[rfe.get_support()]\n",
    "    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n",
    "    \n",
    "df_rfe_cols_cnt.plot(kind='bar', color=base_color, figsize=(15, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df_rfe_cols_cnt[df_rfe_cols_cnt['cnt'] < 4].index\n",
    "X=X.drop(x_cols,axis=1)\n",
    "test=test.drop(x_cols,axis=1)\n",
    "train_X, valid_X,train_y, valid_y = train_test_split(X,y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:28:36,513] Finished trial#0 resulted in value: 0.935196791185645. Current best value is 0.935196791185645 with parameters: {'booster': 'gbtree', 'alpha': 4.60033291811229e-06, 'max_depth': 18, 'eta': 0.008173817332634628, 'gamma': 0.8902721232594177, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:28:56,221] Finished trial#1 resulted in value: -1.5539921181251457. Current best value is 0.935196791185645 with parameters: {'booster': 'gbtree', 'alpha': 4.60033291811229e-06, 'max_depth': 18, 'eta': 0.008173817332634628, 'gamma': 0.8902721232594177, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:02,141] Finished trial#2 resulted in value: 0.9548856586697718. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:15,208] Finished trial#3 resulted in value: -29.40068058137117. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:16,951] Finished trial#4 resulted in value: 0.8069192137502167. Current best value is 0.9548856586697718 with parameters: {'booster': 'gbtree', 'alpha': 0.00036957714154973493, 'max_depth': 7, 'eta': 8.476826495207648e-07, 'gamma': 1.8208331884593898e-08, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:29:23,603] Finished trial#5 resulted in value: 0.9554614147515625. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:41,435] Finished trial#6 resulted in value: -1.4777565969533113. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:47,520] Finished trial#7 resulted in value: 0.9546554963677492. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:52,570] Finished trial#8 resulted in value: 0.80763085397036. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:29:57,916] Finished trial#9 resulted in value: 0.9532127450886515. Current best value is 0.9554614147515625 with parameters: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.0006344243075561812, 'max_depth': 8, 'eta': 1.0840048003283997e-08, 'gamma': 0.010909830452661298, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[00:29:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:30:17,791] Finished trial#0 resulted in value: 0.9521134752192577. Current best value is 0.9521134752192577 with parameters: {'booster': 'dart', 'alpha': 0.0005498199457683365, 'max_depth': 5, 'eta': 2.673192533142511e-07, 'gamma': 1.8888217914863087e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.270287350141747e-05, 'skip_drop': 7.760338396503087e-08}.\n",
      "[I 2019-11-20 00:30:26,926] Finished trial#1 resulted in value: 0.9578058325024463. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:30:48,396] Finished trial#2 resulted in value: 0.7272354906592282. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:23,874] Finished trial#3 resulted in value: 0.9577140218613494. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:38,558] Finished trial#4 resulted in value: 0.9551363613997488. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:31:56,582] Finished trial#5 resulted in value: 0.9518838868793225. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:11,399] Finished trial#6 resulted in value: 0.9348990187528268. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:36,243] Finished trial#7 resulted in value: 0.9570785312201711. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:48,656] Finished trial#8 resulted in value: 0.9558221341072535. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 00:32:59,279] Finished trial#9 resulted in value: 0.9309553740696277. Current best value is 0.9578058325024463 with parameters: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 1.1266429023874118e-06, 'max_depth': 10, 'eta': 0.0001753722465233045, 'gamma': 0.000261715475472335, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[00:32:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 00:33:19,903] Finished trial#0 resulted in value: 0.9540782183180527. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:33:26,744] Finished trial#1 resulted in value: 0.9527068485401523. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:33:34,972] Finished trial#2 resulted in value: -509.39465015773476. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:26,887] Finished trial#3 resulted in value: 0.4727458439331976. Current best value is 0.9540782183180527 with parameters: {'booster': 'gbtree', 'alpha': 4.860611981540524e-07, 'max_depth': 16, 'eta': 0.09288159741639739, 'gamma': 4.111850153124893e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:40,564] Finished trial#4 resulted in value: 0.9545346369428991. Current best value is 0.9545346369428991 with parameters: {'booster': 'gbtree', 'alpha': 0.010153455864032377, 'max_depth': 14, 'eta': 0.0029264792450987144, 'gamma': 4.551552592372101e-07, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:49,776] Finished trial#5 resulted in value: 0.9564613487344698. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:34:52,216] Finished trial#6 resulted in value: 0.8973922426646362. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:35:07,576] Finished trial#7 resulted in value: 0.9542409917899194. Current best value is 0.9564613487344698 with parameters: {'booster': 'gbtree', 'alpha': 0.3058290726751666, 'max_depth': 10, 'eta': 4.449213621589445e-08, 'gamma': 4.469861273240667e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 00:36:00,099] Finished trial#8 resulted in value: 0.957111945972502. Current best value is 0.957111945972502 with parameters: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}.\n",
      "[I 2019-11-20 00:36:11,372] Finished trial#9 resulted in value: 0.9314723147742662. Current best value is 0.957111945972502 with parameters: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 1.6943165841563767e-07, 'max_depth': 14, 'eta': 0.0009645923293830843, 'gamma': 0.013461864078747363, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.7124916831359184e-08, 'skip_drop': 0.00012230598728643746}\n",
      "\n",
      "[00:36:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22.95643615911303, 23.146576434969464, 22.742527656745942]\n",
      "22.948513416942813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class RidgeCV():\n",
    "    model_cls = Ridge\n",
    "\n",
    "    def __init__(self, n_trials=100):\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.reset_index(drop=True)\n",
    "            y = y.reset_index(drop=True)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self, n_trials=self.n_trials)\n",
    "        self.best_trial = study.best_trial\n",
    "\n",
    "        print()\n",
    "        print(\"Best score:\", round(self.best_trial.value, 2))\n",
    "        print(\"Best params:\", self.best_trial.params)\n",
    "        print()\n",
    "\n",
    "        self.best_model = self.model_cls(**self.best_trial.params)\n",
    "        self.best_model.fit(self.X, self.y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = pd.DataFrame(X.values.reshape(1, -1))\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        return self.best_model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "\n",
    "        return self.best_model.score(X, y)\n",
    "\n",
    "    def kfold_cv(self, model, splits=5):\n",
    "        scores = []\n",
    "\n",
    "        kf = KFold(n_splits=splits, shuffle=True)\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            scores.append(r2_score(model.predict(X_test), y_test))\n",
    "\n",
    "        score = np.array(scores).mean()\n",
    "        return score\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class XGBRegressorCV(RidgeCV):\n",
    "    model_cls = xgb.XGBRegressor\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        booster = trial.suggest_categorical('booster', ['gbtree', 'dart'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 1e-8, 1.0)\n",
    "\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "        eta = trial.suggest_loguniform('eta', 1e-8, 1.0)\n",
    "        gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "        grow_policy = trial.suggest_categorical(\n",
    "            'grow_policy', ['depthwise', 'lossguide'])\n",
    "\n",
    "        if booster == 'gbtree':\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy)\n",
    "        elif booster == 'dart':\n",
    "            sample_type = trial.suggest_categorical('sample_type',\n",
    "                                                    ['uniform', 'weighted'])\n",
    "            normalize_type = trial.suggest_categorical('normalize_type',\n",
    "                                                       ['tree', 'forest'])\n",
    "            rate_drop = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
    "            skip_drop = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy,\n",
    "                                   sample_type=sample_type,\n",
    "                                   normalize_type=normalize_type,\n",
    "                                   rate_drop=rate_drop, skip_drop=skip_drop)\n",
    "\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "xgbr = XGBRegressorCV(n_trials=100)\n",
    "# xgbr.fit(train_X, train_y)\n",
    "\n",
    "# pred_y=xgbr.predict(valid_X)\n",
    "# score=mean_absolute_error(np.exp(valid_y),np.exp(xgbr.predict(valid_X)))\n",
    "# print(f'MAE:{score:4f}')\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "model = XGBRegressorCV(n_trials=10)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "#MAE:22.821581(drop)dummy\n",
    "#MAE:22.753628(nodrop)dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:49:33,001] Finished trial#0 resulted in value: 0.9444320369636955. Current best value is 0.9444320369636955 with parameters: {'booster': 'gbtree', 'iterations': 310, 'learning_rate': 0.5356748534001899, 'random_strength': 52, 'bagging_temperature': 0.17025341497792462, 'od_type': 'Iter', 'od_wait': 20, 'lambda_l1': 0.28916744743958905, 'lambda_l2': 2.6040951538148434e-07, 'num_leaves': 249, 'feature_fraction': 0.5161973959560417, 'bagging_fraction': 0.9864977088428162, 'bagging_freq': 2, 'min_child_samples': 69}.\n",
      "[I 2019-11-20 14:49:35,601] Finished trial#1 resulted in value: 0.6812421248745195. Current best value is 0.9444320369636955 with parameters: {'booster': 'gbtree', 'iterations': 310, 'learning_rate': 0.5356748534001899, 'random_strength': 52, 'bagging_temperature': 0.17025341497792462, 'od_type': 'Iter', 'od_wait': 20, 'lambda_l1': 0.28916744743958905, 'lambda_l2': 2.6040951538148434e-07, 'num_leaves': 249, 'feature_fraction': 0.5161973959560417, 'bagging_fraction': 0.9864977088428162, 'bagging_freq': 2, 'min_child_samples': 69}.\n",
      "[I 2019-11-20 14:49:37,737] Finished trial#2 resulted in value: 0.946479655692683. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:41,257] Finished trial#3 resulted in value: 0.9229633342970001. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:44,933] Finished trial#4 resulted in value: 0.9117554963499478. Current best value is 0.946479655692683 with parameters: {'booster': 'gblinear', 'iterations': 147, 'learning_rate': 0.05140776098208592, 'random_strength': 16, 'bagging_temperature': 0.018077204812799792, 'od_type': 'IncToDec', 'od_wait': 16, 'lambda_l1': 1.2397132233228845, 'lambda_l2': 6.203871151301491, 'num_leaves': 221, 'feature_fraction': 0.8861669001781305, 'bagging_fraction': 0.6638335782701525, 'bagging_freq': 4, 'min_child_samples': 40}.\n",
      "[I 2019-11-20 14:49:47,731] Finished trial#5 resulted in value: 0.9535737648164808. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:49,790] Finished trial#6 resulted in value: 0.9170101560951187. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:52,403] Finished trial#7 resulted in value: 0.9491913685191673. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:54,809] Finished trial#8 resulted in value: 0.952089428510396. Current best value is 0.9535737648164808 with parameters: {'booster': 'gbtree', 'iterations': 343, 'learning_rate': 0.17486354336154475, 'random_strength': 68, 'bagging_temperature': 10.619816878324755, 'od_type': 'IncToDec', 'od_wait': 44, 'lambda_l1': 0.007193626299662627, 'lambda_l2': 2.0393007769253266, 'num_leaves': 113, 'feature_fraction': 0.9124528061352463, 'bagging_fraction': 0.8609323784328669, 'bagging_freq': 5, 'min_child_samples': 23}.\n",
      "[I 2019-11-20 14:49:57,576] Finished trial#9 resulted in value: 0.9535738228210354. Current best value is 0.9535738228210354 with parameters: {'booster': 'gbtree', 'iterations': 324, 'learning_rate': 0.19191133141833663, 'random_strength': 58, 'bagging_temperature': 1.3025324129298999, 'od_type': 'IncToDec', 'od_wait': 20, 'lambda_l1': 7.66143544724732e-05, 'lambda_l2': 0.0002223866325696207, 'num_leaves': 76, 'feature_fraction': 0.8249491788761739, 'bagging_fraction': 0.6621871695330719, 'bagging_freq': 5, 'min_child_samples': 13}.\n",
      "[I 2019-11-20 14:49:59,643] Finished trial#10 resulted in value: 0.9544397094191. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:01,689] Finished trial#11 resulted in value: 0.8912676270402354. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:04,101] Finished trial#12 resulted in value: 0.9501701287808487. Current best value is 0.9544397094191 with parameters: {'booster': 'gbtree', 'iterations': 233, 'learning_rate': 0.14491262633605723, 'random_strength': 99, 'bagging_temperature': 99.51821719313563, 'od_type': 'IncToDec', 'od_wait': 25, 'lambda_l1': 3.9069108336289006e-07, 'lambda_l2': 0.010007193148217458, 'num_leaves': 23, 'feature_fraction': 0.7591288444136376, 'bagging_fraction': 0.7846640682698668, 'bagging_freq': 4, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:50:06,404] Finished trial#13 resulted in value: 0.9551434497202755. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:07,828] Finished trial#14 resulted in value: 0.8912601043751215. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:09,855] Finished trial#15 resulted in value: 0.9482017929572482. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:13,464] Finished trial#16 resulted in value: 0.9452702720735692. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:14,839] Finished trial#17 resulted in value: 0.8082902384585313. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:17,131] Finished trial#18 resulted in value: 0.9498923087405512. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 14:50:20,225] Finished trial#19 resulted in value: 0.9515642996217105. Current best value is 0.9551434497202755 with parameters: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 393, 'learning_rate': 0.08430886181317533, 'random_strength': 2, 'bagging_temperature': 3.7728812010310446, 'od_type': 'IncToDec', 'od_wait': 23, 'lambda_l1': 3.6060490924544417e-06, 'lambda_l2': 0.00013197746712333668, 'num_leaves': 57, 'feature_fraction': 0.6675434030420537, 'bagging_fraction': 0.5669304919807483, 'bagging_freq': 5, 'min_child_samples': 19}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:50:22,203] Finished trial#0 resulted in value: 0.9190442462685363. Current best value is 0.9190442462685363 with parameters: {'booster': 'gbtree', 'iterations': 103, 'learning_rate': 0.7597808912677907, 'random_strength': 62, 'bagging_temperature': 50.66977343950774, 'od_type': 'Iter', 'od_wait': 22, 'lambda_l1': 7.434126748681443e-07, 'lambda_l2': 0.00351728452576551, 'num_leaves': 104, 'feature_fraction': 0.4508349977292094, 'bagging_fraction': 0.40809902352411337, 'bagging_freq': 3, 'min_child_samples': 73}.\n",
      "[I 2019-11-20 14:50:26,060] Finished trial#1 resulted in value: 0.9561786264207287. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:27,830] Finished trial#2 resulted in value: 0.7484399047076318. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:29,704] Finished trial#3 resulted in value: 0.9517489634448248. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:31,504] Finished trial#4 resulted in value: 0.9451362832560125. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:33,243] Finished trial#5 resulted in value: 0.9547846621786619. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:35,516] Finished trial#6 resulted in value: 0.950153906190244. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:38,358] Finished trial#7 resulted in value: 0.8794672564179061. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:41,705] Finished trial#8 resulted in value: 0.9463267555279613. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:45,019] Finished trial#9 resulted in value: 0.9491094153626438. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:49,833] Finished trial#10 resulted in value: 0.951389751571497. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:51,252] Finished trial#11 resulted in value: 0.9532864049605327. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:52,882] Finished trial#12 resulted in value: 0.8324258524376896. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:50:59,678] Finished trial#13 resulted in value: 0.9536680802440181. Current best value is 0.9561786264207287 with parameters: {'booster': 'gblinear', 'iterations': 157, 'learning_rate': 0.12165360415228718, 'random_strength': 48, 'bagging_temperature': 0.6866054425106289, 'od_type': 'IncToDec', 'od_wait': 17, 'lambda_l1': 0.27329988248253806, 'lambda_l2': 6.850685344136793e-05, 'num_leaves': 212, 'feature_fraction': 0.8667216729716639, 'bagging_fraction': 0.7738121861927356, 'bagging_freq': 3, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:51:07,116] Finished trial#14 resulted in value: 0.9585452521008078. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:13,532] Finished trial#15 resulted in value: 0.957382636145273. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:21,090] Finished trial#16 resulted in value: 0.9561206288398209. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:23,635] Finished trial#17 resulted in value: 0.890840649928222. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n",
      "[I 2019-11-20 14:51:29,618] Finished trial#18 resulted in value: 0.9576068734294443. Current best value is 0.9585452521008078 with parameters: {'booster': 'gbtree', 'iterations': 151, 'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, 'lambda_l1': 1.2463610788259488e-08, 'lambda_l2': 7.857095132818392e-06, 'num_leaves': 62, 'feature_fraction': 0.9078544999774736, 'bagging_fraction': 0.9963767668938488, 'bagging_freq': 2, 'min_child_samples': 5}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-420-6f2c397560e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-420-6f2c397560e4>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     44\u001b[0m     }\n\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "def min_max_normalization(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    x_norm = (x - x_min) / ( x_max - x_min)\n",
    "    return x_norm\n",
    "def min_max_renormalization(x1,x2):\n",
    "    x_min = x2.min()\n",
    "    x_max = x2.max()\n",
    "#     x_norm = (x2 - x_min) / ( x_max - x_min)\n",
    "    x=x1*  ( x_max - x_min)+x_min\n",
    "    return x\n",
    "\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "cat_list = ['area', 'sex', 'partner','education']\n",
    "class LGBRegressorCV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'eval_metric':'mae',\n",
    "            'booster':trial.suggest_categorical('booster',['gbtree','gblinear']),\n",
    "            'loss_function': 'fair',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "#             'depth' : trial.suggest_int('depth', 4, 25),  \n",
    "            'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50),\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressorCV(n_trials=20)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "# model = XGBRegressorCV(n_trials=20)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "# checktesty=valid_y.sort_values()\n",
    "# checktestX=valid_X.ix[list(checktesty.index)]\n",
    "# checktesty=checktesty.reset_index(drop=True)#これと\n",
    "\n",
    "# checkpred=pd.DataFrame(model.predict(checktestX))#これ\n",
    "# check=pd.concat([checktesty,checkpred], axis=1)\n",
    "# check.columns=[\"actual\",\"predict\"]\n",
    "# check.plot(alpha=0.5)\n",
    "#MAE:23.144563 (nodrop)dummy\n",
    "#MAE:23.742147(drop)dummy\n",
    "#MAE:23.902507nodummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:48:38,249] Finished trial#0 resulted in value: 0.9546877750924008. Current best value is 0.9546877750924008 with parameters: {'iterations': 101, 'learning_rate': 0.05677338448922201, 'random_strength': 74, 'bagging_temperature': 2.0510543184036862, 'od_type': 'Iter', 'od_wait': 35}.\n",
      "[I 2019-11-20 14:48:40,445] Finished trial#1 resulted in value: 0.8725052547829868. Current best value is 0.9546877750924008 with parameters: {'iterations': 101, 'learning_rate': 0.05677338448922201, 'random_strength': 74, 'bagging_temperature': 2.0510543184036862, 'od_type': 'Iter', 'od_wait': 35}.\n",
      "[I 2019-11-20 14:48:41,974] Finished trial#2 resulted in value: 0.956279653164976. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:43,704] Finished trial#3 resulted in value: 0.9526568224980487. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:45,385] Finished trial#4 resulted in value: 0.724462304574427. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:47,379] Finished trial#5 resulted in value: 0.934567977655848. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:49,251] Finished trial#6 resulted in value: 0.8608944531751209. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:50,978] Finished trial#7 resulted in value: 0.9443375210424974. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:52,895] Finished trial#8 resulted in value: 0.6812147720085355. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:55,129] Finished trial#9 resulted in value: 0.9423476110039471. Current best value is 0.956279653164976 with parameters: {'iterations': 77, 'learning_rate': 0.2861080183583399, 'random_strength': 8, 'bagging_temperature': 0.053753584042810526, 'od_type': 'IncToDec', 'od_wait': 11}.\n",
      "[I 2019-11-20 14:48:56,988] Finished trial#10 resulted in value: 0.9578334287160528. Current best value is 0.9578334287160528 with parameters: {'iterations': 52, 'learning_rate': 0.22744116181524698, 'random_strength': 93, 'bagging_temperature': 0.26035526103111334, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:48:59,048] Finished trial#11 resulted in value: 0.958161293343581. Current best value is 0.958161293343581 with parameters: {'iterations': 51, 'learning_rate': 0.24474452006467162, 'random_strength': 95, 'bagging_temperature': 0.27004141678600385, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:01,749] Finished trial#12 resulted in value: 0.9583523351809283. Current best value is 0.9583523351809283 with parameters: {'iterations': 50, 'learning_rate': 0.190908395790108, 'random_strength': 99, 'bagging_temperature': 0.3365228994292324, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:03,349] Finished trial#13 resulted in value: 0.9587674974781338. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:05,061] Finished trial#14 resulted in value: 0.9580861649431325. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:06,865] Finished trial#15 resulted in value: 0.9530357537100704. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n",
      "[I 2019-11-20 14:49:09,409] Finished trial#16 resulted in value: 0.9580107388968948. Current best value is 0.9587674974781338 with parameters: {'iterations': 88, 'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-870cf0fd95d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLGBRegressor1CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtrain_ylgm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_max_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_ylgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_max_renormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;31m# https://github.com/pfnet/optuna/pull/325.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    \n",
    "class LGBRegressor1CV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'xentropy loss',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 8, 20), \n",
    "#             'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressor1CV(n_trials=40)\n",
    "train_ylgm=min_max_normalization(train_y)\n",
    "model.fit(train_X,train_ylgm)\n",
    "\n",
    "pred_y=min_max_renormalization(model.predict(valid_X),train_y)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost+optuna(non recomended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4311223\ttotal: 6.09ms\tremaining: 6.08s\n",
      "1:\tlearn: 0.4122959\ttotal: 15.3ms\tremaining: 7.62s\n",
      "2:\tlearn: 0.3929384\ttotal: 24.4ms\tremaining: 8.12s\n",
      "3:\tlearn: 0.3781482\ttotal: 30.1ms\tremaining: 7.5s\n",
      "4:\tlearn: 0.3617889\ttotal: 44.7ms\tremaining: 8.9s\n",
      "5:\tlearn: 0.3479133\ttotal: 58.7ms\tremaining: 9.72s\n",
      "6:\tlearn: 0.3305254\ttotal: 70.5ms\tremaining: 9.99s\n",
      "7:\tlearn: 0.3156954\ttotal: 83.6ms\tremaining: 10.4s\n",
      "8:\tlearn: 0.3049368\ttotal: 93.7ms\tremaining: 10.3s\n",
      "9:\tlearn: 0.2948312\ttotal: 103ms\tremaining: 10.2s\n",
      "10:\tlearn: 0.2851900\ttotal: 113ms\tremaining: 10.2s\n",
      "11:\tlearn: 0.2806031\ttotal: 123ms\tremaining: 10.1s\n",
      "12:\tlearn: 0.2729884\ttotal: 132ms\tremaining: 10.1s\n",
      "13:\tlearn: 0.2656313\ttotal: 142ms\tremaining: 10s\n",
      "14:\tlearn: 0.2601300\ttotal: 151ms\tremaining: 9.93s\n",
      "15:\tlearn: 0.2539201\ttotal: 170ms\tremaining: 10.5s\n",
      "16:\tlearn: 0.2485246\ttotal: 183ms\tremaining: 10.6s\n",
      "17:\tlearn: 0.2423629\ttotal: 195ms\tremaining: 10.6s\n",
      "18:\tlearn: 0.2380699\ttotal: 247ms\tremaining: 12.8s\n",
      "19:\tlearn: 0.2347966\ttotal: 288ms\tremaining: 14.1s\n",
      "20:\tlearn: 0.2311633\ttotal: 333ms\tremaining: 15.5s\n",
      "21:\tlearn: 0.2280010\ttotal: 357ms\tremaining: 15.9s\n",
      "22:\tlearn: 0.2217537\ttotal: 370ms\tremaining: 15.7s\n",
      "23:\tlearn: 0.2192560\ttotal: 382ms\tremaining: 15.5s\n",
      "24:\tlearn: 0.2161294\ttotal: 409ms\tremaining: 15.9s\n",
      "25:\tlearn: 0.2141041\ttotal: 419ms\tremaining: 15.7s\n",
      "26:\tlearn: 0.2092834\ttotal: 429ms\tremaining: 15.4s\n",
      "27:\tlearn: 0.2043057\ttotal: 436ms\tremaining: 15.1s\n",
      "28:\tlearn: 0.2012852\ttotal: 449ms\tremaining: 15s\n",
      "29:\tlearn: 0.2000804\ttotal: 479ms\tremaining: 15.5s\n",
      "30:\tlearn: 0.1983112\ttotal: 496ms\tremaining: 15.5s\n",
      "31:\tlearn: 0.1972527\ttotal: 505ms\tremaining: 15.3s\n",
      "32:\tlearn: 0.1942226\ttotal: 522ms\tremaining: 15.3s\n",
      "33:\tlearn: 0.1896919\ttotal: 532ms\tremaining: 15.1s\n",
      "34:\tlearn: 0.1858320\ttotal: 543ms\tremaining: 15s\n",
      "35:\tlearn: 0.1852328\ttotal: 552ms\tremaining: 14.8s\n",
      "36:\tlearn: 0.1845956\ttotal: 572ms\tremaining: 14.9s\n",
      "37:\tlearn: 0.1838172\ttotal: 580ms\tremaining: 14.7s\n",
      "38:\tlearn: 0.1826762\ttotal: 599ms\tremaining: 14.8s\n",
      "39:\tlearn: 0.1817929\ttotal: 615ms\tremaining: 14.8s\n",
      "40:\tlearn: 0.1800657\ttotal: 626ms\tremaining: 14.6s\n",
      "41:\tlearn: 0.1787221\ttotal: 633ms\tremaining: 14.4s\n",
      "42:\tlearn: 0.1780508\ttotal: 649ms\tremaining: 14.4s\n",
      "43:\tlearn: 0.1771337\ttotal: 663ms\tremaining: 14.4s\n",
      "44:\tlearn: 0.1767505\ttotal: 674ms\tremaining: 14.3s\n",
      "45:\tlearn: 0.1753077\ttotal: 687ms\tremaining: 14.3s\n",
      "46:\tlearn: 0.1728196\ttotal: 698ms\tremaining: 14.2s\n",
      "47:\tlearn: 0.1723998\ttotal: 713ms\tremaining: 14.1s\n",
      "48:\tlearn: 0.1698267\ttotal: 739ms\tremaining: 14.3s\n",
      "49:\tlearn: 0.1693407\ttotal: 771ms\tremaining: 14.7s\n",
      "50:\tlearn: 0.1670097\ttotal: 807ms\tremaining: 15s\n",
      "51:\tlearn: 0.1667453\ttotal: 840ms\tremaining: 15.3s\n",
      "52:\tlearn: 0.1653919\ttotal: 855ms\tremaining: 15.3s\n",
      "53:\tlearn: 0.1651249\ttotal: 867ms\tremaining: 15.2s\n",
      "54:\tlearn: 0.1651133\ttotal: 878ms\tremaining: 15.1s\n",
      "55:\tlearn: 0.1649510\ttotal: 891ms\tremaining: 15s\n",
      "56:\tlearn: 0.1646254\ttotal: 905ms\tremaining: 15s\n",
      "57:\tlearn: 0.1644612\ttotal: 915ms\tremaining: 14.9s\n",
      "58:\tlearn: 0.1643066\ttotal: 926ms\tremaining: 14.8s\n",
      "59:\tlearn: 0.1632530\ttotal: 936ms\tremaining: 14.7s\n",
      "60:\tlearn: 0.1614734\ttotal: 949ms\tremaining: 14.6s\n",
      "61:\tlearn: 0.1611170\ttotal: 966ms\tremaining: 14.6s\n",
      "62:\tlearn: 0.1607912\ttotal: 978ms\tremaining: 14.5s\n",
      "63:\tlearn: 0.1604157\ttotal: 987ms\tremaining: 14.4s\n",
      "64:\tlearn: 0.1595622\ttotal: 1s\tremaining: 14.4s\n",
      "65:\tlearn: 0.1591387\ttotal: 1.01s\tremaining: 14.3s\n",
      "66:\tlearn: 0.1590066\ttotal: 1.02s\tremaining: 14.3s\n",
      "67:\tlearn: 0.1588404\ttotal: 1.04s\tremaining: 14.3s\n",
      "68:\tlearn: 0.1588055\ttotal: 1.05s\tremaining: 14.2s\n",
      "69:\tlearn: 0.1586660\ttotal: 1.07s\tremaining: 14.2s\n",
      "70:\tlearn: 0.1582629\ttotal: 1.08s\tremaining: 14.1s\n",
      "71:\tlearn: 0.1581518\ttotal: 1.09s\tremaining: 14s\n",
      "72:\tlearn: 0.1580025\ttotal: 1.1s\tremaining: 14s\n",
      "73:\tlearn: 0.1573191\ttotal: 1.11s\tremaining: 13.9s\n",
      "74:\tlearn: 0.1565309\ttotal: 1.12s\tremaining: 13.8s\n",
      "75:\tlearn: 0.1561008\ttotal: 1.13s\tremaining: 13.8s\n",
      "76:\tlearn: 0.1558711\ttotal: 1.14s\tremaining: 13.7s\n",
      "77:\tlearn: 0.1557153\ttotal: 1.16s\tremaining: 13.7s\n",
      "78:\tlearn: 0.1550222\ttotal: 1.17s\tremaining: 13.7s\n",
      "79:\tlearn: 0.1543709\ttotal: 1.18s\tremaining: 13.6s\n",
      "80:\tlearn: 0.1541333\ttotal: 1.21s\tremaining: 13.7s\n",
      "81:\tlearn: 0.1540627\ttotal: 1.22s\tremaining: 13.7s\n",
      "82:\tlearn: 0.1536081\ttotal: 1.24s\tremaining: 13.7s\n",
      "83:\tlearn: 0.1530523\ttotal: 1.25s\tremaining: 13.6s\n",
      "84:\tlearn: 0.1529161\ttotal: 1.27s\tremaining: 13.6s\n",
      "85:\tlearn: 0.1528759\ttotal: 1.28s\tremaining: 13.6s\n",
      "86:\tlearn: 0.1527104\ttotal: 1.29s\tremaining: 13.6s\n",
      "87:\tlearn: 0.1525871\ttotal: 1.3s\tremaining: 13.5s\n",
      "88:\tlearn: 0.1525130\ttotal: 1.32s\tremaining: 13.5s\n",
      "89:\tlearn: 0.1523223\ttotal: 1.35s\tremaining: 13.6s\n",
      "90:\tlearn: 0.1520831\ttotal: 1.37s\tremaining: 13.7s\n",
      "91:\tlearn: 0.1517886\ttotal: 1.38s\tremaining: 13.6s\n",
      "92:\tlearn: 0.1510540\ttotal: 1.39s\tremaining: 13.6s\n",
      "93:\tlearn: 0.1509498\ttotal: 1.4s\tremaining: 13.5s\n",
      "94:\tlearn: 0.1505193\ttotal: 1.41s\tremaining: 13.5s\n",
      "95:\tlearn: 0.1503798\ttotal: 1.43s\tremaining: 13.4s\n",
      "96:\tlearn: 0.1503078\ttotal: 1.44s\tremaining: 13.4s\n",
      "97:\tlearn: 0.1502307\ttotal: 1.44s\tremaining: 13.3s\n",
      "98:\tlearn: 0.1498731\ttotal: 1.45s\tremaining: 13.2s\n",
      "99:\tlearn: 0.1487402\ttotal: 1.47s\tremaining: 13.2s\n",
      "100:\tlearn: 0.1485769\ttotal: 1.47s\tremaining: 13.1s\n",
      "101:\tlearn: 0.1482429\ttotal: 1.48s\tremaining: 13.1s\n",
      "102:\tlearn: 0.1477609\ttotal: 1.49s\tremaining: 13s\n",
      "103:\tlearn: 0.1477561\ttotal: 1.5s\tremaining: 12.9s\n",
      "104:\tlearn: 0.1477085\ttotal: 1.51s\tremaining: 12.9s\n",
      "105:\tlearn: 0.1476461\ttotal: 1.52s\tremaining: 12.8s\n",
      "106:\tlearn: 0.1476081\ttotal: 1.52s\tremaining: 12.7s\n",
      "107:\tlearn: 0.1476074\ttotal: 1.53s\tremaining: 12.6s\n",
      "108:\tlearn: 0.1471139\ttotal: 1.54s\tremaining: 12.6s\n",
      "109:\tlearn: 0.1470467\ttotal: 1.55s\tremaining: 12.5s\n",
      "110:\tlearn: 0.1469894\ttotal: 1.56s\tremaining: 12.5s\n",
      "111:\tlearn: 0.1469754\ttotal: 1.57s\tremaining: 12.4s\n",
      "112:\tlearn: 0.1469007\ttotal: 1.58s\tremaining: 12.4s\n",
      "113:\tlearn: 0.1468391\ttotal: 1.6s\tremaining: 12.4s\n",
      "114:\tlearn: 0.1467413\ttotal: 1.61s\tremaining: 12.4s\n",
      "115:\tlearn: 0.1459982\ttotal: 1.62s\tremaining: 12.3s\n",
      "116:\tlearn: 0.1459376\ttotal: 1.62s\tremaining: 12.2s\n",
      "117:\tlearn: 0.1458638\ttotal: 1.64s\tremaining: 12.2s\n",
      "118:\tlearn: 0.1456469\ttotal: 1.65s\tremaining: 12.2s\n",
      "119:\tlearn: 0.1455688\ttotal: 1.66s\tremaining: 12.1s\n",
      "120:\tlearn: 0.1443474\ttotal: 1.67s\tremaining: 12.1s\n",
      "121:\tlearn: 0.1442534\ttotal: 1.68s\tremaining: 12.1s\n",
      "122:\tlearn: 0.1442181\ttotal: 1.69s\tremaining: 12s\n",
      "123:\tlearn: 0.1441789\ttotal: 1.7s\tremaining: 12s\n",
      "124:\tlearn: 0.1441204\ttotal: 1.7s\tremaining: 11.9s\n",
      "125:\tlearn: 0.1432705\ttotal: 1.71s\tremaining: 11.9s\n",
      "126:\tlearn: 0.1431116\ttotal: 1.72s\tremaining: 11.8s\n",
      "127:\tlearn: 0.1430280\ttotal: 1.73s\tremaining: 11.8s\n",
      "128:\tlearn: 0.1430054\ttotal: 1.76s\tremaining: 11.9s\n",
      "129:\tlearn: 0.1428502\ttotal: 1.78s\tremaining: 11.9s\n",
      "130:\tlearn: 0.1427607\ttotal: 1.81s\tremaining: 12s\n",
      "131:\tlearn: 0.1413635\ttotal: 1.83s\tremaining: 12s\n",
      "132:\tlearn: 0.1412966\ttotal: 1.86s\tremaining: 12.1s\n",
      "133:\tlearn: 0.1407043\ttotal: 1.87s\tremaining: 12.1s\n",
      "134:\tlearn: 0.1405489\ttotal: 1.88s\tremaining: 12s\n",
      "135:\tlearn: 0.1404847\ttotal: 1.88s\tremaining: 12s\n",
      "136:\tlearn: 0.1404707\ttotal: 1.89s\tremaining: 11.9s\n",
      "137:\tlearn: 0.1399496\ttotal: 1.9s\tremaining: 11.9s\n",
      "138:\tlearn: 0.1396927\ttotal: 1.91s\tremaining: 11.8s\n",
      "139:\tlearn: 0.1394807\ttotal: 1.92s\tremaining: 11.8s\n",
      "140:\tlearn: 0.1394378\ttotal: 1.93s\tremaining: 11.8s\n",
      "141:\tlearn: 0.1388223\ttotal: 1.94s\tremaining: 11.7s\n",
      "142:\tlearn: 0.1388124\ttotal: 1.95s\tremaining: 11.7s\n",
      "143:\tlearn: 0.1379689\ttotal: 1.96s\tremaining: 11.6s\n",
      "144:\tlearn: 0.1374252\ttotal: 1.97s\tremaining: 11.6s\n",
      "145:\tlearn: 0.1368803\ttotal: 1.98s\tremaining: 11.6s\n",
      "146:\tlearn: 0.1359654\ttotal: 1.98s\tremaining: 11.5s\n",
      "147:\tlearn: 0.1348713\ttotal: 2s\tremaining: 11.5s\n",
      "148:\tlearn: 0.1344440\ttotal: 2s\tremaining: 11.5s\n",
      "149:\tlearn: 0.1341088\ttotal: 2.02s\tremaining: 11.4s\n",
      "150:\tlearn: 0.1335523\ttotal: 2.02s\tremaining: 11.4s\n",
      "151:\tlearn: 0.1322850\ttotal: 2.03s\tremaining: 11.3s\n",
      "152:\tlearn: 0.1313930\ttotal: 2.04s\tremaining: 11.3s\n",
      "153:\tlearn: 0.1310088\ttotal: 2.05s\tremaining: 11.3s\n",
      "154:\tlearn: 0.1300560\ttotal: 2.06s\tremaining: 11.3s\n",
      "155:\tlearn: 0.1295490\ttotal: 2.07s\tremaining: 11.2s\n",
      "156:\tlearn: 0.1292416\ttotal: 2.09s\tremaining: 11.2s\n",
      "157:\tlearn: 0.1277759\ttotal: 2.1s\tremaining: 11.2s\n",
      "158:\tlearn: 0.1270675\ttotal: 2.11s\tremaining: 11.2s\n",
      "159:\tlearn: 0.1266233\ttotal: 2.12s\tremaining: 11.1s\n",
      "160:\tlearn: 0.1262800\ttotal: 2.13s\tremaining: 11.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.1260394\ttotal: 2.13s\tremaining: 11s\n",
      "162:\tlearn: 0.1253762\ttotal: 2.14s\tremaining: 11s\n",
      "163:\tlearn: 0.1245294\ttotal: 2.15s\tremaining: 11s\n",
      "164:\tlearn: 0.1236547\ttotal: 2.16s\tremaining: 10.9s\n",
      "165:\tlearn: 0.1226551\ttotal: 2.17s\tremaining: 10.9s\n",
      "166:\tlearn: 0.1218153\ttotal: 2.19s\tremaining: 10.9s\n",
      "167:\tlearn: 0.1210012\ttotal: 2.2s\tremaining: 10.9s\n",
      "168:\tlearn: 0.1198229\ttotal: 2.21s\tremaining: 10.9s\n",
      "169:\tlearn: 0.1186641\ttotal: 2.21s\tremaining: 10.8s\n",
      "170:\tlearn: 0.1184452\ttotal: 2.22s\tremaining: 10.8s\n",
      "171:\tlearn: 0.1174584\ttotal: 2.23s\tremaining: 10.7s\n",
      "172:\tlearn: 0.1168594\ttotal: 2.24s\tremaining: 10.7s\n",
      "173:\tlearn: 0.1160382\ttotal: 2.25s\tremaining: 10.7s\n",
      "174:\tlearn: 0.1155777\ttotal: 2.26s\tremaining: 10.6s\n",
      "175:\tlearn: 0.1149415\ttotal: 2.27s\tremaining: 10.6s\n",
      "176:\tlearn: 0.1141265\ttotal: 2.29s\tremaining: 10.6s\n",
      "177:\tlearn: 0.1138118\ttotal: 2.31s\tremaining: 10.6s\n",
      "178:\tlearn: 0.1132746\ttotal: 2.32s\tremaining: 10.7s\n",
      "179:\tlearn: 0.1129659\ttotal: 2.34s\tremaining: 10.7s\n",
      "180:\tlearn: 0.1125093\ttotal: 2.35s\tremaining: 10.7s\n",
      "181:\tlearn: 0.1123111\ttotal: 2.37s\tremaining: 10.6s\n",
      "182:\tlearn: 0.1121614\ttotal: 2.38s\tremaining: 10.6s\n",
      "183:\tlearn: 0.1117889\ttotal: 2.39s\tremaining: 10.6s\n",
      "184:\tlearn: 0.1112220\ttotal: 2.4s\tremaining: 10.6s\n",
      "185:\tlearn: 0.1106817\ttotal: 2.41s\tremaining: 10.5s\n",
      "186:\tlearn: 0.1100388\ttotal: 2.42s\tremaining: 10.5s\n",
      "187:\tlearn: 0.1095495\ttotal: 2.43s\tremaining: 10.5s\n",
      "188:\tlearn: 0.1091457\ttotal: 2.44s\tremaining: 10.5s\n",
      "189:\tlearn: 0.1087893\ttotal: 2.44s\tremaining: 10.4s\n",
      "190:\tlearn: 0.1082639\ttotal: 2.45s\tremaining: 10.4s\n",
      "191:\tlearn: 0.1076691\ttotal: 2.46s\tremaining: 10.3s\n",
      "192:\tlearn: 0.1073450\ttotal: 2.47s\tremaining: 10.3s\n",
      "193:\tlearn: 0.1066572\ttotal: 2.48s\tremaining: 10.3s\n",
      "194:\tlearn: 0.1062926\ttotal: 2.49s\tremaining: 10.3s\n",
      "195:\tlearn: 0.1059169\ttotal: 2.5s\tremaining: 10.2s\n",
      "196:\tlearn: 0.1055321\ttotal: 2.5s\tremaining: 10.2s\n",
      "197:\tlearn: 0.1050727\ttotal: 2.51s\tremaining: 10.2s\n",
      "198:\tlearn: 0.1046798\ttotal: 2.52s\tremaining: 10.1s\n",
      "199:\tlearn: 0.1043478\ttotal: 2.53s\tremaining: 10.1s\n",
      "200:\tlearn: 0.1038878\ttotal: 2.54s\tremaining: 10.1s\n",
      "201:\tlearn: 0.1035696\ttotal: 2.55s\tremaining: 10.1s\n",
      "202:\tlearn: 0.1032663\ttotal: 2.56s\tremaining: 10.1s\n",
      "203:\tlearn: 0.1030018\ttotal: 2.57s\tremaining: 10s\n",
      "204:\tlearn: 0.1027316\ttotal: 2.59s\tremaining: 10s\n",
      "205:\tlearn: 0.1025337\ttotal: 2.6s\tremaining: 10s\n",
      "206:\tlearn: 0.1022917\ttotal: 2.61s\tremaining: 9.99s\n",
      "207:\tlearn: 0.1020399\ttotal: 2.62s\tremaining: 9.97s\n",
      "208:\tlearn: 0.1016837\ttotal: 2.63s\tremaining: 9.95s\n",
      "209:\tlearn: 0.1014508\ttotal: 2.64s\tremaining: 9.93s\n",
      "210:\tlearn: 0.1012881\ttotal: 2.65s\tremaining: 9.91s\n",
      "211:\tlearn: 0.1009841\ttotal: 2.66s\tremaining: 9.89s\n",
      "212:\tlearn: 0.1008057\ttotal: 2.67s\tremaining: 9.87s\n",
      "213:\tlearn: 0.1006113\ttotal: 2.68s\tremaining: 9.85s\n",
      "214:\tlearn: 0.1004135\ttotal: 2.69s\tremaining: 9.82s\n",
      "215:\tlearn: 0.1002820\ttotal: 2.7s\tremaining: 9.81s\n",
      "216:\tlearn: 0.1001487\ttotal: 2.71s\tremaining: 9.79s\n",
      "217:\tlearn: 0.0999552\ttotal: 2.72s\tremaining: 9.77s\n",
      "218:\tlearn: 0.0997712\ttotal: 2.73s\tremaining: 9.75s\n",
      "219:\tlearn: 0.0995869\ttotal: 2.74s\tremaining: 9.73s\n",
      "220:\tlearn: 0.0994833\ttotal: 2.77s\tremaining: 9.75s\n",
      "221:\tlearn: 0.0992720\ttotal: 2.8s\tremaining: 9.81s\n",
      "222:\tlearn: 0.0991120\ttotal: 2.81s\tremaining: 9.81s\n",
      "223:\tlearn: 0.0989766\ttotal: 2.84s\tremaining: 9.82s\n",
      "224:\tlearn: 0.0986919\ttotal: 2.86s\tremaining: 9.86s\n",
      "225:\tlearn: 0.0984873\ttotal: 2.88s\tremaining: 9.86s\n",
      "226:\tlearn: 0.0983647\ttotal: 2.89s\tremaining: 9.84s\n",
      "227:\tlearn: 0.0982691\ttotal: 2.9s\tremaining: 9.82s\n",
      "228:\tlearn: 0.0980235\ttotal: 2.91s\tremaining: 9.8s\n",
      "229:\tlearn: 0.0978615\ttotal: 2.92s\tremaining: 9.78s\n",
      "230:\tlearn: 0.0977867\ttotal: 2.93s\tremaining: 9.77s\n",
      "231:\tlearn: 0.0976870\ttotal: 2.94s\tremaining: 9.75s\n",
      "232:\tlearn: 0.0974900\ttotal: 2.96s\tremaining: 9.74s\n",
      "233:\tlearn: 0.0973135\ttotal: 2.97s\tremaining: 9.73s\n",
      "234:\tlearn: 0.0971796\ttotal: 2.98s\tremaining: 9.7s\n",
      "235:\tlearn: 0.0969793\ttotal: 2.99s\tremaining: 9.68s\n",
      "236:\tlearn: 0.0968243\ttotal: 3s\tremaining: 9.68s\n",
      "237:\tlearn: 0.0966600\ttotal: 3.01s\tremaining: 9.65s\n",
      "238:\tlearn: 0.0965809\ttotal: 3.02s\tremaining: 9.63s\n",
      "239:\tlearn: 0.0964334\ttotal: 3.03s\tremaining: 9.61s\n",
      "240:\tlearn: 0.0963443\ttotal: 3.04s\tremaining: 9.58s\n",
      "241:\tlearn: 0.0961940\ttotal: 3.05s\tremaining: 9.56s\n",
      "242:\tlearn: 0.0960926\ttotal: 3.06s\tremaining: 9.55s\n",
      "243:\tlearn: 0.0959967\ttotal: 3.08s\tremaining: 9.53s\n",
      "244:\tlearn: 0.0959136\ttotal: 3.09s\tremaining: 9.51s\n",
      "245:\tlearn: 0.0957592\ttotal: 3.1s\tremaining: 9.49s\n",
      "246:\tlearn: 0.0956402\ttotal: 3.11s\tremaining: 9.47s\n",
      "247:\tlearn: 0.0955753\ttotal: 3.12s\tremaining: 9.45s\n",
      "248:\tlearn: 0.0954457\ttotal: 3.13s\tremaining: 9.43s\n",
      "249:\tlearn: 0.0953398\ttotal: 3.14s\tremaining: 9.41s\n",
      "250:\tlearn: 0.0951616\ttotal: 3.15s\tremaining: 9.39s\n",
      "251:\tlearn: 0.0950456\ttotal: 3.16s\tremaining: 9.37s\n",
      "252:\tlearn: 0.0949378\ttotal: 3.17s\tremaining: 9.36s\n",
      "253:\tlearn: 0.0947669\ttotal: 3.18s\tremaining: 9.34s\n",
      "254:\tlearn: 0.0945274\ttotal: 3.19s\tremaining: 9.32s\n",
      "255:\tlearn: 0.0944292\ttotal: 3.2s\tremaining: 9.3s\n",
      "256:\tlearn: 0.0943059\ttotal: 3.21s\tremaining: 9.28s\n",
      "257:\tlearn: 0.0942131\ttotal: 3.22s\tremaining: 9.26s\n",
      "258:\tlearn: 0.0940935\ttotal: 3.23s\tremaining: 9.25s\n",
      "259:\tlearn: 0.0940064\ttotal: 3.24s\tremaining: 9.23s\n",
      "260:\tlearn: 0.0938856\ttotal: 3.25s\tremaining: 9.22s\n",
      "261:\tlearn: 0.0937738\ttotal: 3.27s\tremaining: 9.2s\n",
      "262:\tlearn: 0.0936748\ttotal: 3.28s\tremaining: 9.18s\n",
      "263:\tlearn: 0.0934597\ttotal: 3.29s\tremaining: 9.16s\n",
      "264:\tlearn: 0.0933010\ttotal: 3.3s\tremaining: 9.15s\n",
      "265:\tlearn: 0.0932088\ttotal: 3.31s\tremaining: 9.13s\n",
      "266:\tlearn: 0.0931566\ttotal: 3.32s\tremaining: 9.11s\n",
      "267:\tlearn: 0.0931127\ttotal: 3.33s\tremaining: 9.09s\n",
      "268:\tlearn: 0.0930273\ttotal: 3.34s\tremaining: 9.07s\n",
      "269:\tlearn: 0.0929134\ttotal: 3.35s\tremaining: 9.05s\n",
      "270:\tlearn: 0.0928538\ttotal: 3.36s\tremaining: 9.03s\n",
      "271:\tlearn: 0.0926644\ttotal: 3.37s\tremaining: 9.01s\n",
      "272:\tlearn: 0.0925175\ttotal: 3.38s\tremaining: 8.99s\n",
      "273:\tlearn: 0.0924366\ttotal: 3.38s\tremaining: 8.97s\n",
      "274:\tlearn: 0.0923379\ttotal: 3.4s\tremaining: 8.96s\n",
      "275:\tlearn: 0.0922851\ttotal: 3.41s\tremaining: 8.94s\n",
      "276:\tlearn: 0.0922020\ttotal: 3.42s\tremaining: 8.91s\n",
      "277:\tlearn: 0.0920693\ttotal: 3.43s\tremaining: 8.9s\n",
      "278:\tlearn: 0.0919757\ttotal: 3.44s\tremaining: 8.89s\n",
      "279:\tlearn: 0.0919015\ttotal: 3.45s\tremaining: 8.87s\n",
      "280:\tlearn: 0.0917332\ttotal: 3.46s\tremaining: 8.85s\n",
      "281:\tlearn: 0.0916767\ttotal: 3.47s\tremaining: 8.84s\n",
      "282:\tlearn: 0.0915522\ttotal: 3.48s\tremaining: 8.82s\n",
      "283:\tlearn: 0.0914976\ttotal: 3.49s\tremaining: 8.8s\n",
      "284:\tlearn: 0.0913882\ttotal: 3.5s\tremaining: 8.79s\n",
      "285:\tlearn: 0.0913048\ttotal: 3.51s\tremaining: 8.77s\n",
      "286:\tlearn: 0.0912239\ttotal: 3.52s\tremaining: 8.75s\n",
      "287:\tlearn: 0.0911468\ttotal: 3.53s\tremaining: 8.73s\n",
      "288:\tlearn: 0.0910387\ttotal: 3.54s\tremaining: 8.71s\n",
      "289:\tlearn: 0.0909935\ttotal: 3.56s\tremaining: 8.71s\n",
      "290:\tlearn: 0.0909160\ttotal: 3.56s\tremaining: 8.68s\n",
      "291:\tlearn: 0.0907403\ttotal: 3.57s\tremaining: 8.66s\n",
      "292:\tlearn: 0.0906629\ttotal: 3.59s\tremaining: 8.66s\n",
      "293:\tlearn: 0.0906138\ttotal: 3.6s\tremaining: 8.64s\n",
      "294:\tlearn: 0.0905115\ttotal: 3.61s\tremaining: 8.62s\n",
      "295:\tlearn: 0.0904531\ttotal: 3.62s\tremaining: 8.61s\n",
      "296:\tlearn: 0.0904027\ttotal: 3.63s\tremaining: 8.59s\n",
      "297:\tlearn: 0.0902462\ttotal: 3.64s\tremaining: 8.57s\n",
      "298:\tlearn: 0.0901883\ttotal: 3.65s\tremaining: 8.56s\n",
      "299:\tlearn: 0.0900959\ttotal: 3.66s\tremaining: 8.54s\n",
      "300:\tlearn: 0.0900545\ttotal: 3.67s\tremaining: 8.53s\n",
      "301:\tlearn: 0.0899826\ttotal: 3.68s\tremaining: 8.51s\n",
      "302:\tlearn: 0.0898530\ttotal: 3.69s\tremaining: 8.5s\n",
      "303:\tlearn: 0.0898037\ttotal: 3.71s\tremaining: 8.49s\n",
      "304:\tlearn: 0.0897543\ttotal: 3.72s\tremaining: 8.47s\n",
      "305:\tlearn: 0.0896982\ttotal: 3.73s\tremaining: 8.46s\n",
      "306:\tlearn: 0.0896365\ttotal: 3.74s\tremaining: 8.45s\n",
      "307:\tlearn: 0.0895549\ttotal: 3.75s\tremaining: 8.43s\n",
      "308:\tlearn: 0.0895109\ttotal: 3.76s\tremaining: 8.41s\n",
      "309:\tlearn: 0.0894220\ttotal: 3.78s\tremaining: 8.41s\n",
      "310:\tlearn: 0.0893716\ttotal: 3.8s\tremaining: 8.41s\n",
      "311:\tlearn: 0.0892957\ttotal: 3.83s\tremaining: 8.44s\n",
      "312:\tlearn: 0.0892328\ttotal: 3.84s\tremaining: 8.44s\n",
      "313:\tlearn: 0.0891675\ttotal: 3.86s\tremaining: 8.43s\n",
      "314:\tlearn: 0.0891162\ttotal: 3.89s\tremaining: 8.45s\n",
      "315:\tlearn: 0.0890710\ttotal: 3.9s\tremaining: 8.44s\n",
      "316:\tlearn: 0.0890343\ttotal: 3.91s\tremaining: 8.43s\n",
      "317:\tlearn: 0.0890106\ttotal: 3.92s\tremaining: 8.41s\n",
      "318:\tlearn: 0.0889672\ttotal: 3.93s\tremaining: 8.39s\n",
      "319:\tlearn: 0.0889135\ttotal: 3.94s\tremaining: 8.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320:\tlearn: 0.0888810\ttotal: 3.95s\tremaining: 8.36s\n",
      "321:\tlearn: 0.0888278\ttotal: 3.96s\tremaining: 8.35s\n",
      "322:\tlearn: 0.0887025\ttotal: 3.98s\tremaining: 8.34s\n",
      "323:\tlearn: 0.0886652\ttotal: 3.98s\tremaining: 8.32s\n",
      "324:\tlearn: 0.0885920\ttotal: 4s\tremaining: 8.3s\n",
      "325:\tlearn: 0.0885365\ttotal: 4.01s\tremaining: 8.29s\n",
      "326:\tlearn: 0.0884819\ttotal: 4.02s\tremaining: 8.27s\n",
      "327:\tlearn: 0.0884438\ttotal: 4.03s\tremaining: 8.26s\n",
      "328:\tlearn: 0.0883378\ttotal: 4.04s\tremaining: 8.24s\n",
      "329:\tlearn: 0.0883066\ttotal: 4.05s\tremaining: 8.23s\n",
      "330:\tlearn: 0.0882703\ttotal: 4.07s\tremaining: 8.22s\n",
      "331:\tlearn: 0.0882276\ttotal: 4.08s\tremaining: 8.2s\n",
      "332:\tlearn: 0.0881350\ttotal: 4.08s\tremaining: 8.18s\n",
      "333:\tlearn: 0.0880910\ttotal: 4.1s\tremaining: 8.17s\n",
      "334:\tlearn: 0.0880187\ttotal: 4.11s\tremaining: 8.16s\n",
      "335:\tlearn: 0.0879648\ttotal: 4.12s\tremaining: 8.14s\n",
      "336:\tlearn: 0.0878786\ttotal: 4.13s\tremaining: 8.12s\n",
      "337:\tlearn: 0.0878516\ttotal: 4.13s\tremaining: 8.1s\n",
      "338:\tlearn: 0.0877520\ttotal: 4.14s\tremaining: 8.07s\n",
      "339:\tlearn: 0.0876903\ttotal: 4.15s\tremaining: 8.06s\n",
      "340:\tlearn: 0.0876500\ttotal: 4.16s\tremaining: 8.04s\n",
      "341:\tlearn: 0.0876138\ttotal: 4.17s\tremaining: 8.03s\n",
      "342:\tlearn: 0.0875670\ttotal: 4.18s\tremaining: 8.01s\n",
      "343:\tlearn: 0.0875234\ttotal: 4.19s\tremaining: 8s\n",
      "344:\tlearn: 0.0874622\ttotal: 4.2s\tremaining: 7.98s\n",
      "345:\tlearn: 0.0873691\ttotal: 4.21s\tremaining: 7.97s\n",
      "346:\tlearn: 0.0872930\ttotal: 4.22s\tremaining: 7.95s\n",
      "347:\tlearn: 0.0872649\ttotal: 4.23s\tremaining: 7.93s\n",
      "348:\tlearn: 0.0872308\ttotal: 4.25s\tremaining: 7.93s\n",
      "349:\tlearn: 0.0871984\ttotal: 4.26s\tremaining: 7.92s\n",
      "350:\tlearn: 0.0871124\ttotal: 4.28s\tremaining: 7.91s\n",
      "351:\tlearn: 0.0870668\ttotal: 4.29s\tremaining: 7.9s\n",
      "352:\tlearn: 0.0870248\ttotal: 4.3s\tremaining: 7.88s\n",
      "353:\tlearn: 0.0870034\ttotal: 4.31s\tremaining: 7.87s\n",
      "354:\tlearn: 0.0869812\ttotal: 4.32s\tremaining: 7.85s\n",
      "355:\tlearn: 0.0869234\ttotal: 4.33s\tremaining: 7.84s\n",
      "356:\tlearn: 0.0868903\ttotal: 4.34s\tremaining: 7.82s\n",
      "357:\tlearn: 0.0868521\ttotal: 4.35s\tremaining: 7.8s\n",
      "358:\tlearn: 0.0868118\ttotal: 4.36s\tremaining: 7.78s\n",
      "359:\tlearn: 0.0867864\ttotal: 4.37s\tremaining: 7.76s\n",
      "360:\tlearn: 0.0867424\ttotal: 4.38s\tremaining: 7.75s\n",
      "361:\tlearn: 0.0866474\ttotal: 4.39s\tremaining: 7.74s\n",
      "362:\tlearn: 0.0865953\ttotal: 4.4s\tremaining: 7.72s\n",
      "363:\tlearn: 0.0865630\ttotal: 4.41s\tremaining: 7.7s\n",
      "364:\tlearn: 0.0864793\ttotal: 4.42s\tremaining: 7.69s\n",
      "365:\tlearn: 0.0864307\ttotal: 4.43s\tremaining: 7.67s\n",
      "366:\tlearn: 0.0863789\ttotal: 4.43s\tremaining: 7.65s\n",
      "367:\tlearn: 0.0863551\ttotal: 4.44s\tremaining: 7.63s\n",
      "368:\tlearn: 0.0863379\ttotal: 4.45s\tremaining: 7.62s\n",
      "369:\tlearn: 0.0862524\ttotal: 4.46s\tremaining: 7.6s\n",
      "370:\tlearn: 0.0862242\ttotal: 4.47s\tremaining: 7.58s\n",
      "371:\tlearn: 0.0861402\ttotal: 4.48s\tremaining: 7.57s\n",
      "372:\tlearn: 0.0860873\ttotal: 4.49s\tremaining: 7.55s\n",
      "373:\tlearn: 0.0860542\ttotal: 4.5s\tremaining: 7.53s\n",
      "374:\tlearn: 0.0860117\ttotal: 4.51s\tremaining: 7.51s\n",
      "375:\tlearn: 0.0859653\ttotal: 4.52s\tremaining: 7.5s\n",
      "376:\tlearn: 0.0859325\ttotal: 4.53s\tremaining: 7.49s\n",
      "377:\tlearn: 0.0859120\ttotal: 4.54s\tremaining: 7.48s\n",
      "378:\tlearn: 0.0858936\ttotal: 4.56s\tremaining: 7.47s\n",
      "379:\tlearn: 0.0858622\ttotal: 4.57s\tremaining: 7.45s\n",
      "380:\tlearn: 0.0857976\ttotal: 4.58s\tremaining: 7.43s\n",
      "381:\tlearn: 0.0857182\ttotal: 4.58s\tremaining: 7.42s\n",
      "382:\tlearn: 0.0856770\ttotal: 4.59s\tremaining: 7.4s\n",
      "383:\tlearn: 0.0856496\ttotal: 4.61s\tremaining: 7.39s\n",
      "384:\tlearn: 0.0856212\ttotal: 4.62s\tremaining: 7.38s\n",
      "385:\tlearn: 0.0855579\ttotal: 4.63s\tremaining: 7.36s\n",
      "386:\tlearn: 0.0855148\ttotal: 4.64s\tremaining: 7.34s\n",
      "387:\tlearn: 0.0854991\ttotal: 4.64s\tremaining: 7.32s\n",
      "388:\tlearn: 0.0854674\ttotal: 4.66s\tremaining: 7.31s\n",
      "389:\tlearn: 0.0853876\ttotal: 4.66s\tremaining: 7.29s\n",
      "390:\tlearn: 0.0853267\ttotal: 4.67s\tremaining: 7.27s\n",
      "391:\tlearn: 0.0853057\ttotal: 4.68s\tremaining: 7.26s\n",
      "392:\tlearn: 0.0851840\ttotal: 4.69s\tremaining: 7.24s\n",
      "393:\tlearn: 0.0851531\ttotal: 4.7s\tremaining: 7.23s\n",
      "394:\tlearn: 0.0851218\ttotal: 4.71s\tremaining: 7.22s\n",
      "395:\tlearn: 0.0850721\ttotal: 4.72s\tremaining: 7.2s\n",
      "396:\tlearn: 0.0850420\ttotal: 4.73s\tremaining: 7.19s\n",
      "397:\tlearn: 0.0849972\ttotal: 4.75s\tremaining: 7.18s\n",
      "398:\tlearn: 0.0849675\ttotal: 4.75s\tremaining: 7.16s\n",
      "399:\tlearn: 0.0849146\ttotal: 4.76s\tremaining: 7.14s\n",
      "400:\tlearn: 0.0848798\ttotal: 4.77s\tremaining: 7.12s\n",
      "401:\tlearn: 0.0848480\ttotal: 4.78s\tremaining: 7.11s\n",
      "402:\tlearn: 0.0847711\ttotal: 4.79s\tremaining: 7.1s\n",
      "403:\tlearn: 0.0847476\ttotal: 4.82s\tremaining: 7.1s\n",
      "404:\tlearn: 0.0847151\ttotal: 4.84s\tremaining: 7.11s\n",
      "405:\tlearn: 0.0846537\ttotal: 4.86s\tremaining: 7.11s\n",
      "406:\tlearn: 0.0846081\ttotal: 4.9s\tremaining: 7.14s\n",
      "407:\tlearn: 0.0845414\ttotal: 4.92s\tremaining: 7.14s\n",
      "408:\tlearn: 0.0844902\ttotal: 4.93s\tremaining: 7.12s\n",
      "409:\tlearn: 0.0844378\ttotal: 4.94s\tremaining: 7.11s\n",
      "410:\tlearn: 0.0843975\ttotal: 4.94s\tremaining: 7.08s\n",
      "411:\tlearn: 0.0843649\ttotal: 4.96s\tremaining: 7.08s\n",
      "412:\tlearn: 0.0843352\ttotal: 4.97s\tremaining: 7.06s\n",
      "413:\tlearn: 0.0842200\ttotal: 4.98s\tremaining: 7.04s\n",
      "414:\tlearn: 0.0841636\ttotal: 4.99s\tremaining: 7.03s\n",
      "415:\tlearn: 0.0841251\ttotal: 5s\tremaining: 7.02s\n",
      "416:\tlearn: 0.0840708\ttotal: 5.01s\tremaining: 7.01s\n",
      "417:\tlearn: 0.0840443\ttotal: 5.02s\tremaining: 6.99s\n",
      "418:\tlearn: 0.0839953\ttotal: 5.03s\tremaining: 6.97s\n",
      "419:\tlearn: 0.0839660\ttotal: 5.03s\tremaining: 6.95s\n",
      "420:\tlearn: 0.0839119\ttotal: 5.04s\tremaining: 6.94s\n",
      "421:\tlearn: 0.0838529\ttotal: 5.05s\tremaining: 6.92s\n",
      "422:\tlearn: 0.0838342\ttotal: 5.06s\tremaining: 6.9s\n",
      "423:\tlearn: 0.0838077\ttotal: 5.07s\tremaining: 6.89s\n",
      "424:\tlearn: 0.0837879\ttotal: 5.08s\tremaining: 6.88s\n",
      "425:\tlearn: 0.0837679\ttotal: 5.09s\tremaining: 6.86s\n",
      "426:\tlearn: 0.0837338\ttotal: 5.1s\tremaining: 6.85s\n",
      "427:\tlearn: 0.0836964\ttotal: 5.11s\tremaining: 6.83s\n",
      "428:\tlearn: 0.0836569\ttotal: 5.12s\tremaining: 6.82s\n",
      "429:\tlearn: 0.0836258\ttotal: 5.13s\tremaining: 6.81s\n",
      "430:\tlearn: 0.0836091\ttotal: 5.14s\tremaining: 6.79s\n",
      "431:\tlearn: 0.0835654\ttotal: 5.15s\tremaining: 6.77s\n",
      "432:\tlearn: 0.0835105\ttotal: 5.16s\tremaining: 6.76s\n",
      "433:\tlearn: 0.0834885\ttotal: 5.17s\tremaining: 6.75s\n",
      "434:\tlearn: 0.0834522\ttotal: 5.18s\tremaining: 6.73s\n",
      "435:\tlearn: 0.0834083\ttotal: 5.19s\tremaining: 6.72s\n",
      "436:\tlearn: 0.0833847\ttotal: 5.2s\tremaining: 6.7s\n",
      "437:\tlearn: 0.0833385\ttotal: 5.21s\tremaining: 6.69s\n",
      "438:\tlearn: 0.0833168\ttotal: 5.22s\tremaining: 6.67s\n",
      "439:\tlearn: 0.0832846\ttotal: 5.23s\tremaining: 6.66s\n",
      "440:\tlearn: 0.0832561\ttotal: 5.24s\tremaining: 6.64s\n",
      "441:\tlearn: 0.0831736\ttotal: 5.24s\tremaining: 6.62s\n",
      "442:\tlearn: 0.0831516\ttotal: 5.26s\tremaining: 6.61s\n",
      "443:\tlearn: 0.0831345\ttotal: 5.27s\tremaining: 6.59s\n",
      "444:\tlearn: 0.0830984\ttotal: 5.28s\tremaining: 6.58s\n",
      "445:\tlearn: 0.0830802\ttotal: 5.29s\tremaining: 6.57s\n",
      "446:\tlearn: 0.0830497\ttotal: 5.29s\tremaining: 6.55s\n",
      "447:\tlearn: 0.0829569\ttotal: 5.3s\tremaining: 6.53s\n",
      "448:\tlearn: 0.0829454\ttotal: 5.31s\tremaining: 6.52s\n",
      "449:\tlearn: 0.0828988\ttotal: 5.32s\tremaining: 6.5s\n",
      "450:\tlearn: 0.0828788\ttotal: 5.33s\tremaining: 6.49s\n",
      "451:\tlearn: 0.0828386\ttotal: 5.34s\tremaining: 6.48s\n",
      "452:\tlearn: 0.0827881\ttotal: 5.35s\tremaining: 6.46s\n",
      "453:\tlearn: 0.0827593\ttotal: 5.36s\tremaining: 6.44s\n",
      "454:\tlearn: 0.0826700\ttotal: 5.36s\tremaining: 6.42s\n",
      "455:\tlearn: 0.0826496\ttotal: 5.37s\tremaining: 6.41s\n",
      "456:\tlearn: 0.0826252\ttotal: 5.38s\tremaining: 6.39s\n",
      "457:\tlearn: 0.0825832\ttotal: 5.39s\tremaining: 6.37s\n",
      "458:\tlearn: 0.0825642\ttotal: 5.39s\tremaining: 6.36s\n",
      "459:\tlearn: 0.0825418\ttotal: 5.4s\tremaining: 6.34s\n",
      "460:\tlearn: 0.0825012\ttotal: 5.41s\tremaining: 6.33s\n",
      "461:\tlearn: 0.0824771\ttotal: 5.43s\tremaining: 6.32s\n",
      "462:\tlearn: 0.0824474\ttotal: 5.43s\tremaining: 6.3s\n",
      "463:\tlearn: 0.0824105\ttotal: 5.44s\tremaining: 6.29s\n",
      "464:\tlearn: 0.0823821\ttotal: 5.45s\tremaining: 6.27s\n",
      "465:\tlearn: 0.0823563\ttotal: 5.46s\tremaining: 6.26s\n",
      "466:\tlearn: 0.0823041\ttotal: 5.47s\tremaining: 6.24s\n",
      "467:\tlearn: 0.0822883\ttotal: 5.47s\tremaining: 6.22s\n",
      "468:\tlearn: 0.0822703\ttotal: 5.48s\tremaining: 6.2s\n",
      "469:\tlearn: 0.0822349\ttotal: 5.49s\tremaining: 6.19s\n",
      "470:\tlearn: 0.0821939\ttotal: 5.5s\tremaining: 6.18s\n",
      "471:\tlearn: 0.0821791\ttotal: 5.51s\tremaining: 6.17s\n",
      "472:\tlearn: 0.0821593\ttotal: 5.52s\tremaining: 6.15s\n",
      "473:\tlearn: 0.0821131\ttotal: 5.53s\tremaining: 6.14s\n",
      "474:\tlearn: 0.0820654\ttotal: 5.54s\tremaining: 6.12s\n",
      "475:\tlearn: 0.0820420\ttotal: 5.55s\tremaining: 6.11s\n",
      "476:\tlearn: 0.0820181\ttotal: 5.56s\tremaining: 6.09s\n",
      "477:\tlearn: 0.0819950\ttotal: 5.57s\tremaining: 6.08s\n",
      "478:\tlearn: 0.0819424\ttotal: 5.58s\tremaining: 6.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479:\tlearn: 0.0819126\ttotal: 5.59s\tremaining: 6.05s\n",
      "480:\tlearn: 0.0818823\ttotal: 5.59s\tremaining: 6.04s\n",
      "481:\tlearn: 0.0818578\ttotal: 5.6s\tremaining: 6.02s\n",
      "482:\tlearn: 0.0818371\ttotal: 5.61s\tremaining: 6s\n",
      "483:\tlearn: 0.0818036\ttotal: 5.62s\tremaining: 5.99s\n",
      "484:\tlearn: 0.0817769\ttotal: 5.63s\tremaining: 5.97s\n",
      "485:\tlearn: 0.0817513\ttotal: 5.64s\tremaining: 5.96s\n",
      "486:\tlearn: 0.0817293\ttotal: 5.64s\tremaining: 5.95s\n",
      "487:\tlearn: 0.0817076\ttotal: 5.65s\tremaining: 5.93s\n",
      "488:\tlearn: 0.0816822\ttotal: 5.67s\tremaining: 5.92s\n",
      "489:\tlearn: 0.0816284\ttotal: 5.68s\tremaining: 5.91s\n",
      "490:\tlearn: 0.0815935\ttotal: 5.69s\tremaining: 5.9s\n",
      "491:\tlearn: 0.0815864\ttotal: 5.7s\tremaining: 5.88s\n",
      "492:\tlearn: 0.0815727\ttotal: 5.71s\tremaining: 5.87s\n",
      "493:\tlearn: 0.0815573\ttotal: 5.72s\tremaining: 5.86s\n",
      "494:\tlearn: 0.0815413\ttotal: 5.73s\tremaining: 5.84s\n",
      "495:\tlearn: 0.0815091\ttotal: 5.82s\tremaining: 5.92s\n",
      "496:\tlearn: 0.0814835\ttotal: 5.87s\tremaining: 5.94s\n",
      "497:\tlearn: 0.0814680\ttotal: 5.99s\tremaining: 6.04s\n",
      "498:\tlearn: 0.0814180\ttotal: 6s\tremaining: 6.02s\n",
      "499:\tlearn: 0.0813822\ttotal: 6.01s\tremaining: 6.01s\n",
      "500:\tlearn: 0.0813528\ttotal: 6.03s\tremaining: 6s\n",
      "501:\tlearn: 0.0813036\ttotal: 6.04s\tremaining: 5.99s\n",
      "502:\tlearn: 0.0812317\ttotal: 6.05s\tremaining: 5.98s\n",
      "503:\tlearn: 0.0812048\ttotal: 6.06s\tremaining: 5.96s\n",
      "504:\tlearn: 0.0811812\ttotal: 6.07s\tremaining: 5.95s\n",
      "505:\tlearn: 0.0811490\ttotal: 6.08s\tremaining: 5.94s\n",
      "506:\tlearn: 0.0811164\ttotal: 6.09s\tremaining: 5.92s\n",
      "507:\tlearn: 0.0810764\ttotal: 6.1s\tremaining: 5.91s\n",
      "508:\tlearn: 0.0810499\ttotal: 6.11s\tremaining: 5.9s\n",
      "509:\tlearn: 0.0810321\ttotal: 6.12s\tremaining: 5.88s\n",
      "510:\tlearn: 0.0809927\ttotal: 6.13s\tremaining: 5.87s\n",
      "511:\tlearn: 0.0809573\ttotal: 6.14s\tremaining: 5.86s\n",
      "512:\tlearn: 0.0809425\ttotal: 6.16s\tremaining: 5.85s\n",
      "513:\tlearn: 0.0809219\ttotal: 6.17s\tremaining: 5.84s\n",
      "514:\tlearn: 0.0809031\ttotal: 6.18s\tremaining: 5.82s\n",
      "515:\tlearn: 0.0808569\ttotal: 6.19s\tremaining: 5.81s\n",
      "516:\tlearn: 0.0808090\ttotal: 6.21s\tremaining: 5.8s\n",
      "517:\tlearn: 0.0807885\ttotal: 6.22s\tremaining: 5.79s\n",
      "518:\tlearn: 0.0807536\ttotal: 6.23s\tremaining: 5.78s\n",
      "519:\tlearn: 0.0807337\ttotal: 6.25s\tremaining: 5.76s\n",
      "520:\tlearn: 0.0807274\ttotal: 6.26s\tremaining: 5.76s\n",
      "521:\tlearn: 0.0807019\ttotal: 6.27s\tremaining: 5.74s\n",
      "522:\tlearn: 0.0806751\ttotal: 6.29s\tremaining: 5.73s\n",
      "523:\tlearn: 0.0806360\ttotal: 6.3s\tremaining: 5.73s\n",
      "524:\tlearn: 0.0806197\ttotal: 6.31s\tremaining: 5.71s\n",
      "525:\tlearn: 0.0805936\ttotal: 6.32s\tremaining: 5.7s\n",
      "526:\tlearn: 0.0805561\ttotal: 6.33s\tremaining: 5.68s\n",
      "527:\tlearn: 0.0805115\ttotal: 6.34s\tremaining: 5.66s\n",
      "528:\tlearn: 0.0804984\ttotal: 6.35s\tremaining: 5.65s\n",
      "529:\tlearn: 0.0804846\ttotal: 6.38s\tremaining: 5.65s\n",
      "530:\tlearn: 0.0804406\ttotal: 6.4s\tremaining: 5.66s\n",
      "531:\tlearn: 0.0804208\ttotal: 6.42s\tremaining: 5.65s\n",
      "532:\tlearn: 0.0803814\ttotal: 6.44s\tremaining: 5.64s\n",
      "533:\tlearn: 0.0803719\ttotal: 6.46s\tremaining: 5.64s\n",
      "534:\tlearn: 0.0803371\ttotal: 6.47s\tremaining: 5.63s\n",
      "535:\tlearn: 0.0803234\ttotal: 6.5s\tremaining: 5.62s\n",
      "536:\tlearn: 0.0803055\ttotal: 6.51s\tremaining: 5.61s\n",
      "537:\tlearn: 0.0802737\ttotal: 6.54s\tremaining: 5.61s\n",
      "538:\tlearn: 0.0802533\ttotal: 6.57s\tremaining: 5.62s\n",
      "539:\tlearn: 0.0802171\ttotal: 6.58s\tremaining: 5.61s\n",
      "540:\tlearn: 0.0801933\ttotal: 6.6s\tremaining: 5.6s\n",
      "541:\tlearn: 0.0801828\ttotal: 6.62s\tremaining: 5.59s\n",
      "542:\tlearn: 0.0801546\ttotal: 6.64s\tremaining: 5.59s\n",
      "543:\tlearn: 0.0801369\ttotal: 6.66s\tremaining: 5.58s\n",
      "544:\tlearn: 0.0801216\ttotal: 6.68s\tremaining: 5.57s\n",
      "545:\tlearn: 0.0800756\ttotal: 6.69s\tremaining: 5.57s\n",
      "546:\tlearn: 0.0800590\ttotal: 6.71s\tremaining: 5.56s\n",
      "547:\tlearn: 0.0800463\ttotal: 6.73s\tremaining: 5.55s\n",
      "548:\tlearn: 0.0800344\ttotal: 6.77s\tremaining: 5.56s\n",
      "549:\tlearn: 0.0799853\ttotal: 6.79s\tremaining: 5.55s\n",
      "550:\tlearn: 0.0799620\ttotal: 6.8s\tremaining: 5.54s\n",
      "551:\tlearn: 0.0799300\ttotal: 6.83s\tremaining: 5.54s\n",
      "552:\tlearn: 0.0799045\ttotal: 6.84s\tremaining: 5.53s\n",
      "553:\tlearn: 0.0798548\ttotal: 6.85s\tremaining: 5.51s\n",
      "554:\tlearn: 0.0798100\ttotal: 6.86s\tremaining: 5.5s\n",
      "555:\tlearn: 0.0797886\ttotal: 6.88s\tremaining: 5.49s\n",
      "556:\tlearn: 0.0797685\ttotal: 6.9s\tremaining: 5.49s\n",
      "557:\tlearn: 0.0797439\ttotal: 6.92s\tremaining: 5.48s\n",
      "558:\tlearn: 0.0797191\ttotal: 6.97s\tremaining: 5.5s\n",
      "559:\tlearn: 0.0797152\ttotal: 7.05s\tremaining: 5.54s\n",
      "560:\tlearn: 0.0796906\ttotal: 7.06s\tremaining: 5.53s\n",
      "561:\tlearn: 0.0796687\ttotal: 7.07s\tremaining: 5.51s\n",
      "562:\tlearn: 0.0796497\ttotal: 7.08s\tremaining: 5.49s\n",
      "563:\tlearn: 0.0796359\ttotal: 7.09s\tremaining: 5.48s\n",
      "564:\tlearn: 0.0796113\ttotal: 7.1s\tremaining: 5.47s\n",
      "565:\tlearn: 0.0795979\ttotal: 7.11s\tremaining: 5.45s\n",
      "566:\tlearn: 0.0795701\ttotal: 7.12s\tremaining: 5.43s\n",
      "567:\tlearn: 0.0795427\ttotal: 7.12s\tremaining: 5.42s\n",
      "568:\tlearn: 0.0794877\ttotal: 7.13s\tremaining: 5.4s\n",
      "569:\tlearn: 0.0794728\ttotal: 7.13s\tremaining: 5.38s\n",
      "570:\tlearn: 0.0794609\ttotal: 7.14s\tremaining: 5.37s\n",
      "571:\tlearn: 0.0794204\ttotal: 7.15s\tremaining: 5.35s\n",
      "572:\tlearn: 0.0793795\ttotal: 7.16s\tremaining: 5.33s\n",
      "573:\tlearn: 0.0793541\ttotal: 7.16s\tremaining: 5.32s\n",
      "574:\tlearn: 0.0793418\ttotal: 7.17s\tremaining: 5.3s\n",
      "575:\tlearn: 0.0793243\ttotal: 7.17s\tremaining: 5.28s\n",
      "576:\tlearn: 0.0792863\ttotal: 7.18s\tremaining: 5.26s\n",
      "577:\tlearn: 0.0792728\ttotal: 7.19s\tremaining: 5.25s\n",
      "578:\tlearn: 0.0792589\ttotal: 7.19s\tremaining: 5.23s\n",
      "579:\tlearn: 0.0792171\ttotal: 7.2s\tremaining: 5.21s\n",
      "580:\tlearn: 0.0791834\ttotal: 7.21s\tremaining: 5.2s\n",
      "581:\tlearn: 0.0791603\ttotal: 7.21s\tremaining: 5.18s\n",
      "582:\tlearn: 0.0791275\ttotal: 7.22s\tremaining: 5.16s\n",
      "583:\tlearn: 0.0790970\ttotal: 7.22s\tremaining: 5.15s\n",
      "584:\tlearn: 0.0790719\ttotal: 7.23s\tremaining: 5.13s\n",
      "585:\tlearn: 0.0790426\ttotal: 7.24s\tremaining: 5.11s\n",
      "586:\tlearn: 0.0789974\ttotal: 7.24s\tremaining: 5.09s\n",
      "587:\tlearn: 0.0789760\ttotal: 7.25s\tremaining: 5.08s\n",
      "588:\tlearn: 0.0789121\ttotal: 7.27s\tremaining: 5.07s\n",
      "589:\tlearn: 0.0788806\ttotal: 7.28s\tremaining: 5.06s\n",
      "590:\tlearn: 0.0788544\ttotal: 7.29s\tremaining: 5.05s\n",
      "591:\tlearn: 0.0788065\ttotal: 7.3s\tremaining: 5.03s\n",
      "592:\tlearn: 0.0787829\ttotal: 7.33s\tremaining: 5.03s\n",
      "593:\tlearn: 0.0787580\ttotal: 7.34s\tremaining: 5.02s\n",
      "594:\tlearn: 0.0787454\ttotal: 7.35s\tremaining: 5s\n",
      "595:\tlearn: 0.0787277\ttotal: 7.37s\tremaining: 4.99s\n",
      "596:\tlearn: 0.0786995\ttotal: 7.38s\tremaining: 4.98s\n",
      "597:\tlearn: 0.0786746\ttotal: 7.39s\tremaining: 4.97s\n",
      "598:\tlearn: 0.0786603\ttotal: 7.41s\tremaining: 4.96s\n",
      "599:\tlearn: 0.0786409\ttotal: 7.42s\tremaining: 4.94s\n",
      "600:\tlearn: 0.0786139\ttotal: 7.42s\tremaining: 4.93s\n",
      "601:\tlearn: 0.0785900\ttotal: 7.45s\tremaining: 4.92s\n",
      "602:\tlearn: 0.0785455\ttotal: 7.46s\tremaining: 4.91s\n",
      "603:\tlearn: 0.0785155\ttotal: 7.47s\tremaining: 4.9s\n",
      "604:\tlearn: 0.0785006\ttotal: 7.48s\tremaining: 4.88s\n",
      "605:\tlearn: 0.0784870\ttotal: 7.5s\tremaining: 4.88s\n",
      "606:\tlearn: 0.0784738\ttotal: 7.51s\tremaining: 4.86s\n",
      "607:\tlearn: 0.0784330\ttotal: 7.53s\tremaining: 4.85s\n",
      "608:\tlearn: 0.0784148\ttotal: 7.54s\tremaining: 4.84s\n",
      "609:\tlearn: 0.0783928\ttotal: 7.55s\tremaining: 4.83s\n",
      "610:\tlearn: 0.0783665\ttotal: 7.56s\tremaining: 4.81s\n",
      "611:\tlearn: 0.0783254\ttotal: 7.57s\tremaining: 4.8s\n",
      "612:\tlearn: 0.0782812\ttotal: 7.58s\tremaining: 4.79s\n",
      "613:\tlearn: 0.0782614\ttotal: 7.6s\tremaining: 4.78s\n",
      "614:\tlearn: 0.0782272\ttotal: 7.61s\tremaining: 4.76s\n",
      "615:\tlearn: 0.0781968\ttotal: 7.62s\tremaining: 4.75s\n",
      "616:\tlearn: 0.0781732\ttotal: 7.63s\tremaining: 4.74s\n",
      "617:\tlearn: 0.0781469\ttotal: 7.64s\tremaining: 4.72s\n",
      "618:\tlearn: 0.0781228\ttotal: 7.65s\tremaining: 4.71s\n",
      "619:\tlearn: 0.0780675\ttotal: 7.67s\tremaining: 4.7s\n",
      "620:\tlearn: 0.0780482\ttotal: 7.68s\tremaining: 4.69s\n",
      "621:\tlearn: 0.0780278\ttotal: 7.69s\tremaining: 4.67s\n",
      "622:\tlearn: 0.0780064\ttotal: 7.71s\tremaining: 4.67s\n",
      "623:\tlearn: 0.0779807\ttotal: 7.72s\tremaining: 4.65s\n",
      "624:\tlearn: 0.0779556\ttotal: 7.74s\tremaining: 4.64s\n",
      "625:\tlearn: 0.0779394\ttotal: 7.75s\tremaining: 4.63s\n",
      "626:\tlearn: 0.0779219\ttotal: 7.77s\tremaining: 4.62s\n",
      "627:\tlearn: 0.0778995\ttotal: 7.79s\tremaining: 4.61s\n",
      "628:\tlearn: 0.0778697\ttotal: 7.79s\tremaining: 4.6s\n",
      "629:\tlearn: 0.0778469\ttotal: 7.8s\tremaining: 4.58s\n",
      "630:\tlearn: 0.0778251\ttotal: 7.81s\tremaining: 4.57s\n",
      "631:\tlearn: 0.0778123\ttotal: 7.82s\tremaining: 4.55s\n",
      "632:\tlearn: 0.0777862\ttotal: 7.83s\tremaining: 4.54s\n",
      "633:\tlearn: 0.0777746\ttotal: 7.84s\tremaining: 4.53s\n",
      "634:\tlearn: 0.0777663\ttotal: 7.85s\tremaining: 4.51s\n",
      "635:\tlearn: 0.0777571\ttotal: 7.86s\tremaining: 4.5s\n",
      "636:\tlearn: 0.0777474\ttotal: 7.87s\tremaining: 4.48s\n",
      "637:\tlearn: 0.0777252\ttotal: 7.87s\tremaining: 4.47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638:\tlearn: 0.0777096\ttotal: 7.88s\tremaining: 4.45s\n",
      "639:\tlearn: 0.0776699\ttotal: 7.89s\tremaining: 4.44s\n",
      "640:\tlearn: 0.0776200\ttotal: 7.89s\tremaining: 4.42s\n",
      "641:\tlearn: 0.0776051\ttotal: 7.9s\tremaining: 4.41s\n",
      "642:\tlearn: 0.0775866\ttotal: 7.91s\tremaining: 4.39s\n",
      "643:\tlearn: 0.0775585\ttotal: 7.91s\tremaining: 4.37s\n",
      "644:\tlearn: 0.0775452\ttotal: 7.92s\tremaining: 4.36s\n",
      "645:\tlearn: 0.0775320\ttotal: 7.92s\tremaining: 4.34s\n",
      "646:\tlearn: 0.0775059\ttotal: 7.93s\tremaining: 4.33s\n",
      "647:\tlearn: 0.0774929\ttotal: 7.94s\tremaining: 4.32s\n",
      "648:\tlearn: 0.0774865\ttotal: 7.96s\tremaining: 4.31s\n",
      "649:\tlearn: 0.0774516\ttotal: 7.98s\tremaining: 4.3s\n",
      "650:\tlearn: 0.0774122\ttotal: 8s\tremaining: 4.29s\n",
      "651:\tlearn: 0.0773967\ttotal: 8.01s\tremaining: 4.27s\n",
      "652:\tlearn: 0.0773711\ttotal: 8.04s\tremaining: 4.27s\n",
      "653:\tlearn: 0.0773426\ttotal: 8.06s\tremaining: 4.27s\n",
      "654:\tlearn: 0.0773192\ttotal: 8.07s\tremaining: 4.25s\n",
      "655:\tlearn: 0.0773029\ttotal: 8.09s\tremaining: 4.24s\n",
      "656:\tlearn: 0.0772901\ttotal: 8.09s\tremaining: 4.22s\n",
      "657:\tlearn: 0.0772819\ttotal: 8.1s\tremaining: 4.21s\n",
      "658:\tlearn: 0.0772725\ttotal: 8.12s\tremaining: 4.2s\n",
      "659:\tlearn: 0.0772432\ttotal: 8.12s\tremaining: 4.18s\n",
      "660:\tlearn: 0.0772263\ttotal: 8.13s\tremaining: 4.17s\n",
      "661:\tlearn: 0.0772132\ttotal: 8.15s\tremaining: 4.16s\n",
      "662:\tlearn: 0.0771799\ttotal: 8.15s\tremaining: 4.14s\n",
      "663:\tlearn: 0.0771457\ttotal: 8.16s\tremaining: 4.13s\n",
      "664:\tlearn: 0.0771234\ttotal: 8.17s\tremaining: 4.12s\n",
      "665:\tlearn: 0.0771060\ttotal: 8.18s\tremaining: 4.1s\n",
      "666:\tlearn: 0.0770902\ttotal: 8.18s\tremaining: 4.09s\n",
      "667:\tlearn: 0.0770560\ttotal: 8.19s\tremaining: 4.07s\n",
      "668:\tlearn: 0.0770449\ttotal: 8.2s\tremaining: 4.05s\n",
      "669:\tlearn: 0.0770282\ttotal: 8.2s\tremaining: 4.04s\n",
      "670:\tlearn: 0.0770093\ttotal: 8.21s\tremaining: 4.03s\n",
      "671:\tlearn: 0.0769989\ttotal: 8.21s\tremaining: 4.01s\n",
      "672:\tlearn: 0.0769564\ttotal: 8.22s\tremaining: 3.99s\n",
      "673:\tlearn: 0.0769431\ttotal: 8.23s\tremaining: 3.98s\n",
      "674:\tlearn: 0.0769326\ttotal: 8.23s\tremaining: 3.96s\n",
      "675:\tlearn: 0.0769088\ttotal: 8.24s\tremaining: 3.95s\n",
      "676:\tlearn: 0.0768881\ttotal: 8.25s\tremaining: 3.93s\n",
      "677:\tlearn: 0.0768738\ttotal: 8.25s\tremaining: 3.92s\n",
      "678:\tlearn: 0.0768464\ttotal: 8.26s\tremaining: 3.9s\n",
      "679:\tlearn: 0.0768317\ttotal: 8.26s\tremaining: 3.89s\n",
      "680:\tlearn: 0.0768169\ttotal: 8.27s\tremaining: 3.87s\n",
      "681:\tlearn: 0.0768044\ttotal: 8.28s\tremaining: 3.86s\n",
      "682:\tlearn: 0.0767878\ttotal: 8.28s\tremaining: 3.84s\n",
      "683:\tlearn: 0.0767786\ttotal: 8.29s\tremaining: 3.83s\n",
      "684:\tlearn: 0.0767702\ttotal: 8.29s\tremaining: 3.81s\n",
      "685:\tlearn: 0.0767463\ttotal: 8.3s\tremaining: 3.8s\n",
      "686:\tlearn: 0.0767269\ttotal: 8.31s\tremaining: 3.79s\n",
      "687:\tlearn: 0.0766968\ttotal: 8.31s\tremaining: 3.77s\n",
      "688:\tlearn: 0.0766837\ttotal: 8.32s\tremaining: 3.76s\n",
      "689:\tlearn: 0.0766587\ttotal: 8.34s\tremaining: 3.74s\n",
      "690:\tlearn: 0.0766180\ttotal: 8.34s\tremaining: 3.73s\n",
      "691:\tlearn: 0.0765969\ttotal: 8.35s\tremaining: 3.72s\n",
      "692:\tlearn: 0.0765806\ttotal: 8.36s\tremaining: 3.7s\n",
      "693:\tlearn: 0.0765598\ttotal: 8.36s\tremaining: 3.69s\n",
      "694:\tlearn: 0.0765395\ttotal: 8.37s\tremaining: 3.67s\n",
      "695:\tlearn: 0.0765144\ttotal: 8.38s\tremaining: 3.66s\n",
      "696:\tlearn: 0.0764811\ttotal: 8.39s\tremaining: 3.65s\n",
      "697:\tlearn: 0.0764635\ttotal: 8.4s\tremaining: 3.63s\n",
      "698:\tlearn: 0.0764518\ttotal: 8.4s\tremaining: 3.62s\n",
      "699:\tlearn: 0.0764376\ttotal: 8.41s\tremaining: 3.6s\n",
      "700:\tlearn: 0.0764179\ttotal: 8.42s\tremaining: 3.59s\n",
      "701:\tlearn: 0.0764048\ttotal: 8.43s\tremaining: 3.58s\n",
      "702:\tlearn: 0.0763854\ttotal: 8.44s\tremaining: 3.56s\n",
      "703:\tlearn: 0.0763715\ttotal: 8.44s\tremaining: 3.55s\n",
      "704:\tlearn: 0.0763514\ttotal: 8.45s\tremaining: 3.54s\n",
      "705:\tlearn: 0.0763217\ttotal: 8.46s\tremaining: 3.52s\n",
      "706:\tlearn: 0.0763023\ttotal: 8.47s\tremaining: 3.51s\n",
      "707:\tlearn: 0.0762999\ttotal: 8.47s\tremaining: 3.49s\n",
      "708:\tlearn: 0.0762836\ttotal: 8.48s\tremaining: 3.48s\n",
      "709:\tlearn: 0.0762699\ttotal: 8.49s\tremaining: 3.47s\n",
      "710:\tlearn: 0.0762595\ttotal: 8.49s\tremaining: 3.45s\n",
      "711:\tlearn: 0.0762516\ttotal: 8.5s\tremaining: 3.44s\n",
      "712:\tlearn: 0.0762107\ttotal: 8.51s\tremaining: 3.42s\n",
      "713:\tlearn: 0.0761960\ttotal: 8.52s\tremaining: 3.41s\n",
      "714:\tlearn: 0.0761791\ttotal: 8.52s\tremaining: 3.4s\n",
      "715:\tlearn: 0.0761672\ttotal: 8.53s\tremaining: 3.38s\n",
      "716:\tlearn: 0.0761466\ttotal: 8.54s\tremaining: 3.37s\n",
      "717:\tlearn: 0.0761375\ttotal: 8.55s\tremaining: 3.36s\n",
      "718:\tlearn: 0.0761218\ttotal: 8.55s\tremaining: 3.34s\n",
      "719:\tlearn: 0.0760951\ttotal: 8.56s\tremaining: 3.33s\n",
      "720:\tlearn: 0.0760796\ttotal: 8.57s\tremaining: 3.31s\n",
      "721:\tlearn: 0.0760658\ttotal: 8.58s\tremaining: 3.3s\n",
      "722:\tlearn: 0.0760520\ttotal: 8.58s\tremaining: 3.29s\n",
      "723:\tlearn: 0.0760395\ttotal: 8.59s\tremaining: 3.27s\n",
      "724:\tlearn: 0.0760197\ttotal: 8.6s\tremaining: 3.26s\n",
      "725:\tlearn: 0.0760092\ttotal: 8.61s\tremaining: 3.25s\n",
      "726:\tlearn: 0.0759931\ttotal: 8.62s\tremaining: 3.23s\n",
      "727:\tlearn: 0.0759688\ttotal: 8.62s\tremaining: 3.22s\n",
      "728:\tlearn: 0.0759539\ttotal: 8.63s\tremaining: 3.21s\n",
      "729:\tlearn: 0.0759354\ttotal: 8.64s\tremaining: 3.19s\n",
      "730:\tlearn: 0.0759198\ttotal: 8.65s\tremaining: 3.18s\n",
      "731:\tlearn: 0.0759042\ttotal: 8.65s\tremaining: 3.17s\n",
      "732:\tlearn: 0.0758927\ttotal: 8.66s\tremaining: 3.15s\n",
      "733:\tlearn: 0.0758721\ttotal: 8.67s\tremaining: 3.14s\n",
      "734:\tlearn: 0.0758595\ttotal: 8.67s\tremaining: 3.13s\n",
      "735:\tlearn: 0.0758430\ttotal: 8.68s\tremaining: 3.11s\n",
      "736:\tlearn: 0.0758250\ttotal: 8.69s\tremaining: 3.1s\n",
      "737:\tlearn: 0.0758133\ttotal: 8.69s\tremaining: 3.09s\n",
      "738:\tlearn: 0.0758022\ttotal: 8.7s\tremaining: 3.07s\n",
      "739:\tlearn: 0.0757638\ttotal: 8.71s\tremaining: 3.06s\n",
      "740:\tlearn: 0.0757280\ttotal: 8.71s\tremaining: 3.04s\n",
      "741:\tlearn: 0.0757106\ttotal: 8.72s\tremaining: 3.03s\n",
      "742:\tlearn: 0.0756816\ttotal: 8.73s\tremaining: 3.02s\n",
      "743:\tlearn: 0.0756677\ttotal: 8.73s\tremaining: 3s\n",
      "744:\tlearn: 0.0756514\ttotal: 8.74s\tremaining: 2.99s\n",
      "745:\tlearn: 0.0756347\ttotal: 8.74s\tremaining: 2.98s\n",
      "746:\tlearn: 0.0756092\ttotal: 8.75s\tremaining: 2.96s\n",
      "747:\tlearn: 0.0755877\ttotal: 8.76s\tremaining: 2.95s\n",
      "748:\tlearn: 0.0755654\ttotal: 8.76s\tremaining: 2.94s\n",
      "749:\tlearn: 0.0755516\ttotal: 8.77s\tremaining: 2.92s\n",
      "750:\tlearn: 0.0755343\ttotal: 8.78s\tremaining: 2.91s\n",
      "751:\tlearn: 0.0755156\ttotal: 8.78s\tremaining: 2.9s\n",
      "752:\tlearn: 0.0754881\ttotal: 8.79s\tremaining: 2.88s\n",
      "753:\tlearn: 0.0754723\ttotal: 8.8s\tremaining: 2.87s\n",
      "754:\tlearn: 0.0754630\ttotal: 8.8s\tremaining: 2.86s\n",
      "755:\tlearn: 0.0754511\ttotal: 8.81s\tremaining: 2.84s\n",
      "756:\tlearn: 0.0754238\ttotal: 8.82s\tremaining: 2.83s\n",
      "757:\tlearn: 0.0754055\ttotal: 8.82s\tremaining: 2.82s\n",
      "758:\tlearn: 0.0753959\ttotal: 8.83s\tremaining: 2.8s\n",
      "759:\tlearn: 0.0753867\ttotal: 8.84s\tremaining: 2.79s\n",
      "760:\tlearn: 0.0753752\ttotal: 8.85s\tremaining: 2.78s\n",
      "761:\tlearn: 0.0753550\ttotal: 8.85s\tremaining: 2.76s\n",
      "762:\tlearn: 0.0753244\ttotal: 8.86s\tremaining: 2.75s\n",
      "763:\tlearn: 0.0753087\ttotal: 8.86s\tremaining: 2.74s\n",
      "764:\tlearn: 0.0752814\ttotal: 8.87s\tremaining: 2.72s\n",
      "765:\tlearn: 0.0752603\ttotal: 8.88s\tremaining: 2.71s\n",
      "766:\tlearn: 0.0752386\ttotal: 8.88s\tremaining: 2.7s\n",
      "767:\tlearn: 0.0752259\ttotal: 8.89s\tremaining: 2.68s\n",
      "768:\tlearn: 0.0752010\ttotal: 8.89s\tremaining: 2.67s\n",
      "769:\tlearn: 0.0751817\ttotal: 8.9s\tremaining: 2.66s\n",
      "770:\tlearn: 0.0751518\ttotal: 8.9s\tremaining: 2.64s\n",
      "771:\tlearn: 0.0751412\ttotal: 8.91s\tremaining: 2.63s\n",
      "772:\tlearn: 0.0751243\ttotal: 8.92s\tremaining: 2.62s\n",
      "773:\tlearn: 0.0751079\ttotal: 8.92s\tremaining: 2.6s\n",
      "774:\tlearn: 0.0750939\ttotal: 8.93s\tremaining: 2.59s\n",
      "775:\tlearn: 0.0750738\ttotal: 8.93s\tremaining: 2.58s\n",
      "776:\tlearn: 0.0750555\ttotal: 8.95s\tremaining: 2.57s\n",
      "777:\tlearn: 0.0750529\ttotal: 8.96s\tremaining: 2.56s\n",
      "778:\tlearn: 0.0750276\ttotal: 8.99s\tremaining: 2.55s\n",
      "779:\tlearn: 0.0750157\ttotal: 9.01s\tremaining: 2.54s\n",
      "780:\tlearn: 0.0750070\ttotal: 9.07s\tremaining: 2.54s\n",
      "781:\tlearn: 0.0749876\ttotal: 9.09s\tremaining: 2.53s\n",
      "782:\tlearn: 0.0749839\ttotal: 9.12s\tremaining: 2.53s\n",
      "783:\tlearn: 0.0749612\ttotal: 9.15s\tremaining: 2.52s\n",
      "784:\tlearn: 0.0749518\ttotal: 9.16s\tremaining: 2.51s\n",
      "785:\tlearn: 0.0749224\ttotal: 9.18s\tremaining: 2.5s\n",
      "786:\tlearn: 0.0749008\ttotal: 9.18s\tremaining: 2.48s\n",
      "787:\tlearn: 0.0748933\ttotal: 9.19s\tremaining: 2.47s\n",
      "788:\tlearn: 0.0748690\ttotal: 9.2s\tremaining: 2.46s\n",
      "789:\tlearn: 0.0748534\ttotal: 9.21s\tremaining: 2.45s\n",
      "790:\tlearn: 0.0748190\ttotal: 9.22s\tremaining: 2.44s\n",
      "791:\tlearn: 0.0748088\ttotal: 9.22s\tremaining: 2.42s\n",
      "792:\tlearn: 0.0747845\ttotal: 9.23s\tremaining: 2.41s\n",
      "793:\tlearn: 0.0747635\ttotal: 9.24s\tremaining: 2.4s\n",
      "794:\tlearn: 0.0747498\ttotal: 9.25s\tremaining: 2.38s\n",
      "795:\tlearn: 0.0747187\ttotal: 9.27s\tremaining: 2.37s\n",
      "796:\tlearn: 0.0747099\ttotal: 9.29s\tremaining: 2.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797:\tlearn: 0.0747011\ttotal: 9.31s\tremaining: 2.36s\n",
      "798:\tlearn: 0.0746856\ttotal: 9.32s\tremaining: 2.35s\n",
      "799:\tlearn: 0.0746578\ttotal: 9.33s\tremaining: 2.33s\n",
      "800:\tlearn: 0.0746125\ttotal: 9.34s\tremaining: 2.32s\n",
      "801:\tlearn: 0.0745864\ttotal: 9.35s\tremaining: 2.31s\n",
      "802:\tlearn: 0.0745643\ttotal: 9.36s\tremaining: 2.29s\n",
      "803:\tlearn: 0.0745479\ttotal: 9.36s\tremaining: 2.28s\n",
      "804:\tlearn: 0.0745386\ttotal: 9.37s\tremaining: 2.27s\n",
      "805:\tlearn: 0.0745288\ttotal: 9.38s\tremaining: 2.26s\n",
      "806:\tlearn: 0.0745143\ttotal: 9.39s\tremaining: 2.25s\n",
      "807:\tlearn: 0.0745050\ttotal: 9.4s\tremaining: 2.23s\n",
      "808:\tlearn: 0.0744827\ttotal: 9.4s\tremaining: 2.22s\n",
      "809:\tlearn: 0.0744600\ttotal: 9.41s\tremaining: 2.21s\n",
      "810:\tlearn: 0.0744410\ttotal: 9.42s\tremaining: 2.2s\n",
      "811:\tlearn: 0.0744283\ttotal: 9.44s\tremaining: 2.19s\n",
      "812:\tlearn: 0.0743981\ttotal: 9.45s\tremaining: 2.17s\n",
      "813:\tlearn: 0.0743852\ttotal: 9.46s\tremaining: 2.16s\n",
      "814:\tlearn: 0.0743754\ttotal: 9.47s\tremaining: 2.15s\n",
      "815:\tlearn: 0.0743592\ttotal: 9.48s\tremaining: 2.14s\n",
      "816:\tlearn: 0.0743276\ttotal: 9.5s\tremaining: 2.13s\n",
      "817:\tlearn: 0.0743185\ttotal: 9.5s\tremaining: 2.11s\n",
      "818:\tlearn: 0.0743101\ttotal: 9.52s\tremaining: 2.1s\n",
      "819:\tlearn: 0.0742954\ttotal: 9.53s\tremaining: 2.09s\n",
      "820:\tlearn: 0.0742836\ttotal: 9.54s\tremaining: 2.08s\n",
      "821:\tlearn: 0.0742575\ttotal: 9.54s\tremaining: 2.07s\n",
      "822:\tlearn: 0.0742396\ttotal: 9.55s\tremaining: 2.05s\n",
      "823:\tlearn: 0.0742288\ttotal: 9.56s\tremaining: 2.04s\n",
      "824:\tlearn: 0.0742178\ttotal: 9.57s\tremaining: 2.03s\n",
      "825:\tlearn: 0.0742039\ttotal: 9.57s\tremaining: 2.02s\n",
      "826:\tlearn: 0.0741962\ttotal: 9.58s\tremaining: 2s\n",
      "827:\tlearn: 0.0741569\ttotal: 9.59s\tremaining: 1.99s\n",
      "828:\tlearn: 0.0741324\ttotal: 9.6s\tremaining: 1.98s\n",
      "829:\tlearn: 0.0741055\ttotal: 9.6s\tremaining: 1.97s\n",
      "830:\tlearn: 0.0740924\ttotal: 9.61s\tremaining: 1.95s\n",
      "831:\tlearn: 0.0740841\ttotal: 9.62s\tremaining: 1.94s\n",
      "832:\tlearn: 0.0740638\ttotal: 9.63s\tremaining: 1.93s\n",
      "833:\tlearn: 0.0740395\ttotal: 9.63s\tremaining: 1.92s\n",
      "834:\tlearn: 0.0740308\ttotal: 9.64s\tremaining: 1.91s\n",
      "835:\tlearn: 0.0740050\ttotal: 9.65s\tremaining: 1.89s\n",
      "836:\tlearn: 0.0739872\ttotal: 9.66s\tremaining: 1.88s\n",
      "837:\tlearn: 0.0739724\ttotal: 9.66s\tremaining: 1.87s\n",
      "838:\tlearn: 0.0739668\ttotal: 9.67s\tremaining: 1.85s\n",
      "839:\tlearn: 0.0739432\ttotal: 9.67s\tremaining: 1.84s\n",
      "840:\tlearn: 0.0739221\ttotal: 9.68s\tremaining: 1.83s\n",
      "841:\tlearn: 0.0739102\ttotal: 9.69s\tremaining: 1.82s\n",
      "842:\tlearn: 0.0738938\ttotal: 9.69s\tremaining: 1.8s\n",
      "843:\tlearn: 0.0738802\ttotal: 9.7s\tremaining: 1.79s\n",
      "844:\tlearn: 0.0738772\ttotal: 9.71s\tremaining: 1.78s\n",
      "845:\tlearn: 0.0738512\ttotal: 9.72s\tremaining: 1.77s\n",
      "846:\tlearn: 0.0738432\ttotal: 9.73s\tremaining: 1.76s\n",
      "847:\tlearn: 0.0738354\ttotal: 9.74s\tremaining: 1.75s\n",
      "848:\tlearn: 0.0738290\ttotal: 9.75s\tremaining: 1.73s\n",
      "849:\tlearn: 0.0738115\ttotal: 9.76s\tremaining: 1.72s\n",
      "850:\tlearn: 0.0737895\ttotal: 9.77s\tremaining: 1.71s\n",
      "851:\tlearn: 0.0737754\ttotal: 9.78s\tremaining: 1.7s\n",
      "852:\tlearn: 0.0737615\ttotal: 9.79s\tremaining: 1.69s\n",
      "853:\tlearn: 0.0737443\ttotal: 9.8s\tremaining: 1.68s\n",
      "854:\tlearn: 0.0737344\ttotal: 9.81s\tremaining: 1.66s\n",
      "855:\tlearn: 0.0737162\ttotal: 9.82s\tremaining: 1.65s\n",
      "856:\tlearn: 0.0736935\ttotal: 9.83s\tremaining: 1.64s\n",
      "857:\tlearn: 0.0736839\ttotal: 9.84s\tremaining: 1.63s\n",
      "858:\tlearn: 0.0736678\ttotal: 9.85s\tremaining: 1.62s\n",
      "859:\tlearn: 0.0736436\ttotal: 9.86s\tremaining: 1.6s\n",
      "860:\tlearn: 0.0736280\ttotal: 9.87s\tremaining: 1.59s\n",
      "861:\tlearn: 0.0736074\ttotal: 9.88s\tremaining: 1.58s\n",
      "862:\tlearn: 0.0735884\ttotal: 9.89s\tremaining: 1.57s\n",
      "863:\tlearn: 0.0735601\ttotal: 9.9s\tremaining: 1.56s\n",
      "864:\tlearn: 0.0735383\ttotal: 9.91s\tremaining: 1.55s\n",
      "865:\tlearn: 0.0735268\ttotal: 9.92s\tremaining: 1.53s\n",
      "866:\tlearn: 0.0735193\ttotal: 9.93s\tremaining: 1.52s\n",
      "867:\tlearn: 0.0734958\ttotal: 9.94s\tremaining: 1.51s\n",
      "868:\tlearn: 0.0734862\ttotal: 9.95s\tremaining: 1.5s\n",
      "869:\tlearn: 0.0734684\ttotal: 9.96s\tremaining: 1.49s\n",
      "870:\tlearn: 0.0734581\ttotal: 9.98s\tremaining: 1.48s\n",
      "871:\tlearn: 0.0734490\ttotal: 10s\tremaining: 1.47s\n",
      "872:\tlearn: 0.0734314\ttotal: 10s\tremaining: 1.46s\n",
      "873:\tlearn: 0.0734204\ttotal: 10s\tremaining: 1.45s\n",
      "874:\tlearn: 0.0734178\ttotal: 10.1s\tremaining: 1.44s\n",
      "875:\tlearn: 0.0734035\ttotal: 10.1s\tremaining: 1.44s\n",
      "876:\tlearn: 0.0733879\ttotal: 10.2s\tremaining: 1.42s\n",
      "877:\tlearn: 0.0733638\ttotal: 10.2s\tremaining: 1.41s\n",
      "878:\tlearn: 0.0733549\ttotal: 10.2s\tremaining: 1.4s\n",
      "879:\tlearn: 0.0733323\ttotal: 10.2s\tremaining: 1.39s\n",
      "880:\tlearn: 0.0733231\ttotal: 10.2s\tremaining: 1.38s\n",
      "881:\tlearn: 0.0733122\ttotal: 10.2s\tremaining: 1.37s\n",
      "882:\tlearn: 0.0732956\ttotal: 10.2s\tremaining: 1.35s\n",
      "883:\tlearn: 0.0732718\ttotal: 10.2s\tremaining: 1.34s\n",
      "884:\tlearn: 0.0732528\ttotal: 10.2s\tremaining: 1.33s\n",
      "885:\tlearn: 0.0732438\ttotal: 10.3s\tremaining: 1.32s\n",
      "886:\tlearn: 0.0732303\ttotal: 10.3s\tremaining: 1.31s\n",
      "887:\tlearn: 0.0732029\ttotal: 10.3s\tremaining: 1.29s\n",
      "888:\tlearn: 0.0731722\ttotal: 10.3s\tremaining: 1.28s\n",
      "889:\tlearn: 0.0731504\ttotal: 10.3s\tremaining: 1.27s\n",
      "890:\tlearn: 0.0731476\ttotal: 10.3s\tremaining: 1.26s\n",
      "891:\tlearn: 0.0731291\ttotal: 10.3s\tremaining: 1.25s\n",
      "892:\tlearn: 0.0731204\ttotal: 10.3s\tremaining: 1.24s\n",
      "893:\tlearn: 0.0731038\ttotal: 10.3s\tremaining: 1.22s\n",
      "894:\tlearn: 0.0730914\ttotal: 10.3s\tremaining: 1.21s\n",
      "895:\tlearn: 0.0730790\ttotal: 10.4s\tremaining: 1.2s\n",
      "896:\tlearn: 0.0730503\ttotal: 10.4s\tremaining: 1.19s\n",
      "897:\tlearn: 0.0730432\ttotal: 10.4s\tremaining: 1.18s\n",
      "898:\tlearn: 0.0730154\ttotal: 10.4s\tremaining: 1.17s\n",
      "899:\tlearn: 0.0729969\ttotal: 10.4s\tremaining: 1.16s\n",
      "900:\tlearn: 0.0729672\ttotal: 10.4s\tremaining: 1.14s\n",
      "901:\tlearn: 0.0729586\ttotal: 10.4s\tremaining: 1.13s\n",
      "902:\tlearn: 0.0729493\ttotal: 10.4s\tremaining: 1.12s\n",
      "903:\tlearn: 0.0729332\ttotal: 10.4s\tremaining: 1.11s\n",
      "904:\tlearn: 0.0729173\ttotal: 10.4s\tremaining: 1.09s\n",
      "905:\tlearn: 0.0729010\ttotal: 10.4s\tremaining: 1.08s\n",
      "906:\tlearn: 0.0728826\ttotal: 10.4s\tremaining: 1.07s\n",
      "907:\tlearn: 0.0728696\ttotal: 10.5s\tremaining: 1.06s\n",
      "908:\tlearn: 0.0728531\ttotal: 10.5s\tremaining: 1.05s\n",
      "909:\tlearn: 0.0728275\ttotal: 10.5s\tremaining: 1.03s\n",
      "910:\tlearn: 0.0728151\ttotal: 10.5s\tremaining: 1.02s\n",
      "911:\tlearn: 0.0727973\ttotal: 10.5s\tremaining: 1.01s\n",
      "912:\tlearn: 0.0727707\ttotal: 10.5s\tremaining: 999ms\n",
      "913:\tlearn: 0.0727609\ttotal: 10.5s\tremaining: 987ms\n",
      "914:\tlearn: 0.0727320\ttotal: 10.5s\tremaining: 975ms\n",
      "915:\tlearn: 0.0727224\ttotal: 10.5s\tremaining: 963ms\n",
      "916:\tlearn: 0.0727092\ttotal: 10.5s\tremaining: 951ms\n",
      "917:\tlearn: 0.0726906\ttotal: 10.5s\tremaining: 940ms\n",
      "918:\tlearn: 0.0726855\ttotal: 10.5s\tremaining: 928ms\n",
      "919:\tlearn: 0.0726762\ttotal: 10.5s\tremaining: 916ms\n",
      "920:\tlearn: 0.0726545\ttotal: 10.5s\tremaining: 904ms\n",
      "921:\tlearn: 0.0726393\ttotal: 10.5s\tremaining: 892ms\n",
      "922:\tlearn: 0.0726294\ttotal: 10.6s\tremaining: 880ms\n",
      "923:\tlearn: 0.0726178\ttotal: 10.6s\tremaining: 868ms\n",
      "924:\tlearn: 0.0726075\ttotal: 10.6s\tremaining: 857ms\n",
      "925:\tlearn: 0.0725914\ttotal: 10.6s\tremaining: 845ms\n",
      "926:\tlearn: 0.0725583\ttotal: 10.6s\tremaining: 833ms\n",
      "927:\tlearn: 0.0725487\ttotal: 10.6s\tremaining: 821ms\n",
      "928:\tlearn: 0.0725352\ttotal: 10.6s\tremaining: 809ms\n",
      "929:\tlearn: 0.0725168\ttotal: 10.6s\tremaining: 798ms\n",
      "930:\tlearn: 0.0725093\ttotal: 10.6s\tremaining: 786ms\n",
      "931:\tlearn: 0.0724831\ttotal: 10.6s\tremaining: 774ms\n",
      "932:\tlearn: 0.0724747\ttotal: 10.6s\tremaining: 762ms\n",
      "933:\tlearn: 0.0724692\ttotal: 10.6s\tremaining: 751ms\n",
      "934:\tlearn: 0.0724590\ttotal: 10.6s\tremaining: 739ms\n",
      "935:\tlearn: 0.0724448\ttotal: 10.6s\tremaining: 727ms\n",
      "936:\tlearn: 0.0724377\ttotal: 10.6s\tremaining: 715ms\n",
      "937:\tlearn: 0.0724268\ttotal: 10.6s\tremaining: 704ms\n",
      "938:\tlearn: 0.0724161\ttotal: 10.7s\tremaining: 692ms\n",
      "939:\tlearn: 0.0724049\ttotal: 10.7s\tremaining: 680ms\n",
      "940:\tlearn: 0.0723901\ttotal: 10.7s\tremaining: 669ms\n",
      "941:\tlearn: 0.0723816\ttotal: 10.7s\tremaining: 657ms\n",
      "942:\tlearn: 0.0723682\ttotal: 10.7s\tremaining: 645ms\n",
      "943:\tlearn: 0.0723517\ttotal: 10.7s\tremaining: 634ms\n",
      "944:\tlearn: 0.0723441\ttotal: 10.7s\tremaining: 622ms\n",
      "945:\tlearn: 0.0723072\ttotal: 10.7s\tremaining: 611ms\n",
      "946:\tlearn: 0.0723046\ttotal: 10.7s\tremaining: 599ms\n",
      "947:\tlearn: 0.0722796\ttotal: 10.7s\tremaining: 587ms\n",
      "948:\tlearn: 0.0722617\ttotal: 10.7s\tremaining: 576ms\n",
      "949:\tlearn: 0.0722493\ttotal: 10.7s\tremaining: 564ms\n",
      "950:\tlearn: 0.0722287\ttotal: 10.7s\tremaining: 553ms\n",
      "951:\tlearn: 0.0722165\ttotal: 10.7s\tremaining: 541ms\n",
      "952:\tlearn: 0.0721959\ttotal: 10.7s\tremaining: 530ms\n",
      "953:\tlearn: 0.0721830\ttotal: 10.7s\tremaining: 518ms\n",
      "954:\tlearn: 0.0721702\ttotal: 10.8s\tremaining: 507ms\n",
      "955:\tlearn: 0.0721555\ttotal: 10.8s\tremaining: 495ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956:\tlearn: 0.0721371\ttotal: 10.8s\tremaining: 484ms\n",
      "957:\tlearn: 0.0721162\ttotal: 10.8s\tremaining: 472ms\n",
      "958:\tlearn: 0.0720910\ttotal: 10.8s\tremaining: 461ms\n",
      "959:\tlearn: 0.0720777\ttotal: 10.8s\tremaining: 450ms\n",
      "960:\tlearn: 0.0720675\ttotal: 10.8s\tremaining: 438ms\n",
      "961:\tlearn: 0.0720430\ttotal: 10.8s\tremaining: 427ms\n",
      "962:\tlearn: 0.0720237\ttotal: 10.8s\tremaining: 415ms\n",
      "963:\tlearn: 0.0720122\ttotal: 10.8s\tremaining: 404ms\n",
      "964:\tlearn: 0.0719990\ttotal: 10.8s\tremaining: 392ms\n",
      "965:\tlearn: 0.0719877\ttotal: 10.8s\tremaining: 381ms\n",
      "966:\tlearn: 0.0719730\ttotal: 10.8s\tremaining: 370ms\n",
      "967:\tlearn: 0.0719579\ttotal: 10.8s\tremaining: 358ms\n",
      "968:\tlearn: 0.0719401\ttotal: 10.8s\tremaining: 347ms\n",
      "969:\tlearn: 0.0719309\ttotal: 10.9s\tremaining: 336ms\n",
      "970:\tlearn: 0.0719133\ttotal: 10.9s\tremaining: 324ms\n",
      "971:\tlearn: 0.0718980\ttotal: 10.9s\tremaining: 313ms\n",
      "972:\tlearn: 0.0718867\ttotal: 10.9s\tremaining: 302ms\n",
      "973:\tlearn: 0.0718669\ttotal: 10.9s\tremaining: 290ms\n",
      "974:\tlearn: 0.0718536\ttotal: 10.9s\tremaining: 279ms\n",
      "975:\tlearn: 0.0718463\ttotal: 10.9s\tremaining: 268ms\n",
      "976:\tlearn: 0.0718319\ttotal: 10.9s\tremaining: 257ms\n",
      "977:\tlearn: 0.0718212\ttotal: 10.9s\tremaining: 245ms\n",
      "978:\tlearn: 0.0717963\ttotal: 10.9s\tremaining: 234ms\n",
      "979:\tlearn: 0.0717877\ttotal: 10.9s\tremaining: 223ms\n",
      "980:\tlearn: 0.0717773\ttotal: 10.9s\tremaining: 212ms\n",
      "981:\tlearn: 0.0717681\ttotal: 10.9s\tremaining: 200ms\n",
      "982:\tlearn: 0.0717484\ttotal: 10.9s\tremaining: 189ms\n",
      "983:\tlearn: 0.0717327\ttotal: 10.9s\tremaining: 178ms\n",
      "984:\tlearn: 0.0717254\ttotal: 10.9s\tremaining: 167ms\n",
      "985:\tlearn: 0.0717078\ttotal: 11s\tremaining: 156ms\n",
      "986:\tlearn: 0.0716913\ttotal: 11s\tremaining: 144ms\n",
      "987:\tlearn: 0.0716748\ttotal: 11s\tremaining: 133ms\n",
      "988:\tlearn: 0.0716605\ttotal: 11s\tremaining: 122ms\n",
      "989:\tlearn: 0.0716469\ttotal: 11s\tremaining: 111ms\n",
      "990:\tlearn: 0.0716133\ttotal: 11s\tremaining: 99.8ms\n",
      "991:\tlearn: 0.0715970\ttotal: 11s\tremaining: 88.7ms\n",
      "992:\tlearn: 0.0715885\ttotal: 11s\tremaining: 77.6ms\n",
      "993:\tlearn: 0.0715712\ttotal: 11s\tremaining: 66.5ms\n",
      "994:\tlearn: 0.0715645\ttotal: 11s\tremaining: 55.4ms\n",
      "995:\tlearn: 0.0715516\ttotal: 11s\tremaining: 44.3ms\n",
      "996:\tlearn: 0.0715379\ttotal: 11s\tremaining: 33.2ms\n",
      "997:\tlearn: 0.0715095\ttotal: 11.1s\tremaining: 22.2ms\n",
      "998:\tlearn: 0.0714965\ttotal: 11.1s\tremaining: 11.1ms\n",
      "999:\tlearn: 0.0714915\ttotal: 11.1s\tremaining: 0us\n",
      "MAE:22.720843\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoost\n",
    "# from sklearn.metrics import log_loss\n",
    "\n",
    "class CatRegressorCV(RidgeCV):\n",
    "    model_cls = CatBoost\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'MAE',\n",
    "#             'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 6, 10),                                      \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "# model=CatRegressorCV(n_trials=10)\n",
    "model=CatBoost({'depth': 7, 'learning_rate': 0.074638569770399, 'random_strength': 59, 'bagging_temperature': 0.011038111194790014, 'od_type': 'IncToDec', 'od_wait': 12})\n",
    "# model=CatBoost({'depth':16,'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, })\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(model.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomforest +optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 19:48:02,789] Finished trial#0 resulted in value: 0.8931599296970697. Current best value is 0.8931599296970697 with parameters: {'n_estimators': 135, 'max_features': 1, 'min_samples_split': 12, 'max_depth': 85}.\n",
      "[I 2019-11-20 19:48:23,929] Finished trial#1 resulted in value: 0.9566515640548288. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:48:32,150] Finished trial#2 resulted in value: 0.9458261198997718. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:48:43,064] Finished trial#3 resulted in value: 0.9380937454515014. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:48:45,291] Finished trial#4 resulted in value: 0.874440118510343. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:48:46,885] Finished trial#5 resulted in value: 0.8794096559749092. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:49:01,952] Finished trial#6 resulted in value: 0.9492194941048879. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:49:05,831] Finished trial#7 resulted in value: 0.9516607824364339. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:49:26,812] Finished trial#8 resulted in value: 0.9301315620457153. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:49:31,021] Finished trial#9 resulted in value: 0.9382178838048233. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:49:58,174] Finished trial#10 resulted in value: 0.9534965345209484. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:50:26,999] Finished trial#11 resulted in value: 0.9529789440706278. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:50:43,308] Finished trial#12 resulted in value: 0.950133321196809. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:51:03,310] Finished trial#13 resulted in value: 0.9530008814870399. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:51:17,968] Finished trial#14 resulted in value: 0.9514553470257769. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n",
      "[I 2019-11-20 19:51:38,799] Finished trial#15 resulted in value: 0.9471480336222481. Current best value is 0.9566515640548288 with parameters: {'n_estimators': 93, 'max_features': 12, 'min_samples_split': 9, 'max_depth': 54}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-490-64d28725d7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#{'depth': 7, 'learning_rate': 0.074638569770399, 'random_strength': 59, 'bagging_temperature': 0.011038111194790014, 'od_type': 'IncToDec', 'od_wait': 12})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# model=CatBoost({'depth':16,'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, })\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-490-64d28725d7e0>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     11\u001b[0m         }\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRFRegressorCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class RFRegressorCV(RidgeCV):\n",
    "    model_cls =  RFR\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'n_estimators'      : trial.suggest_int('n_estimators', 10, 200),\n",
    "            'max_features'      : trial.suggest_int('max_features', 1,self.X.shape[1]),\n",
    "            'random_state'      : 123,\n",
    "            'n_jobs'            : 1,\n",
    "            'min_samples_split' :trial.suggest_int('min_samples_split', 3, 100) ,\n",
    "            'max_depth'         : trial.suggest_int('max_depth' , 3, 100)\n",
    "        }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "model=RFRegressorCV(n_trials=20)\n",
    "#{'depth': 7, 'learning_rate': 0.074638569770399, 'random_strength': 59, 'bagging_temperature': 0.011038111194790014, 'od_type': 'IncToDec', 'od_wait': 12})\n",
    "# model=CatBoost({'depth':16,'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, })\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(model.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engeneering for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "categorical_variable_names = [\"position\",\"sex\",\"education\"]\n",
    "# カテゴリ変数に一括変換\n",
    "# x_dummy = pd.get_dummies(df[categorical_variable_names], drop_first=True,columns=categorical_variable_names)\n",
    "# X_nn=df.drop(categorical_variable_names, axis=1)\n",
    "X_nn=df\n",
    "sscaler =StandardScaler()\n",
    "sscaler.fit(X_nn)  \n",
    "\n",
    "x_datas_std =sscaler.transform(X_nn)\n",
    "x_datas_std = pd.DataFrame(x_datas_std, columns=X_nn.columns)\n",
    "X_nn= pd.concat([x_datas_std,], axis=1)\n",
    "\n",
    "\n",
    "train_X_nn=X_nn.drop(\"id\",axis=1)\n",
    "test_nn=X_nn[X_nn.salary.isnull()].drop(\"id\",axis=1)\n",
    "test_nn= test_nn.drop([\"salary\",],axis=1)\n",
    "train_X_nn= train_X_nn.dropna().drop([\"salary\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>1.027084</td>\n",
       "      <td>1.544468</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>1.097992</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>0.818742</td>\n",
       "      <td>-0.243349</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>-0.192567</td>\n",
       "      <td>0.292875</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>0.069682</td>\n",
       "      <td>1.584501</td>\n",
       "      <td>-0.539406</td>\n",
       "      <td>0.283116</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.070094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>0.145629</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>1.023458</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.691715</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>-0.753363</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.755483</td>\n",
       "      <td>-0.737849</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.678180</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>-0.950787</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.567845</td>\n",
       "      <td>-0.516979</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.491214</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.760128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.470845</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.447636</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>1.463169</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>1.249976</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.388501</td>\n",
       "      <td>0.332472</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.876595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.130761</td>\n",
       "      <td>0.808237</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-1.145594</td>\n",
       "      <td>-0.840184</td>\n",
       "      <td>-0.086690</td>\n",
       "      <td>0.036335</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.902451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>-0.286387</td>\n",
       "      <td>-1.621326</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>1.707344</td>\n",
       "      <td>-0.584697</td>\n",
       "      <td>-1.143269</td>\n",
       "      <td>-0.841217</td>\n",
       "      <td>1.500567</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.451760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position       age      area       sex   partner  num_child  education  \\\n",
       "0     -0.176420  1.027084  1.544468  1.009041  0.998734   0.708483  -0.085928   \n",
       "1      0.643375 -0.192567  0.292875 -0.991040 -1.001267  -0.708058  -0.982563   \n",
       "2      0.643375  0.276529  0.145629 -0.991040 -1.001267  -0.708058   0.810708   \n",
       "3     -0.996214 -1.036941  1.691715  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "4     -0.996214 -0.755483 -0.737849  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "...         ...       ...       ...       ...       ...        ...        ...   \n",
       "20995 -0.996214 -0.567845 -0.516979  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "20996 -0.996214 -1.036941  1.470845  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "20997  1.463169  0.276529  1.249976  1.009041 -1.001267  -0.708058   0.810708   \n",
       "20998 -0.996214 -1.130761  0.808237  1.009041  0.998734   0.708483  -0.085928   \n",
       "20999 -0.176420 -0.286387 -1.621326 -0.991040 -1.001267  -0.708058   1.707344   \n",
       "\n",
       "       service_length  study_time   commute  overtime  familiy_num  \\\n",
       "0            1.097992   -0.537098  0.818742 -0.243349     0.832908   \n",
       "1            0.069682    1.584501 -0.539406  0.283116    -0.833279   \n",
       "2            0.163165    0.069073 -0.992122  1.023458    -0.833279   \n",
       "3           -0.771663   -0.234012 -0.992122 -0.753363    -0.833279   \n",
       "4           -0.678180   -0.234012 -1.293932 -0.950787    -0.833279   \n",
       "...               ...         ...       ...       ...          ...   \n",
       "20995       -0.491214   -0.537098 -1.293932  0.793129    -0.833279   \n",
       "20996       -0.771663   -0.234012 -1.293932  0.447636    -0.833279   \n",
       "20997        0.163165    0.069073 -0.388501  0.332472    -0.833279   \n",
       "20998       -1.145594   -0.840184 -0.086690  0.036335     0.832908   \n",
       "20999       -0.584697   -1.143269 -0.841217  1.500567    -0.833279   \n",
       "\n",
       "       agexposition  \n",
       "0         -0.008975  \n",
       "1          0.070094  \n",
       "2          0.307300  \n",
       "3         -0.878731  \n",
       "4         -0.807569  \n",
       "...             ...  \n",
       "20995     -0.760128  \n",
       "20996     -0.878731  \n",
       "20997      0.876595  \n",
       "20998     -0.902451  \n",
       "20999     -0.451760  \n",
       "\n",
       "[21000 rows x 13 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16800 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "16800/16800 [==============================] - ETA: 31s - loss: 32.925 - ETA: 13s - loss: 24.315 - ETA: 9s - loss: 16.529 - ETA: 6s - loss: 12.08 - ETA: 5s - loss: 9.5510 - ETA: 4s - loss: 7.818 - ETA: 4s - loss: 6.887 - ETA: 3s - loss: 6.059 - ETA: 3s - loss: 5.394 - ETA: 3s - loss: 4.912 - ETA: 3s - loss: 4.621 - ETA: 3s - loss: 4.368 - ETA: 2s - loss: 4.060 - ETA: 2s - loss: 3.879 - ETA: 2s - loss: 3.705 - ETA: 2s - loss: 3.481 - ETA: 2s - loss: 3.279 - ETA: 2s - loss: 3.098 - ETA: 2s - loss: 2.938 - ETA: 2s - loss: 2.839 - ETA: 2s - loss: 2.749 - ETA: 2s - loss: 2.664 - ETA: 2s - loss: 2.587 - ETA: 2s - loss: 2.509 - ETA: 2s - loss: 2.438 - ETA: 1s - loss: 2.339 - ETA: 1s - loss: 2.276 - ETA: 1s - loss: 2.218 - ETA: 1s - loss: 2.162 - ETA: 1s - loss: 2.109 - ETA: 1s - loss: 2.035 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.924 - ETA: 1s - loss: 1.882 - ETA: 1s - loss: 1.823 - ETA: 1s - loss: 1.785 - ETA: 1s - loss: 1.749 - ETA: 1s - loss: 1.698 - ETA: 1s - loss: 1.665 - ETA: 0s - loss: 1.634 - ETA: 0s - loss: 1.588 - ETA: 0s - loss: 1.559 - ETA: 0s - loss: 1.531 - ETA: 0s - loss: 1.491 - ETA: 0s - loss: 1.453 - ETA: 0s - loss: 1.429 - ETA: 0s - loss: 1.393 - ETA: 0s - loss: 1.371 - ETA: 0s - loss: 1.338 - ETA: 0s - loss: 1.307 - ETA: 0s - loss: 1.287 - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 1.248 - ETA: 0s - loss: 1.230 - 4s 221us/step - loss: 1.2191 - val_loss: 0.0552\n",
      "Epoch 2/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.061 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.055 - ETA: 3s - loss: 0.052 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.049 - ETA: 3s - loss: 0.047 - ETA: 3s - loss: 0.046 - ETA: 2s - loss: 0.045 - ETA: 2s - loss: 0.044 - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.038 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 1s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - 4s 247us/step - loss: 0.0292 - val_loss: 0.0201\n",
      "Epoch 3/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 219us/step - loss: 0.0206 - val_loss: 0.0210\n",
      "Epoch 4/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 223us/step - loss: 0.0201 - val_loss: 0.0187\n",
      "Epoch 5/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 202us/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 6/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 201us/step - loss: 0.0185 - val_loss: 0.0249\n",
      "Epoch 7/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 4s 217us/step - loss: 0.0189 - val_loss: 0.0243\n",
      "Epoch 8/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 242us/step - loss: 0.0175 - val_loss: 0.0164\n",
      "Epoch 9/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 296us/step - loss: 0.0173 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 314us/step - loss: 0.0173 - val_loss: 0.0235\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 7s - loss: 0.015 - ETA: 8s - loss: 0.019 - ETA: 5s - loss: 0.019 - ETA: 4s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 4s 256us/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 12/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 281us/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 13/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 316us/step - loss: 0.0168 - val_loss: 0.0299\n",
      "Epoch 14/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.031 - ETA: 2s - loss: 0.024 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 231us/step - loss: 0.0178 - val_loss: 0.0198\n",
      "Epoch 15/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 214us/step - loss: 0.0149 - val_loss: 0.0159\n",
      "Epoch 16/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 234us/step - loss: 0.0145 - val_loss: 0.0136\n",
      "Epoch 17/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - 4s 247us/step - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 18/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 10s - loss: 0.01 - ETA: 8s - loss: 0.0118 - ETA: 8s - loss: 0.012 - ETA: 8s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 320us/step - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 5s 321us/step - loss: 0.0140 - val_loss: 0.0179\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.016 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 6s 329us/step - loss: 0.0147 - val_loss: 0.0158\n",
      "Epoch 21/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 235us/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 22/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 205us/step - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 23/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 3s 182us/step - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 24/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 209us/step - loss: 0.0145 - val_loss: 0.0188\n",
      "Epoch 25/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.023 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.021 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 5s 304us/step - loss: 0.0131 - val_loss: 0.0220\n",
      "Epoch 26/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - 5s 278us/step - loss: 0.0120 - val_loss: 0.0129\n",
      "Epoch 27/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 257us/step - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 28/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 255us/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 240us/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 30/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 232us/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 31/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 299us/step - loss: 0.0111 - val_loss: 0.0151\n",
      "Epoch 32/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 317us/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 33/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 276us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 34/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 238us/step - loss: 0.0131 - val_loss: 0.0284\n",
      "Epoch 35/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.024 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 224us/step - loss: 0.0126 - val_loss: 0.0142\n",
      "Epoch 36/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 6s 334us/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 6s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 384us/step - loss: 0.0107 - val_loss: 0.0207\n",
      "Epoch 38/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 7s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 7s 429us/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 39/100\n",
      "16800/16800 [==============================] - ETA: 7s - loss: 0.009 - ETA: 9s - loss: 0.009 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.009 - ETA: 7s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 346us/step - loss: 0.0108 - val_loss: 0.0198\n",
      "Epoch 40/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.022 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 219us/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 41/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 292us/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 42/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 218us/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 43/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 238us/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 44/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 239us/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 45/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 223us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 244us/step - loss: 0.0114 - val_loss: 0.0130\n",
      "Epoch 47/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 195us/step - loss: 0.0107 - val_loss: 0.0170\n",
      "Epoch 48/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 250us/step - loss: 0.0115 - val_loss: 0.0137\n",
      "Epoch 49/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 7s - loss: 0.009 - ETA: 7s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 212us/step - loss: 0.0098 - val_loss: 0.0130\n",
      "Epoch 50/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 300us/step - loss: 0.0115 - val_loss: 0.0210\n",
      "Epoch 51/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 256us/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 52/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 256us/step - loss: 0.0094 - val_loss: 0.0201\n",
      "Epoch 53/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 247us/step - loss: 0.0104 - val_loss: 0.0181\n",
      "Epoch 54/100\n",
      "16800/16800 [==============================] - ETA: 14s - loss: 0.01 - ETA: 8s - loss: 0.0142 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 278us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 277us/step - loss: 0.0094 - val_loss: 0.0138\n",
      "Epoch 56/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 322us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 57/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 280us/step - loss: 0.0100 - val_loss: 0.0185\n",
      "Epoch 58/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 6s - loss: 0.013 - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 339us/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 59/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.005 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 7s 422us/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 60/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 320us/step - loss: 0.0095 - val_loss: 0.0103\n",
      "Epoch 61/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 280us/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 62/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 312us/step - loss: 0.0107 - val_loss: 0.0154\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 311us/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 64/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 297us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 65/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0109\n",
      "Epoch 66/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 291us/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 67/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0110\n",
      "Epoch 68/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 289us/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 69/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 271us/step - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 70/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 171us/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 71/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 170us/step - loss: 0.0092 - val_loss: 0.0109\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 224us/step - loss: 0.0092 - val_loss: 0.0121\n",
      "Epoch 73/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 336us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 74/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 338us/step - loss: 0.0091 - val_loss: 0.0104\n",
      "Epoch 75/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 318us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 76/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 293us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 77/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 285us/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 78/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 292us/step - loss: 0.0082 - val_loss: 0.0094\n",
      "Epoch 79/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 8s - loss: 0.007 - ETA: 7s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 274us/step - loss: 0.0087 - val_loss: 0.0161\n",
      "Epoch 81/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 3s 183us/step - loss: 0.0085 - val_loss: 0.0107\n",
      "Epoch 82/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 173us/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 83/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 318us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 84/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 312us/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 85/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 86/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0082 - val_loss: 0.0103\n",
      "Epoch 87/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 281us/step - loss: 0.0083 - val_loss: 0.0112\n",
      "Epoch 88/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 280us/step - loss: 0.0079 - val_loss: 0.0103\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 296us/step - loss: 0.0082 - val_loss: 0.0127\n",
      "Epoch 90/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0111\n",
      "Epoch 91/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 286us/step - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 92/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 298us/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 93/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 294us/step - loss: 0.0086 - val_loss: 0.0109\n",
      "Epoch 94/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0085 - val_loss: 0.0095\n",
      "Epoch 95/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0114\n",
      "Epoch 96/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 301us/step - loss: 0.0081 - val_loss: 0.0104\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 296us/step - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 98/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0080 - val_loss: 0.0106\n",
      "Epoch 99/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 293us/step - loss: 0.0086 - val_loss: 0.0169\n",
      "Epoch 100/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 279us/step - loss: 0.0081 - val_loss: 0.0091\n"
     ]
    }
   ],
   "source": [
    "#ニューラルネットワークモデルの生成\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(800, activation = 'relu'))\n",
    "model.add(Dense(100, activation =  'relu'))\n",
    "model.add(Dense(1))\n",
    "# モデルをコンパイル \n",
    "from keras.optimizers import Adam\n",
    "# ylgm=min_max_normalization(y)\n",
    "train_X_nn, valid_X_nn,train_y_nn, valid_y_nn = train_test_split(train_X_nn,y,test_size=0.2,random_state=43)\n",
    "\n",
    "model.compile(Adam(lr=1e-3), loss=\"mean_squared_error\")\n",
    "#トレーニングデータで学習し，テストデータで評価（平均2乗誤差を用いる）\n",
    "history = model.fit(np.array(train_X_nn), np.array(train_y_nn), batch_size=128, epochs=100, verbose=1, \n",
    "          validation_data=(np.array(valid_X_nn), np.array(valid_y_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 138us/step\n",
      "0.009095980622583912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134851e10>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#結果の表示\n",
    "import matplotlib.pyplot as plt #プロット用のライブラリを利用\n",
    "\n",
    "print(model.evaluate(valid_X_nn, valid_y_nn))\n",
    "\n",
    "train_acc = history.history['loss']\n",
    "test_acc = history.history['val_loss']\n",
    "x = np.arange(len(train_acc))\n",
    "plt.plot(x, train_acc, label = 'train mse')\n",
    "plt.plot(x, test_acc, label = 'test mse')\n",
    "plt.legend() #グラフの線の説明を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:25.749117\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(valid_X_nn)\n",
    "score=mean_absolute_error(np.exp(valid_y_nn),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>230.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>147.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  age  area  sex  partner  num_child  education  service_length  \\\n",
       "0            4   39    14    1        1          5          2              19   \n",
       "1            2   31    27    0        0          0          5               0   \n",
       "2            1   20    45    1        1          2          1               2   \n",
       "3            1   28    37    1        0          0          1              10   \n",
       "4            2   41    16    1        0          0          1              23   \n",
       "...        ...  ...   ...  ...      ...        ...        ...             ...   \n",
       "8995         2   43    14    1        0          0          1              25   \n",
       "8996         3   40     5    1        0          0          1              22   \n",
       "8997         5   46    24    0        0          0          1              28   \n",
       "8998         1   22    13    0        0          0          1               4   \n",
       "8999         3   49    14    0        1          0          3              27   \n",
       "\n",
       "      study_time  commute  overtime  familiy_num  agexposition  cluster  \n",
       "0            1.0      1.8      14.2            7         156.0       19  \n",
       "1            0.0      0.5      18.6            1          62.0        0  \n",
       "2            2.0      1.2       2.3            4          30.0       14  \n",
       "3            3.0      0.3       0.0            1          42.0        9  \n",
       "4            3.0      0.5      10.1            1          82.0        4  \n",
       "...          ...      ...       ...          ...           ...      ...  \n",
       "8995         3.0      0.7       0.0            1          86.0        4  \n",
       "8996         8.0      0.7       5.7            1         120.0        7  \n",
       "8997         2.0      0.8       0.0            1         230.0       17  \n",
       "8998         0.0      0.1       0.7            1          33.0       12  \n",
       "8999         0.0      1.7      11.0            2         147.0       19  \n",
       "\n",
       "[9000 rows x 14 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:13:17] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:13:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:13:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:13:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:14:07,447] Finished trial#0 resulted in value: 0.9143755837764973. Current best value is 0.9143755837764973 with parameters: {'booster': 'gbtree', 'iterations': 135, 'learning_rate': 0.8151368878154551, 'random_strength': 94, 'bagging_temperature': 6.938280999202821, 'od_type': 'IncToDec', 'od_wait': 19, 'lambda_l1': 9.699582345379795e-08, 'lambda_l2': 0.0018368260823978786, 'num_leaves': 157, 'feature_fraction': 0.7664785013828086, 'bagging_fraction': 0.6302200992159606, 'bagging_freq': 4, 'min_child_samples': 49}.\n",
      "[I 2019-11-20 22:14:09,217] Finished trial#1 resulted in value: 0.9546423755060085. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:12,565] Finished trial#2 resulted in value: 0.948242783130086. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:13,982] Finished trial#3 resulted in value: 0.9269750034706646. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:16,621] Finished trial#4 resulted in value: 0.8797833850923066. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:20,069] Finished trial#5 resulted in value: 0.7652653007887137. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:22,004] Finished trial#6 resulted in value: 0.9437476767419801. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:25,084] Finished trial#7 resulted in value: 0.9524717056659113. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:27,805] Finished trial#8 resulted in value: 0.954194581729546. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n",
      "[I 2019-11-20 22:14:30,611] Finished trial#9 resulted in value: 0.9434507788782129. Current best value is 0.9546423755060085 with parameters: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.95\n",
      "Best params: {'booster': 'gblinear', 'iterations': 180, 'learning_rate': 0.16773953349424586, 'random_strength': 66, 'bagging_temperature': 0.027908674738595336, 'od_type': 'IncToDec', 'od_wait': 43, 'lambda_l1': 7.71987468312203e-07, 'lambda_l2': 1.5951076717793644, 'num_leaves': 53, 'feature_fraction': 0.497757371866325, 'bagging_fraction': 0.7724507844007065, 'bagging_freq': 2, 'min_child_samples': 19}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:14:33,823] Finished trial#0 resulted in value: 0.94704061569658. Current best value is 0.94704061569658 with parameters: {'booster': 'gblinear', 'iterations': 304, 'learning_rate': 0.04127893448549056, 'random_strength': 27, 'bagging_temperature': 50.328958880835046, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 0.4158578413536228, 'lambda_l2': 5.9115592804708186e-05, 'num_leaves': 225, 'feature_fraction': 0.5035461792628737, 'bagging_fraction': 0.6952287675309226, 'bagging_freq': 4, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:14:36,576] Finished trial#1 resulted in value: 0.9550118655159926. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:38,906] Finished trial#2 resulted in value: 0.9071454070082563. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:41,190] Finished trial#3 resulted in value: 0.9537631941621794. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:42,707] Finished trial#4 resulted in value: 0.947153716607317. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:44,291] Finished trial#5 resulted in value: 0.9449035676821493. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:46,454] Finished trial#6 resulted in value: 0.9404390794377436. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:48,586] Finished trial#7 resulted in value: 0.9378045313465039. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:53,418] Finished trial#8 resulted in value: 0.9229302515071062. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n",
      "[I 2019-11-20 22:14:54,530] Finished trial#9 resulted in value: 0.927631710005065. Current best value is 0.9550118655159926 with parameters: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 108, 'learning_rate': 0.15772881137717037, 'random_strength': 12, 'bagging_temperature': 0.013901216674422201, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.6349504363110887, 'lambda_l2': 1.7242187482734048e-05, 'num_leaves': 142, 'feature_fraction': 0.570213403089252, 'bagging_fraction': 0.7007043967450461, 'bagging_freq': 6, 'min_child_samples': 8}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:14:59,041] Finished trial#0 resulted in value: 0.9504733069802713. Current best value is 0.9504733069802713 with parameters: {'booster': 'gblinear', 'iterations': 143, 'learning_rate': 0.03353529918819856, 'random_strength': 62, 'bagging_temperature': 4.87860425405438, 'od_type': 'Iter', 'od_wait': 42, 'lambda_l1': 0.04308223230888693, 'lambda_l2': 6.975506696146087e-07, 'num_leaves': 155, 'feature_fraction': 0.784836886731884, 'bagging_fraction': 0.5882181838758687, 'bagging_freq': 7, 'min_child_samples': 12}.\n",
      "[I 2019-11-20 22:15:01,522] Finished trial#1 resulted in value: 0.9540602696564477. Current best value is 0.9540602696564477 with parameters: {'booster': 'gbtree', 'iterations': 341, 'learning_rate': 0.19637163428952623, 'random_strength': 2, 'bagging_temperature': 1.5407039413491679, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.0007873776598892658, 'lambda_l2': 1.1298379433824174e-08, 'num_leaves': 111, 'feature_fraction': 0.5065244612627627, 'bagging_fraction': 0.885828417758962, 'bagging_freq': 2, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:15:03,220] Finished trial#2 resulted in value: 0.9519285318589343. Current best value is 0.9540602696564477 with parameters: {'booster': 'gbtree', 'iterations': 341, 'learning_rate': 0.19637163428952623, 'random_strength': 2, 'bagging_temperature': 1.5407039413491679, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.0007873776598892658, 'lambda_l2': 1.1298379433824174e-08, 'num_leaves': 111, 'feature_fraction': 0.5065244612627627, 'bagging_fraction': 0.885828417758962, 'bagging_freq': 2, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:15:04,640] Finished trial#3 resulted in value: 0.9464138688970148. Current best value is 0.9540602696564477 with parameters: {'booster': 'gbtree', 'iterations': 341, 'learning_rate': 0.19637163428952623, 'random_strength': 2, 'bagging_temperature': 1.5407039413491679, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.0007873776598892658, 'lambda_l2': 1.1298379433824174e-08, 'num_leaves': 111, 'feature_fraction': 0.5065244612627627, 'bagging_fraction': 0.885828417758962, 'bagging_freq': 2, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:15:07,499] Finished trial#4 resulted in value: 0.9533520895365053. Current best value is 0.9540602696564477 with parameters: {'booster': 'gbtree', 'iterations': 341, 'learning_rate': 0.19637163428952623, 'random_strength': 2, 'bagging_temperature': 1.5407039413491679, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.0007873776598892658, 'lambda_l2': 1.1298379433824174e-08, 'num_leaves': 111, 'feature_fraction': 0.5065244612627627, 'bagging_fraction': 0.885828417758962, 'bagging_freq': 2, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:15:09,425] Finished trial#5 resulted in value: 0.9525059715601716. Current best value is 0.9540602696564477 with parameters: {'booster': 'gbtree', 'iterations': 341, 'learning_rate': 0.19637163428952623, 'random_strength': 2, 'bagging_temperature': 1.5407039413491679, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.0007873776598892658, 'lambda_l2': 1.1298379433824174e-08, 'num_leaves': 111, 'feature_fraction': 0.5065244612627627, 'bagging_fraction': 0.885828417758962, 'bagging_freq': 2, 'min_child_samples': 15}.\n",
      "[I 2019-11-20 22:15:12,063] Finished trial#6 resulted in value: 0.95495110740883. Current best value is 0.95495110740883 with parameters: {'booster': 'gbtree', 'iterations': 224, 'learning_rate': 0.09225612104697202, 'random_strength': 94, 'bagging_temperature': 0.2825900533936127, 'od_type': 'Iter', 'od_wait': 12, 'lambda_l1': 1.3143075638431384e-08, 'lambda_l2': 0.004300637156614743, 'num_leaves': 212, 'feature_fraction': 0.8557425015964467, 'bagging_fraction': 0.4723789103181295, 'bagging_freq': 1, 'min_child_samples': 39}.\n",
      "[I 2019-11-20 22:15:13,531] Finished trial#7 resulted in value: 0.9460650175017478. Current best value is 0.95495110740883 with parameters: {'booster': 'gbtree', 'iterations': 224, 'learning_rate': 0.09225612104697202, 'random_strength': 94, 'bagging_temperature': 0.2825900533936127, 'od_type': 'Iter', 'od_wait': 12, 'lambda_l1': 1.3143075638431384e-08, 'lambda_l2': 0.004300637156614743, 'num_leaves': 212, 'feature_fraction': 0.8557425015964467, 'bagging_fraction': 0.4723789103181295, 'bagging_freq': 1, 'min_child_samples': 39}.\n",
      "[I 2019-11-20 22:15:15,173] Finished trial#8 resulted in value: 0.7918442732549118. Current best value is 0.95495110740883 with parameters: {'booster': 'gbtree', 'iterations': 224, 'learning_rate': 0.09225612104697202, 'random_strength': 94, 'bagging_temperature': 0.2825900533936127, 'od_type': 'Iter', 'od_wait': 12, 'lambda_l1': 1.3143075638431384e-08, 'lambda_l2': 0.004300637156614743, 'num_leaves': 212, 'feature_fraction': 0.8557425015964467, 'bagging_fraction': 0.4723789103181295, 'bagging_freq': 1, 'min_child_samples': 39}.\n",
      "[I 2019-11-20 22:15:18,798] Finished trial#9 resulted in value: 0.9294627348072524. Current best value is 0.95495110740883 with parameters: {'booster': 'gbtree', 'iterations': 224, 'learning_rate': 0.09225612104697202, 'random_strength': 94, 'bagging_temperature': 0.2825900533936127, 'od_type': 'Iter', 'od_wait': 12, 'lambda_l1': 1.3143075638431384e-08, 'lambda_l2': 0.004300637156614743, 'num_leaves': 212, 'feature_fraction': 0.8557425015964467, 'bagging_fraction': 0.4723789103181295, 'bagging_freq': 1, 'min_child_samples': 39}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.95\n",
      "Best params: {'booster': 'gbtree', 'iterations': 224, 'learning_rate': 0.09225612104697202, 'random_strength': 94, 'bagging_temperature': 0.2825900533936127, 'od_type': 'Iter', 'od_wait': 12, 'lambda_l1': 1.3143075638431384e-08, 'lambda_l2': 0.004300637156614743, 'num_leaves': 212, 'feature_fraction': 0.8557425015964467, 'bagging_fraction': 0.4723789103181295, 'bagging_freq': 1, 'min_child_samples': 39}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:15:22,494] Finished trial#0 resulted in value: 0.9538188499877602. Current best value is 0.9538188499877602 with parameters: {'booster': 'gbtree', 'iterations': 133, 'learning_rate': 0.09890559191914468, 'random_strength': 32, 'bagging_temperature': 0.09663216383949942, 'od_type': 'IncToDec', 'od_wait': 19, 'lambda_l1': 0.1713769169194866, 'lambda_l2': 0.006942364511099813, 'num_leaves': 243, 'feature_fraction': 0.4974983173880728, 'bagging_fraction': 0.8145793724156651, 'bagging_freq': 4, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 22:15:24,833] Finished trial#1 resulted in value: 0.9565228410311374. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:27,202] Finished trial#2 resulted in value: 0.607197944947816. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:31,059] Finished trial#3 resulted in value: 0.9532685752692963. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:32,775] Finished trial#4 resulted in value: 0.9026090233198968. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:35,276] Finished trial#5 resulted in value: 0.6882226953221121. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:36,891] Finished trial#6 resulted in value: 0.9453271299456472. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:40,093] Finished trial#7 resulted in value: 0.8670900853980796. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:42,483] Finished trial#8 resulted in value: 0.953219036507879. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n",
      "[I 2019-11-20 22:15:45,826] Finished trial#9 resulted in value: 0.9546442022074176. Current best value is 0.9565228410311374 with parameters: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 201, 'learning_rate': 0.10026616749630636, 'random_strength': 99, 'bagging_temperature': 43.751518096315124, 'od_type': 'Iter', 'od_wait': 15, 'lambda_l1': 0.0033900493889292968, 'lambda_l2': 0.40925196088280513, 'num_leaves': 87, 'feature_fraction': 0.5121846411931855, 'bagging_fraction': 0.8977678637442313, 'bagging_freq': 5, 'min_child_samples': 31}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:15:47,479] Finished trial#0 resulted in value: 0.9098631337997892. Current best value is 0.9098631337997892 with parameters: {'n_estimators': 11, 'max_features': 2, 'min_samples_split': 19, 'max_depth': 63}.\n",
      "[I 2019-11-20 22:15:49,810] Finished trial#1 resulted in value: 0.9016829986684515. Current best value is 0.9098631337997892 with parameters: {'n_estimators': 11, 'max_features': 2, 'min_samples_split': 19, 'max_depth': 63}.\n",
      "[I 2019-11-20 22:16:02,288] Finished trial#2 resulted in value: 0.9384689135926493. Current best value is 0.9384689135926493 with parameters: {'n_estimators': 138, 'max_features': 8, 'min_samples_split': 92, 'max_depth': 87}.\n",
      "[I 2019-11-20 22:16:03,701] Finished trial#3 resulted in value: 0.9313762053722648. Current best value is 0.9384689135926493 with parameters: {'n_estimators': 138, 'max_features': 8, 'min_samples_split': 92, 'max_depth': 87}.\n",
      "[I 2019-11-20 22:16:13,944] Finished trial#4 resulted in value: 0.9376281643115604. Current best value is 0.9384689135926493 with parameters: {'n_estimators': 138, 'max_features': 8, 'min_samples_split': 92, 'max_depth': 87}.\n",
      "[I 2019-11-20 22:16:25,095] Finished trial#5 resulted in value: 0.9443057668007364. Current best value is 0.9443057668007364 with parameters: {'n_estimators': 126, 'max_features': 7, 'min_samples_split': 54, 'max_depth': 12}.\n",
      "[I 2019-11-20 22:16:46,500] Finished trial#6 resulted in value: 0.9434128499156007. Current best value is 0.9443057668007364 with parameters: {'n_estimators': 126, 'max_features': 7, 'min_samples_split': 54, 'max_depth': 12}.\n",
      "[I 2019-11-20 22:16:49,451] Finished trial#7 resulted in value: 0.9113853848025062. Current best value is 0.9443057668007364 with parameters: {'n_estimators': 126, 'max_features': 7, 'min_samples_split': 54, 'max_depth': 12}.\n",
      "[I 2019-11-20 22:17:03,110] Finished trial#8 resulted in value: 0.9487583628502486. Current best value is 0.9487583628502486 with parameters: {'n_estimators': 155, 'max_features': 6, 'min_samples_split': 30, 'max_depth': 15}.\n",
      "[I 2019-11-20 22:17:22,737] Finished trial#9 resulted in value: 0.9460637323336437. Current best value is 0.9487583628502486 with parameters: {'n_estimators': 155, 'max_features': 6, 'min_samples_split': 30, 'max_depth': 15}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.95\n",
      "Best params: {'n_estimators': 155, 'max_features': 6, 'min_samples_split': 30, 'max_depth': 15}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:17:45,807] Finished trial#0 resulted in value: 0.9452188807883987. Current best value is 0.9452188807883987 with parameters: {'n_estimators': 126, 'max_features': 13, 'min_samples_split': 69, 'max_depth': 93}.\n",
      "[I 2019-11-20 22:17:50,319] Finished trial#1 resulted in value: 0.9432082420632095. Current best value is 0.9452188807883987 with parameters: {'n_estimators': 126, 'max_features': 13, 'min_samples_split': 69, 'max_depth': 93}.\n",
      "[I 2019-11-20 22:17:52,131] Finished trial#2 resulted in value: 0.8996349622102139. Current best value is 0.9452188807883987 with parameters: {'n_estimators': 126, 'max_features': 13, 'min_samples_split': 69, 'max_depth': 93}.\n",
      "[I 2019-11-20 22:20:16,849] Finished trial#3 resulted in value: 0.9551288903639952. Current best value is 0.9551288903639952 with parameters: {'n_estimators': 163, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 48}.\n",
      "[I 2019-11-20 22:20:26,844] Finished trial#4 resulted in value: 0.9436628402990099. Current best value is 0.9551288903639952 with parameters: {'n_estimators': 163, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 48}.\n",
      "[I 2019-11-20 22:20:36,399] Finished trial#5 resulted in value: 0.9447933397143125. Current best value is 0.9551288903639952 with parameters: {'n_estimators': 163, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 48}.\n",
      "[I 2019-11-20 22:20:58,296] Finished trial#6 resulted in value: 0.9558888953365499. Current best value is 0.9558888953365499 with parameters: {'n_estimators': 130, 'max_features': 10, 'min_samples_split': 9, 'max_depth': 76}.\n",
      "[I 2019-11-20 22:20:59,659] Finished trial#7 resulted in value: 0.8931957213870731. Current best value is 0.9558888953365499 with parameters: {'n_estimators': 130, 'max_features': 10, 'min_samples_split': 9, 'max_depth': 76}.\n",
      "[I 2019-11-20 22:21:06,427] Finished trial#8 resulted in value: 0.9435421609134191. Current best value is 0.9558888953365499 with parameters: {'n_estimators': 130, 'max_features': 10, 'min_samples_split': 9, 'max_depth': 76}.\n",
      "[I 2019-11-20 22:21:09,302] Finished trial#9 resulted in value: 0.7150259030999965. Current best value is 0.9558888953365499 with parameters: {'n_estimators': 130, 'max_features': 10, 'min_samples_split': 9, 'max_depth': 76}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'n_estimators': 130, 'max_features': 10, 'min_samples_split': 9, 'max_depth': 76}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:21:32,640] Finished trial#0 resulted in value: 0.9469273275509188. Current best value is 0.9469273275509188 with parameters: {'n_estimators': 190, 'max_features': 8, 'min_samples_split': 60, 'max_depth': 64}.\n",
      "[I 2019-11-20 22:21:55,774] Finished trial#1 resulted in value: 0.9549629412847797. Current best value is 0.9549629412847797 with parameters: {'n_estimators': 193, 'max_features': 7, 'min_samples_split': 12, 'max_depth': 90}.\n",
      "[I 2019-11-20 22:21:58,252] Finished trial#2 resulted in value: 0.8347125462202161. Current best value is 0.9549629412847797 with parameters: {'n_estimators': 193, 'max_features': 7, 'min_samples_split': 12, 'max_depth': 90}.\n",
      "[I 2019-11-20 22:22:07,220] Finished trial#3 resulted in value: 0.9453861622215122. Current best value is 0.9549629412847797 with parameters: {'n_estimators': 193, 'max_features': 7, 'min_samples_split': 12, 'max_depth': 90}.\n",
      "[I 2019-11-20 22:22:25,398] Finished trial#4 resulted in value: 0.9542778205098813. Current best value is 0.9549629412847797 with parameters: {'n_estimators': 193, 'max_features': 7, 'min_samples_split': 12, 'max_depth': 90}.\n",
      "[I 2019-11-20 22:22:36,385] Finished trial#5 resulted in value: 0.9262471070227367. Current best value is 0.9549629412847797 with parameters: {'n_estimators': 193, 'max_features': 7, 'min_samples_split': 12, 'max_depth': 90}.\n",
      "[I 2019-11-20 22:22:56,600] Finished trial#6 resulted in value: 0.9550700892181521. Current best value is 0.9550700892181521 with parameters: {'n_estimators': 114, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 36}.\n",
      "[I 2019-11-20 22:23:00,010] Finished trial#7 resulted in value: 0.917738158372434. Current best value is 0.9550700892181521 with parameters: {'n_estimators': 114, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 36}.\n",
      "[I 2019-11-20 22:23:07,546] Finished trial#8 resulted in value: 0.9357513178154259. Current best value is 0.9550700892181521 with parameters: {'n_estimators': 114, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 36}.\n",
      "[I 2019-11-20 22:23:10,415] Finished trial#9 resulted in value: 0.9031285784783923. Current best value is 0.9550700892181521 with parameters: {'n_estimators': 114, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 36}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'n_estimators': 114, 'max_features': 12, 'min_samples_split': 20, 'max_depth': 36}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:23:23,078] Finished trial#0 resulted in value: 0.9162962515707646. Current best value is 0.9162962515707646 with parameters: {'n_estimators': 149, 'max_features': 2, 'min_samples_split': 22, 'max_depth': 82}.\n",
      "[I 2019-11-20 22:23:27,407] Finished trial#1 resulted in value: 0.947898840167851. Current best value is 0.947898840167851 with parameters: {'n_estimators': 42, 'max_features': 4, 'min_samples_split': 8, 'max_depth': 92}.\n",
      "[I 2019-11-20 22:23:33,804] Finished trial#2 resulted in value: 0.9038808555600181. Current best value is 0.947898840167851 with parameters: {'n_estimators': 42, 'max_features': 4, 'min_samples_split': 8, 'max_depth': 92}.\n",
      "[I 2019-11-20 22:23:46,423] Finished trial#3 resulted in value: 0.9469090226830715. Current best value is 0.947898840167851 with parameters: {'n_estimators': 42, 'max_features': 4, 'min_samples_split': 8, 'max_depth': 92}.\n",
      "[I 2019-11-20 22:24:00,608] Finished trial#4 resulted in value: 0.9479277915431471. Current best value is 0.9479277915431471 with parameters: {'n_estimators': 96, 'max_features': 11, 'min_samples_split': 62, 'max_depth': 13}.\n",
      "[I 2019-11-20 22:24:23,132] Finished trial#5 resulted in value: 0.9535204386693221. Current best value is 0.9535204386693221 with parameters: {'n_estimators': 189, 'max_features': 8, 'min_samples_split': 27, 'max_depth': 49}.\n",
      "[I 2019-11-20 22:24:56,364] Finished trial#6 resulted in value: 0.9568331922432327. Current best value is 0.9568331922432327 with parameters: {'n_estimators': 160, 'max_features': 11, 'min_samples_split': 7, 'max_depth': 70}.\n",
      "[I 2019-11-20 22:25:22,428] Finished trial#7 resulted in value: 0.9482161068842458. Current best value is 0.9568331922432327 with parameters: {'n_estimators': 160, 'max_features': 11, 'min_samples_split': 7, 'max_depth': 70}.\n",
      "[I 2019-11-20 22:25:38,016] Finished trial#8 resulted in value: 0.9549707502592139. Current best value is 0.9568331922432327 with parameters: {'n_estimators': 160, 'max_features': 11, 'min_samples_split': 7, 'max_depth': 70}.\n",
      "[I 2019-11-20 22:25:56,299] Finished trial#9 resulted in value: 0.952426968565263. Current best value is 0.9568331922432327 with parameters: {'n_estimators': 160, 'max_features': 11, 'min_samples_split': 7, 'max_depth': 70}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'n_estimators': 160, 'max_features': 11, 'min_samples_split': 7, 'max_depth': 70}\n",
      "\n",
      "0:\tlearn: 0.4007749\ttotal: 4.7ms\tremaining: 4.69s\n",
      "1:\tlearn: 0.3628360\ttotal: 12.3ms\tremaining: 6.12s\n",
      "2:\tlearn: 0.3355615\ttotal: 23ms\tremaining: 7.65s\n",
      "3:\tlearn: 0.3116683\ttotal: 36.1ms\tremaining: 9s\n",
      "4:\tlearn: 0.2928513\ttotal: 67.3ms\tremaining: 13.4s\n",
      "5:\tlearn: 0.2788128\ttotal: 80.8ms\tremaining: 13.4s\n",
      "6:\tlearn: 0.2651759\ttotal: 112ms\tremaining: 15.8s\n",
      "7:\tlearn: 0.2569273\ttotal: 119ms\tremaining: 14.7s\n",
      "8:\tlearn: 0.2461002\ttotal: 127ms\tremaining: 13.9s\n",
      "9:\tlearn: 0.2340684\ttotal: 139ms\tremaining: 13.8s\n",
      "10:\tlearn: 0.2220914\ttotal: 160ms\tremaining: 14.4s\n",
      "11:\tlearn: 0.2161497\ttotal: 170ms\tremaining: 14s\n",
      "12:\tlearn: 0.2095868\ttotal: 181ms\tremaining: 13.7s\n",
      "13:\tlearn: 0.2050956\ttotal: 213ms\tremaining: 15s\n",
      "14:\tlearn: 0.2022421\ttotal: 238ms\tremaining: 15.6s\n",
      "15:\tlearn: 0.1991205\ttotal: 259ms\tremaining: 15.9s\n",
      "16:\tlearn: 0.1959589\ttotal: 286ms\tremaining: 16.5s\n",
      "17:\tlearn: 0.1921920\ttotal: 300ms\tremaining: 16.3s\n",
      "18:\tlearn: 0.1886862\ttotal: 310ms\tremaining: 16s\n",
      "19:\tlearn: 0.1872826\ttotal: 318ms\tremaining: 15.6s\n",
      "20:\tlearn: 0.1864769\ttotal: 332ms\tremaining: 15.5s\n",
      "21:\tlearn: 0.1850101\ttotal: 347ms\tremaining: 15.4s\n",
      "22:\tlearn: 0.1847986\ttotal: 357ms\tremaining: 15.2s\n",
      "23:\tlearn: 0.1844052\ttotal: 365ms\tremaining: 14.8s\n",
      "24:\tlearn: 0.1767632\ttotal: 386ms\tremaining: 15s\n",
      "25:\tlearn: 0.1758911\ttotal: 397ms\tremaining: 14.9s\n",
      "26:\tlearn: 0.1756461\ttotal: 408ms\tremaining: 14.7s\n",
      "27:\tlearn: 0.1752657\ttotal: 417ms\tremaining: 14.5s\n",
      "28:\tlearn: 0.1747235\ttotal: 431ms\tremaining: 14.4s\n",
      "29:\tlearn: 0.1746033\ttotal: 442ms\tremaining: 14.3s\n",
      "30:\tlearn: 0.1743272\ttotal: 450ms\tremaining: 14.1s\n",
      "31:\tlearn: 0.1739461\ttotal: 462ms\tremaining: 14s\n",
      "32:\tlearn: 0.1739248\ttotal: 471ms\tremaining: 13.8s\n",
      "33:\tlearn: 0.1736002\ttotal: 480ms\tremaining: 13.6s\n",
      "34:\tlearn: 0.1735042\ttotal: 492ms\tremaining: 13.6s\n",
      "35:\tlearn: 0.1721851\ttotal: 501ms\tremaining: 13.4s\n",
      "36:\tlearn: 0.1720714\ttotal: 512ms\tremaining: 13.3s\n",
      "37:\tlearn: 0.1720073\ttotal: 523ms\tremaining: 13.2s\n",
      "38:\tlearn: 0.1659767\ttotal: 530ms\tremaining: 13.1s\n",
      "39:\tlearn: 0.1657791\ttotal: 542ms\tremaining: 13s\n",
      "40:\tlearn: 0.1655733\ttotal: 550ms\tremaining: 12.9s\n",
      "41:\tlearn: 0.1655200\ttotal: 560ms\tremaining: 12.8s\n",
      "42:\tlearn: 0.1649523\ttotal: 568ms\tremaining: 12.6s\n",
      "43:\tlearn: 0.1639508\ttotal: 579ms\tremaining: 12.6s\n",
      "44:\tlearn: 0.1634523\ttotal: 591ms\tremaining: 12.6s\n",
      "45:\tlearn: 0.1621174\ttotal: 599ms\tremaining: 12.4s\n",
      "46:\tlearn: 0.1616145\ttotal: 612ms\tremaining: 12.4s\n",
      "47:\tlearn: 0.1613835\ttotal: 621ms\tremaining: 12.3s\n",
      "48:\tlearn: 0.1611290\ttotal: 629ms\tremaining: 12.2s\n",
      "49:\tlearn: 0.1610261\ttotal: 639ms\tremaining: 12.1s\n",
      "50:\tlearn: 0.1606529\ttotal: 651ms\tremaining: 12.1s\n",
      "51:\tlearn: 0.1605969\ttotal: 663ms\tremaining: 12.1s\n",
      "52:\tlearn: 0.1564139\ttotal: 671ms\tremaining: 12s\n",
      "53:\tlearn: 0.1561758\ttotal: 683ms\tremaining: 12s\n",
      "54:\tlearn: 0.1561000\ttotal: 694ms\tremaining: 11.9s\n",
      "55:\tlearn: 0.1558707\ttotal: 703ms\tremaining: 11.8s\n",
      "56:\tlearn: 0.1555808\ttotal: 714ms\tremaining: 11.8s\n",
      "57:\tlearn: 0.1552498\ttotal: 723ms\tremaining: 11.7s\n",
      "58:\tlearn: 0.1551421\ttotal: 735ms\tremaining: 11.7s\n",
      "59:\tlearn: 0.1523483\ttotal: 742ms\tremaining: 11.6s\n",
      "60:\tlearn: 0.1471042\ttotal: 753ms\tremaining: 11.6s\n",
      "61:\tlearn: 0.1469384\ttotal: 761ms\tremaining: 11.5s\n",
      "62:\tlearn: 0.1468367\ttotal: 773ms\tremaining: 11.5s\n",
      "63:\tlearn: 0.1455646\ttotal: 781ms\tremaining: 11.4s\n",
      "64:\tlearn: 0.1452673\ttotal: 792ms\tremaining: 11.4s\n",
      "65:\tlearn: 0.1450730\ttotal: 800ms\tremaining: 11.3s\n",
      "66:\tlearn: 0.1423320\ttotal: 811ms\tremaining: 11.3s\n",
      "67:\tlearn: 0.1422277\ttotal: 818ms\tremaining: 11.2s\n",
      "68:\tlearn: 0.1421492\ttotal: 828ms\tremaining: 11.2s\n",
      "69:\tlearn: 0.1414166\ttotal: 836ms\tremaining: 11.1s\n",
      "70:\tlearn: 0.1379268\ttotal: 847ms\tremaining: 11.1s\n",
      "71:\tlearn: 0.1373455\ttotal: 856ms\tremaining: 11s\n",
      "72:\tlearn: 0.1348406\ttotal: 867ms\tremaining: 11s\n",
      "73:\tlearn: 0.1338875\ttotal: 874ms\tremaining: 10.9s\n",
      "74:\tlearn: 0.1314027\ttotal: 890ms\tremaining: 11s\n",
      "75:\tlearn: 0.1298976\ttotal: 903ms\tremaining: 11s\n",
      "76:\tlearn: 0.1271289\ttotal: 923ms\tremaining: 11.1s\n",
      "77:\tlearn: 0.1251048\ttotal: 935ms\tremaining: 11s\n",
      "78:\tlearn: 0.1231309\ttotal: 945ms\tremaining: 11s\n",
      "79:\tlearn: 0.1213035\ttotal: 956ms\tremaining: 11s\n",
      "80:\tlearn: 0.1194469\ttotal: 963ms\tremaining: 10.9s\n",
      "81:\tlearn: 0.1174378\ttotal: 974ms\tremaining: 10.9s\n",
      "82:\tlearn: 0.1162710\ttotal: 981ms\tremaining: 10.8s\n",
      "83:\tlearn: 0.1149746\ttotal: 993ms\tremaining: 10.8s\n",
      "84:\tlearn: 0.1138003\ttotal: 1s\tremaining: 10.8s\n",
      "85:\tlearn: 0.1130935\ttotal: 1.01s\tremaining: 10.7s\n",
      "86:\tlearn: 0.1125796\ttotal: 1.02s\tremaining: 10.7s\n",
      "87:\tlearn: 0.1120451\ttotal: 1.03s\tremaining: 10.6s\n",
      "88:\tlearn: 0.1115188\ttotal: 1.04s\tremaining: 10.6s\n",
      "89:\tlearn: 0.1104549\ttotal: 1.04s\tremaining: 10.6s\n",
      "90:\tlearn: 0.1096750\ttotal: 1.06s\tremaining: 10.6s\n",
      "91:\tlearn: 0.1090812\ttotal: 1.06s\tremaining: 10.5s\n",
      "92:\tlearn: 0.1082148\ttotal: 1.08s\tremaining: 10.5s\n",
      "93:\tlearn: 0.1077787\ttotal: 1.08s\tremaining: 10.4s\n",
      "94:\tlearn: 0.1073298\ttotal: 1.1s\tremaining: 10.5s\n",
      "95:\tlearn: 0.1069225\ttotal: 1.11s\tremaining: 10.4s\n",
      "96:\tlearn: 0.1065955\ttotal: 1.11s\tremaining: 10.4s\n",
      "97:\tlearn: 0.1063300\ttotal: 1.12s\tremaining: 10.4s\n",
      "98:\tlearn: 0.1056764\ttotal: 1.14s\tremaining: 10.4s\n",
      "99:\tlearn: 0.1050956\ttotal: 1.16s\tremaining: 10.5s\n",
      "100:\tlearn: 0.1047703\ttotal: 1.19s\tremaining: 10.6s\n",
      "101:\tlearn: 0.1042980\ttotal: 1.23s\tremaining: 10.8s\n",
      "102:\tlearn: 0.1040741\ttotal: 1.24s\tremaining: 10.8s\n",
      "103:\tlearn: 0.1037857\ttotal: 1.28s\tremaining: 11s\n",
      "104:\tlearn: 0.1032922\ttotal: 1.29s\tremaining: 11s\n",
      "105:\tlearn: 0.1029806\ttotal: 1.3s\tremaining: 10.9s\n",
      "106:\tlearn: 0.1028168\ttotal: 1.3s\tremaining: 10.9s\n",
      "107:\tlearn: 0.1025602\ttotal: 1.32s\tremaining: 10.9s\n",
      "108:\tlearn: 0.1021486\ttotal: 1.32s\tremaining: 10.8s\n",
      "109:\tlearn: 0.1014468\ttotal: 1.33s\tremaining: 10.8s\n",
      "110:\tlearn: 0.1012653\ttotal: 1.34s\tremaining: 10.8s\n",
      "111:\tlearn: 0.1011273\ttotal: 1.35s\tremaining: 10.7s\n",
      "112:\tlearn: 0.1008176\ttotal: 1.36s\tremaining: 10.7s\n",
      "113:\tlearn: 0.1006787\ttotal: 1.37s\tremaining: 10.7s\n",
      "114:\tlearn: 0.1001725\ttotal: 1.38s\tremaining: 10.6s\n",
      "115:\tlearn: 0.0998680\ttotal: 1.39s\tremaining: 10.6s\n",
      "116:\tlearn: 0.0995763\ttotal: 1.4s\tremaining: 10.6s\n",
      "117:\tlearn: 0.0993855\ttotal: 1.41s\tremaining: 10.5s\n",
      "118:\tlearn: 0.0991707\ttotal: 1.42s\tremaining: 10.5s\n",
      "119:\tlearn: 0.0986972\ttotal: 1.43s\tremaining: 10.5s\n",
      "120:\tlearn: 0.0984314\ttotal: 1.44s\tremaining: 10.4s\n",
      "121:\tlearn: 0.0981356\ttotal: 1.44s\tremaining: 10.4s\n",
      "122:\tlearn: 0.0979814\ttotal: 1.47s\tremaining: 10.5s\n",
      "123:\tlearn: 0.0978426\ttotal: 1.49s\tremaining: 10.5s\n",
      "124:\tlearn: 0.0977492\ttotal: 1.5s\tremaining: 10.5s\n",
      "125:\tlearn: 0.0975551\ttotal: 1.51s\tremaining: 10.5s\n",
      "126:\tlearn: 0.0971407\ttotal: 1.52s\tremaining: 10.5s\n",
      "127:\tlearn: 0.0968398\ttotal: 1.53s\tremaining: 10.4s\n",
      "128:\tlearn: 0.0965651\ttotal: 1.54s\tremaining: 10.4s\n",
      "129:\tlearn: 0.0962967\ttotal: 1.55s\tremaining: 10.4s\n",
      "130:\tlearn: 0.0961288\ttotal: 1.56s\tremaining: 10.4s\n",
      "131:\tlearn: 0.0959623\ttotal: 1.57s\tremaining: 10.3s\n",
      "132:\tlearn: 0.0956683\ttotal: 1.58s\tremaining: 10.3s\n",
      "133:\tlearn: 0.0955090\ttotal: 1.59s\tremaining: 10.3s\n",
      "134:\tlearn: 0.0953868\ttotal: 1.6s\tremaining: 10.3s\n",
      "135:\tlearn: 0.0952436\ttotal: 1.61s\tremaining: 10.2s\n",
      "136:\tlearn: 0.0950318\ttotal: 1.62s\tremaining: 10.2s\n",
      "137:\tlearn: 0.0947789\ttotal: 1.63s\tremaining: 10.2s\n",
      "138:\tlearn: 0.0946735\ttotal: 1.64s\tremaining: 10.1s\n",
      "139:\tlearn: 0.0944793\ttotal: 1.65s\tremaining: 10.1s\n",
      "140:\tlearn: 0.0943818\ttotal: 1.65s\tremaining: 10.1s\n",
      "141:\tlearn: 0.0942192\ttotal: 1.66s\tremaining: 10.1s\n",
      "142:\tlearn: 0.0941123\ttotal: 1.67s\tremaining: 10s\n",
      "143:\tlearn: 0.0939030\ttotal: 1.68s\tremaining: 10s\n",
      "144:\tlearn: 0.0937305\ttotal: 1.69s\tremaining: 9.96s\n",
      "145:\tlearn: 0.0936641\ttotal: 1.7s\tremaining: 9.93s\n",
      "146:\tlearn: 0.0935022\ttotal: 1.71s\tremaining: 9.91s\n",
      "147:\tlearn: 0.0931490\ttotal: 1.72s\tremaining: 9.89s\n",
      "148:\tlearn: 0.0930567\ttotal: 1.73s\tremaining: 9.86s\n",
      "149:\tlearn: 0.0929376\ttotal: 1.74s\tremaining: 9.84s\n",
      "150:\tlearn: 0.0928513\ttotal: 1.75s\tremaining: 9.81s\n",
      "151:\tlearn: 0.0927307\ttotal: 1.75s\tremaining: 9.78s\n",
      "152:\tlearn: 0.0925250\ttotal: 1.76s\tremaining: 9.76s\n",
      "153:\tlearn: 0.0923326\ttotal: 1.77s\tremaining: 9.73s\n",
      "154:\tlearn: 0.0922404\ttotal: 1.78s\tremaining: 9.71s\n",
      "155:\tlearn: 0.0919534\ttotal: 1.79s\tremaining: 9.69s\n",
      "156:\tlearn: 0.0918101\ttotal: 1.8s\tremaining: 9.66s\n",
      "157:\tlearn: 0.0916757\ttotal: 1.81s\tremaining: 9.65s\n",
      "158:\tlearn: 0.0915909\ttotal: 1.82s\tremaining: 9.63s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159:\tlearn: 0.0915319\ttotal: 1.83s\tremaining: 9.59s\n",
      "160:\tlearn: 0.0914265\ttotal: 1.84s\tremaining: 9.58s\n",
      "161:\tlearn: 0.0913063\ttotal: 1.85s\tremaining: 9.55s\n",
      "162:\tlearn: 0.0912392\ttotal: 1.85s\tremaining: 9.52s\n",
      "163:\tlearn: 0.0911143\ttotal: 1.86s\tremaining: 9.51s\n",
      "164:\tlearn: 0.0910040\ttotal: 1.87s\tremaining: 9.48s\n",
      "165:\tlearn: 0.0909483\ttotal: 1.88s\tremaining: 9.45s\n",
      "166:\tlearn: 0.0908742\ttotal: 1.89s\tremaining: 9.43s\n",
      "167:\tlearn: 0.0908231\ttotal: 1.9s\tremaining: 9.41s\n",
      "168:\tlearn: 0.0906985\ttotal: 1.91s\tremaining: 9.4s\n",
      "169:\tlearn: 0.0906550\ttotal: 1.92s\tremaining: 9.39s\n",
      "170:\tlearn: 0.0905911\ttotal: 1.93s\tremaining: 9.36s\n",
      "171:\tlearn: 0.0905172\ttotal: 1.94s\tremaining: 9.35s\n",
      "172:\tlearn: 0.0904518\ttotal: 1.95s\tremaining: 9.34s\n",
      "173:\tlearn: 0.0903354\ttotal: 1.96s\tremaining: 9.31s\n",
      "174:\tlearn: 0.0900922\ttotal: 1.97s\tremaining: 9.3s\n",
      "175:\tlearn: 0.0900091\ttotal: 1.99s\tremaining: 9.29s\n",
      "176:\tlearn: 0.0898601\ttotal: 2s\tremaining: 9.28s\n",
      "177:\tlearn: 0.0897850\ttotal: 2s\tremaining: 9.26s\n",
      "178:\tlearn: 0.0897004\ttotal: 2.02s\tremaining: 9.25s\n",
      "179:\tlearn: 0.0895801\ttotal: 2.02s\tremaining: 9.22s\n",
      "180:\tlearn: 0.0895483\ttotal: 2.04s\tremaining: 9.21s\n",
      "181:\tlearn: 0.0893497\ttotal: 2.04s\tremaining: 9.19s\n",
      "182:\tlearn: 0.0892580\ttotal: 2.05s\tremaining: 9.17s\n",
      "183:\tlearn: 0.0891952\ttotal: 2.07s\tremaining: 9.16s\n",
      "184:\tlearn: 0.0890981\ttotal: 2.08s\tremaining: 9.19s\n",
      "185:\tlearn: 0.0889166\ttotal: 2.12s\tremaining: 9.26s\n",
      "186:\tlearn: 0.0887857\ttotal: 2.13s\tremaining: 9.24s\n",
      "187:\tlearn: 0.0887158\ttotal: 2.13s\tremaining: 9.22s\n",
      "188:\tlearn: 0.0885965\ttotal: 2.15s\tremaining: 9.23s\n",
      "189:\tlearn: 0.0885009\ttotal: 2.17s\tremaining: 9.24s\n",
      "190:\tlearn: 0.0883007\ttotal: 2.18s\tremaining: 9.25s\n",
      "191:\tlearn: 0.0882463\ttotal: 2.21s\tremaining: 9.3s\n",
      "192:\tlearn: 0.0881877\ttotal: 2.23s\tremaining: 9.34s\n",
      "193:\tlearn: 0.0881163\ttotal: 2.27s\tremaining: 9.42s\n",
      "194:\tlearn: 0.0879709\ttotal: 2.28s\tremaining: 9.4s\n",
      "195:\tlearn: 0.0879103\ttotal: 2.29s\tremaining: 9.41s\n",
      "196:\tlearn: 0.0878113\ttotal: 2.31s\tremaining: 9.4s\n",
      "197:\tlearn: 0.0877486\ttotal: 2.31s\tremaining: 9.37s\n",
      "198:\tlearn: 0.0876126\ttotal: 2.32s\tremaining: 9.36s\n",
      "199:\tlearn: 0.0875430\ttotal: 2.34s\tremaining: 9.38s\n",
      "200:\tlearn: 0.0874886\ttotal: 2.36s\tremaining: 9.38s\n",
      "201:\tlearn: 0.0874111\ttotal: 2.37s\tremaining: 9.36s\n",
      "202:\tlearn: 0.0873603\ttotal: 2.39s\tremaining: 9.38s\n",
      "203:\tlearn: 0.0872916\ttotal: 2.4s\tremaining: 9.37s\n",
      "204:\tlearn: 0.0872519\ttotal: 2.41s\tremaining: 9.34s\n",
      "205:\tlearn: 0.0871503\ttotal: 2.42s\tremaining: 9.33s\n",
      "206:\tlearn: 0.0871131\ttotal: 2.44s\tremaining: 9.33s\n",
      "207:\tlearn: 0.0870157\ttotal: 2.44s\tremaining: 9.31s\n",
      "208:\tlearn: 0.0869624\ttotal: 2.46s\tremaining: 9.3s\n",
      "209:\tlearn: 0.0869246\ttotal: 2.46s\tremaining: 9.27s\n",
      "210:\tlearn: 0.0868657\ttotal: 2.48s\tremaining: 9.27s\n",
      "211:\tlearn: 0.0867615\ttotal: 2.49s\tremaining: 9.25s\n",
      "212:\tlearn: 0.0866687\ttotal: 2.5s\tremaining: 9.23s\n",
      "213:\tlearn: 0.0866006\ttotal: 2.51s\tremaining: 9.21s\n",
      "214:\tlearn: 0.0865361\ttotal: 2.52s\tremaining: 9.2s\n",
      "215:\tlearn: 0.0864726\ttotal: 2.53s\tremaining: 9.18s\n",
      "216:\tlearn: 0.0864280\ttotal: 2.54s\tremaining: 9.15s\n",
      "217:\tlearn: 0.0863935\ttotal: 2.55s\tremaining: 9.13s\n",
      "218:\tlearn: 0.0863289\ttotal: 2.56s\tremaining: 9.11s\n",
      "219:\tlearn: 0.0862703\ttotal: 2.57s\tremaining: 9.11s\n",
      "220:\tlearn: 0.0862394\ttotal: 2.58s\tremaining: 9.09s\n",
      "221:\tlearn: 0.0861787\ttotal: 2.59s\tremaining: 9.08s\n",
      "222:\tlearn: 0.0860675\ttotal: 2.61s\tremaining: 9.09s\n",
      "223:\tlearn: 0.0859507\ttotal: 2.62s\tremaining: 9.07s\n",
      "224:\tlearn: 0.0859007\ttotal: 2.63s\tremaining: 9.05s\n",
      "225:\tlearn: 0.0858275\ttotal: 2.64s\tremaining: 9.03s\n",
      "226:\tlearn: 0.0857840\ttotal: 2.65s\tremaining: 9.01s\n",
      "227:\tlearn: 0.0856957\ttotal: 2.65s\tremaining: 8.99s\n",
      "228:\tlearn: 0.0856202\ttotal: 2.67s\tremaining: 8.97s\n",
      "229:\tlearn: 0.0855851\ttotal: 2.67s\tremaining: 8.96s\n",
      "230:\tlearn: 0.0855391\ttotal: 2.69s\tremaining: 8.95s\n",
      "231:\tlearn: 0.0854724\ttotal: 2.7s\tremaining: 8.93s\n",
      "232:\tlearn: 0.0854528\ttotal: 2.71s\tremaining: 8.92s\n",
      "233:\tlearn: 0.0853802\ttotal: 2.72s\tremaining: 8.9s\n",
      "234:\tlearn: 0.0852997\ttotal: 2.73s\tremaining: 8.89s\n",
      "235:\tlearn: 0.0852360\ttotal: 2.74s\tremaining: 8.87s\n",
      "236:\tlearn: 0.0851119\ttotal: 2.75s\tremaining: 8.85s\n",
      "237:\tlearn: 0.0850161\ttotal: 2.76s\tremaining: 8.83s\n",
      "238:\tlearn: 0.0849791\ttotal: 2.77s\tremaining: 8.81s\n",
      "239:\tlearn: 0.0849460\ttotal: 2.78s\tremaining: 8.79s\n",
      "240:\tlearn: 0.0849178\ttotal: 2.79s\tremaining: 8.77s\n",
      "241:\tlearn: 0.0848508\ttotal: 2.79s\tremaining: 8.75s\n",
      "242:\tlearn: 0.0847163\ttotal: 2.8s\tremaining: 8.73s\n",
      "243:\tlearn: 0.0846899\ttotal: 2.81s\tremaining: 8.72s\n",
      "244:\tlearn: 0.0845522\ttotal: 2.82s\tremaining: 8.7s\n",
      "245:\tlearn: 0.0844915\ttotal: 2.83s\tremaining: 8.69s\n",
      "246:\tlearn: 0.0844491\ttotal: 2.84s\tremaining: 8.67s\n",
      "247:\tlearn: 0.0843916\ttotal: 2.85s\tremaining: 8.65s\n",
      "248:\tlearn: 0.0843071\ttotal: 2.86s\tremaining: 8.63s\n",
      "249:\tlearn: 0.0842835\ttotal: 2.87s\tremaining: 8.62s\n",
      "250:\tlearn: 0.0842108\ttotal: 2.88s\tremaining: 8.6s\n",
      "251:\tlearn: 0.0841404\ttotal: 2.89s\tremaining: 8.59s\n",
      "252:\tlearn: 0.0840881\ttotal: 2.9s\tremaining: 8.57s\n",
      "253:\tlearn: 0.0840301\ttotal: 2.92s\tremaining: 8.59s\n",
      "254:\tlearn: 0.0839590\ttotal: 2.93s\tremaining: 8.57s\n",
      "255:\tlearn: 0.0839150\ttotal: 2.94s\tremaining: 8.56s\n",
      "256:\tlearn: 0.0838344\ttotal: 2.96s\tremaining: 8.54s\n",
      "257:\tlearn: 0.0838006\ttotal: 2.96s\tremaining: 8.52s\n",
      "258:\tlearn: 0.0837758\ttotal: 2.98s\tremaining: 8.51s\n",
      "259:\tlearn: 0.0837300\ttotal: 2.98s\tremaining: 8.49s\n",
      "260:\tlearn: 0.0837054\ttotal: 3s\tremaining: 8.48s\n",
      "261:\tlearn: 0.0836458\ttotal: 3s\tremaining: 8.46s\n",
      "262:\tlearn: 0.0836142\ttotal: 3.01s\tremaining: 8.45s\n",
      "263:\tlearn: 0.0835718\ttotal: 3.02s\tremaining: 8.43s\n",
      "264:\tlearn: 0.0835306\ttotal: 3.03s\tremaining: 8.41s\n",
      "265:\tlearn: 0.0834846\ttotal: 3.05s\tremaining: 8.41s\n",
      "266:\tlearn: 0.0834434\ttotal: 3.06s\tremaining: 8.41s\n",
      "267:\tlearn: 0.0833747\ttotal: 3.07s\tremaining: 8.4s\n",
      "268:\tlearn: 0.0833255\ttotal: 3.08s\tremaining: 8.38s\n",
      "269:\tlearn: 0.0832720\ttotal: 3.09s\tremaining: 8.37s\n",
      "270:\tlearn: 0.0832113\ttotal: 3.1s\tremaining: 8.34s\n",
      "271:\tlearn: 0.0831592\ttotal: 3.11s\tremaining: 8.32s\n",
      "272:\tlearn: 0.0831174\ttotal: 3.12s\tremaining: 8.31s\n",
      "273:\tlearn: 0.0830472\ttotal: 3.13s\tremaining: 8.29s\n",
      "274:\tlearn: 0.0829884\ttotal: 3.14s\tremaining: 8.27s\n",
      "275:\tlearn: 0.0829423\ttotal: 3.15s\tremaining: 8.25s\n",
      "276:\tlearn: 0.0828627\ttotal: 3.16s\tremaining: 8.24s\n",
      "277:\tlearn: 0.0828331\ttotal: 3.17s\tremaining: 8.24s\n",
      "278:\tlearn: 0.0827463\ttotal: 3.19s\tremaining: 8.23s\n",
      "279:\tlearn: 0.0827128\ttotal: 3.21s\tremaining: 8.26s\n",
      "280:\tlearn: 0.0826266\ttotal: 3.23s\tremaining: 8.26s\n",
      "281:\tlearn: 0.0825905\ttotal: 3.24s\tremaining: 8.25s\n",
      "282:\tlearn: 0.0825360\ttotal: 3.26s\tremaining: 8.27s\n",
      "283:\tlearn: 0.0824997\ttotal: 3.27s\tremaining: 8.25s\n",
      "284:\tlearn: 0.0824679\ttotal: 3.28s\tremaining: 8.23s\n",
      "285:\tlearn: 0.0824294\ttotal: 3.29s\tremaining: 8.21s\n",
      "286:\tlearn: 0.0823911\ttotal: 3.3s\tremaining: 8.19s\n",
      "287:\tlearn: 0.0823589\ttotal: 3.31s\tremaining: 8.17s\n",
      "288:\tlearn: 0.0823260\ttotal: 3.32s\tremaining: 8.16s\n",
      "289:\tlearn: 0.0822927\ttotal: 3.33s\tremaining: 8.14s\n",
      "290:\tlearn: 0.0822530\ttotal: 3.34s\tremaining: 8.13s\n",
      "291:\tlearn: 0.0822188\ttotal: 3.35s\tremaining: 8.11s\n",
      "292:\tlearn: 0.0821828\ttotal: 3.35s\tremaining: 8.1s\n",
      "293:\tlearn: 0.0820758\ttotal: 3.37s\tremaining: 8.08s\n",
      "294:\tlearn: 0.0820184\ttotal: 3.37s\tremaining: 8.06s\n",
      "295:\tlearn: 0.0819715\ttotal: 3.38s\tremaining: 8.05s\n",
      "296:\tlearn: 0.0819473\ttotal: 3.39s\tremaining: 8.03s\n",
      "297:\tlearn: 0.0818937\ttotal: 3.4s\tremaining: 8.01s\n",
      "298:\tlearn: 0.0818759\ttotal: 3.41s\tremaining: 7.99s\n",
      "299:\tlearn: 0.0818432\ttotal: 3.42s\tremaining: 7.97s\n",
      "300:\tlearn: 0.0818094\ttotal: 3.43s\tremaining: 7.96s\n",
      "301:\tlearn: 0.0817697\ttotal: 3.44s\tremaining: 7.94s\n",
      "302:\tlearn: 0.0816695\ttotal: 3.44s\tremaining: 7.92s\n",
      "303:\tlearn: 0.0816075\ttotal: 3.45s\tremaining: 7.91s\n",
      "304:\tlearn: 0.0815466\ttotal: 3.46s\tremaining: 7.89s\n",
      "305:\tlearn: 0.0814782\ttotal: 3.47s\tremaining: 7.87s\n",
      "306:\tlearn: 0.0814316\ttotal: 3.48s\tremaining: 7.85s\n",
      "307:\tlearn: 0.0814058\ttotal: 3.49s\tremaining: 7.83s\n",
      "308:\tlearn: 0.0813635\ttotal: 3.5s\tremaining: 7.82s\n",
      "309:\tlearn: 0.0812965\ttotal: 3.5s\tremaining: 7.8s\n",
      "310:\tlearn: 0.0812370\ttotal: 3.51s\tremaining: 7.79s\n",
      "311:\tlearn: 0.0811809\ttotal: 3.52s\tremaining: 7.77s\n",
      "312:\tlearn: 0.0811070\ttotal: 3.53s\tremaining: 7.75s\n",
      "313:\tlearn: 0.0810312\ttotal: 3.54s\tremaining: 7.73s\n",
      "314:\tlearn: 0.0810061\ttotal: 3.55s\tremaining: 7.72s\n",
      "315:\tlearn: 0.0809528\ttotal: 3.56s\tremaining: 7.7s\n",
      "316:\tlearn: 0.0808906\ttotal: 3.57s\tremaining: 7.68s\n",
      "317:\tlearn: 0.0808557\ttotal: 3.58s\tremaining: 7.67s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318:\tlearn: 0.0808243\ttotal: 3.58s\tremaining: 7.65s\n",
      "319:\tlearn: 0.0807595\ttotal: 3.59s\tremaining: 7.63s\n",
      "320:\tlearn: 0.0806970\ttotal: 3.6s\tremaining: 7.62s\n",
      "321:\tlearn: 0.0806563\ttotal: 3.61s\tremaining: 7.61s\n",
      "322:\tlearn: 0.0806247\ttotal: 3.62s\tremaining: 7.59s\n",
      "323:\tlearn: 0.0805453\ttotal: 3.63s\tremaining: 7.58s\n",
      "324:\tlearn: 0.0805317\ttotal: 3.64s\tremaining: 7.56s\n",
      "325:\tlearn: 0.0804885\ttotal: 3.65s\tremaining: 7.54s\n",
      "326:\tlearn: 0.0804648\ttotal: 3.66s\tremaining: 7.53s\n",
      "327:\tlearn: 0.0804315\ttotal: 3.67s\tremaining: 7.52s\n",
      "328:\tlearn: 0.0804022\ttotal: 3.68s\tremaining: 7.5s\n",
      "329:\tlearn: 0.0803584\ttotal: 3.69s\tremaining: 7.49s\n",
      "330:\tlearn: 0.0803366\ttotal: 3.7s\tremaining: 7.48s\n",
      "331:\tlearn: 0.0802961\ttotal: 3.71s\tremaining: 7.46s\n",
      "332:\tlearn: 0.0802667\ttotal: 3.72s\tremaining: 7.45s\n",
      "333:\tlearn: 0.0802341\ttotal: 3.73s\tremaining: 7.44s\n",
      "334:\tlearn: 0.0801569\ttotal: 3.74s\tremaining: 7.43s\n",
      "335:\tlearn: 0.0801295\ttotal: 3.75s\tremaining: 7.42s\n",
      "336:\tlearn: 0.0800770\ttotal: 3.77s\tremaining: 7.41s\n",
      "337:\tlearn: 0.0800449\ttotal: 3.77s\tremaining: 7.39s\n",
      "338:\tlearn: 0.0800115\ttotal: 3.79s\tremaining: 7.38s\n",
      "339:\tlearn: 0.0799815\ttotal: 3.8s\tremaining: 7.37s\n",
      "340:\tlearn: 0.0799485\ttotal: 3.8s\tremaining: 7.35s\n",
      "341:\tlearn: 0.0799194\ttotal: 3.82s\tremaining: 7.34s\n",
      "342:\tlearn: 0.0798829\ttotal: 3.83s\tremaining: 7.33s\n",
      "343:\tlearn: 0.0798395\ttotal: 3.84s\tremaining: 7.32s\n",
      "344:\tlearn: 0.0798132\ttotal: 3.85s\tremaining: 7.31s\n",
      "345:\tlearn: 0.0797438\ttotal: 3.86s\tremaining: 7.3s\n",
      "346:\tlearn: 0.0797155\ttotal: 3.87s\tremaining: 7.28s\n",
      "347:\tlearn: 0.0796768\ttotal: 3.88s\tremaining: 7.27s\n",
      "348:\tlearn: 0.0796476\ttotal: 3.89s\tremaining: 7.25s\n",
      "349:\tlearn: 0.0796055\ttotal: 3.9s\tremaining: 7.24s\n",
      "350:\tlearn: 0.0795585\ttotal: 3.91s\tremaining: 7.23s\n",
      "351:\tlearn: 0.0795216\ttotal: 3.92s\tremaining: 7.22s\n",
      "352:\tlearn: 0.0794996\ttotal: 3.94s\tremaining: 7.22s\n",
      "353:\tlearn: 0.0794671\ttotal: 3.95s\tremaining: 7.2s\n",
      "354:\tlearn: 0.0794324\ttotal: 3.96s\tremaining: 7.19s\n",
      "355:\tlearn: 0.0794022\ttotal: 3.97s\tremaining: 7.18s\n",
      "356:\tlearn: 0.0793501\ttotal: 3.98s\tremaining: 7.16s\n",
      "357:\tlearn: 0.0793207\ttotal: 3.99s\tremaining: 7.15s\n",
      "358:\tlearn: 0.0792794\ttotal: 4s\tremaining: 7.13s\n",
      "359:\tlearn: 0.0792284\ttotal: 4.01s\tremaining: 7.13s\n",
      "360:\tlearn: 0.0791868\ttotal: 4.02s\tremaining: 7.11s\n",
      "361:\tlearn: 0.0791293\ttotal: 4.03s\tremaining: 7.1s\n",
      "362:\tlearn: 0.0790790\ttotal: 4.04s\tremaining: 7.09s\n",
      "363:\tlearn: 0.0790475\ttotal: 4.05s\tremaining: 7.07s\n",
      "364:\tlearn: 0.0790003\ttotal: 4.06s\tremaining: 7.06s\n",
      "365:\tlearn: 0.0789475\ttotal: 4.07s\tremaining: 7.05s\n",
      "366:\tlearn: 0.0789064\ttotal: 4.08s\tremaining: 7.03s\n",
      "367:\tlearn: 0.0788885\ttotal: 4.09s\tremaining: 7.02s\n",
      "368:\tlearn: 0.0788578\ttotal: 4.1s\tremaining: 7s\n",
      "369:\tlearn: 0.0788095\ttotal: 4.11s\tremaining: 6.99s\n",
      "370:\tlearn: 0.0787713\ttotal: 4.12s\tremaining: 6.98s\n",
      "371:\tlearn: 0.0787465\ttotal: 4.12s\tremaining: 6.96s\n",
      "372:\tlearn: 0.0787231\ttotal: 4.13s\tremaining: 6.95s\n",
      "373:\tlearn: 0.0786829\ttotal: 4.14s\tremaining: 6.93s\n",
      "374:\tlearn: 0.0786516\ttotal: 4.17s\tremaining: 6.95s\n",
      "375:\tlearn: 0.0786256\ttotal: 4.19s\tremaining: 6.96s\n",
      "376:\tlearn: 0.0785840\ttotal: 4.21s\tremaining: 6.96s\n",
      "377:\tlearn: 0.0785558\ttotal: 4.23s\tremaining: 6.97s\n",
      "378:\tlearn: 0.0785346\ttotal: 4.25s\tremaining: 6.96s\n",
      "379:\tlearn: 0.0785177\ttotal: 4.27s\tremaining: 6.97s\n",
      "380:\tlearn: 0.0784661\ttotal: 4.28s\tremaining: 6.95s\n",
      "381:\tlearn: 0.0784353\ttotal: 4.29s\tremaining: 6.94s\n",
      "382:\tlearn: 0.0783758\ttotal: 4.3s\tremaining: 6.93s\n",
      "383:\tlearn: 0.0783358\ttotal: 4.31s\tremaining: 6.92s\n",
      "384:\tlearn: 0.0782897\ttotal: 4.32s\tremaining: 6.91s\n",
      "385:\tlearn: 0.0782489\ttotal: 4.33s\tremaining: 6.89s\n",
      "386:\tlearn: 0.0782309\ttotal: 4.34s\tremaining: 6.88s\n",
      "387:\tlearn: 0.0782138\ttotal: 4.35s\tremaining: 6.86s\n",
      "388:\tlearn: 0.0781841\ttotal: 4.36s\tremaining: 6.85s\n",
      "389:\tlearn: 0.0781259\ttotal: 4.37s\tremaining: 6.84s\n",
      "390:\tlearn: 0.0780780\ttotal: 4.38s\tremaining: 6.82s\n",
      "391:\tlearn: 0.0780455\ttotal: 4.39s\tremaining: 6.81s\n",
      "392:\tlearn: 0.0779939\ttotal: 4.4s\tremaining: 6.8s\n",
      "393:\tlearn: 0.0779481\ttotal: 4.41s\tremaining: 6.78s\n",
      "394:\tlearn: 0.0779206\ttotal: 4.42s\tremaining: 6.77s\n",
      "395:\tlearn: 0.0778990\ttotal: 4.43s\tremaining: 6.75s\n",
      "396:\tlearn: 0.0778609\ttotal: 4.44s\tremaining: 6.74s\n",
      "397:\tlearn: 0.0778360\ttotal: 4.45s\tremaining: 6.72s\n",
      "398:\tlearn: 0.0778177\ttotal: 4.46s\tremaining: 6.71s\n",
      "399:\tlearn: 0.0777439\ttotal: 4.46s\tremaining: 6.7s\n",
      "400:\tlearn: 0.0777211\ttotal: 4.47s\tremaining: 6.68s\n",
      "401:\tlearn: 0.0777038\ttotal: 4.48s\tremaining: 6.67s\n",
      "402:\tlearn: 0.0776666\ttotal: 4.5s\tremaining: 6.66s\n",
      "403:\tlearn: 0.0776488\ttotal: 4.5s\tremaining: 6.64s\n",
      "404:\tlearn: 0.0776349\ttotal: 4.51s\tremaining: 6.63s\n",
      "405:\tlearn: 0.0776168\ttotal: 4.52s\tremaining: 6.62s\n",
      "406:\tlearn: 0.0775885\ttotal: 4.53s\tremaining: 6.6s\n",
      "407:\tlearn: 0.0775434\ttotal: 4.54s\tremaining: 6.59s\n",
      "408:\tlearn: 0.0775032\ttotal: 4.55s\tremaining: 6.58s\n",
      "409:\tlearn: 0.0774876\ttotal: 4.56s\tremaining: 6.56s\n",
      "410:\tlearn: 0.0774660\ttotal: 4.57s\tremaining: 6.55s\n",
      "411:\tlearn: 0.0774333\ttotal: 4.58s\tremaining: 6.54s\n",
      "412:\tlearn: 0.0773997\ttotal: 4.59s\tremaining: 6.53s\n",
      "413:\tlearn: 0.0773460\ttotal: 4.6s\tremaining: 6.51s\n",
      "414:\tlearn: 0.0773124\ttotal: 4.61s\tremaining: 6.5s\n",
      "415:\tlearn: 0.0772429\ttotal: 4.62s\tremaining: 6.48s\n",
      "416:\tlearn: 0.0772089\ttotal: 4.63s\tremaining: 6.47s\n",
      "417:\tlearn: 0.0771767\ttotal: 4.64s\tremaining: 6.46s\n",
      "418:\tlearn: 0.0771327\ttotal: 4.65s\tremaining: 6.45s\n",
      "419:\tlearn: 0.0770984\ttotal: 4.66s\tremaining: 6.43s\n",
      "420:\tlearn: 0.0770847\ttotal: 4.67s\tremaining: 6.42s\n",
      "421:\tlearn: 0.0770405\ttotal: 4.68s\tremaining: 6.4s\n",
      "422:\tlearn: 0.0769979\ttotal: 4.68s\tremaining: 6.39s\n",
      "423:\tlearn: 0.0769560\ttotal: 4.7s\tremaining: 6.38s\n",
      "424:\tlearn: 0.0769224\ttotal: 4.7s\tremaining: 6.36s\n",
      "425:\tlearn: 0.0768928\ttotal: 4.71s\tremaining: 6.35s\n",
      "426:\tlearn: 0.0768613\ttotal: 4.72s\tremaining: 6.34s\n",
      "427:\tlearn: 0.0768422\ttotal: 4.73s\tremaining: 6.33s\n",
      "428:\tlearn: 0.0768051\ttotal: 4.74s\tremaining: 6.31s\n",
      "429:\tlearn: 0.0767666\ttotal: 4.75s\tremaining: 6.3s\n",
      "430:\tlearn: 0.0767325\ttotal: 4.76s\tremaining: 6.28s\n",
      "431:\tlearn: 0.0767016\ttotal: 4.77s\tremaining: 6.27s\n",
      "432:\tlearn: 0.0766517\ttotal: 4.78s\tremaining: 6.26s\n",
      "433:\tlearn: 0.0766111\ttotal: 4.79s\tremaining: 6.24s\n",
      "434:\tlearn: 0.0765854\ttotal: 4.8s\tremaining: 6.23s\n",
      "435:\tlearn: 0.0765506\ttotal: 4.81s\tremaining: 6.22s\n",
      "436:\tlearn: 0.0765351\ttotal: 4.82s\tremaining: 6.21s\n",
      "437:\tlearn: 0.0765060\ttotal: 4.83s\tremaining: 6.19s\n",
      "438:\tlearn: 0.0764884\ttotal: 4.83s\tremaining: 6.18s\n",
      "439:\tlearn: 0.0764586\ttotal: 4.85s\tremaining: 6.17s\n",
      "440:\tlearn: 0.0764303\ttotal: 4.86s\tremaining: 6.16s\n",
      "441:\tlearn: 0.0763894\ttotal: 4.87s\tremaining: 6.15s\n",
      "442:\tlearn: 0.0763513\ttotal: 4.88s\tremaining: 6.13s\n",
      "443:\tlearn: 0.0763047\ttotal: 4.89s\tremaining: 6.12s\n",
      "444:\tlearn: 0.0762759\ttotal: 4.89s\tremaining: 6.11s\n",
      "445:\tlearn: 0.0762513\ttotal: 4.9s\tremaining: 6.09s\n",
      "446:\tlearn: 0.0762270\ttotal: 4.91s\tremaining: 6.08s\n",
      "447:\tlearn: 0.0761860\ttotal: 4.92s\tremaining: 6.06s\n",
      "448:\tlearn: 0.0761633\ttotal: 4.93s\tremaining: 6.04s\n",
      "449:\tlearn: 0.0761468\ttotal: 4.94s\tremaining: 6.03s\n",
      "450:\tlearn: 0.0761241\ttotal: 4.95s\tremaining: 6.02s\n",
      "451:\tlearn: 0.0760727\ttotal: 4.96s\tremaining: 6.01s\n",
      "452:\tlearn: 0.0760520\ttotal: 4.97s\tremaining: 6s\n",
      "453:\tlearn: 0.0760140\ttotal: 4.98s\tremaining: 5.98s\n",
      "454:\tlearn: 0.0759998\ttotal: 4.99s\tremaining: 5.97s\n",
      "455:\tlearn: 0.0759595\ttotal: 4.99s\tremaining: 5.96s\n",
      "456:\tlearn: 0.0759336\ttotal: 5s\tremaining: 5.95s\n",
      "457:\tlearn: 0.0759049\ttotal: 5.01s\tremaining: 5.93s\n",
      "458:\tlearn: 0.0758703\ttotal: 5.02s\tremaining: 5.92s\n",
      "459:\tlearn: 0.0758318\ttotal: 5.03s\tremaining: 5.91s\n",
      "460:\tlearn: 0.0758013\ttotal: 5.04s\tremaining: 5.89s\n",
      "461:\tlearn: 0.0757831\ttotal: 5.05s\tremaining: 5.88s\n",
      "462:\tlearn: 0.0757571\ttotal: 5.06s\tremaining: 5.87s\n",
      "463:\tlearn: 0.0757293\ttotal: 5.07s\tremaining: 5.85s\n",
      "464:\tlearn: 0.0756959\ttotal: 5.08s\tremaining: 5.84s\n",
      "465:\tlearn: 0.0756524\ttotal: 5.09s\tremaining: 5.83s\n",
      "466:\tlearn: 0.0756258\ttotal: 5.1s\tremaining: 5.82s\n",
      "467:\tlearn: 0.0755922\ttotal: 5.11s\tremaining: 5.81s\n",
      "468:\tlearn: 0.0755704\ttotal: 5.12s\tremaining: 5.79s\n",
      "469:\tlearn: 0.0755485\ttotal: 5.13s\tremaining: 5.78s\n",
      "470:\tlearn: 0.0754905\ttotal: 5.13s\tremaining: 5.77s\n",
      "471:\tlearn: 0.0754654\ttotal: 5.14s\tremaining: 5.75s\n",
      "472:\tlearn: 0.0754277\ttotal: 5.16s\tremaining: 5.75s\n",
      "473:\tlearn: 0.0753718\ttotal: 5.17s\tremaining: 5.74s\n",
      "474:\tlearn: 0.0753445\ttotal: 5.2s\tremaining: 5.75s\n",
      "475:\tlearn: 0.0753228\ttotal: 5.22s\tremaining: 5.75s\n",
      "476:\tlearn: 0.0752727\ttotal: 5.24s\tremaining: 5.75s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477:\tlearn: 0.0752524\ttotal: 5.26s\tremaining: 5.75s\n",
      "478:\tlearn: 0.0752150\ttotal: 5.27s\tremaining: 5.74s\n",
      "479:\tlearn: 0.0751810\ttotal: 5.28s\tremaining: 5.72s\n",
      "480:\tlearn: 0.0751662\ttotal: 5.29s\tremaining: 5.71s\n",
      "481:\tlearn: 0.0751355\ttotal: 5.3s\tremaining: 5.69s\n",
      "482:\tlearn: 0.0751002\ttotal: 5.31s\tremaining: 5.68s\n",
      "483:\tlearn: 0.0750820\ttotal: 5.32s\tremaining: 5.67s\n",
      "484:\tlearn: 0.0750548\ttotal: 5.33s\tremaining: 5.66s\n",
      "485:\tlearn: 0.0750350\ttotal: 5.34s\tremaining: 5.65s\n",
      "486:\tlearn: 0.0750071\ttotal: 5.35s\tremaining: 5.64s\n",
      "487:\tlearn: 0.0749921\ttotal: 5.36s\tremaining: 5.63s\n",
      "488:\tlearn: 0.0749542\ttotal: 5.38s\tremaining: 5.62s\n",
      "489:\tlearn: 0.0749331\ttotal: 5.39s\tremaining: 5.61s\n",
      "490:\tlearn: 0.0749261\ttotal: 5.4s\tremaining: 5.59s\n",
      "491:\tlearn: 0.0749011\ttotal: 5.41s\tremaining: 5.58s\n",
      "492:\tlearn: 0.0748674\ttotal: 5.42s\tremaining: 5.57s\n",
      "493:\tlearn: 0.0748506\ttotal: 5.43s\tremaining: 5.56s\n",
      "494:\tlearn: 0.0748209\ttotal: 5.44s\tremaining: 5.55s\n",
      "495:\tlearn: 0.0748085\ttotal: 5.45s\tremaining: 5.54s\n",
      "496:\tlearn: 0.0747892\ttotal: 5.46s\tremaining: 5.53s\n",
      "497:\tlearn: 0.0747641\ttotal: 5.47s\tremaining: 5.51s\n",
      "498:\tlearn: 0.0747146\ttotal: 5.48s\tremaining: 5.5s\n",
      "499:\tlearn: 0.0746873\ttotal: 5.49s\tremaining: 5.49s\n",
      "500:\tlearn: 0.0746485\ttotal: 5.5s\tremaining: 5.48s\n",
      "501:\tlearn: 0.0746201\ttotal: 5.51s\tremaining: 5.47s\n",
      "502:\tlearn: 0.0745943\ttotal: 5.53s\tremaining: 5.46s\n",
      "503:\tlearn: 0.0745549\ttotal: 5.53s\tremaining: 5.44s\n",
      "504:\tlearn: 0.0745381\ttotal: 5.54s\tremaining: 5.43s\n",
      "505:\tlearn: 0.0745175\ttotal: 5.55s\tremaining: 5.42s\n",
      "506:\tlearn: 0.0744972\ttotal: 5.56s\tremaining: 5.41s\n",
      "507:\tlearn: 0.0744566\ttotal: 5.58s\tremaining: 5.4s\n",
      "508:\tlearn: 0.0744278\ttotal: 5.58s\tremaining: 5.39s\n",
      "509:\tlearn: 0.0744130\ttotal: 5.59s\tremaining: 5.38s\n",
      "510:\tlearn: 0.0743831\ttotal: 5.61s\tremaining: 5.37s\n",
      "511:\tlearn: 0.0743600\ttotal: 5.62s\tremaining: 5.35s\n",
      "512:\tlearn: 0.0743327\ttotal: 5.63s\tremaining: 5.34s\n",
      "513:\tlearn: 0.0743061\ttotal: 5.63s\tremaining: 5.33s\n",
      "514:\tlearn: 0.0742817\ttotal: 5.64s\tremaining: 5.32s\n",
      "515:\tlearn: 0.0742594\ttotal: 5.66s\tremaining: 5.3s\n",
      "516:\tlearn: 0.0742285\ttotal: 5.66s\tremaining: 5.29s\n",
      "517:\tlearn: 0.0742128\ttotal: 5.67s\tremaining: 5.28s\n",
      "518:\tlearn: 0.0741591\ttotal: 5.68s\tremaining: 5.27s\n",
      "519:\tlearn: 0.0741469\ttotal: 5.69s\tremaining: 5.25s\n",
      "520:\tlearn: 0.0741358\ttotal: 5.71s\tremaining: 5.25s\n",
      "521:\tlearn: 0.0741219\ttotal: 5.71s\tremaining: 5.23s\n",
      "522:\tlearn: 0.0740917\ttotal: 5.72s\tremaining: 5.22s\n",
      "523:\tlearn: 0.0740647\ttotal: 5.73s\tremaining: 5.21s\n",
      "524:\tlearn: 0.0740379\ttotal: 5.74s\tremaining: 5.2s\n",
      "525:\tlearn: 0.0740085\ttotal: 5.76s\tremaining: 5.19s\n",
      "526:\tlearn: 0.0739953\ttotal: 5.76s\tremaining: 5.17s\n",
      "527:\tlearn: 0.0739708\ttotal: 5.78s\tremaining: 5.16s\n",
      "528:\tlearn: 0.0739553\ttotal: 5.79s\tremaining: 5.15s\n",
      "529:\tlearn: 0.0739252\ttotal: 5.8s\tremaining: 5.14s\n",
      "530:\tlearn: 0.0739105\ttotal: 5.8s\tremaining: 5.13s\n",
      "531:\tlearn: 0.0738862\ttotal: 5.82s\tremaining: 5.12s\n",
      "532:\tlearn: 0.0738596\ttotal: 5.83s\tremaining: 5.1s\n",
      "533:\tlearn: 0.0738384\ttotal: 5.83s\tremaining: 5.09s\n",
      "534:\tlearn: 0.0738290\ttotal: 5.84s\tremaining: 5.08s\n",
      "535:\tlearn: 0.0738062\ttotal: 5.85s\tremaining: 5.07s\n",
      "536:\tlearn: 0.0737792\ttotal: 5.86s\tremaining: 5.06s\n",
      "537:\tlearn: 0.0737560\ttotal: 5.87s\tremaining: 5.04s\n",
      "538:\tlearn: 0.0737321\ttotal: 5.88s\tremaining: 5.03s\n",
      "539:\tlearn: 0.0737032\ttotal: 5.89s\tremaining: 5.02s\n",
      "540:\tlearn: 0.0736703\ttotal: 5.9s\tremaining: 5.01s\n",
      "541:\tlearn: 0.0736406\ttotal: 5.91s\tremaining: 5s\n",
      "542:\tlearn: 0.0736194\ttotal: 5.92s\tremaining: 4.98s\n",
      "543:\tlearn: 0.0735911\ttotal: 5.93s\tremaining: 4.97s\n",
      "544:\tlearn: 0.0735702\ttotal: 5.94s\tremaining: 4.96s\n",
      "545:\tlearn: 0.0735456\ttotal: 5.95s\tremaining: 4.95s\n",
      "546:\tlearn: 0.0735288\ttotal: 5.96s\tremaining: 4.94s\n",
      "547:\tlearn: 0.0735111\ttotal: 5.97s\tremaining: 4.93s\n",
      "548:\tlearn: 0.0734872\ttotal: 5.98s\tremaining: 4.92s\n",
      "549:\tlearn: 0.0734649\ttotal: 5.99s\tremaining: 4.9s\n",
      "550:\tlearn: 0.0734364\ttotal: 6s\tremaining: 4.89s\n",
      "551:\tlearn: 0.0734224\ttotal: 6.01s\tremaining: 4.88s\n",
      "552:\tlearn: 0.0733968\ttotal: 6.02s\tremaining: 4.87s\n",
      "553:\tlearn: 0.0733738\ttotal: 6.03s\tremaining: 4.85s\n",
      "554:\tlearn: 0.0733517\ttotal: 6.04s\tremaining: 4.84s\n",
      "555:\tlearn: 0.0733208\ttotal: 6.05s\tremaining: 4.83s\n",
      "556:\tlearn: 0.0732979\ttotal: 6.06s\tremaining: 4.82s\n",
      "557:\tlearn: 0.0732842\ttotal: 6.07s\tremaining: 4.81s\n",
      "558:\tlearn: 0.0732449\ttotal: 6.08s\tremaining: 4.79s\n",
      "559:\tlearn: 0.0732294\ttotal: 6.08s\tremaining: 4.78s\n",
      "560:\tlearn: 0.0732080\ttotal: 6.09s\tremaining: 4.77s\n",
      "561:\tlearn: 0.0731929\ttotal: 6.11s\tremaining: 4.76s\n",
      "562:\tlearn: 0.0731482\ttotal: 6.11s\tremaining: 4.75s\n",
      "563:\tlearn: 0.0731292\ttotal: 6.13s\tremaining: 4.74s\n",
      "564:\tlearn: 0.0731056\ttotal: 6.13s\tremaining: 4.72s\n",
      "565:\tlearn: 0.0730877\ttotal: 6.14s\tremaining: 4.71s\n",
      "566:\tlearn: 0.0730653\ttotal: 6.15s\tremaining: 4.7s\n",
      "567:\tlearn: 0.0730480\ttotal: 6.17s\tremaining: 4.7s\n",
      "568:\tlearn: 0.0730306\ttotal: 6.19s\tremaining: 4.69s\n",
      "569:\tlearn: 0.0730058\ttotal: 6.21s\tremaining: 4.68s\n",
      "570:\tlearn: 0.0729862\ttotal: 6.22s\tremaining: 4.68s\n",
      "571:\tlearn: 0.0729542\ttotal: 6.25s\tremaining: 4.67s\n",
      "572:\tlearn: 0.0729321\ttotal: 6.26s\tremaining: 4.67s\n",
      "573:\tlearn: 0.0729146\ttotal: 6.28s\tremaining: 4.66s\n",
      "574:\tlearn: 0.0728743\ttotal: 6.29s\tremaining: 4.65s\n",
      "575:\tlearn: 0.0728624\ttotal: 6.3s\tremaining: 4.63s\n",
      "576:\tlearn: 0.0728458\ttotal: 6.32s\tremaining: 4.64s\n",
      "577:\tlearn: 0.0728311\ttotal: 6.33s\tremaining: 4.62s\n",
      "578:\tlearn: 0.0728125\ttotal: 6.34s\tremaining: 4.61s\n",
      "579:\tlearn: 0.0727776\ttotal: 6.35s\tremaining: 4.6s\n",
      "580:\tlearn: 0.0727463\ttotal: 6.36s\tremaining: 4.59s\n",
      "581:\tlearn: 0.0727123\ttotal: 6.37s\tremaining: 4.58s\n",
      "582:\tlearn: 0.0726922\ttotal: 6.38s\tremaining: 4.57s\n",
      "583:\tlearn: 0.0726585\ttotal: 6.39s\tremaining: 4.55s\n",
      "584:\tlearn: 0.0726421\ttotal: 6.4s\tremaining: 4.54s\n",
      "585:\tlearn: 0.0726312\ttotal: 6.41s\tremaining: 4.53s\n",
      "586:\tlearn: 0.0726041\ttotal: 6.42s\tremaining: 4.52s\n",
      "587:\tlearn: 0.0725862\ttotal: 6.43s\tremaining: 4.5s\n",
      "588:\tlearn: 0.0725356\ttotal: 6.44s\tremaining: 4.49s\n",
      "589:\tlearn: 0.0725033\ttotal: 6.45s\tremaining: 4.48s\n",
      "590:\tlearn: 0.0724900\ttotal: 6.46s\tremaining: 4.47s\n",
      "591:\tlearn: 0.0724579\ttotal: 6.47s\tremaining: 4.46s\n",
      "592:\tlearn: 0.0724310\ttotal: 6.47s\tremaining: 4.44s\n",
      "593:\tlearn: 0.0724095\ttotal: 6.5s\tremaining: 4.44s\n",
      "594:\tlearn: 0.0723978\ttotal: 6.5s\tremaining: 4.43s\n",
      "595:\tlearn: 0.0723769\ttotal: 6.51s\tremaining: 4.42s\n",
      "596:\tlearn: 0.0723325\ttotal: 6.52s\tremaining: 4.4s\n",
      "597:\tlearn: 0.0723098\ttotal: 6.53s\tremaining: 4.39s\n",
      "598:\tlearn: 0.0722961\ttotal: 6.54s\tremaining: 4.38s\n",
      "599:\tlearn: 0.0722704\ttotal: 6.55s\tremaining: 4.37s\n",
      "600:\tlearn: 0.0722410\ttotal: 6.56s\tremaining: 4.36s\n",
      "601:\tlearn: 0.0722277\ttotal: 6.57s\tremaining: 4.34s\n",
      "602:\tlearn: 0.0722141\ttotal: 6.58s\tremaining: 4.33s\n",
      "603:\tlearn: 0.0721930\ttotal: 6.59s\tremaining: 4.32s\n",
      "604:\tlearn: 0.0721834\ttotal: 6.6s\tremaining: 4.31s\n",
      "605:\tlearn: 0.0721592\ttotal: 6.61s\tremaining: 4.3s\n",
      "606:\tlearn: 0.0721249\ttotal: 6.62s\tremaining: 4.29s\n",
      "607:\tlearn: 0.0720977\ttotal: 6.63s\tremaining: 4.27s\n",
      "608:\tlearn: 0.0720836\ttotal: 6.64s\tremaining: 4.26s\n",
      "609:\tlearn: 0.0720404\ttotal: 6.65s\tremaining: 4.25s\n",
      "610:\tlearn: 0.0720148\ttotal: 6.66s\tremaining: 4.24s\n",
      "611:\tlearn: 0.0720005\ttotal: 6.66s\tremaining: 4.22s\n",
      "612:\tlearn: 0.0719809\ttotal: 6.67s\tremaining: 4.21s\n",
      "613:\tlearn: 0.0719602\ttotal: 6.68s\tremaining: 4.2s\n",
      "614:\tlearn: 0.0719422\ttotal: 6.69s\tremaining: 4.19s\n",
      "615:\tlearn: 0.0719274\ttotal: 6.7s\tremaining: 4.18s\n",
      "616:\tlearn: 0.0719113\ttotal: 6.71s\tremaining: 4.16s\n",
      "617:\tlearn: 0.0718888\ttotal: 6.72s\tremaining: 4.15s\n",
      "618:\tlearn: 0.0718344\ttotal: 6.73s\tremaining: 4.14s\n",
      "619:\tlearn: 0.0718228\ttotal: 6.73s\tremaining: 4.13s\n",
      "620:\tlearn: 0.0718038\ttotal: 6.74s\tremaining: 4.12s\n",
      "621:\tlearn: 0.0717787\ttotal: 6.75s\tremaining: 4.1s\n",
      "622:\tlearn: 0.0717634\ttotal: 6.76s\tremaining: 4.09s\n",
      "623:\tlearn: 0.0717496\ttotal: 6.77s\tremaining: 4.08s\n",
      "624:\tlearn: 0.0717190\ttotal: 6.78s\tremaining: 4.07s\n",
      "625:\tlearn: 0.0717021\ttotal: 6.79s\tremaining: 4.06s\n",
      "626:\tlearn: 0.0716867\ttotal: 6.8s\tremaining: 4.04s\n",
      "627:\tlearn: 0.0716794\ttotal: 6.8s\tremaining: 4.03s\n",
      "628:\tlearn: 0.0716592\ttotal: 6.82s\tremaining: 4.02s\n",
      "629:\tlearn: 0.0716305\ttotal: 6.82s\tremaining: 4.01s\n",
      "630:\tlearn: 0.0716103\ttotal: 6.83s\tremaining: 4s\n",
      "631:\tlearn: 0.0716002\ttotal: 6.84s\tremaining: 3.98s\n",
      "632:\tlearn: 0.0715759\ttotal: 6.85s\tremaining: 3.97s\n",
      "633:\tlearn: 0.0715556\ttotal: 6.86s\tremaining: 3.96s\n",
      "634:\tlearn: 0.0715487\ttotal: 6.87s\tremaining: 3.95s\n",
      "635:\tlearn: 0.0715280\ttotal: 6.88s\tremaining: 3.94s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636:\tlearn: 0.0715199\ttotal: 6.89s\tremaining: 3.93s\n",
      "637:\tlearn: 0.0715122\ttotal: 6.9s\tremaining: 3.91s\n",
      "638:\tlearn: 0.0714770\ttotal: 6.91s\tremaining: 3.9s\n",
      "639:\tlearn: 0.0714666\ttotal: 6.92s\tremaining: 3.89s\n",
      "640:\tlearn: 0.0714524\ttotal: 6.92s\tremaining: 3.88s\n",
      "641:\tlearn: 0.0714275\ttotal: 6.93s\tremaining: 3.87s\n",
      "642:\tlearn: 0.0714079\ttotal: 6.94s\tremaining: 3.85s\n",
      "643:\tlearn: 0.0713614\ttotal: 6.95s\tremaining: 3.84s\n",
      "644:\tlearn: 0.0713371\ttotal: 6.96s\tremaining: 3.83s\n",
      "645:\tlearn: 0.0713179\ttotal: 6.97s\tremaining: 3.82s\n",
      "646:\tlearn: 0.0713077\ttotal: 6.98s\tremaining: 3.81s\n",
      "647:\tlearn: 0.0712988\ttotal: 7s\tremaining: 3.8s\n",
      "648:\tlearn: 0.0712723\ttotal: 7.01s\tremaining: 3.79s\n",
      "649:\tlearn: 0.0712580\ttotal: 7.02s\tremaining: 3.78s\n",
      "650:\tlearn: 0.0712440\ttotal: 7.03s\tremaining: 3.77s\n",
      "651:\tlearn: 0.0712278\ttotal: 7.04s\tremaining: 3.76s\n",
      "652:\tlearn: 0.0712059\ttotal: 7.05s\tremaining: 3.75s\n",
      "653:\tlearn: 0.0711851\ttotal: 7.06s\tremaining: 3.73s\n",
      "654:\tlearn: 0.0711546\ttotal: 7.07s\tremaining: 3.72s\n",
      "655:\tlearn: 0.0711428\ttotal: 7.08s\tremaining: 3.71s\n",
      "656:\tlearn: 0.0711269\ttotal: 7.09s\tremaining: 3.7s\n",
      "657:\tlearn: 0.0711076\ttotal: 7.1s\tremaining: 3.69s\n",
      "658:\tlearn: 0.0710928\ttotal: 7.11s\tremaining: 3.68s\n",
      "659:\tlearn: 0.0710765\ttotal: 7.12s\tremaining: 3.67s\n",
      "660:\tlearn: 0.0710469\ttotal: 7.13s\tremaining: 3.66s\n",
      "661:\tlearn: 0.0710298\ttotal: 7.14s\tremaining: 3.65s\n",
      "662:\tlearn: 0.0710042\ttotal: 7.15s\tremaining: 3.63s\n",
      "663:\tlearn: 0.0709885\ttotal: 7.16s\tremaining: 3.63s\n",
      "664:\tlearn: 0.0709625\ttotal: 7.19s\tremaining: 3.62s\n",
      "665:\tlearn: 0.0709446\ttotal: 7.21s\tremaining: 3.62s\n",
      "666:\tlearn: 0.0709067\ttotal: 7.23s\tremaining: 3.61s\n",
      "667:\tlearn: 0.0708867\ttotal: 7.25s\tremaining: 3.6s\n",
      "668:\tlearn: 0.0708711\ttotal: 7.26s\tremaining: 3.59s\n",
      "669:\tlearn: 0.0708472\ttotal: 7.29s\tremaining: 3.59s\n",
      "670:\tlearn: 0.0707908\ttotal: 7.29s\tremaining: 3.58s\n",
      "671:\tlearn: 0.0707649\ttotal: 7.3s\tremaining: 3.56s\n",
      "672:\tlearn: 0.0707448\ttotal: 7.32s\tremaining: 3.55s\n",
      "673:\tlearn: 0.0707190\ttotal: 7.32s\tremaining: 3.54s\n",
      "674:\tlearn: 0.0707050\ttotal: 7.33s\tremaining: 3.53s\n",
      "675:\tlearn: 0.0706971\ttotal: 7.34s\tremaining: 3.52s\n",
      "676:\tlearn: 0.0706772\ttotal: 7.35s\tremaining: 3.51s\n",
      "677:\tlearn: 0.0706609\ttotal: 7.36s\tremaining: 3.5s\n",
      "678:\tlearn: 0.0706546\ttotal: 7.37s\tremaining: 3.48s\n",
      "679:\tlearn: 0.0706116\ttotal: 7.38s\tremaining: 3.47s\n",
      "680:\tlearn: 0.0705925\ttotal: 7.39s\tremaining: 3.46s\n",
      "681:\tlearn: 0.0705588\ttotal: 7.4s\tremaining: 3.45s\n",
      "682:\tlearn: 0.0705386\ttotal: 7.41s\tremaining: 3.44s\n",
      "683:\tlearn: 0.0705207\ttotal: 7.42s\tremaining: 3.43s\n",
      "684:\tlearn: 0.0704947\ttotal: 7.43s\tremaining: 3.42s\n",
      "685:\tlearn: 0.0704834\ttotal: 7.44s\tremaining: 3.41s\n",
      "686:\tlearn: 0.0704484\ttotal: 7.46s\tremaining: 3.4s\n",
      "687:\tlearn: 0.0704302\ttotal: 7.46s\tremaining: 3.38s\n",
      "688:\tlearn: 0.0704061\ttotal: 7.48s\tremaining: 3.37s\n",
      "689:\tlearn: 0.0703831\ttotal: 7.48s\tremaining: 3.36s\n",
      "690:\tlearn: 0.0703535\ttotal: 7.5s\tremaining: 3.35s\n",
      "691:\tlearn: 0.0703402\ttotal: 7.5s\tremaining: 3.34s\n",
      "692:\tlearn: 0.0703215\ttotal: 7.51s\tremaining: 3.33s\n",
      "693:\tlearn: 0.0703006\ttotal: 7.52s\tremaining: 3.32s\n",
      "694:\tlearn: 0.0702849\ttotal: 7.53s\tremaining: 3.31s\n",
      "695:\tlearn: 0.0702529\ttotal: 7.54s\tremaining: 3.29s\n",
      "696:\tlearn: 0.0702425\ttotal: 7.55s\tremaining: 3.28s\n",
      "697:\tlearn: 0.0702196\ttotal: 7.56s\tremaining: 3.27s\n",
      "698:\tlearn: 0.0702019\ttotal: 7.58s\tremaining: 3.26s\n",
      "699:\tlearn: 0.0701832\ttotal: 7.6s\tremaining: 3.26s\n",
      "700:\tlearn: 0.0701590\ttotal: 7.61s\tremaining: 3.25s\n",
      "701:\tlearn: 0.0701330\ttotal: 7.62s\tremaining: 3.23s\n",
      "702:\tlearn: 0.0701182\ttotal: 7.63s\tremaining: 3.22s\n",
      "703:\tlearn: 0.0701003\ttotal: 7.65s\tremaining: 3.22s\n",
      "704:\tlearn: 0.0700751\ttotal: 7.66s\tremaining: 3.2s\n",
      "705:\tlearn: 0.0700434\ttotal: 7.67s\tremaining: 3.19s\n",
      "706:\tlearn: 0.0700167\ttotal: 7.68s\tremaining: 3.18s\n",
      "707:\tlearn: 0.0699876\ttotal: 7.7s\tremaining: 3.17s\n",
      "708:\tlearn: 0.0699662\ttotal: 7.7s\tremaining: 3.16s\n",
      "709:\tlearn: 0.0699414\ttotal: 7.71s\tremaining: 3.15s\n",
      "710:\tlearn: 0.0699123\ttotal: 7.72s\tremaining: 3.14s\n",
      "711:\tlearn: 0.0698994\ttotal: 7.74s\tremaining: 3.13s\n",
      "712:\tlearn: 0.0698894\ttotal: 7.75s\tremaining: 3.12s\n",
      "713:\tlearn: 0.0698726\ttotal: 7.76s\tremaining: 3.11s\n",
      "714:\tlearn: 0.0698494\ttotal: 7.77s\tremaining: 3.1s\n",
      "715:\tlearn: 0.0698303\ttotal: 7.78s\tremaining: 3.09s\n",
      "716:\tlearn: 0.0698132\ttotal: 7.79s\tremaining: 3.08s\n",
      "717:\tlearn: 0.0697972\ttotal: 7.8s\tremaining: 3.06s\n",
      "718:\tlearn: 0.0697833\ttotal: 7.81s\tremaining: 3.05s\n",
      "719:\tlearn: 0.0697648\ttotal: 7.82s\tremaining: 3.04s\n",
      "720:\tlearn: 0.0697093\ttotal: 7.83s\tremaining: 3.03s\n",
      "721:\tlearn: 0.0696886\ttotal: 7.84s\tremaining: 3.02s\n",
      "722:\tlearn: 0.0696789\ttotal: 7.85s\tremaining: 3.01s\n",
      "723:\tlearn: 0.0696613\ttotal: 7.86s\tremaining: 2.99s\n",
      "724:\tlearn: 0.0696311\ttotal: 7.86s\tremaining: 2.98s\n",
      "725:\tlearn: 0.0696140\ttotal: 7.88s\tremaining: 2.97s\n",
      "726:\tlearn: 0.0695956\ttotal: 7.88s\tremaining: 2.96s\n",
      "727:\tlearn: 0.0695861\ttotal: 7.89s\tremaining: 2.95s\n",
      "728:\tlearn: 0.0695692\ttotal: 7.9s\tremaining: 2.94s\n",
      "729:\tlearn: 0.0695479\ttotal: 7.91s\tremaining: 2.93s\n",
      "730:\tlearn: 0.0695297\ttotal: 7.92s\tremaining: 2.92s\n",
      "731:\tlearn: 0.0695154\ttotal: 7.93s\tremaining: 2.9s\n",
      "732:\tlearn: 0.0694958\ttotal: 7.94s\tremaining: 2.89s\n",
      "733:\tlearn: 0.0694809\ttotal: 7.95s\tremaining: 2.88s\n",
      "734:\tlearn: 0.0694684\ttotal: 7.96s\tremaining: 2.87s\n",
      "735:\tlearn: 0.0694529\ttotal: 7.96s\tremaining: 2.86s\n",
      "736:\tlearn: 0.0694227\ttotal: 7.97s\tremaining: 2.85s\n",
      "737:\tlearn: 0.0694030\ttotal: 7.98s\tremaining: 2.83s\n",
      "738:\tlearn: 0.0693851\ttotal: 7.99s\tremaining: 2.82s\n",
      "739:\tlearn: 0.0693659\ttotal: 8s\tremaining: 2.81s\n",
      "740:\tlearn: 0.0693529\ttotal: 8.01s\tremaining: 2.8s\n",
      "741:\tlearn: 0.0693135\ttotal: 8.02s\tremaining: 2.79s\n",
      "742:\tlearn: 0.0692944\ttotal: 8.02s\tremaining: 2.77s\n",
      "743:\tlearn: 0.0692588\ttotal: 8.03s\tremaining: 2.76s\n",
      "744:\tlearn: 0.0692408\ttotal: 8.04s\tremaining: 2.75s\n",
      "745:\tlearn: 0.0692246\ttotal: 8.05s\tremaining: 2.74s\n",
      "746:\tlearn: 0.0692077\ttotal: 8.06s\tremaining: 2.73s\n",
      "747:\tlearn: 0.0691921\ttotal: 8.07s\tremaining: 2.72s\n",
      "748:\tlearn: 0.0691745\ttotal: 8.08s\tremaining: 2.71s\n",
      "749:\tlearn: 0.0691428\ttotal: 8.09s\tremaining: 2.69s\n",
      "750:\tlearn: 0.0691237\ttotal: 8.09s\tremaining: 2.68s\n",
      "751:\tlearn: 0.0690933\ttotal: 8.1s\tremaining: 2.67s\n",
      "752:\tlearn: 0.0690706\ttotal: 8.11s\tremaining: 2.66s\n",
      "753:\tlearn: 0.0690499\ttotal: 8.12s\tremaining: 2.65s\n",
      "754:\tlearn: 0.0690383\ttotal: 8.13s\tremaining: 2.64s\n",
      "755:\tlearn: 0.0690251\ttotal: 8.14s\tremaining: 2.63s\n",
      "756:\tlearn: 0.0689987\ttotal: 8.15s\tremaining: 2.62s\n",
      "757:\tlearn: 0.0689772\ttotal: 8.15s\tremaining: 2.6s\n",
      "758:\tlearn: 0.0689548\ttotal: 8.16s\tremaining: 2.59s\n",
      "759:\tlearn: 0.0689436\ttotal: 8.18s\tremaining: 2.58s\n",
      "760:\tlearn: 0.0689216\ttotal: 8.19s\tremaining: 2.57s\n",
      "761:\tlearn: 0.0688968\ttotal: 8.22s\tremaining: 2.57s\n",
      "762:\tlearn: 0.0688886\ttotal: 8.24s\tremaining: 2.56s\n",
      "763:\tlearn: 0.0688640\ttotal: 8.26s\tremaining: 2.55s\n",
      "764:\tlearn: 0.0688433\ttotal: 8.28s\tremaining: 2.54s\n",
      "765:\tlearn: 0.0688277\ttotal: 8.29s\tremaining: 2.53s\n",
      "766:\tlearn: 0.0688043\ttotal: 8.3s\tremaining: 2.52s\n",
      "767:\tlearn: 0.0687797\ttotal: 8.3s\tremaining: 2.51s\n",
      "768:\tlearn: 0.0687594\ttotal: 8.31s\tremaining: 2.5s\n",
      "769:\tlearn: 0.0687482\ttotal: 8.32s\tremaining: 2.48s\n",
      "770:\tlearn: 0.0687318\ttotal: 8.33s\tremaining: 2.47s\n",
      "771:\tlearn: 0.0687126\ttotal: 8.34s\tremaining: 2.46s\n",
      "772:\tlearn: 0.0686944\ttotal: 8.35s\tremaining: 2.45s\n",
      "773:\tlearn: 0.0686826\ttotal: 8.36s\tremaining: 2.44s\n",
      "774:\tlearn: 0.0686696\ttotal: 8.37s\tremaining: 2.43s\n",
      "775:\tlearn: 0.0686573\ttotal: 8.38s\tremaining: 2.42s\n",
      "776:\tlearn: 0.0686534\ttotal: 8.38s\tremaining: 2.41s\n",
      "777:\tlearn: 0.0686336\ttotal: 8.39s\tremaining: 2.39s\n",
      "778:\tlearn: 0.0686213\ttotal: 8.4s\tremaining: 2.38s\n",
      "779:\tlearn: 0.0685976\ttotal: 8.41s\tremaining: 2.37s\n",
      "780:\tlearn: 0.0685778\ttotal: 8.42s\tremaining: 2.36s\n",
      "781:\tlearn: 0.0685674\ttotal: 8.43s\tremaining: 2.35s\n",
      "782:\tlearn: 0.0685515\ttotal: 8.43s\tremaining: 2.34s\n",
      "783:\tlearn: 0.0685329\ttotal: 8.44s\tremaining: 2.33s\n",
      "784:\tlearn: 0.0685127\ttotal: 8.45s\tremaining: 2.31s\n",
      "785:\tlearn: 0.0684922\ttotal: 8.46s\tremaining: 2.3s\n",
      "786:\tlearn: 0.0684700\ttotal: 8.47s\tremaining: 2.29s\n",
      "787:\tlearn: 0.0684539\ttotal: 8.48s\tremaining: 2.28s\n",
      "788:\tlearn: 0.0684409\ttotal: 8.49s\tremaining: 2.27s\n",
      "789:\tlearn: 0.0684258\ttotal: 8.49s\tremaining: 2.26s\n",
      "790:\tlearn: 0.0684053\ttotal: 8.5s\tremaining: 2.25s\n",
      "791:\tlearn: 0.0683946\ttotal: 8.51s\tremaining: 2.23s\n",
      "792:\tlearn: 0.0683807\ttotal: 8.52s\tremaining: 2.22s\n",
      "793:\tlearn: 0.0683648\ttotal: 8.53s\tremaining: 2.21s\n",
      "794:\tlearn: 0.0683454\ttotal: 8.54s\tremaining: 2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795:\tlearn: 0.0683351\ttotal: 8.54s\tremaining: 2.19s\n",
      "796:\tlearn: 0.0683107\ttotal: 8.55s\tremaining: 2.18s\n",
      "797:\tlearn: 0.0682957\ttotal: 8.56s\tremaining: 2.17s\n",
      "798:\tlearn: 0.0682756\ttotal: 8.57s\tremaining: 2.15s\n",
      "799:\tlearn: 0.0682584\ttotal: 8.57s\tremaining: 2.14s\n",
      "800:\tlearn: 0.0682353\ttotal: 8.58s\tremaining: 2.13s\n",
      "801:\tlearn: 0.0682233\ttotal: 8.59s\tremaining: 2.12s\n",
      "802:\tlearn: 0.0682085\ttotal: 8.6s\tremaining: 2.11s\n",
      "803:\tlearn: 0.0682005\ttotal: 8.61s\tremaining: 2.1s\n",
      "804:\tlearn: 0.0681869\ttotal: 8.62s\tremaining: 2.09s\n",
      "805:\tlearn: 0.0681762\ttotal: 8.63s\tremaining: 2.08s\n",
      "806:\tlearn: 0.0681540\ttotal: 8.64s\tremaining: 2.07s\n",
      "807:\tlearn: 0.0681377\ttotal: 8.65s\tremaining: 2.06s\n",
      "808:\tlearn: 0.0681242\ttotal: 8.66s\tremaining: 2.04s\n",
      "809:\tlearn: 0.0681063\ttotal: 8.67s\tremaining: 2.03s\n",
      "810:\tlearn: 0.0680958\ttotal: 8.68s\tremaining: 2.02s\n",
      "811:\tlearn: 0.0680769\ttotal: 8.69s\tremaining: 2.01s\n",
      "812:\tlearn: 0.0680622\ttotal: 8.7s\tremaining: 2s\n",
      "813:\tlearn: 0.0680445\ttotal: 8.71s\tremaining: 1.99s\n",
      "814:\tlearn: 0.0680320\ttotal: 8.72s\tremaining: 1.98s\n",
      "815:\tlearn: 0.0680083\ttotal: 8.73s\tremaining: 1.97s\n",
      "816:\tlearn: 0.0679878\ttotal: 8.73s\tremaining: 1.96s\n",
      "817:\tlearn: 0.0679754\ttotal: 8.75s\tremaining: 1.95s\n",
      "818:\tlearn: 0.0679654\ttotal: 8.76s\tremaining: 1.93s\n",
      "819:\tlearn: 0.0679563\ttotal: 8.76s\tremaining: 1.92s\n",
      "820:\tlearn: 0.0679405\ttotal: 8.77s\tremaining: 1.91s\n",
      "821:\tlearn: 0.0679276\ttotal: 8.79s\tremaining: 1.9s\n",
      "822:\tlearn: 0.0679137\ttotal: 8.79s\tremaining: 1.89s\n",
      "823:\tlearn: 0.0679013\ttotal: 8.8s\tremaining: 1.88s\n",
      "824:\tlearn: 0.0678812\ttotal: 8.81s\tremaining: 1.87s\n",
      "825:\tlearn: 0.0678739\ttotal: 8.82s\tremaining: 1.86s\n",
      "826:\tlearn: 0.0678352\ttotal: 8.84s\tremaining: 1.85s\n",
      "827:\tlearn: 0.0678246\ttotal: 8.84s\tremaining: 1.84s\n",
      "828:\tlearn: 0.0677972\ttotal: 8.85s\tremaining: 1.83s\n",
      "829:\tlearn: 0.0677804\ttotal: 8.86s\tremaining: 1.81s\n",
      "830:\tlearn: 0.0677675\ttotal: 8.87s\tremaining: 1.8s\n",
      "831:\tlearn: 0.0677526\ttotal: 8.88s\tremaining: 1.79s\n",
      "832:\tlearn: 0.0677426\ttotal: 8.89s\tremaining: 1.78s\n",
      "833:\tlearn: 0.0677272\ttotal: 8.9s\tremaining: 1.77s\n",
      "834:\tlearn: 0.0677085\ttotal: 8.91s\tremaining: 1.76s\n",
      "835:\tlearn: 0.0677020\ttotal: 8.92s\tremaining: 1.75s\n",
      "836:\tlearn: 0.0676839\ttotal: 8.93s\tremaining: 1.74s\n",
      "837:\tlearn: 0.0676666\ttotal: 8.94s\tremaining: 1.73s\n",
      "838:\tlearn: 0.0676422\ttotal: 8.95s\tremaining: 1.72s\n",
      "839:\tlearn: 0.0676183\ttotal: 8.95s\tremaining: 1.71s\n",
      "840:\tlearn: 0.0676025\ttotal: 8.96s\tremaining: 1.69s\n",
      "841:\tlearn: 0.0675858\ttotal: 8.97s\tremaining: 1.68s\n",
      "842:\tlearn: 0.0675796\ttotal: 8.98s\tremaining: 1.67s\n",
      "843:\tlearn: 0.0675538\ttotal: 8.99s\tremaining: 1.66s\n",
      "844:\tlearn: 0.0675308\ttotal: 9s\tremaining: 1.65s\n",
      "845:\tlearn: 0.0675063\ttotal: 9.01s\tremaining: 1.64s\n",
      "846:\tlearn: 0.0674535\ttotal: 9.02s\tremaining: 1.63s\n",
      "847:\tlearn: 0.0674450\ttotal: 9.03s\tremaining: 1.62s\n",
      "848:\tlearn: 0.0674280\ttotal: 9.04s\tremaining: 1.61s\n",
      "849:\tlearn: 0.0674186\ttotal: 9.06s\tremaining: 1.6s\n",
      "850:\tlearn: 0.0674017\ttotal: 9.06s\tremaining: 1.59s\n",
      "851:\tlearn: 0.0673954\ttotal: 9.07s\tremaining: 1.58s\n",
      "852:\tlearn: 0.0673876\ttotal: 9.08s\tremaining: 1.56s\n",
      "853:\tlearn: 0.0673628\ttotal: 9.09s\tremaining: 1.55s\n",
      "854:\tlearn: 0.0673506\ttotal: 9.1s\tremaining: 1.54s\n",
      "855:\tlearn: 0.0673350\ttotal: 9.11s\tremaining: 1.53s\n",
      "856:\tlearn: 0.0673050\ttotal: 9.12s\tremaining: 1.52s\n",
      "857:\tlearn: 0.0672852\ttotal: 9.13s\tremaining: 1.51s\n",
      "858:\tlearn: 0.0672686\ttotal: 9.13s\tremaining: 1.5s\n",
      "859:\tlearn: 0.0672440\ttotal: 9.14s\tremaining: 1.49s\n",
      "860:\tlearn: 0.0672403\ttotal: 9.17s\tremaining: 1.48s\n",
      "861:\tlearn: 0.0672122\ttotal: 9.24s\tremaining: 1.48s\n",
      "862:\tlearn: 0.0671991\ttotal: 9.25s\tremaining: 1.47s\n",
      "863:\tlearn: 0.0671760\ttotal: 9.29s\tremaining: 1.46s\n",
      "864:\tlearn: 0.0671672\ttotal: 9.3s\tremaining: 1.45s\n",
      "865:\tlearn: 0.0671576\ttotal: 9.31s\tremaining: 1.44s\n",
      "866:\tlearn: 0.0671508\ttotal: 9.32s\tremaining: 1.43s\n",
      "867:\tlearn: 0.0671358\ttotal: 9.33s\tremaining: 1.42s\n",
      "868:\tlearn: 0.0671168\ttotal: 9.34s\tremaining: 1.41s\n",
      "869:\tlearn: 0.0670986\ttotal: 9.34s\tremaining: 1.4s\n",
      "870:\tlearn: 0.0670749\ttotal: 9.35s\tremaining: 1.39s\n",
      "871:\tlearn: 0.0670501\ttotal: 9.36s\tremaining: 1.37s\n",
      "872:\tlearn: 0.0670360\ttotal: 9.37s\tremaining: 1.36s\n",
      "873:\tlearn: 0.0670196\ttotal: 9.38s\tremaining: 1.35s\n",
      "874:\tlearn: 0.0670017\ttotal: 9.38s\tremaining: 1.34s\n",
      "875:\tlearn: 0.0669860\ttotal: 9.4s\tremaining: 1.33s\n",
      "876:\tlearn: 0.0669772\ttotal: 9.4s\tremaining: 1.32s\n",
      "877:\tlearn: 0.0669656\ttotal: 9.41s\tremaining: 1.31s\n",
      "878:\tlearn: 0.0669285\ttotal: 9.42s\tremaining: 1.3s\n",
      "879:\tlearn: 0.0668983\ttotal: 9.43s\tremaining: 1.29s\n",
      "880:\tlearn: 0.0668782\ttotal: 9.44s\tremaining: 1.27s\n",
      "881:\tlearn: 0.0668603\ttotal: 9.45s\tremaining: 1.26s\n",
      "882:\tlearn: 0.0668380\ttotal: 9.46s\tremaining: 1.25s\n",
      "883:\tlearn: 0.0668228\ttotal: 9.47s\tremaining: 1.24s\n",
      "884:\tlearn: 0.0667927\ttotal: 9.48s\tremaining: 1.23s\n",
      "885:\tlearn: 0.0667841\ttotal: 9.49s\tremaining: 1.22s\n",
      "886:\tlearn: 0.0667622\ttotal: 9.5s\tremaining: 1.21s\n",
      "887:\tlearn: 0.0667476\ttotal: 9.51s\tremaining: 1.2s\n",
      "888:\tlearn: 0.0667228\ttotal: 9.52s\tremaining: 1.19s\n",
      "889:\tlearn: 0.0666951\ttotal: 9.53s\tremaining: 1.18s\n",
      "890:\tlearn: 0.0666642\ttotal: 9.54s\tremaining: 1.17s\n",
      "891:\tlearn: 0.0666529\ttotal: 9.55s\tremaining: 1.16s\n",
      "892:\tlearn: 0.0666391\ttotal: 9.56s\tremaining: 1.15s\n",
      "893:\tlearn: 0.0666149\ttotal: 9.57s\tremaining: 1.13s\n",
      "894:\tlearn: 0.0666067\ttotal: 9.58s\tremaining: 1.12s\n",
      "895:\tlearn: 0.0665941\ttotal: 9.59s\tremaining: 1.11s\n",
      "896:\tlearn: 0.0665791\ttotal: 9.6s\tremaining: 1.1s\n",
      "897:\tlearn: 0.0665608\ttotal: 9.61s\tremaining: 1.09s\n",
      "898:\tlearn: 0.0665422\ttotal: 9.62s\tremaining: 1.08s\n",
      "899:\tlearn: 0.0665122\ttotal: 9.63s\tremaining: 1.07s\n",
      "900:\tlearn: 0.0665005\ttotal: 9.63s\tremaining: 1.06s\n",
      "901:\tlearn: 0.0664861\ttotal: 9.64s\tremaining: 1.05s\n",
      "902:\tlearn: 0.0664706\ttotal: 9.65s\tremaining: 1.04s\n",
      "903:\tlearn: 0.0664263\ttotal: 9.66s\tremaining: 1.03s\n",
      "904:\tlearn: 0.0664082\ttotal: 9.67s\tremaining: 1.01s\n",
      "905:\tlearn: 0.0663910\ttotal: 9.68s\tremaining: 1s\n",
      "906:\tlearn: 0.0663745\ttotal: 9.69s\tremaining: 994ms\n",
      "907:\tlearn: 0.0663502\ttotal: 9.7s\tremaining: 983ms\n",
      "908:\tlearn: 0.0663293\ttotal: 9.71s\tremaining: 972ms\n",
      "909:\tlearn: 0.0663190\ttotal: 9.72s\tremaining: 961ms\n",
      "910:\tlearn: 0.0663107\ttotal: 9.73s\tremaining: 950ms\n",
      "911:\tlearn: 0.0663019\ttotal: 9.73s\tremaining: 939ms\n",
      "912:\tlearn: 0.0662855\ttotal: 9.74s\tremaining: 929ms\n",
      "913:\tlearn: 0.0662705\ttotal: 9.75s\tremaining: 918ms\n",
      "914:\tlearn: 0.0662582\ttotal: 9.77s\tremaining: 907ms\n",
      "915:\tlearn: 0.0662361\ttotal: 9.78s\tremaining: 897ms\n",
      "916:\tlearn: 0.0662181\ttotal: 9.79s\tremaining: 886ms\n",
      "917:\tlearn: 0.0661983\ttotal: 9.79s\tremaining: 875ms\n",
      "918:\tlearn: 0.0661854\ttotal: 9.8s\tremaining: 864ms\n",
      "919:\tlearn: 0.0661772\ttotal: 9.81s\tremaining: 853ms\n",
      "920:\tlearn: 0.0661540\ttotal: 9.82s\tremaining: 842ms\n",
      "921:\tlearn: 0.0661322\ttotal: 9.83s\tremaining: 832ms\n",
      "922:\tlearn: 0.0661109\ttotal: 9.84s\tremaining: 821ms\n",
      "923:\tlearn: 0.0660993\ttotal: 9.85s\tremaining: 810ms\n",
      "924:\tlearn: 0.0660683\ttotal: 9.86s\tremaining: 799ms\n",
      "925:\tlearn: 0.0660437\ttotal: 9.87s\tremaining: 788ms\n",
      "926:\tlearn: 0.0660281\ttotal: 9.88s\tremaining: 778ms\n",
      "927:\tlearn: 0.0660122\ttotal: 9.88s\tremaining: 767ms\n",
      "928:\tlearn: 0.0659874\ttotal: 9.89s\tremaining: 756ms\n",
      "929:\tlearn: 0.0659653\ttotal: 9.9s\tremaining: 745ms\n",
      "930:\tlearn: 0.0659472\ttotal: 9.91s\tremaining: 735ms\n",
      "931:\tlearn: 0.0659398\ttotal: 9.92s\tremaining: 724ms\n",
      "932:\tlearn: 0.0659224\ttotal: 9.93s\tremaining: 713ms\n",
      "933:\tlearn: 0.0659059\ttotal: 9.94s\tremaining: 703ms\n",
      "934:\tlearn: 0.0658818\ttotal: 9.95s\tremaining: 692ms\n",
      "935:\tlearn: 0.0658669\ttotal: 9.96s\tremaining: 681ms\n",
      "936:\tlearn: 0.0658537\ttotal: 9.97s\tremaining: 670ms\n",
      "937:\tlearn: 0.0658478\ttotal: 9.98s\tremaining: 660ms\n",
      "938:\tlearn: 0.0658317\ttotal: 9.99s\tremaining: 649ms\n",
      "939:\tlearn: 0.0658218\ttotal: 10s\tremaining: 638ms\n",
      "940:\tlearn: 0.0658057\ttotal: 10s\tremaining: 627ms\n",
      "941:\tlearn: 0.0657947\ttotal: 10s\tremaining: 617ms\n",
      "942:\tlearn: 0.0657837\ttotal: 10s\tremaining: 606ms\n",
      "943:\tlearn: 0.0657681\ttotal: 10s\tremaining: 595ms\n",
      "944:\tlearn: 0.0657466\ttotal: 10s\tremaining: 584ms\n",
      "945:\tlearn: 0.0657321\ttotal: 10s\tremaining: 574ms\n",
      "946:\tlearn: 0.0657275\ttotal: 10.1s\tremaining: 563ms\n",
      "947:\tlearn: 0.0657196\ttotal: 10.1s\tremaining: 552ms\n",
      "948:\tlearn: 0.0657044\ttotal: 10.1s\tremaining: 542ms\n",
      "949:\tlearn: 0.0656796\ttotal: 10.1s\tremaining: 531ms\n",
      "950:\tlearn: 0.0656641\ttotal: 10.1s\tremaining: 520ms\n",
      "951:\tlearn: 0.0656509\ttotal: 10.1s\tremaining: 509ms\n",
      "952:\tlearn: 0.0656358\ttotal: 10.1s\tremaining: 499ms\n",
      "953:\tlearn: 0.0656212\ttotal: 10.1s\tremaining: 488ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954:\tlearn: 0.0656079\ttotal: 10.1s\tremaining: 477ms\n",
      "955:\tlearn: 0.0655808\ttotal: 10.1s\tremaining: 467ms\n",
      "956:\tlearn: 0.0655626\ttotal: 10.2s\tremaining: 456ms\n",
      "957:\tlearn: 0.0655565\ttotal: 10.2s\tremaining: 445ms\n",
      "958:\tlearn: 0.0655424\ttotal: 10.2s\tremaining: 435ms\n",
      "959:\tlearn: 0.0655356\ttotal: 10.2s\tremaining: 424ms\n",
      "960:\tlearn: 0.0655253\ttotal: 10.2s\tremaining: 414ms\n",
      "961:\tlearn: 0.0655145\ttotal: 10.2s\tremaining: 404ms\n",
      "962:\tlearn: 0.0654923\ttotal: 10.3s\tremaining: 394ms\n",
      "963:\tlearn: 0.0654728\ttotal: 10.3s\tremaining: 384ms\n",
      "964:\tlearn: 0.0654656\ttotal: 10.3s\tremaining: 373ms\n",
      "965:\tlearn: 0.0654509\ttotal: 10.3s\tremaining: 363ms\n",
      "966:\tlearn: 0.0654410\ttotal: 10.3s\tremaining: 352ms\n",
      "967:\tlearn: 0.0654226\ttotal: 10.3s\tremaining: 341ms\n",
      "968:\tlearn: 0.0653974\ttotal: 10.3s\tremaining: 331ms\n",
      "969:\tlearn: 0.0653756\ttotal: 10.3s\tremaining: 320ms\n",
      "970:\tlearn: 0.0653589\ttotal: 10.4s\tremaining: 309ms\n",
      "971:\tlearn: 0.0653433\ttotal: 10.4s\tremaining: 299ms\n",
      "972:\tlearn: 0.0653235\ttotal: 10.4s\tremaining: 288ms\n",
      "973:\tlearn: 0.0653119\ttotal: 10.4s\tremaining: 277ms\n",
      "974:\tlearn: 0.0652955\ttotal: 10.4s\tremaining: 267ms\n",
      "975:\tlearn: 0.0652826\ttotal: 10.4s\tremaining: 256ms\n",
      "976:\tlearn: 0.0652727\ttotal: 10.4s\tremaining: 245ms\n",
      "977:\tlearn: 0.0652453\ttotal: 10.4s\tremaining: 235ms\n",
      "978:\tlearn: 0.0652381\ttotal: 10.4s\tremaining: 224ms\n",
      "979:\tlearn: 0.0652220\ttotal: 10.5s\tremaining: 213ms\n",
      "980:\tlearn: 0.0652109\ttotal: 10.5s\tremaining: 203ms\n",
      "981:\tlearn: 0.0651858\ttotal: 10.5s\tremaining: 192ms\n",
      "982:\tlearn: 0.0651688\ttotal: 10.5s\tremaining: 181ms\n",
      "983:\tlearn: 0.0651444\ttotal: 10.5s\tremaining: 171ms\n",
      "984:\tlearn: 0.0651290\ttotal: 10.5s\tremaining: 160ms\n",
      "985:\tlearn: 0.0651171\ttotal: 10.5s\tremaining: 149ms\n",
      "986:\tlearn: 0.0650990\ttotal: 10.5s\tremaining: 139ms\n",
      "987:\tlearn: 0.0650700\ttotal: 10.5s\tremaining: 128ms\n",
      "988:\tlearn: 0.0650510\ttotal: 10.5s\tremaining: 117ms\n",
      "989:\tlearn: 0.0650409\ttotal: 10.6s\tremaining: 107ms\n",
      "990:\tlearn: 0.0650209\ttotal: 10.6s\tremaining: 96ms\n",
      "991:\tlearn: 0.0649946\ttotal: 10.6s\tremaining: 85.3ms\n",
      "992:\tlearn: 0.0649847\ttotal: 10.6s\tremaining: 74.6ms\n",
      "993:\tlearn: 0.0649765\ttotal: 10.6s\tremaining: 64ms\n",
      "994:\tlearn: 0.0649725\ttotal: 10.6s\tremaining: 53.3ms\n",
      "995:\tlearn: 0.0649636\ttotal: 10.6s\tremaining: 42.6ms\n",
      "996:\tlearn: 0.0649574\ttotal: 10.6s\tremaining: 32ms\n",
      "997:\tlearn: 0.0649367\ttotal: 10.6s\tremaining: 21.3ms\n",
      "998:\tlearn: 0.0649202\ttotal: 10.6s\tremaining: 10.7ms\n",
      "999:\tlearn: 0.0649066\ttotal: 10.7s\tremaining: 0us\n",
      "0:\tlearn: 0.4003724\ttotal: 4.49ms\tremaining: 4.49s\n",
      "1:\tlearn: 0.3624150\ttotal: 12ms\tremaining: 5.97s\n",
      "2:\tlearn: 0.3350824\ttotal: 32.2ms\tremaining: 10.7s\n",
      "3:\tlearn: 0.3114450\ttotal: 40.4ms\tremaining: 10.1s\n",
      "4:\tlearn: 0.2927771\ttotal: 50.5ms\tremaining: 10.1s\n",
      "5:\tlearn: 0.2791815\ttotal: 59ms\tremaining: 9.77s\n",
      "6:\tlearn: 0.2658008\ttotal: 76.6ms\tremaining: 10.9s\n",
      "7:\tlearn: 0.2574898\ttotal: 88.3ms\tremaining: 11s\n",
      "8:\tlearn: 0.2466839\ttotal: 98.3ms\tremaining: 10.8s\n",
      "9:\tlearn: 0.2344982\ttotal: 107ms\tremaining: 10.5s\n",
      "10:\tlearn: 0.2225758\ttotal: 129ms\tremaining: 11.6s\n",
      "11:\tlearn: 0.2084417\ttotal: 139ms\tremaining: 11.5s\n",
      "12:\tlearn: 0.2019560\ttotal: 148ms\tremaining: 11.2s\n",
      "13:\tlearn: 0.1977420\ttotal: 169ms\tremaining: 11.9s\n",
      "14:\tlearn: 0.1952003\ttotal: 179ms\tremaining: 11.8s\n",
      "15:\tlearn: 0.1924293\ttotal: 188ms\tremaining: 11.6s\n",
      "16:\tlearn: 0.1893674\ttotal: 199ms\tremaining: 11.5s\n",
      "17:\tlearn: 0.1857952\ttotal: 214ms\tremaining: 11.7s\n",
      "18:\tlearn: 0.1821690\ttotal: 224ms\tremaining: 11.5s\n",
      "19:\tlearn: 0.1808524\ttotal: 234ms\tremaining: 11.5s\n",
      "20:\tlearn: 0.1800191\ttotal: 244ms\tremaining: 11.4s\n",
      "21:\tlearn: 0.1786627\ttotal: 255ms\tremaining: 11.3s\n",
      "22:\tlearn: 0.1784899\ttotal: 262ms\tremaining: 11.1s\n",
      "23:\tlearn: 0.1780853\ttotal: 271ms\tremaining: 11s\n",
      "24:\tlearn: 0.1703532\ttotal: 280ms\tremaining: 10.9s\n",
      "25:\tlearn: 0.1694296\ttotal: 290ms\tremaining: 10.8s\n",
      "26:\tlearn: 0.1691729\ttotal: 297ms\tremaining: 10.7s\n",
      "27:\tlearn: 0.1687860\ttotal: 307ms\tremaining: 10.6s\n",
      "28:\tlearn: 0.1682626\ttotal: 316ms\tremaining: 10.6s\n",
      "29:\tlearn: 0.1681346\ttotal: 323ms\tremaining: 10.4s\n",
      "30:\tlearn: 0.1678639\ttotal: 333ms\tremaining: 10.4s\n",
      "31:\tlearn: 0.1675156\ttotal: 341ms\tremaining: 10.3s\n",
      "32:\tlearn: 0.1674919\ttotal: 351ms\tremaining: 10.3s\n",
      "33:\tlearn: 0.1671596\ttotal: 358ms\tremaining: 10.2s\n",
      "34:\tlearn: 0.1670447\ttotal: 381ms\tremaining: 10.5s\n",
      "35:\tlearn: 0.1657337\ttotal: 392ms\tremaining: 10.5s\n",
      "36:\tlearn: 0.1656185\ttotal: 401ms\tremaining: 10.4s\n",
      "37:\tlearn: 0.1655475\ttotal: 410ms\tremaining: 10.4s\n",
      "38:\tlearn: 0.1650988\ttotal: 419ms\tremaining: 10.3s\n",
      "39:\tlearn: 0.1648909\ttotal: 428ms\tremaining: 10.3s\n",
      "40:\tlearn: 0.1646932\ttotal: 453ms\tremaining: 10.6s\n",
      "41:\tlearn: 0.1646341\ttotal: 476ms\tremaining: 10.9s\n",
      "42:\tlearn: 0.1640347\ttotal: 497ms\tremaining: 11.1s\n",
      "43:\tlearn: 0.1629558\ttotal: 513ms\tremaining: 11.1s\n",
      "44:\tlearn: 0.1624107\ttotal: 535ms\tremaining: 11.3s\n",
      "45:\tlearn: 0.1610699\ttotal: 555ms\tremaining: 11.5s\n",
      "46:\tlearn: 0.1604984\ttotal: 564ms\tremaining: 11.4s\n",
      "47:\tlearn: 0.1602554\ttotal: 572ms\tremaining: 11.3s\n",
      "48:\tlearn: 0.1600092\ttotal: 581ms\tremaining: 11.3s\n",
      "49:\tlearn: 0.1598737\ttotal: 589ms\tremaining: 11.2s\n",
      "50:\tlearn: 0.1595727\ttotal: 600ms\tremaining: 11.2s\n",
      "51:\tlearn: 0.1595135\ttotal: 607ms\tremaining: 11.1s\n",
      "52:\tlearn: 0.1551799\ttotal: 617ms\tremaining: 11s\n",
      "53:\tlearn: 0.1549739\ttotal: 626ms\tremaining: 11s\n",
      "54:\tlearn: 0.1548994\ttotal: 636ms\tremaining: 10.9s\n",
      "55:\tlearn: 0.1546714\ttotal: 646ms\tremaining: 10.9s\n",
      "56:\tlearn: 0.1544113\ttotal: 653ms\tremaining: 10.8s\n",
      "57:\tlearn: 0.1540978\ttotal: 664ms\tremaining: 10.8s\n",
      "58:\tlearn: 0.1539793\ttotal: 671ms\tremaining: 10.7s\n",
      "59:\tlearn: 0.1513000\ttotal: 690ms\tremaining: 10.8s\n",
      "60:\tlearn: 0.1459786\ttotal: 698ms\tremaining: 10.7s\n",
      "61:\tlearn: 0.1458114\ttotal: 708ms\tremaining: 10.7s\n",
      "62:\tlearn: 0.1457026\ttotal: 715ms\tremaining: 10.6s\n",
      "63:\tlearn: 0.1446104\ttotal: 751ms\tremaining: 11s\n",
      "64:\tlearn: 0.1442875\ttotal: 761ms\tremaining: 10.9s\n",
      "65:\tlearn: 0.1440969\ttotal: 769ms\tremaining: 10.9s\n",
      "66:\tlearn: 0.1404221\ttotal: 782ms\tremaining: 10.9s\n",
      "67:\tlearn: 0.1403382\ttotal: 791ms\tremaining: 10.8s\n",
      "68:\tlearn: 0.1381966\ttotal: 800ms\tremaining: 10.8s\n",
      "69:\tlearn: 0.1374670\ttotal: 810ms\tremaining: 10.8s\n",
      "70:\tlearn: 0.1345190\ttotal: 817ms\tremaining: 10.7s\n",
      "71:\tlearn: 0.1326537\ttotal: 828ms\tremaining: 10.7s\n",
      "72:\tlearn: 0.1309415\ttotal: 838ms\tremaining: 10.6s\n",
      "73:\tlearn: 0.1296086\ttotal: 849ms\tremaining: 10.6s\n",
      "74:\tlearn: 0.1280615\ttotal: 859ms\tremaining: 10.6s\n",
      "75:\tlearn: 0.1263456\ttotal: 868ms\tremaining: 10.5s\n",
      "76:\tlearn: 0.1241449\ttotal: 892ms\tremaining: 10.7s\n",
      "77:\tlearn: 0.1224024\ttotal: 902ms\tremaining: 10.7s\n",
      "78:\tlearn: 0.1207029\ttotal: 915ms\tremaining: 10.7s\n",
      "79:\tlearn: 0.1199855\ttotal: 924ms\tremaining: 10.6s\n",
      "80:\tlearn: 0.1184573\ttotal: 935ms\tremaining: 10.6s\n",
      "81:\tlearn: 0.1176315\ttotal: 943ms\tremaining: 10.6s\n",
      "82:\tlearn: 0.1163381\ttotal: 954ms\tremaining: 10.5s\n",
      "83:\tlearn: 0.1146343\ttotal: 967ms\tremaining: 10.5s\n",
      "84:\tlearn: 0.1134480\ttotal: 976ms\tremaining: 10.5s\n",
      "85:\tlearn: 0.1124289\ttotal: 987ms\tremaining: 10.5s\n",
      "86:\tlearn: 0.1117166\ttotal: 994ms\tremaining: 10.4s\n",
      "87:\tlearn: 0.1110539\ttotal: 1s\tremaining: 10.4s\n",
      "88:\tlearn: 0.1106051\ttotal: 1.01s\tremaining: 10.4s\n",
      "89:\tlearn: 0.1095248\ttotal: 1.02s\tremaining: 10.3s\n",
      "90:\tlearn: 0.1085734\ttotal: 1.03s\tremaining: 10.3s\n",
      "91:\tlearn: 0.1079884\ttotal: 1.04s\tremaining: 10.3s\n",
      "92:\tlearn: 0.1073951\ttotal: 1.06s\tremaining: 10.3s\n",
      "93:\tlearn: 0.1068054\ttotal: 1.07s\tremaining: 10.3s\n",
      "94:\tlearn: 0.1064087\ttotal: 1.08s\tremaining: 10.3s\n",
      "95:\tlearn: 0.1060618\ttotal: 1.1s\tremaining: 10.4s\n",
      "96:\tlearn: 0.1054755\ttotal: 1.11s\tremaining: 10.3s\n",
      "97:\tlearn: 0.1048352\ttotal: 1.12s\tremaining: 10.3s\n",
      "98:\tlearn: 0.1045700\ttotal: 1.13s\tremaining: 10.3s\n",
      "99:\tlearn: 0.1040800\ttotal: 1.15s\tremaining: 10.3s\n",
      "100:\tlearn: 0.1037299\ttotal: 1.16s\tremaining: 10.3s\n",
      "101:\tlearn: 0.1034452\ttotal: 1.17s\tremaining: 10.3s\n",
      "102:\tlearn: 0.1031811\ttotal: 1.18s\tremaining: 10.3s\n",
      "103:\tlearn: 0.1026447\ttotal: 1.19s\tremaining: 10.2s\n",
      "104:\tlearn: 0.1024152\ttotal: 1.2s\tremaining: 10.2s\n",
      "105:\tlearn: 0.1022507\ttotal: 1.21s\tremaining: 10.2s\n",
      "106:\tlearn: 0.1018401\ttotal: 1.22s\tremaining: 10.1s\n",
      "107:\tlearn: 0.1016044\ttotal: 1.23s\tremaining: 10.1s\n",
      "108:\tlearn: 0.1011783\ttotal: 1.24s\tremaining: 10.1s\n",
      "109:\tlearn: 0.1008342\ttotal: 1.25s\tremaining: 10.1s\n",
      "110:\tlearn: 0.1003903\ttotal: 1.26s\tremaining: 10.1s\n",
      "111:\tlearn: 0.1000488\ttotal: 1.26s\tremaining: 10s\n",
      "112:\tlearn: 0.0999016\ttotal: 1.27s\tremaining: 10s\n",
      "113:\tlearn: 0.0997167\ttotal: 1.28s\tremaining: 9.98s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114:\tlearn: 0.0993173\ttotal: 1.29s\tremaining: 9.94s\n",
      "115:\tlearn: 0.0991173\ttotal: 1.3s\tremaining: 9.91s\n",
      "116:\tlearn: 0.0986166\ttotal: 1.31s\tremaining: 9.87s\n",
      "117:\tlearn: 0.0983636\ttotal: 1.32s\tremaining: 9.85s\n",
      "118:\tlearn: 0.0982223\ttotal: 1.33s\tremaining: 9.84s\n",
      "119:\tlearn: 0.0980736\ttotal: 1.34s\tremaining: 9.8s\n",
      "120:\tlearn: 0.0978674\ttotal: 1.34s\tremaining: 9.77s\n",
      "121:\tlearn: 0.0973507\ttotal: 1.35s\tremaining: 9.75s\n",
      "122:\tlearn: 0.0971902\ttotal: 1.37s\tremaining: 9.75s\n",
      "123:\tlearn: 0.0970613\ttotal: 1.38s\tremaining: 9.72s\n",
      "124:\tlearn: 0.0968319\ttotal: 1.39s\tremaining: 9.71s\n",
      "125:\tlearn: 0.0966445\ttotal: 1.4s\tremaining: 9.69s\n",
      "126:\tlearn: 0.0964411\ttotal: 1.41s\tremaining: 9.68s\n",
      "127:\tlearn: 0.0963119\ttotal: 1.42s\tremaining: 9.67s\n",
      "128:\tlearn: 0.0961182\ttotal: 1.43s\tremaining: 9.66s\n",
      "129:\tlearn: 0.0958501\ttotal: 1.44s\tremaining: 9.63s\n",
      "130:\tlearn: 0.0956422\ttotal: 1.46s\tremaining: 9.7s\n",
      "131:\tlearn: 0.0954858\ttotal: 1.48s\tremaining: 9.76s\n",
      "132:\tlearn: 0.0953529\ttotal: 1.51s\tremaining: 9.87s\n",
      "133:\tlearn: 0.0951236\ttotal: 1.53s\tremaining: 9.92s\n",
      "134:\tlearn: 0.0950423\ttotal: 1.56s\tremaining: 9.98s\n",
      "135:\tlearn: 0.0947708\ttotal: 1.58s\tremaining: 10s\n",
      "136:\tlearn: 0.0944828\ttotal: 1.59s\tremaining: 10s\n",
      "137:\tlearn: 0.0942680\ttotal: 1.6s\tremaining: 9.98s\n",
      "138:\tlearn: 0.0941782\ttotal: 1.61s\tremaining: 9.95s\n",
      "139:\tlearn: 0.0940164\ttotal: 1.62s\tremaining: 9.95s\n",
      "140:\tlearn: 0.0938615\ttotal: 1.63s\tremaining: 9.91s\n",
      "141:\tlearn: 0.0937567\ttotal: 1.64s\tremaining: 9.89s\n",
      "142:\tlearn: 0.0936403\ttotal: 1.65s\tremaining: 9.88s\n",
      "143:\tlearn: 0.0935007\ttotal: 1.66s\tremaining: 9.84s\n",
      "144:\tlearn: 0.0933850\ttotal: 1.67s\tremaining: 9.84s\n",
      "145:\tlearn: 0.0932980\ttotal: 1.68s\tremaining: 9.81s\n",
      "146:\tlearn: 0.0931845\ttotal: 1.69s\tremaining: 9.79s\n",
      "147:\tlearn: 0.0931271\ttotal: 1.7s\tremaining: 9.78s\n",
      "148:\tlearn: 0.0929719\ttotal: 1.71s\tremaining: 9.75s\n",
      "149:\tlearn: 0.0928901\ttotal: 1.72s\tremaining: 9.74s\n",
      "150:\tlearn: 0.0928260\ttotal: 1.73s\tremaining: 9.7s\n",
      "151:\tlearn: 0.0926270\ttotal: 1.74s\tremaining: 9.69s\n",
      "152:\tlearn: 0.0924831\ttotal: 1.75s\tremaining: 9.68s\n",
      "153:\tlearn: 0.0923259\ttotal: 1.76s\tremaining: 9.65s\n",
      "154:\tlearn: 0.0921663\ttotal: 1.77s\tremaining: 9.64s\n",
      "155:\tlearn: 0.0920612\ttotal: 1.78s\tremaining: 9.62s\n",
      "156:\tlearn: 0.0919107\ttotal: 1.79s\tremaining: 9.6s\n",
      "157:\tlearn: 0.0918490\ttotal: 1.8s\tremaining: 9.59s\n",
      "158:\tlearn: 0.0917747\ttotal: 1.81s\tremaining: 9.57s\n",
      "159:\tlearn: 0.0916525\ttotal: 1.82s\tremaining: 9.56s\n",
      "160:\tlearn: 0.0914476\ttotal: 1.83s\tremaining: 9.54s\n",
      "161:\tlearn: 0.0913730\ttotal: 1.84s\tremaining: 9.53s\n",
      "162:\tlearn: 0.0912687\ttotal: 1.86s\tremaining: 9.55s\n",
      "163:\tlearn: 0.0911537\ttotal: 1.87s\tremaining: 9.53s\n",
      "164:\tlearn: 0.0910480\ttotal: 1.88s\tremaining: 9.52s\n",
      "165:\tlearn: 0.0909692\ttotal: 1.89s\tremaining: 9.49s\n",
      "166:\tlearn: 0.0908616\ttotal: 1.9s\tremaining: 9.5s\n",
      "167:\tlearn: 0.0906721\ttotal: 1.92s\tremaining: 9.49s\n",
      "168:\tlearn: 0.0905269\ttotal: 1.92s\tremaining: 9.46s\n",
      "169:\tlearn: 0.0904057\ttotal: 1.94s\tremaining: 9.45s\n",
      "170:\tlearn: 0.0902484\ttotal: 1.95s\tremaining: 9.47s\n",
      "171:\tlearn: 0.0901717\ttotal: 1.96s\tremaining: 9.45s\n",
      "172:\tlearn: 0.0900647\ttotal: 1.97s\tremaining: 9.43s\n",
      "173:\tlearn: 0.0899961\ttotal: 1.99s\tremaining: 9.44s\n",
      "174:\tlearn: 0.0899361\ttotal: 2s\tremaining: 9.42s\n",
      "175:\tlearn: 0.0898459\ttotal: 2.01s\tremaining: 9.4s\n",
      "176:\tlearn: 0.0897156\ttotal: 2.02s\tremaining: 9.38s\n",
      "177:\tlearn: 0.0896533\ttotal: 2.03s\tremaining: 9.38s\n",
      "178:\tlearn: 0.0896010\ttotal: 2.04s\tremaining: 9.38s\n",
      "179:\tlearn: 0.0895385\ttotal: 2.05s\tremaining: 9.36s\n",
      "180:\tlearn: 0.0894828\ttotal: 2.06s\tremaining: 9.33s\n",
      "181:\tlearn: 0.0894271\ttotal: 2.07s\tremaining: 9.31s\n",
      "182:\tlearn: 0.0893539\ttotal: 2.08s\tremaining: 9.31s\n",
      "183:\tlearn: 0.0891962\ttotal: 2.1s\tremaining: 9.29s\n",
      "184:\tlearn: 0.0890906\ttotal: 2.11s\tremaining: 9.28s\n",
      "185:\tlearn: 0.0890363\ttotal: 2.12s\tremaining: 9.28s\n",
      "186:\tlearn: 0.0889499\ttotal: 2.13s\tremaining: 9.27s\n",
      "187:\tlearn: 0.0888814\ttotal: 2.14s\tremaining: 9.25s\n",
      "188:\tlearn: 0.0886714\ttotal: 2.15s\tremaining: 9.24s\n",
      "189:\tlearn: 0.0886082\ttotal: 2.16s\tremaining: 9.22s\n",
      "190:\tlearn: 0.0885395\ttotal: 2.17s\tremaining: 9.21s\n",
      "191:\tlearn: 0.0884467\ttotal: 2.18s\tremaining: 9.19s\n",
      "192:\tlearn: 0.0883428\ttotal: 2.19s\tremaining: 9.17s\n",
      "193:\tlearn: 0.0882993\ttotal: 2.2s\tremaining: 9.15s\n",
      "194:\tlearn: 0.0881988\ttotal: 2.22s\tremaining: 9.15s\n",
      "195:\tlearn: 0.0881614\ttotal: 2.22s\tremaining: 9.13s\n",
      "196:\tlearn: 0.0880657\ttotal: 2.23s\tremaining: 9.11s\n",
      "197:\tlearn: 0.0880086\ttotal: 2.24s\tremaining: 9.09s\n",
      "198:\tlearn: 0.0879193\ttotal: 2.26s\tremaining: 9.09s\n",
      "199:\tlearn: 0.0878727\ttotal: 2.27s\tremaining: 9.06s\n",
      "200:\tlearn: 0.0878292\ttotal: 2.27s\tremaining: 9.04s\n",
      "201:\tlearn: 0.0877341\ttotal: 2.28s\tremaining: 9.02s\n",
      "202:\tlearn: 0.0876099\ttotal: 2.29s\tremaining: 9.01s\n",
      "203:\tlearn: 0.0875600\ttotal: 2.3s\tremaining: 8.99s\n",
      "204:\tlearn: 0.0874975\ttotal: 2.31s\tremaining: 8.97s\n",
      "205:\tlearn: 0.0873493\ttotal: 2.32s\tremaining: 8.96s\n",
      "206:\tlearn: 0.0872795\ttotal: 2.33s\tremaining: 8.94s\n",
      "207:\tlearn: 0.0871942\ttotal: 2.34s\tremaining: 8.93s\n",
      "208:\tlearn: 0.0871251\ttotal: 2.35s\tremaining: 8.91s\n",
      "209:\tlearn: 0.0870735\ttotal: 2.36s\tremaining: 8.89s\n",
      "210:\tlearn: 0.0869747\ttotal: 2.37s\tremaining: 8.87s\n",
      "211:\tlearn: 0.0869302\ttotal: 2.38s\tremaining: 8.86s\n",
      "212:\tlearn: 0.0868771\ttotal: 2.39s\tremaining: 8.84s\n",
      "213:\tlearn: 0.0867789\ttotal: 2.4s\tremaining: 8.83s\n",
      "214:\tlearn: 0.0867260\ttotal: 2.41s\tremaining: 8.8s\n",
      "215:\tlearn: 0.0866624\ttotal: 2.42s\tremaining: 8.79s\n",
      "216:\tlearn: 0.0865997\ttotal: 2.43s\tremaining: 8.77s\n",
      "217:\tlearn: 0.0865360\ttotal: 2.44s\tremaining: 8.76s\n",
      "218:\tlearn: 0.0864362\ttotal: 2.45s\tremaining: 8.74s\n",
      "219:\tlearn: 0.0863737\ttotal: 2.46s\tremaining: 8.71s\n",
      "220:\tlearn: 0.0863310\ttotal: 2.47s\tremaining: 8.71s\n",
      "221:\tlearn: 0.0861969\ttotal: 2.49s\tremaining: 8.72s\n",
      "222:\tlearn: 0.0861307\ttotal: 2.51s\tremaining: 8.74s\n",
      "223:\tlearn: 0.0860647\ttotal: 2.53s\tremaining: 8.76s\n",
      "224:\tlearn: 0.0860419\ttotal: 2.55s\tremaining: 8.79s\n",
      "225:\tlearn: 0.0859769\ttotal: 2.56s\tremaining: 8.78s\n",
      "226:\tlearn: 0.0859244\ttotal: 2.59s\tremaining: 8.83s\n",
      "227:\tlearn: 0.0858542\ttotal: 2.6s\tremaining: 8.8s\n",
      "228:\tlearn: 0.0858152\ttotal: 2.61s\tremaining: 8.79s\n",
      "229:\tlearn: 0.0857206\ttotal: 2.62s\tremaining: 8.77s\n",
      "230:\tlearn: 0.0856739\ttotal: 2.63s\tremaining: 8.74s\n",
      "231:\tlearn: 0.0856189\ttotal: 2.64s\tremaining: 8.73s\n",
      "232:\tlearn: 0.0855349\ttotal: 2.64s\tremaining: 8.7s\n",
      "233:\tlearn: 0.0854575\ttotal: 2.66s\tremaining: 8.7s\n",
      "234:\tlearn: 0.0854165\ttotal: 2.67s\tremaining: 8.68s\n",
      "235:\tlearn: 0.0853570\ttotal: 2.67s\tremaining: 8.66s\n",
      "236:\tlearn: 0.0853025\ttotal: 2.68s\tremaining: 8.64s\n",
      "237:\tlearn: 0.0852020\ttotal: 2.69s\tremaining: 8.62s\n",
      "238:\tlearn: 0.0851537\ttotal: 2.7s\tremaining: 8.6s\n",
      "239:\tlearn: 0.0850887\ttotal: 2.71s\tremaining: 8.59s\n",
      "240:\tlearn: 0.0850444\ttotal: 2.72s\tremaining: 8.57s\n",
      "241:\tlearn: 0.0849887\ttotal: 2.73s\tremaining: 8.55s\n",
      "242:\tlearn: 0.0849566\ttotal: 2.74s\tremaining: 8.53s\n",
      "243:\tlearn: 0.0848903\ttotal: 2.75s\tremaining: 8.52s\n",
      "244:\tlearn: 0.0847901\ttotal: 2.76s\tremaining: 8.5s\n",
      "245:\tlearn: 0.0846770\ttotal: 2.77s\tremaining: 8.49s\n",
      "246:\tlearn: 0.0846205\ttotal: 2.78s\tremaining: 8.47s\n",
      "247:\tlearn: 0.0845795\ttotal: 2.79s\tremaining: 8.45s\n",
      "248:\tlearn: 0.0844982\ttotal: 2.8s\tremaining: 8.44s\n",
      "249:\tlearn: 0.0844512\ttotal: 2.81s\tremaining: 8.42s\n",
      "250:\tlearn: 0.0844180\ttotal: 2.81s\tremaining: 8.4s\n",
      "251:\tlearn: 0.0843711\ttotal: 2.82s\tremaining: 8.38s\n",
      "252:\tlearn: 0.0843357\ttotal: 2.83s\tremaining: 8.36s\n",
      "253:\tlearn: 0.0842914\ttotal: 2.84s\tremaining: 8.35s\n",
      "254:\tlearn: 0.0842456\ttotal: 2.85s\tremaining: 8.33s\n",
      "255:\tlearn: 0.0842161\ttotal: 2.86s\tremaining: 8.31s\n",
      "256:\tlearn: 0.0841665\ttotal: 2.87s\tremaining: 8.29s\n",
      "257:\tlearn: 0.0841235\ttotal: 2.88s\tremaining: 8.27s\n",
      "258:\tlearn: 0.0840893\ttotal: 2.89s\tremaining: 8.26s\n",
      "259:\tlearn: 0.0840485\ttotal: 2.9s\tremaining: 8.24s\n",
      "260:\tlearn: 0.0840134\ttotal: 2.9s\tremaining: 8.22s\n",
      "261:\tlearn: 0.0839515\ttotal: 2.92s\tremaining: 8.22s\n",
      "262:\tlearn: 0.0838193\ttotal: 2.92s\tremaining: 8.2s\n",
      "263:\tlearn: 0.0837936\ttotal: 2.94s\tremaining: 8.18s\n",
      "264:\tlearn: 0.0837412\ttotal: 2.94s\tremaining: 8.17s\n",
      "265:\tlearn: 0.0836733\ttotal: 2.95s\tremaining: 8.15s\n",
      "266:\tlearn: 0.0836354\ttotal: 2.96s\tremaining: 8.14s\n",
      "267:\tlearn: 0.0835877\ttotal: 2.97s\tremaining: 8.12s\n",
      "268:\tlearn: 0.0835620\ttotal: 2.98s\tremaining: 8.1s\n",
      "269:\tlearn: 0.0835090\ttotal: 2.99s\tremaining: 8.09s\n",
      "270:\tlearn: 0.0834842\ttotal: 3s\tremaining: 8.07s\n",
      "271:\tlearn: 0.0834613\ttotal: 3.02s\tremaining: 8.08s\n",
      "272:\tlearn: 0.0834321\ttotal: 3.03s\tremaining: 8.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273:\tlearn: 0.0833713\ttotal: 3.05s\tremaining: 8.07s\n",
      "274:\tlearn: 0.0833453\ttotal: 3.06s\tremaining: 8.07s\n",
      "275:\tlearn: 0.0831859\ttotal: 3.07s\tremaining: 8.05s\n",
      "276:\tlearn: 0.0831532\ttotal: 3.08s\tremaining: 8.04s\n",
      "277:\tlearn: 0.0830966\ttotal: 3.09s\tremaining: 8.02s\n",
      "278:\tlearn: 0.0830595\ttotal: 3.1s\tremaining: 8.02s\n",
      "279:\tlearn: 0.0830081\ttotal: 3.11s\tremaining: 8.01s\n",
      "280:\tlearn: 0.0829784\ttotal: 3.12s\tremaining: 8s\n",
      "281:\tlearn: 0.0829440\ttotal: 3.13s\tremaining: 7.98s\n",
      "282:\tlearn: 0.0829220\ttotal: 3.15s\tremaining: 7.97s\n",
      "283:\tlearn: 0.0828422\ttotal: 3.16s\tremaining: 7.96s\n",
      "284:\tlearn: 0.0827873\ttotal: 3.17s\tremaining: 7.95s\n",
      "285:\tlearn: 0.0827108\ttotal: 3.18s\tremaining: 7.93s\n",
      "286:\tlearn: 0.0826668\ttotal: 3.19s\tremaining: 7.93s\n",
      "287:\tlearn: 0.0826120\ttotal: 3.2s\tremaining: 7.92s\n",
      "288:\tlearn: 0.0825289\ttotal: 3.21s\tremaining: 7.9s\n",
      "289:\tlearn: 0.0824596\ttotal: 3.22s\tremaining: 7.89s\n",
      "290:\tlearn: 0.0824199\ttotal: 3.23s\tremaining: 7.88s\n",
      "291:\tlearn: 0.0823666\ttotal: 3.24s\tremaining: 7.87s\n",
      "292:\tlearn: 0.0822938\ttotal: 3.25s\tremaining: 7.85s\n",
      "293:\tlearn: 0.0822432\ttotal: 3.26s\tremaining: 7.83s\n",
      "294:\tlearn: 0.0821839\ttotal: 3.28s\tremaining: 7.84s\n",
      "295:\tlearn: 0.0821550\ttotal: 3.29s\tremaining: 7.83s\n",
      "296:\tlearn: 0.0821166\ttotal: 3.3s\tremaining: 7.81s\n",
      "297:\tlearn: 0.0820497\ttotal: 3.31s\tremaining: 7.8s\n",
      "298:\tlearn: 0.0820129\ttotal: 3.32s\tremaining: 7.79s\n",
      "299:\tlearn: 0.0819680\ttotal: 3.33s\tremaining: 7.77s\n",
      "300:\tlearn: 0.0819359\ttotal: 3.34s\tremaining: 7.76s\n",
      "301:\tlearn: 0.0819051\ttotal: 3.35s\tremaining: 7.75s\n",
      "302:\tlearn: 0.0818715\ttotal: 3.36s\tremaining: 7.74s\n",
      "303:\tlearn: 0.0818371\ttotal: 3.37s\tremaining: 7.72s\n",
      "304:\tlearn: 0.0817138\ttotal: 3.38s\tremaining: 7.71s\n",
      "305:\tlearn: 0.0816847\ttotal: 3.39s\tremaining: 7.7s\n",
      "306:\tlearn: 0.0816722\ttotal: 3.4s\tremaining: 7.68s\n",
      "307:\tlearn: 0.0816327\ttotal: 3.41s\tremaining: 7.67s\n",
      "308:\tlearn: 0.0815423\ttotal: 3.42s\tremaining: 7.66s\n",
      "309:\tlearn: 0.0814399\ttotal: 3.43s\tremaining: 7.64s\n",
      "310:\tlearn: 0.0814079\ttotal: 3.45s\tremaining: 7.63s\n",
      "311:\tlearn: 0.0813439\ttotal: 3.46s\tremaining: 7.63s\n",
      "312:\tlearn: 0.0813152\ttotal: 3.47s\tremaining: 7.61s\n",
      "313:\tlearn: 0.0812617\ttotal: 3.48s\tremaining: 7.6s\n",
      "314:\tlearn: 0.0811976\ttotal: 3.5s\tremaining: 7.61s\n",
      "315:\tlearn: 0.0811307\ttotal: 3.52s\tremaining: 7.61s\n",
      "316:\tlearn: 0.0811049\ttotal: 3.54s\tremaining: 7.63s\n",
      "317:\tlearn: 0.0810643\ttotal: 3.56s\tremaining: 7.63s\n",
      "318:\tlearn: 0.0810132\ttotal: 3.58s\tremaining: 7.65s\n",
      "319:\tlearn: 0.0809836\ttotal: 3.6s\tremaining: 7.65s\n",
      "320:\tlearn: 0.0809551\ttotal: 3.61s\tremaining: 7.64s\n",
      "321:\tlearn: 0.0809195\ttotal: 3.62s\tremaining: 7.62s\n",
      "322:\tlearn: 0.0808887\ttotal: 3.63s\tremaining: 7.61s\n",
      "323:\tlearn: 0.0808529\ttotal: 3.64s\tremaining: 7.59s\n",
      "324:\tlearn: 0.0808114\ttotal: 3.65s\tremaining: 7.58s\n",
      "325:\tlearn: 0.0807722\ttotal: 3.66s\tremaining: 7.56s\n",
      "326:\tlearn: 0.0807406\ttotal: 3.67s\tremaining: 7.55s\n",
      "327:\tlearn: 0.0807156\ttotal: 3.68s\tremaining: 7.53s\n",
      "328:\tlearn: 0.0806982\ttotal: 3.69s\tremaining: 7.52s\n",
      "329:\tlearn: 0.0806685\ttotal: 3.69s\tremaining: 7.5s\n",
      "330:\tlearn: 0.0806109\ttotal: 3.71s\tremaining: 7.49s\n",
      "331:\tlearn: 0.0805851\ttotal: 3.71s\tremaining: 7.48s\n",
      "332:\tlearn: 0.0805316\ttotal: 3.73s\tremaining: 7.46s\n",
      "333:\tlearn: 0.0804619\ttotal: 3.73s\tremaining: 7.45s\n",
      "334:\tlearn: 0.0804040\ttotal: 3.74s\tremaining: 7.43s\n",
      "335:\tlearn: 0.0803571\ttotal: 3.75s\tremaining: 7.41s\n",
      "336:\tlearn: 0.0803141\ttotal: 3.77s\tremaining: 7.41s\n",
      "337:\tlearn: 0.0802844\ttotal: 3.77s\tremaining: 7.39s\n",
      "338:\tlearn: 0.0802044\ttotal: 3.79s\tremaining: 7.39s\n",
      "339:\tlearn: 0.0801693\ttotal: 3.8s\tremaining: 7.38s\n",
      "340:\tlearn: 0.0801393\ttotal: 3.81s\tremaining: 7.37s\n",
      "341:\tlearn: 0.0800924\ttotal: 3.82s\tremaining: 7.36s\n",
      "342:\tlearn: 0.0800684\ttotal: 3.83s\tremaining: 7.34s\n",
      "343:\tlearn: 0.0800444\ttotal: 3.84s\tremaining: 7.33s\n",
      "344:\tlearn: 0.0799939\ttotal: 3.85s\tremaining: 7.31s\n",
      "345:\tlearn: 0.0799631\ttotal: 3.86s\tremaining: 7.3s\n",
      "346:\tlearn: 0.0799185\ttotal: 3.87s\tremaining: 7.29s\n",
      "347:\tlearn: 0.0798773\ttotal: 3.88s\tremaining: 7.27s\n",
      "348:\tlearn: 0.0798457\ttotal: 3.89s\tremaining: 7.26s\n",
      "349:\tlearn: 0.0798129\ttotal: 3.9s\tremaining: 7.24s\n",
      "350:\tlearn: 0.0797479\ttotal: 3.91s\tremaining: 7.23s\n",
      "351:\tlearn: 0.0797221\ttotal: 3.92s\tremaining: 7.22s\n",
      "352:\tlearn: 0.0796972\ttotal: 3.93s\tremaining: 7.2s\n",
      "353:\tlearn: 0.0796553\ttotal: 3.94s\tremaining: 7.19s\n",
      "354:\tlearn: 0.0796091\ttotal: 3.95s\tremaining: 7.17s\n",
      "355:\tlearn: 0.0795969\ttotal: 3.96s\tremaining: 7.16s\n",
      "356:\tlearn: 0.0795788\ttotal: 3.97s\tremaining: 7.15s\n",
      "357:\tlearn: 0.0795561\ttotal: 3.98s\tremaining: 7.13s\n",
      "358:\tlearn: 0.0795106\ttotal: 3.98s\tremaining: 7.12s\n",
      "359:\tlearn: 0.0794701\ttotal: 3.99s\tremaining: 7.1s\n",
      "360:\tlearn: 0.0794220\ttotal: 4s\tremaining: 7.09s\n",
      "361:\tlearn: 0.0793873\ttotal: 4.01s\tremaining: 7.07s\n",
      "362:\tlearn: 0.0793583\ttotal: 4.02s\tremaining: 7.06s\n",
      "363:\tlearn: 0.0793107\ttotal: 4.03s\tremaining: 7.05s\n",
      "364:\tlearn: 0.0792748\ttotal: 4.04s\tremaining: 7.03s\n",
      "365:\tlearn: 0.0792360\ttotal: 4.05s\tremaining: 7.02s\n",
      "366:\tlearn: 0.0792158\ttotal: 4.06s\tremaining: 7s\n",
      "367:\tlearn: 0.0791984\ttotal: 4.07s\tremaining: 7s\n",
      "368:\tlearn: 0.0791764\ttotal: 4.08s\tremaining: 6.98s\n",
      "369:\tlearn: 0.0790537\ttotal: 4.09s\tremaining: 6.97s\n",
      "370:\tlearn: 0.0790067\ttotal: 4.1s\tremaining: 6.96s\n",
      "371:\tlearn: 0.0789608\ttotal: 4.11s\tremaining: 6.94s\n",
      "372:\tlearn: 0.0789177\ttotal: 4.12s\tremaining: 6.93s\n",
      "373:\tlearn: 0.0788903\ttotal: 4.13s\tremaining: 6.92s\n",
      "374:\tlearn: 0.0788308\ttotal: 4.14s\tremaining: 6.9s\n",
      "375:\tlearn: 0.0788029\ttotal: 4.15s\tremaining: 6.89s\n",
      "376:\tlearn: 0.0787832\ttotal: 4.16s\tremaining: 6.88s\n",
      "377:\tlearn: 0.0787526\ttotal: 4.17s\tremaining: 6.86s\n",
      "378:\tlearn: 0.0787232\ttotal: 4.18s\tremaining: 6.85s\n",
      "379:\tlearn: 0.0786989\ttotal: 4.19s\tremaining: 6.83s\n",
      "380:\tlearn: 0.0786570\ttotal: 4.2s\tremaining: 6.82s\n",
      "381:\tlearn: 0.0786397\ttotal: 4.21s\tremaining: 6.81s\n",
      "382:\tlearn: 0.0786204\ttotal: 4.21s\tremaining: 6.79s\n",
      "383:\tlearn: 0.0785977\ttotal: 4.23s\tremaining: 6.78s\n",
      "384:\tlearn: 0.0785800\ttotal: 4.23s\tremaining: 6.76s\n",
      "385:\tlearn: 0.0785243\ttotal: 4.25s\tremaining: 6.76s\n",
      "386:\tlearn: 0.0784935\ttotal: 4.26s\tremaining: 6.75s\n",
      "387:\tlearn: 0.0784666\ttotal: 4.27s\tremaining: 6.73s\n",
      "388:\tlearn: 0.0784333\ttotal: 4.28s\tremaining: 6.72s\n",
      "389:\tlearn: 0.0783999\ttotal: 4.29s\tremaining: 6.71s\n",
      "390:\tlearn: 0.0783618\ttotal: 4.3s\tremaining: 6.7s\n",
      "391:\tlearn: 0.0783247\ttotal: 4.31s\tremaining: 6.68s\n",
      "392:\tlearn: 0.0782885\ttotal: 4.32s\tremaining: 6.67s\n",
      "393:\tlearn: 0.0782567\ttotal: 4.33s\tremaining: 6.66s\n",
      "394:\tlearn: 0.0782148\ttotal: 4.34s\tremaining: 6.64s\n",
      "395:\tlearn: 0.0781861\ttotal: 4.35s\tremaining: 6.63s\n",
      "396:\tlearn: 0.0781644\ttotal: 4.35s\tremaining: 6.61s\n",
      "397:\tlearn: 0.0781193\ttotal: 4.37s\tremaining: 6.6s\n",
      "398:\tlearn: 0.0780912\ttotal: 4.37s\tremaining: 6.59s\n",
      "399:\tlearn: 0.0780649\ttotal: 4.39s\tremaining: 6.58s\n",
      "400:\tlearn: 0.0780431\ttotal: 4.39s\tremaining: 6.56s\n",
      "401:\tlearn: 0.0780144\ttotal: 4.4s\tremaining: 6.55s\n",
      "402:\tlearn: 0.0779642\ttotal: 4.41s\tremaining: 6.54s\n",
      "403:\tlearn: 0.0779398\ttotal: 4.42s\tremaining: 6.52s\n",
      "404:\tlearn: 0.0779243\ttotal: 4.43s\tremaining: 6.51s\n",
      "405:\tlearn: 0.0778879\ttotal: 4.44s\tremaining: 6.5s\n",
      "406:\tlearn: 0.0778664\ttotal: 4.45s\tremaining: 6.49s\n",
      "407:\tlearn: 0.0778459\ttotal: 4.46s\tremaining: 6.47s\n",
      "408:\tlearn: 0.0778184\ttotal: 4.47s\tremaining: 6.46s\n",
      "409:\tlearn: 0.0777824\ttotal: 4.48s\tremaining: 6.45s\n",
      "410:\tlearn: 0.0777460\ttotal: 4.5s\tremaining: 6.44s\n",
      "411:\tlearn: 0.0776991\ttotal: 4.52s\tremaining: 6.45s\n",
      "412:\tlearn: 0.0776698\ttotal: 4.54s\tremaining: 6.45s\n",
      "413:\tlearn: 0.0775856\ttotal: 4.56s\tremaining: 6.46s\n",
      "414:\tlearn: 0.0774916\ttotal: 4.59s\tremaining: 6.46s\n",
      "415:\tlearn: 0.0774678\ttotal: 4.61s\tremaining: 6.47s\n",
      "416:\tlearn: 0.0774364\ttotal: 4.62s\tremaining: 6.46s\n",
      "417:\tlearn: 0.0774213\ttotal: 4.63s\tremaining: 6.44s\n",
      "418:\tlearn: 0.0773578\ttotal: 4.63s\tremaining: 6.43s\n",
      "419:\tlearn: 0.0773159\ttotal: 4.65s\tremaining: 6.42s\n",
      "420:\tlearn: 0.0772896\ttotal: 4.65s\tremaining: 6.4s\n",
      "421:\tlearn: 0.0772488\ttotal: 4.66s\tremaining: 6.39s\n",
      "422:\tlearn: 0.0772200\ttotal: 4.67s\tremaining: 6.37s\n",
      "423:\tlearn: 0.0771975\ttotal: 4.68s\tremaining: 6.36s\n",
      "424:\tlearn: 0.0771878\ttotal: 4.69s\tremaining: 6.34s\n",
      "425:\tlearn: 0.0771635\ttotal: 4.7s\tremaining: 6.33s\n",
      "426:\tlearn: 0.0771330\ttotal: 4.71s\tremaining: 6.32s\n",
      "427:\tlearn: 0.0771007\ttotal: 4.72s\tremaining: 6.3s\n",
      "428:\tlearn: 0.0770657\ttotal: 4.73s\tremaining: 6.29s\n",
      "429:\tlearn: 0.0770342\ttotal: 4.74s\tremaining: 6.28s\n",
      "430:\tlearn: 0.0770041\ttotal: 4.75s\tremaining: 6.26s\n",
      "431:\tlearn: 0.0769630\ttotal: 4.75s\tremaining: 6.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432:\tlearn: 0.0769221\ttotal: 4.76s\tremaining: 6.24s\n",
      "433:\tlearn: 0.0769035\ttotal: 4.77s\tremaining: 6.22s\n",
      "434:\tlearn: 0.0768741\ttotal: 4.78s\tremaining: 6.21s\n",
      "435:\tlearn: 0.0768318\ttotal: 4.79s\tremaining: 6.2s\n",
      "436:\tlearn: 0.0767956\ttotal: 4.8s\tremaining: 6.18s\n",
      "437:\tlearn: 0.0767384\ttotal: 4.81s\tremaining: 6.17s\n",
      "438:\tlearn: 0.0767098\ttotal: 4.82s\tremaining: 6.15s\n",
      "439:\tlearn: 0.0766955\ttotal: 4.83s\tremaining: 6.14s\n",
      "440:\tlearn: 0.0766691\ttotal: 4.84s\tremaining: 6.13s\n",
      "441:\tlearn: 0.0766415\ttotal: 4.85s\tremaining: 6.12s\n",
      "442:\tlearn: 0.0766095\ttotal: 4.86s\tremaining: 6.11s\n",
      "443:\tlearn: 0.0765922\ttotal: 4.87s\tremaining: 6.09s\n",
      "444:\tlearn: 0.0765671\ttotal: 4.88s\tremaining: 6.08s\n",
      "445:\tlearn: 0.0764755\ttotal: 4.88s\tremaining: 6.07s\n",
      "446:\tlearn: 0.0764595\ttotal: 4.89s\tremaining: 6.05s\n",
      "447:\tlearn: 0.0764157\ttotal: 4.91s\tremaining: 6.04s\n",
      "448:\tlearn: 0.0763895\ttotal: 4.91s\tremaining: 6.03s\n",
      "449:\tlearn: 0.0763698\ttotal: 4.93s\tremaining: 6.02s\n",
      "450:\tlearn: 0.0763454\ttotal: 4.94s\tremaining: 6.01s\n",
      "451:\tlearn: 0.0763156\ttotal: 4.95s\tremaining: 6s\n",
      "452:\tlearn: 0.0762884\ttotal: 4.96s\tremaining: 5.99s\n",
      "453:\tlearn: 0.0762687\ttotal: 4.97s\tremaining: 5.98s\n",
      "454:\tlearn: 0.0762389\ttotal: 4.98s\tremaining: 5.96s\n",
      "455:\tlearn: 0.0762143\ttotal: 4.99s\tremaining: 5.95s\n",
      "456:\tlearn: 0.0761791\ttotal: 5s\tremaining: 5.94s\n",
      "457:\tlearn: 0.0761614\ttotal: 5.01s\tremaining: 5.93s\n",
      "458:\tlearn: 0.0761265\ttotal: 5.03s\tremaining: 5.93s\n",
      "459:\tlearn: 0.0760829\ttotal: 5.04s\tremaining: 5.92s\n",
      "460:\tlearn: 0.0760611\ttotal: 5.05s\tremaining: 5.9s\n",
      "461:\tlearn: 0.0760305\ttotal: 5.06s\tremaining: 5.89s\n",
      "462:\tlearn: 0.0760073\ttotal: 5.07s\tremaining: 5.88s\n",
      "463:\tlearn: 0.0759819\ttotal: 5.08s\tremaining: 5.87s\n",
      "464:\tlearn: 0.0759505\ttotal: 5.09s\tremaining: 5.86s\n",
      "465:\tlearn: 0.0759318\ttotal: 5.1s\tremaining: 5.84s\n",
      "466:\tlearn: 0.0758907\ttotal: 5.11s\tremaining: 5.83s\n",
      "467:\tlearn: 0.0758647\ttotal: 5.12s\tremaining: 5.82s\n",
      "468:\tlearn: 0.0758315\ttotal: 5.13s\tremaining: 5.81s\n",
      "469:\tlearn: 0.0758080\ttotal: 5.14s\tremaining: 5.8s\n",
      "470:\tlearn: 0.0757547\ttotal: 5.15s\tremaining: 5.79s\n",
      "471:\tlearn: 0.0757169\ttotal: 5.16s\tremaining: 5.77s\n",
      "472:\tlearn: 0.0756934\ttotal: 5.17s\tremaining: 5.76s\n",
      "473:\tlearn: 0.0756512\ttotal: 5.18s\tremaining: 5.75s\n",
      "474:\tlearn: 0.0756029\ttotal: 5.19s\tremaining: 5.74s\n",
      "475:\tlearn: 0.0755837\ttotal: 5.2s\tremaining: 5.72s\n",
      "476:\tlearn: 0.0755609\ttotal: 5.21s\tremaining: 5.71s\n",
      "477:\tlearn: 0.0755363\ttotal: 5.22s\tremaining: 5.7s\n",
      "478:\tlearn: 0.0755138\ttotal: 5.23s\tremaining: 5.69s\n",
      "479:\tlearn: 0.0754857\ttotal: 5.24s\tremaining: 5.68s\n",
      "480:\tlearn: 0.0754645\ttotal: 5.25s\tremaining: 5.66s\n",
      "481:\tlearn: 0.0754478\ttotal: 5.26s\tremaining: 5.65s\n",
      "482:\tlearn: 0.0754246\ttotal: 5.27s\tremaining: 5.64s\n",
      "483:\tlearn: 0.0754129\ttotal: 5.28s\tremaining: 5.63s\n",
      "484:\tlearn: 0.0753911\ttotal: 5.29s\tremaining: 5.62s\n",
      "485:\tlearn: 0.0753765\ttotal: 5.3s\tremaining: 5.6s\n",
      "486:\tlearn: 0.0753545\ttotal: 5.31s\tremaining: 5.59s\n",
      "487:\tlearn: 0.0753397\ttotal: 5.32s\tremaining: 5.58s\n",
      "488:\tlearn: 0.0753108\ttotal: 5.33s\tremaining: 5.57s\n",
      "489:\tlearn: 0.0752962\ttotal: 5.34s\tremaining: 5.55s\n",
      "490:\tlearn: 0.0752672\ttotal: 5.35s\tremaining: 5.54s\n",
      "491:\tlearn: 0.0752471\ttotal: 5.36s\tremaining: 5.53s\n",
      "492:\tlearn: 0.0752209\ttotal: 5.37s\tremaining: 5.52s\n",
      "493:\tlearn: 0.0752031\ttotal: 5.38s\tremaining: 5.51s\n",
      "494:\tlearn: 0.0751879\ttotal: 5.39s\tremaining: 5.5s\n",
      "495:\tlearn: 0.0751482\ttotal: 5.4s\tremaining: 5.48s\n",
      "496:\tlearn: 0.0751341\ttotal: 5.41s\tremaining: 5.47s\n",
      "497:\tlearn: 0.0751116\ttotal: 5.42s\tremaining: 5.46s\n",
      "498:\tlearn: 0.0750753\ttotal: 5.43s\tremaining: 5.45s\n",
      "499:\tlearn: 0.0750514\ttotal: 5.44s\tremaining: 5.44s\n",
      "500:\tlearn: 0.0750165\ttotal: 5.45s\tremaining: 5.43s\n",
      "501:\tlearn: 0.0750033\ttotal: 5.46s\tremaining: 5.41s\n",
      "502:\tlearn: 0.0749896\ttotal: 5.47s\tremaining: 5.4s\n",
      "503:\tlearn: 0.0749695\ttotal: 5.48s\tremaining: 5.39s\n",
      "504:\tlearn: 0.0749455\ttotal: 5.49s\tremaining: 5.38s\n",
      "505:\tlearn: 0.0749211\ttotal: 5.5s\tremaining: 5.37s\n",
      "506:\tlearn: 0.0748965\ttotal: 5.52s\tremaining: 5.37s\n",
      "507:\tlearn: 0.0748639\ttotal: 5.54s\tremaining: 5.37s\n",
      "508:\tlearn: 0.0748507\ttotal: 5.57s\tremaining: 5.37s\n",
      "509:\tlearn: 0.0748216\ttotal: 5.6s\tremaining: 5.38s\n",
      "510:\tlearn: 0.0748027\ttotal: 5.61s\tremaining: 5.37s\n",
      "511:\tlearn: 0.0747795\ttotal: 5.64s\tremaining: 5.37s\n",
      "512:\tlearn: 0.0747673\ttotal: 5.65s\tremaining: 5.36s\n",
      "513:\tlearn: 0.0747446\ttotal: 5.66s\tremaining: 5.35s\n",
      "514:\tlearn: 0.0747179\ttotal: 5.67s\tremaining: 5.33s\n",
      "515:\tlearn: 0.0746957\ttotal: 5.68s\tremaining: 5.33s\n",
      "516:\tlearn: 0.0746798\ttotal: 5.69s\tremaining: 5.31s\n",
      "517:\tlearn: 0.0746485\ttotal: 5.7s\tremaining: 5.3s\n",
      "518:\tlearn: 0.0746344\ttotal: 5.7s\tremaining: 5.29s\n",
      "519:\tlearn: 0.0746190\ttotal: 5.72s\tremaining: 5.28s\n",
      "520:\tlearn: 0.0745958\ttotal: 5.72s\tremaining: 5.26s\n",
      "521:\tlearn: 0.0745628\ttotal: 5.74s\tremaining: 5.25s\n",
      "522:\tlearn: 0.0745265\ttotal: 5.74s\tremaining: 5.24s\n",
      "523:\tlearn: 0.0745125\ttotal: 5.75s\tremaining: 5.23s\n",
      "524:\tlearn: 0.0744326\ttotal: 5.76s\tremaining: 5.21s\n",
      "525:\tlearn: 0.0744201\ttotal: 5.77s\tremaining: 5.2s\n",
      "526:\tlearn: 0.0744041\ttotal: 5.78s\tremaining: 5.19s\n",
      "527:\tlearn: 0.0743639\ttotal: 5.79s\tremaining: 5.18s\n",
      "528:\tlearn: 0.0743437\ttotal: 5.8s\tremaining: 5.17s\n",
      "529:\tlearn: 0.0743267\ttotal: 5.81s\tremaining: 5.15s\n",
      "530:\tlearn: 0.0743127\ttotal: 5.82s\tremaining: 5.14s\n",
      "531:\tlearn: 0.0742657\ttotal: 5.83s\tremaining: 5.13s\n",
      "532:\tlearn: 0.0742345\ttotal: 5.84s\tremaining: 5.12s\n",
      "533:\tlearn: 0.0742143\ttotal: 5.85s\tremaining: 5.11s\n",
      "534:\tlearn: 0.0741848\ttotal: 5.86s\tremaining: 5.09s\n",
      "535:\tlearn: 0.0741629\ttotal: 5.87s\tremaining: 5.08s\n",
      "536:\tlearn: 0.0741362\ttotal: 5.88s\tremaining: 5.07s\n",
      "537:\tlearn: 0.0740537\ttotal: 5.89s\tremaining: 5.06s\n",
      "538:\tlearn: 0.0740319\ttotal: 5.9s\tremaining: 5.05s\n",
      "539:\tlearn: 0.0739993\ttotal: 5.92s\tremaining: 5.04s\n",
      "540:\tlearn: 0.0739750\ttotal: 5.92s\tremaining: 5.03s\n",
      "541:\tlearn: 0.0739569\ttotal: 5.93s\tremaining: 5.01s\n",
      "542:\tlearn: 0.0739405\ttotal: 5.94s\tremaining: 5s\n",
      "543:\tlearn: 0.0739232\ttotal: 5.95s\tremaining: 4.99s\n",
      "544:\tlearn: 0.0738976\ttotal: 5.96s\tremaining: 4.98s\n",
      "545:\tlearn: 0.0738789\ttotal: 5.97s\tremaining: 4.96s\n",
      "546:\tlearn: 0.0738638\ttotal: 5.98s\tremaining: 4.95s\n",
      "547:\tlearn: 0.0738305\ttotal: 5.99s\tremaining: 4.94s\n",
      "548:\tlearn: 0.0738123\ttotal: 6.02s\tremaining: 4.94s\n",
      "549:\tlearn: 0.0737897\ttotal: 6.04s\tremaining: 4.94s\n",
      "550:\tlearn: 0.0737480\ttotal: 6.05s\tremaining: 4.93s\n",
      "551:\tlearn: 0.0737290\ttotal: 6.06s\tremaining: 4.92s\n",
      "552:\tlearn: 0.0736928\ttotal: 6.07s\tremaining: 4.91s\n",
      "553:\tlearn: 0.0736773\ttotal: 6.08s\tremaining: 4.9s\n",
      "554:\tlearn: 0.0736529\ttotal: 6.09s\tremaining: 4.88s\n",
      "555:\tlearn: 0.0736356\ttotal: 6.1s\tremaining: 4.87s\n",
      "556:\tlearn: 0.0736299\ttotal: 6.11s\tremaining: 4.86s\n",
      "557:\tlearn: 0.0736094\ttotal: 6.12s\tremaining: 4.85s\n",
      "558:\tlearn: 0.0735881\ttotal: 6.13s\tremaining: 4.84s\n",
      "559:\tlearn: 0.0735513\ttotal: 6.14s\tremaining: 4.83s\n",
      "560:\tlearn: 0.0735206\ttotal: 6.15s\tremaining: 4.81s\n",
      "561:\tlearn: 0.0735027\ttotal: 6.16s\tremaining: 4.8s\n",
      "562:\tlearn: 0.0734941\ttotal: 6.18s\tremaining: 4.79s\n",
      "563:\tlearn: 0.0734839\ttotal: 6.19s\tremaining: 4.78s\n",
      "564:\tlearn: 0.0734460\ttotal: 6.2s\tremaining: 4.77s\n",
      "565:\tlearn: 0.0734255\ttotal: 6.21s\tremaining: 4.76s\n",
      "566:\tlearn: 0.0734051\ttotal: 6.22s\tremaining: 4.75s\n",
      "567:\tlearn: 0.0733765\ttotal: 6.23s\tremaining: 4.74s\n",
      "568:\tlearn: 0.0733549\ttotal: 6.24s\tremaining: 4.73s\n",
      "569:\tlearn: 0.0733395\ttotal: 6.25s\tremaining: 4.72s\n",
      "570:\tlearn: 0.0733220\ttotal: 6.26s\tremaining: 4.7s\n",
      "571:\tlearn: 0.0733000\ttotal: 6.27s\tremaining: 4.69s\n",
      "572:\tlearn: 0.0732762\ttotal: 6.28s\tremaining: 4.68s\n",
      "573:\tlearn: 0.0732585\ttotal: 6.29s\tremaining: 4.67s\n",
      "574:\tlearn: 0.0732288\ttotal: 6.3s\tremaining: 4.66s\n",
      "575:\tlearn: 0.0732034\ttotal: 6.31s\tremaining: 4.65s\n",
      "576:\tlearn: 0.0731920\ttotal: 6.32s\tremaining: 4.63s\n",
      "577:\tlearn: 0.0731629\ttotal: 6.33s\tremaining: 4.62s\n",
      "578:\tlearn: 0.0731217\ttotal: 6.34s\tremaining: 4.61s\n",
      "579:\tlearn: 0.0731009\ttotal: 6.35s\tremaining: 4.6s\n",
      "580:\tlearn: 0.0730730\ttotal: 6.36s\tremaining: 4.59s\n",
      "581:\tlearn: 0.0730543\ttotal: 6.37s\tremaining: 4.57s\n",
      "582:\tlearn: 0.0730398\ttotal: 6.38s\tremaining: 4.56s\n",
      "583:\tlearn: 0.0730291\ttotal: 6.39s\tremaining: 4.55s\n",
      "584:\tlearn: 0.0730153\ttotal: 6.39s\tremaining: 4.54s\n",
      "585:\tlearn: 0.0729859\ttotal: 6.4s\tremaining: 4.52s\n",
      "586:\tlearn: 0.0729711\ttotal: 6.41s\tremaining: 4.51s\n",
      "587:\tlearn: 0.0729574\ttotal: 6.42s\tremaining: 4.5s\n",
      "588:\tlearn: 0.0729422\ttotal: 6.43s\tremaining: 4.49s\n",
      "589:\tlearn: 0.0728993\ttotal: 6.44s\tremaining: 4.47s\n",
      "590:\tlearn: 0.0728667\ttotal: 6.45s\tremaining: 4.46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591:\tlearn: 0.0728460\ttotal: 6.46s\tremaining: 4.45s\n",
      "592:\tlearn: 0.0728380\ttotal: 6.47s\tremaining: 4.44s\n",
      "593:\tlearn: 0.0728223\ttotal: 6.48s\tremaining: 4.43s\n",
      "594:\tlearn: 0.0727968\ttotal: 6.49s\tremaining: 4.42s\n",
      "595:\tlearn: 0.0727778\ttotal: 6.5s\tremaining: 4.4s\n",
      "596:\tlearn: 0.0727518\ttotal: 6.5s\tremaining: 4.39s\n",
      "597:\tlearn: 0.0727227\ttotal: 6.54s\tremaining: 4.4s\n",
      "598:\tlearn: 0.0727031\ttotal: 6.59s\tremaining: 4.41s\n",
      "599:\tlearn: 0.0726672\ttotal: 6.64s\tremaining: 4.42s\n",
      "600:\tlearn: 0.0726544\ttotal: 6.67s\tremaining: 4.43s\n",
      "601:\tlearn: 0.0726451\ttotal: 6.69s\tremaining: 4.42s\n",
      "602:\tlearn: 0.0726322\ttotal: 6.71s\tremaining: 4.41s\n",
      "603:\tlearn: 0.0726158\ttotal: 6.71s\tremaining: 4.4s\n",
      "604:\tlearn: 0.0725722\ttotal: 6.73s\tremaining: 4.39s\n",
      "605:\tlearn: 0.0725505\ttotal: 6.73s\tremaining: 4.38s\n",
      "606:\tlearn: 0.0725351\ttotal: 6.75s\tremaining: 4.37s\n",
      "607:\tlearn: 0.0725183\ttotal: 6.76s\tremaining: 4.36s\n",
      "608:\tlearn: 0.0724869\ttotal: 6.77s\tremaining: 4.35s\n",
      "609:\tlearn: 0.0724686\ttotal: 6.78s\tremaining: 4.34s\n",
      "610:\tlearn: 0.0724590\ttotal: 6.79s\tremaining: 4.32s\n",
      "611:\tlearn: 0.0724403\ttotal: 6.8s\tremaining: 4.31s\n",
      "612:\tlearn: 0.0724234\ttotal: 6.81s\tremaining: 4.3s\n",
      "613:\tlearn: 0.0723990\ttotal: 6.82s\tremaining: 4.29s\n",
      "614:\tlearn: 0.0723827\ttotal: 6.84s\tremaining: 4.28s\n",
      "615:\tlearn: 0.0723648\ttotal: 6.85s\tremaining: 4.27s\n",
      "616:\tlearn: 0.0723390\ttotal: 6.86s\tremaining: 4.26s\n",
      "617:\tlearn: 0.0723024\ttotal: 6.88s\tremaining: 4.25s\n",
      "618:\tlearn: 0.0722850\ttotal: 6.89s\tremaining: 4.24s\n",
      "619:\tlearn: 0.0722687\ttotal: 6.9s\tremaining: 4.23s\n",
      "620:\tlearn: 0.0722415\ttotal: 6.91s\tremaining: 4.22s\n",
      "621:\tlearn: 0.0722276\ttotal: 6.92s\tremaining: 4.21s\n",
      "622:\tlearn: 0.0722065\ttotal: 6.93s\tremaining: 4.2s\n",
      "623:\tlearn: 0.0721946\ttotal: 6.94s\tremaining: 4.18s\n",
      "624:\tlearn: 0.0721761\ttotal: 6.96s\tremaining: 4.17s\n",
      "625:\tlearn: 0.0721427\ttotal: 6.97s\tremaining: 4.16s\n",
      "626:\tlearn: 0.0721053\ttotal: 6.98s\tremaining: 4.15s\n",
      "627:\tlearn: 0.0720868\ttotal: 6.99s\tremaining: 4.14s\n",
      "628:\tlearn: 0.0720683\ttotal: 7s\tremaining: 4.13s\n",
      "629:\tlearn: 0.0720460\ttotal: 7.02s\tremaining: 4.12s\n",
      "630:\tlearn: 0.0720345\ttotal: 7.03s\tremaining: 4.11s\n",
      "631:\tlearn: 0.0720175\ttotal: 7.04s\tremaining: 4.1s\n",
      "632:\tlearn: 0.0720025\ttotal: 7.05s\tremaining: 4.09s\n",
      "633:\tlearn: 0.0719785\ttotal: 7.06s\tremaining: 4.07s\n",
      "634:\tlearn: 0.0719398\ttotal: 7.07s\tremaining: 4.06s\n",
      "635:\tlearn: 0.0719135\ttotal: 7.08s\tremaining: 4.05s\n",
      "636:\tlearn: 0.0719020\ttotal: 7.09s\tremaining: 4.04s\n",
      "637:\tlearn: 0.0718822\ttotal: 7.1s\tremaining: 4.03s\n",
      "638:\tlearn: 0.0718562\ttotal: 7.11s\tremaining: 4.02s\n",
      "639:\tlearn: 0.0718477\ttotal: 7.12s\tremaining: 4.01s\n",
      "640:\tlearn: 0.0718326\ttotal: 7.14s\tremaining: 4s\n",
      "641:\tlearn: 0.0718097\ttotal: 7.15s\tremaining: 3.98s\n",
      "642:\tlearn: 0.0717765\ttotal: 7.16s\tremaining: 3.97s\n",
      "643:\tlearn: 0.0717670\ttotal: 7.17s\tremaining: 3.96s\n",
      "644:\tlearn: 0.0717480\ttotal: 7.18s\tremaining: 3.95s\n",
      "645:\tlearn: 0.0717410\ttotal: 7.19s\tremaining: 3.94s\n",
      "646:\tlearn: 0.0717112\ttotal: 7.2s\tremaining: 3.93s\n",
      "647:\tlearn: 0.0717039\ttotal: 7.21s\tremaining: 3.92s\n",
      "648:\tlearn: 0.0716717\ttotal: 7.22s\tremaining: 3.91s\n",
      "649:\tlearn: 0.0716463\ttotal: 7.23s\tremaining: 3.9s\n",
      "650:\tlearn: 0.0716008\ttotal: 7.24s\tremaining: 3.88s\n",
      "651:\tlearn: 0.0715815\ttotal: 7.26s\tremaining: 3.87s\n",
      "652:\tlearn: 0.0715604\ttotal: 7.27s\tremaining: 3.86s\n",
      "653:\tlearn: 0.0715562\ttotal: 7.28s\tremaining: 3.85s\n",
      "654:\tlearn: 0.0715398\ttotal: 7.29s\tremaining: 3.84s\n",
      "655:\tlearn: 0.0715103\ttotal: 7.3s\tremaining: 3.83s\n",
      "656:\tlearn: 0.0714977\ttotal: 7.31s\tremaining: 3.82s\n",
      "657:\tlearn: 0.0714817\ttotal: 7.33s\tremaining: 3.81s\n",
      "658:\tlearn: 0.0714701\ttotal: 7.34s\tremaining: 3.8s\n",
      "659:\tlearn: 0.0714495\ttotal: 7.35s\tremaining: 3.79s\n",
      "660:\tlearn: 0.0714261\ttotal: 7.37s\tremaining: 3.78s\n",
      "661:\tlearn: 0.0713849\ttotal: 7.38s\tremaining: 3.77s\n",
      "662:\tlearn: 0.0713379\ttotal: 7.39s\tremaining: 3.76s\n",
      "663:\tlearn: 0.0713241\ttotal: 7.4s\tremaining: 3.75s\n",
      "664:\tlearn: 0.0713067\ttotal: 7.42s\tremaining: 3.74s\n",
      "665:\tlearn: 0.0712935\ttotal: 7.43s\tremaining: 3.73s\n",
      "666:\tlearn: 0.0712634\ttotal: 7.44s\tremaining: 3.72s\n",
      "667:\tlearn: 0.0712398\ttotal: 7.46s\tremaining: 3.71s\n",
      "668:\tlearn: 0.0712207\ttotal: 7.47s\tremaining: 3.7s\n",
      "669:\tlearn: 0.0712034\ttotal: 7.48s\tremaining: 3.69s\n",
      "670:\tlearn: 0.0711848\ttotal: 7.51s\tremaining: 3.68s\n",
      "671:\tlearn: 0.0711627\ttotal: 7.53s\tremaining: 3.67s\n",
      "672:\tlearn: 0.0711269\ttotal: 7.54s\tremaining: 3.66s\n",
      "673:\tlearn: 0.0711002\ttotal: 7.55s\tremaining: 3.65s\n",
      "674:\tlearn: 0.0710799\ttotal: 7.57s\tremaining: 3.64s\n",
      "675:\tlearn: 0.0710688\ttotal: 7.58s\tremaining: 3.63s\n",
      "676:\tlearn: 0.0710516\ttotal: 7.61s\tremaining: 3.63s\n",
      "677:\tlearn: 0.0710276\ttotal: 7.63s\tremaining: 3.62s\n",
      "678:\tlearn: 0.0710084\ttotal: 7.66s\tremaining: 3.62s\n",
      "679:\tlearn: 0.0709954\ttotal: 7.68s\tremaining: 3.62s\n",
      "680:\tlearn: 0.0709681\ttotal: 7.71s\tremaining: 3.61s\n",
      "681:\tlearn: 0.0709328\ttotal: 7.74s\tremaining: 3.61s\n",
      "682:\tlearn: 0.0709101\ttotal: 7.75s\tremaining: 3.6s\n",
      "683:\tlearn: 0.0708943\ttotal: 7.77s\tremaining: 3.59s\n",
      "684:\tlearn: 0.0708678\ttotal: 7.78s\tremaining: 3.58s\n",
      "685:\tlearn: 0.0708346\ttotal: 7.79s\tremaining: 3.57s\n",
      "686:\tlearn: 0.0708120\ttotal: 7.8s\tremaining: 3.55s\n",
      "687:\tlearn: 0.0708068\ttotal: 7.82s\tremaining: 3.54s\n",
      "688:\tlearn: 0.0707892\ttotal: 7.82s\tremaining: 3.53s\n",
      "689:\tlearn: 0.0707633\ttotal: 7.84s\tremaining: 3.52s\n",
      "690:\tlearn: 0.0707331\ttotal: 7.85s\tremaining: 3.51s\n",
      "691:\tlearn: 0.0707180\ttotal: 7.86s\tremaining: 3.5s\n",
      "692:\tlearn: 0.0707036\ttotal: 7.87s\tremaining: 3.49s\n",
      "693:\tlearn: 0.0706791\ttotal: 7.88s\tremaining: 3.48s\n",
      "694:\tlearn: 0.0706637\ttotal: 7.89s\tremaining: 3.46s\n",
      "695:\tlearn: 0.0706447\ttotal: 7.91s\tremaining: 3.45s\n",
      "696:\tlearn: 0.0706144\ttotal: 7.92s\tremaining: 3.44s\n",
      "697:\tlearn: 0.0706066\ttotal: 7.93s\tremaining: 3.43s\n",
      "698:\tlearn: 0.0705915\ttotal: 7.94s\tremaining: 3.42s\n",
      "699:\tlearn: 0.0705655\ttotal: 7.96s\tremaining: 3.41s\n",
      "700:\tlearn: 0.0705611\ttotal: 7.97s\tremaining: 3.4s\n",
      "701:\tlearn: 0.0705399\ttotal: 7.99s\tremaining: 3.39s\n",
      "702:\tlearn: 0.0705191\ttotal: 8s\tremaining: 3.38s\n",
      "703:\tlearn: 0.0705065\ttotal: 8.02s\tremaining: 3.37s\n",
      "704:\tlearn: 0.0704770\ttotal: 8.04s\tremaining: 3.36s\n",
      "705:\tlearn: 0.0704479\ttotal: 8.05s\tremaining: 3.35s\n",
      "706:\tlearn: 0.0704163\ttotal: 8.06s\tremaining: 3.34s\n",
      "707:\tlearn: 0.0704012\ttotal: 8.07s\tremaining: 3.33s\n",
      "708:\tlearn: 0.0703827\ttotal: 8.09s\tremaining: 3.32s\n",
      "709:\tlearn: 0.0703466\ttotal: 8.1s\tremaining: 3.31s\n",
      "710:\tlearn: 0.0703238\ttotal: 8.11s\tremaining: 3.29s\n",
      "711:\tlearn: 0.0703127\ttotal: 8.12s\tremaining: 3.29s\n",
      "712:\tlearn: 0.0702969\ttotal: 8.13s\tremaining: 3.27s\n",
      "713:\tlearn: 0.0702369\ttotal: 8.15s\tremaining: 3.26s\n",
      "714:\tlearn: 0.0702224\ttotal: 8.16s\tremaining: 3.25s\n",
      "715:\tlearn: 0.0701932\ttotal: 8.17s\tremaining: 3.24s\n",
      "716:\tlearn: 0.0701854\ttotal: 8.18s\tremaining: 3.23s\n",
      "717:\tlearn: 0.0701756\ttotal: 8.19s\tremaining: 3.22s\n",
      "718:\tlearn: 0.0701519\ttotal: 8.2s\tremaining: 3.21s\n",
      "719:\tlearn: 0.0701275\ttotal: 8.21s\tremaining: 3.19s\n",
      "720:\tlearn: 0.0701137\ttotal: 8.23s\tremaining: 3.18s\n",
      "721:\tlearn: 0.0700895\ttotal: 8.24s\tremaining: 3.17s\n",
      "722:\tlearn: 0.0700808\ttotal: 8.25s\tremaining: 3.16s\n",
      "723:\tlearn: 0.0700634\ttotal: 8.26s\tremaining: 3.15s\n",
      "724:\tlearn: 0.0700427\ttotal: 8.27s\tremaining: 3.14s\n",
      "725:\tlearn: 0.0700225\ttotal: 8.29s\tremaining: 3.13s\n",
      "726:\tlearn: 0.0699923\ttotal: 8.3s\tremaining: 3.12s\n",
      "727:\tlearn: 0.0699644\ttotal: 8.31s\tremaining: 3.1s\n",
      "728:\tlearn: 0.0699417\ttotal: 8.32s\tremaining: 3.09s\n",
      "729:\tlearn: 0.0699234\ttotal: 8.33s\tremaining: 3.08s\n",
      "730:\tlearn: 0.0699032\ttotal: 8.34s\tremaining: 3.07s\n",
      "731:\tlearn: 0.0698766\ttotal: 8.35s\tremaining: 3.06s\n",
      "732:\tlearn: 0.0698511\ttotal: 8.37s\tremaining: 3.05s\n",
      "733:\tlearn: 0.0698395\ttotal: 8.38s\tremaining: 3.04s\n",
      "734:\tlearn: 0.0698193\ttotal: 8.39s\tremaining: 3.03s\n",
      "735:\tlearn: 0.0697884\ttotal: 8.4s\tremaining: 3.01s\n",
      "736:\tlearn: 0.0697640\ttotal: 8.42s\tremaining: 3s\n",
      "737:\tlearn: 0.0697487\ttotal: 8.43s\tremaining: 2.99s\n",
      "738:\tlearn: 0.0697369\ttotal: 8.44s\tremaining: 2.98s\n",
      "739:\tlearn: 0.0697219\ttotal: 8.45s\tremaining: 2.97s\n",
      "740:\tlearn: 0.0697028\ttotal: 8.47s\tremaining: 2.96s\n",
      "741:\tlearn: 0.0696796\ttotal: 8.48s\tremaining: 2.95s\n",
      "742:\tlearn: 0.0696730\ttotal: 8.49s\tremaining: 2.94s\n",
      "743:\tlearn: 0.0696495\ttotal: 8.51s\tremaining: 2.93s\n",
      "744:\tlearn: 0.0696180\ttotal: 8.52s\tremaining: 2.92s\n",
      "745:\tlearn: 0.0696054\ttotal: 8.54s\tremaining: 2.91s\n",
      "746:\tlearn: 0.0695894\ttotal: 8.55s\tremaining: 2.9s\n",
      "747:\tlearn: 0.0695611\ttotal: 8.56s\tremaining: 2.88s\n",
      "748:\tlearn: 0.0695387\ttotal: 8.57s\tremaining: 2.87s\n",
      "749:\tlearn: 0.0695225\ttotal: 8.58s\tremaining: 2.86s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750:\tlearn: 0.0695090\ttotal: 8.6s\tremaining: 2.85s\n",
      "751:\tlearn: 0.0694950\ttotal: 8.61s\tremaining: 2.84s\n",
      "752:\tlearn: 0.0694835\ttotal: 8.62s\tremaining: 2.83s\n",
      "753:\tlearn: 0.0694486\ttotal: 8.63s\tremaining: 2.82s\n",
      "754:\tlearn: 0.0694441\ttotal: 8.65s\tremaining: 2.81s\n",
      "755:\tlearn: 0.0694298\ttotal: 8.67s\tremaining: 2.8s\n",
      "756:\tlearn: 0.0693957\ttotal: 8.7s\tremaining: 2.79s\n",
      "757:\tlearn: 0.0693836\ttotal: 8.72s\tremaining: 2.79s\n",
      "758:\tlearn: 0.0693610\ttotal: 8.75s\tremaining: 2.78s\n",
      "759:\tlearn: 0.0693493\ttotal: 8.77s\tremaining: 2.77s\n",
      "760:\tlearn: 0.0693388\ttotal: 8.78s\tremaining: 2.75s\n",
      "761:\tlearn: 0.0693274\ttotal: 8.79s\tremaining: 2.75s\n",
      "762:\tlearn: 0.0693122\ttotal: 8.8s\tremaining: 2.73s\n",
      "763:\tlearn: 0.0692898\ttotal: 8.82s\tremaining: 2.72s\n",
      "764:\tlearn: 0.0692807\ttotal: 8.84s\tremaining: 2.71s\n",
      "765:\tlearn: 0.0692694\ttotal: 8.85s\tremaining: 2.7s\n",
      "766:\tlearn: 0.0692561\ttotal: 8.86s\tremaining: 2.69s\n",
      "767:\tlearn: 0.0692364\ttotal: 8.87s\tremaining: 2.68s\n",
      "768:\tlearn: 0.0692278\ttotal: 8.88s\tremaining: 2.67s\n",
      "769:\tlearn: 0.0692128\ttotal: 8.89s\tremaining: 2.66s\n",
      "770:\tlearn: 0.0691753\ttotal: 8.91s\tremaining: 2.65s\n",
      "771:\tlearn: 0.0691533\ttotal: 8.93s\tremaining: 2.64s\n",
      "772:\tlearn: 0.0691383\ttotal: 8.94s\tremaining: 2.62s\n",
      "773:\tlearn: 0.0691241\ttotal: 8.95s\tremaining: 2.61s\n",
      "774:\tlearn: 0.0691158\ttotal: 8.96s\tremaining: 2.6s\n",
      "775:\tlearn: 0.0690976\ttotal: 8.98s\tremaining: 2.59s\n",
      "776:\tlearn: 0.0690794\ttotal: 8.99s\tremaining: 2.58s\n",
      "777:\tlearn: 0.0690713\ttotal: 9s\tremaining: 2.57s\n",
      "778:\tlearn: 0.0690570\ttotal: 9.03s\tremaining: 2.56s\n",
      "779:\tlearn: 0.0690437\ttotal: 9.04s\tremaining: 2.55s\n",
      "780:\tlearn: 0.0690339\ttotal: 9.06s\tremaining: 2.54s\n",
      "781:\tlearn: 0.0690093\ttotal: 9.07s\tremaining: 2.53s\n",
      "782:\tlearn: 0.0689949\ttotal: 9.09s\tremaining: 2.52s\n",
      "783:\tlearn: 0.0689853\ttotal: 9.1s\tremaining: 2.51s\n",
      "784:\tlearn: 0.0689632\ttotal: 9.11s\tremaining: 2.5s\n",
      "785:\tlearn: 0.0689409\ttotal: 9.12s\tremaining: 2.48s\n",
      "786:\tlearn: 0.0689283\ttotal: 9.13s\tremaining: 2.47s\n",
      "787:\tlearn: 0.0689224\ttotal: 9.14s\tremaining: 2.46s\n",
      "788:\tlearn: 0.0689069\ttotal: 9.16s\tremaining: 2.45s\n",
      "789:\tlearn: 0.0688796\ttotal: 9.17s\tremaining: 2.44s\n",
      "790:\tlearn: 0.0688648\ttotal: 9.19s\tremaining: 2.43s\n",
      "791:\tlearn: 0.0688514\ttotal: 9.2s\tremaining: 2.42s\n",
      "792:\tlearn: 0.0688309\ttotal: 9.22s\tremaining: 2.41s\n",
      "793:\tlearn: 0.0688132\ttotal: 9.23s\tremaining: 2.39s\n",
      "794:\tlearn: 0.0687909\ttotal: 9.24s\tremaining: 2.38s\n",
      "795:\tlearn: 0.0687768\ttotal: 9.25s\tremaining: 2.37s\n",
      "796:\tlearn: 0.0687558\ttotal: 9.27s\tremaining: 2.36s\n",
      "797:\tlearn: 0.0687423\ttotal: 9.28s\tremaining: 2.35s\n",
      "798:\tlearn: 0.0687049\ttotal: 9.29s\tremaining: 2.34s\n",
      "799:\tlearn: 0.0686959\ttotal: 9.3s\tremaining: 2.33s\n",
      "800:\tlearn: 0.0686671\ttotal: 9.31s\tremaining: 2.31s\n",
      "801:\tlearn: 0.0686586\ttotal: 9.32s\tremaining: 2.3s\n",
      "802:\tlearn: 0.0686467\ttotal: 9.34s\tremaining: 2.29s\n",
      "803:\tlearn: 0.0686330\ttotal: 9.35s\tremaining: 2.28s\n",
      "804:\tlearn: 0.0686114\ttotal: 9.36s\tremaining: 2.27s\n",
      "805:\tlearn: 0.0685910\ttotal: 9.38s\tremaining: 2.26s\n",
      "806:\tlearn: 0.0685752\ttotal: 9.38s\tremaining: 2.24s\n",
      "807:\tlearn: 0.0685636\ttotal: 9.4s\tremaining: 2.23s\n",
      "808:\tlearn: 0.0685457\ttotal: 9.41s\tremaining: 2.22s\n",
      "809:\tlearn: 0.0685356\ttotal: 9.43s\tremaining: 2.21s\n",
      "810:\tlearn: 0.0685065\ttotal: 9.44s\tremaining: 2.2s\n",
      "811:\tlearn: 0.0684780\ttotal: 9.45s\tremaining: 2.19s\n",
      "812:\tlearn: 0.0684654\ttotal: 9.47s\tremaining: 2.18s\n",
      "813:\tlearn: 0.0684569\ttotal: 9.48s\tremaining: 2.17s\n",
      "814:\tlearn: 0.0684475\ttotal: 9.49s\tremaining: 2.15s\n",
      "815:\tlearn: 0.0684306\ttotal: 9.51s\tremaining: 2.14s\n",
      "816:\tlearn: 0.0684154\ttotal: 9.51s\tremaining: 2.13s\n",
      "817:\tlearn: 0.0683975\ttotal: 9.53s\tremaining: 2.12s\n",
      "818:\tlearn: 0.0683904\ttotal: 9.54s\tremaining: 2.11s\n",
      "819:\tlearn: 0.0683672\ttotal: 9.55s\tremaining: 2.1s\n",
      "820:\tlearn: 0.0683457\ttotal: 9.56s\tremaining: 2.08s\n",
      "821:\tlearn: 0.0683299\ttotal: 9.58s\tremaining: 2.07s\n",
      "822:\tlearn: 0.0683096\ttotal: 9.59s\tremaining: 2.06s\n",
      "823:\tlearn: 0.0682978\ttotal: 9.6s\tremaining: 2.05s\n",
      "824:\tlearn: 0.0682879\ttotal: 9.62s\tremaining: 2.04s\n",
      "825:\tlearn: 0.0682716\ttotal: 9.63s\tremaining: 2.03s\n",
      "826:\tlearn: 0.0682326\ttotal: 9.64s\tremaining: 2.02s\n",
      "827:\tlearn: 0.0682168\ttotal: 9.65s\tremaining: 2s\n",
      "828:\tlearn: 0.0681957\ttotal: 9.68s\tremaining: 2s\n",
      "829:\tlearn: 0.0681873\ttotal: 9.7s\tremaining: 1.99s\n",
      "830:\tlearn: 0.0681699\ttotal: 9.74s\tremaining: 1.98s\n",
      "831:\tlearn: 0.0681491\ttotal: 9.77s\tremaining: 1.97s\n",
      "832:\tlearn: 0.0681334\ttotal: 9.79s\tremaining: 1.96s\n",
      "833:\tlearn: 0.0681137\ttotal: 9.8s\tremaining: 1.95s\n",
      "834:\tlearn: 0.0680814\ttotal: 9.82s\tremaining: 1.94s\n",
      "835:\tlearn: 0.0680636\ttotal: 9.83s\tremaining: 1.93s\n",
      "836:\tlearn: 0.0680453\ttotal: 9.85s\tremaining: 1.92s\n",
      "837:\tlearn: 0.0680322\ttotal: 9.86s\tremaining: 1.91s\n",
      "838:\tlearn: 0.0680231\ttotal: 9.88s\tremaining: 1.9s\n",
      "839:\tlearn: 0.0680073\ttotal: 9.88s\tremaining: 1.88s\n",
      "840:\tlearn: 0.0679920\ttotal: 9.9s\tremaining: 1.87s\n",
      "841:\tlearn: 0.0679756\ttotal: 9.92s\tremaining: 1.86s\n",
      "842:\tlearn: 0.0679513\ttotal: 9.93s\tremaining: 1.85s\n",
      "843:\tlearn: 0.0679420\ttotal: 9.94s\tremaining: 1.84s\n",
      "844:\tlearn: 0.0679166\ttotal: 9.95s\tremaining: 1.82s\n",
      "845:\tlearn: 0.0679044\ttotal: 9.96s\tremaining: 1.81s\n",
      "846:\tlearn: 0.0678930\ttotal: 9.98s\tremaining: 1.8s\n",
      "847:\tlearn: 0.0678566\ttotal: 10s\tremaining: 1.79s\n",
      "848:\tlearn: 0.0678402\ttotal: 10s\tremaining: 1.78s\n",
      "849:\tlearn: 0.0678181\ttotal: 10s\tremaining: 1.77s\n",
      "850:\tlearn: 0.0678090\ttotal: 10s\tremaining: 1.76s\n",
      "851:\tlearn: 0.0677989\ttotal: 10.1s\tremaining: 1.75s\n",
      "852:\tlearn: 0.0677742\ttotal: 10.1s\tremaining: 1.74s\n",
      "853:\tlearn: 0.0677652\ttotal: 10.1s\tremaining: 1.72s\n",
      "854:\tlearn: 0.0677389\ttotal: 10.1s\tremaining: 1.71s\n",
      "855:\tlearn: 0.0677276\ttotal: 10.1s\tremaining: 1.7s\n",
      "856:\tlearn: 0.0676996\ttotal: 10.1s\tremaining: 1.69s\n",
      "857:\tlearn: 0.0676756\ttotal: 10.1s\tremaining: 1.68s\n",
      "858:\tlearn: 0.0676581\ttotal: 10.1s\tremaining: 1.67s\n",
      "859:\tlearn: 0.0676151\ttotal: 10.2s\tremaining: 1.65s\n",
      "860:\tlearn: 0.0675948\ttotal: 10.2s\tremaining: 1.64s\n",
      "861:\tlearn: 0.0675713\ttotal: 10.2s\tremaining: 1.63s\n",
      "862:\tlearn: 0.0675525\ttotal: 10.2s\tremaining: 1.62s\n",
      "863:\tlearn: 0.0675270\ttotal: 10.2s\tremaining: 1.61s\n",
      "864:\tlearn: 0.0675144\ttotal: 10.2s\tremaining: 1.59s\n",
      "865:\tlearn: 0.0674897\ttotal: 10.2s\tremaining: 1.58s\n",
      "866:\tlearn: 0.0674816\ttotal: 10.2s\tremaining: 1.57s\n",
      "867:\tlearn: 0.0674659\ttotal: 10.3s\tremaining: 1.56s\n",
      "868:\tlearn: 0.0674428\ttotal: 10.3s\tremaining: 1.55s\n",
      "869:\tlearn: 0.0674305\ttotal: 10.3s\tremaining: 1.53s\n",
      "870:\tlearn: 0.0673942\ttotal: 10.3s\tremaining: 1.52s\n",
      "871:\tlearn: 0.0673784\ttotal: 10.3s\tremaining: 1.51s\n",
      "872:\tlearn: 0.0673642\ttotal: 10.3s\tremaining: 1.5s\n",
      "873:\tlearn: 0.0673501\ttotal: 10.3s\tremaining: 1.49s\n",
      "874:\tlearn: 0.0673303\ttotal: 10.3s\tremaining: 1.48s\n",
      "875:\tlearn: 0.0673141\ttotal: 10.4s\tremaining: 1.47s\n",
      "876:\tlearn: 0.0672980\ttotal: 10.4s\tremaining: 1.45s\n",
      "877:\tlearn: 0.0672785\ttotal: 10.4s\tremaining: 1.44s\n",
      "878:\tlearn: 0.0672662\ttotal: 10.4s\tremaining: 1.43s\n",
      "879:\tlearn: 0.0672347\ttotal: 10.4s\tremaining: 1.42s\n",
      "880:\tlearn: 0.0672086\ttotal: 10.4s\tremaining: 1.41s\n",
      "881:\tlearn: 0.0671903\ttotal: 10.4s\tremaining: 1.4s\n",
      "882:\tlearn: 0.0671627\ttotal: 10.5s\tremaining: 1.39s\n",
      "883:\tlearn: 0.0671395\ttotal: 10.5s\tremaining: 1.37s\n",
      "884:\tlearn: 0.0671255\ttotal: 10.5s\tremaining: 1.36s\n",
      "885:\tlearn: 0.0671136\ttotal: 10.5s\tremaining: 1.35s\n",
      "886:\tlearn: 0.0670853\ttotal: 10.5s\tremaining: 1.34s\n",
      "887:\tlearn: 0.0670689\ttotal: 10.5s\tremaining: 1.33s\n",
      "888:\tlearn: 0.0670601\ttotal: 10.5s\tremaining: 1.31s\n",
      "889:\tlearn: 0.0670473\ttotal: 10.5s\tremaining: 1.3s\n",
      "890:\tlearn: 0.0670308\ttotal: 10.6s\tremaining: 1.29s\n",
      "891:\tlearn: 0.0670170\ttotal: 10.6s\tremaining: 1.28s\n",
      "892:\tlearn: 0.0669949\ttotal: 10.6s\tremaining: 1.27s\n",
      "893:\tlearn: 0.0669692\ttotal: 10.6s\tremaining: 1.26s\n",
      "894:\tlearn: 0.0669546\ttotal: 10.6s\tremaining: 1.25s\n",
      "895:\tlearn: 0.0669256\ttotal: 10.6s\tremaining: 1.23s\n",
      "896:\tlearn: 0.0669072\ttotal: 10.6s\tremaining: 1.22s\n",
      "897:\tlearn: 0.0668977\ttotal: 10.6s\tremaining: 1.21s\n",
      "898:\tlearn: 0.0668664\ttotal: 10.7s\tremaining: 1.2s\n",
      "899:\tlearn: 0.0668534\ttotal: 10.7s\tremaining: 1.19s\n",
      "900:\tlearn: 0.0668409\ttotal: 10.7s\tremaining: 1.17s\n",
      "901:\tlearn: 0.0668342\ttotal: 10.7s\tremaining: 1.16s\n",
      "902:\tlearn: 0.0668132\ttotal: 10.7s\tremaining: 1.15s\n",
      "903:\tlearn: 0.0668023\ttotal: 10.8s\tremaining: 1.14s\n",
      "904:\tlearn: 0.0667976\ttotal: 10.8s\tremaining: 1.13s\n",
      "905:\tlearn: 0.0667736\ttotal: 10.8s\tremaining: 1.12s\n",
      "906:\tlearn: 0.0667639\ttotal: 10.8s\tremaining: 1.11s\n",
      "907:\tlearn: 0.0667514\ttotal: 10.8s\tremaining: 1.1s\n",
      "908:\tlearn: 0.0667336\ttotal: 10.8s\tremaining: 1.09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909:\tlearn: 0.0667211\ttotal: 10.9s\tremaining: 1.07s\n",
      "910:\tlearn: 0.0666991\ttotal: 10.9s\tremaining: 1.06s\n",
      "911:\tlearn: 0.0666894\ttotal: 10.9s\tremaining: 1.05s\n",
      "912:\tlearn: 0.0666840\ttotal: 10.9s\tremaining: 1.04s\n",
      "913:\tlearn: 0.0666654\ttotal: 10.9s\tremaining: 1.02s\n",
      "914:\tlearn: 0.0666344\ttotal: 10.9s\tremaining: 1.01s\n",
      "915:\tlearn: 0.0666169\ttotal: 10.9s\tremaining: 1s\n",
      "916:\tlearn: 0.0665926\ttotal: 10.9s\tremaining: 990ms\n",
      "917:\tlearn: 0.0665829\ttotal: 11s\tremaining: 978ms\n",
      "918:\tlearn: 0.0665592\ttotal: 11s\tremaining: 967ms\n",
      "919:\tlearn: 0.0665488\ttotal: 11s\tremaining: 955ms\n",
      "920:\tlearn: 0.0665278\ttotal: 11s\tremaining: 943ms\n",
      "921:\tlearn: 0.0665183\ttotal: 11s\tremaining: 931ms\n",
      "922:\tlearn: 0.0665082\ttotal: 11s\tremaining: 919ms\n",
      "923:\tlearn: 0.0664967\ttotal: 11s\tremaining: 907ms\n",
      "924:\tlearn: 0.0664825\ttotal: 11s\tremaining: 895ms\n",
      "925:\tlearn: 0.0664600\ttotal: 11.1s\tremaining: 884ms\n",
      "926:\tlearn: 0.0664373\ttotal: 11.1s\tremaining: 872ms\n",
      "927:\tlearn: 0.0664264\ttotal: 11.1s\tremaining: 861ms\n",
      "928:\tlearn: 0.0664168\ttotal: 11.1s\tremaining: 849ms\n",
      "929:\tlearn: 0.0664038\ttotal: 11.1s\tremaining: 837ms\n",
      "930:\tlearn: 0.0663957\ttotal: 11.1s\tremaining: 826ms\n",
      "931:\tlearn: 0.0663879\ttotal: 11.2s\tremaining: 814ms\n",
      "932:\tlearn: 0.0663759\ttotal: 11.2s\tremaining: 803ms\n",
      "933:\tlearn: 0.0663612\ttotal: 11.2s\tremaining: 791ms\n",
      "934:\tlearn: 0.0663428\ttotal: 11.2s\tremaining: 779ms\n",
      "935:\tlearn: 0.0663298\ttotal: 11.2s\tremaining: 767ms\n",
      "936:\tlearn: 0.0663171\ttotal: 11.2s\tremaining: 755ms\n",
      "937:\tlearn: 0.0662958\ttotal: 11.2s\tremaining: 743ms\n",
      "938:\tlearn: 0.0662874\ttotal: 11.2s\tremaining: 731ms\n",
      "939:\tlearn: 0.0662713\ttotal: 11.3s\tremaining: 719ms\n",
      "940:\tlearn: 0.0662540\ttotal: 11.3s\tremaining: 707ms\n",
      "941:\tlearn: 0.0662319\ttotal: 11.3s\tremaining: 695ms\n",
      "942:\tlearn: 0.0661908\ttotal: 11.3s\tremaining: 684ms\n",
      "943:\tlearn: 0.0661840\ttotal: 11.3s\tremaining: 672ms\n",
      "944:\tlearn: 0.0661650\ttotal: 11.3s\tremaining: 660ms\n",
      "945:\tlearn: 0.0661501\ttotal: 11.4s\tremaining: 648ms\n",
      "946:\tlearn: 0.0661238\ttotal: 11.4s\tremaining: 636ms\n",
      "947:\tlearn: 0.0661097\ttotal: 11.4s\tremaining: 626ms\n",
      "948:\tlearn: 0.0660723\ttotal: 11.4s\tremaining: 615ms\n",
      "949:\tlearn: 0.0660511\ttotal: 11.4s\tremaining: 603ms\n",
      "950:\tlearn: 0.0660401\ttotal: 11.5s\tremaining: 591ms\n",
      "951:\tlearn: 0.0660238\ttotal: 11.5s\tremaining: 579ms\n",
      "952:\tlearn: 0.0660086\ttotal: 11.5s\tremaining: 567ms\n",
      "953:\tlearn: 0.0659870\ttotal: 11.5s\tremaining: 555ms\n",
      "954:\tlearn: 0.0659801\ttotal: 11.5s\tremaining: 543ms\n",
      "955:\tlearn: 0.0659580\ttotal: 11.5s\tremaining: 530ms\n",
      "956:\tlearn: 0.0659406\ttotal: 11.5s\tremaining: 518ms\n",
      "957:\tlearn: 0.0659290\ttotal: 11.5s\tremaining: 506ms\n",
      "958:\tlearn: 0.0659180\ttotal: 11.6s\tremaining: 494ms\n",
      "959:\tlearn: 0.0659060\ttotal: 11.6s\tremaining: 482ms\n",
      "960:\tlearn: 0.0658834\ttotal: 11.6s\tremaining: 470ms\n",
      "961:\tlearn: 0.0658510\ttotal: 11.6s\tremaining: 458ms\n",
      "962:\tlearn: 0.0658328\ttotal: 11.6s\tremaining: 446ms\n",
      "963:\tlearn: 0.0658145\ttotal: 11.6s\tremaining: 434ms\n",
      "964:\tlearn: 0.0657960\ttotal: 11.6s\tremaining: 422ms\n",
      "965:\tlearn: 0.0657855\ttotal: 11.6s\tremaining: 410ms\n",
      "966:\tlearn: 0.0657646\ttotal: 11.6s\tremaining: 397ms\n",
      "967:\tlearn: 0.0657611\ttotal: 11.7s\tremaining: 385ms\n",
      "968:\tlearn: 0.0657372\ttotal: 11.7s\tremaining: 373ms\n",
      "969:\tlearn: 0.0657078\ttotal: 11.7s\tremaining: 361ms\n",
      "970:\tlearn: 0.0656852\ttotal: 11.7s\tremaining: 349ms\n",
      "971:\tlearn: 0.0656564\ttotal: 11.7s\tremaining: 337ms\n",
      "972:\tlearn: 0.0656425\ttotal: 11.7s\tremaining: 325ms\n",
      "973:\tlearn: 0.0656261\ttotal: 11.7s\tremaining: 314ms\n",
      "974:\tlearn: 0.0655931\ttotal: 11.8s\tremaining: 302ms\n",
      "975:\tlearn: 0.0655751\ttotal: 11.8s\tremaining: 290ms\n",
      "976:\tlearn: 0.0655597\ttotal: 11.8s\tremaining: 278ms\n",
      "977:\tlearn: 0.0655413\ttotal: 11.8s\tremaining: 266ms\n",
      "978:\tlearn: 0.0655211\ttotal: 11.9s\tremaining: 254ms\n",
      "979:\tlearn: 0.0654976\ttotal: 11.9s\tremaining: 242ms\n",
      "980:\tlearn: 0.0654764\ttotal: 11.9s\tremaining: 230ms\n",
      "981:\tlearn: 0.0654679\ttotal: 11.9s\tremaining: 218ms\n",
      "982:\tlearn: 0.0654526\ttotal: 11.9s\tremaining: 206ms\n",
      "983:\tlearn: 0.0654182\ttotal: 11.9s\tremaining: 194ms\n",
      "984:\tlearn: 0.0653961\ttotal: 11.9s\tremaining: 182ms\n",
      "985:\tlearn: 0.0653748\ttotal: 11.9s\tremaining: 169ms\n",
      "986:\tlearn: 0.0653469\ttotal: 11.9s\tremaining: 157ms\n",
      "987:\tlearn: 0.0653356\ttotal: 12s\tremaining: 145ms\n",
      "988:\tlearn: 0.0653073\ttotal: 12s\tremaining: 133ms\n",
      "989:\tlearn: 0.0652731\ttotal: 12s\tremaining: 121ms\n",
      "990:\tlearn: 0.0652464\ttotal: 12s\tremaining: 109ms\n",
      "991:\tlearn: 0.0652404\ttotal: 12s\tremaining: 96.7ms\n",
      "992:\tlearn: 0.0652301\ttotal: 12s\tremaining: 84.7ms\n",
      "993:\tlearn: 0.0652229\ttotal: 12s\tremaining: 72.6ms\n",
      "994:\tlearn: 0.0652104\ttotal: 12s\tremaining: 60.4ms\n",
      "995:\tlearn: 0.0652023\ttotal: 12s\tremaining: 48.4ms\n",
      "996:\tlearn: 0.0651962\ttotal: 12.1s\tremaining: 36.3ms\n",
      "997:\tlearn: 0.0651819\ttotal: 12.1s\tremaining: 24.2ms\n",
      "998:\tlearn: 0.0651626\ttotal: 12.1s\tremaining: 12.1ms\n",
      "999:\tlearn: 0.0651287\ttotal: 12.1s\tremaining: 0us\n",
      "0:\tlearn: 0.3999188\ttotal: 18.6ms\tremaining: 18.6s\n",
      "1:\tlearn: 0.3618420\ttotal: 40.8ms\tremaining: 20.3s\n",
      "2:\tlearn: 0.3343247\ttotal: 59.3ms\tremaining: 19.7s\n",
      "3:\tlearn: 0.3106462\ttotal: 70ms\tremaining: 17.4s\n",
      "4:\tlearn: 0.2916778\ttotal: 86.2ms\tremaining: 17.1s\n",
      "5:\tlearn: 0.2776008\ttotal: 98.5ms\tremaining: 16.3s\n",
      "6:\tlearn: 0.2640998\ttotal: 107ms\tremaining: 15.2s\n",
      "7:\tlearn: 0.2557554\ttotal: 116ms\tremaining: 14.4s\n",
      "8:\tlearn: 0.2452911\ttotal: 132ms\tremaining: 14.5s\n",
      "9:\tlearn: 0.2334152\ttotal: 149ms\tremaining: 14.7s\n",
      "10:\tlearn: 0.2215118\ttotal: 157ms\tremaining: 14.1s\n",
      "11:\tlearn: 0.2155173\ttotal: 175ms\tremaining: 14.4s\n",
      "12:\tlearn: 0.2088710\ttotal: 184ms\tremaining: 14s\n",
      "13:\tlearn: 0.2043719\ttotal: 197ms\tremaining: 13.9s\n",
      "14:\tlearn: 0.2013999\ttotal: 210ms\tremaining: 13.8s\n",
      "15:\tlearn: 0.1983129\ttotal: 220ms\tremaining: 13.5s\n",
      "16:\tlearn: 0.1952370\ttotal: 231ms\tremaining: 13.4s\n",
      "17:\tlearn: 0.1914517\ttotal: 244ms\tremaining: 13.3s\n",
      "18:\tlearn: 0.1879258\ttotal: 254ms\tremaining: 13.1s\n",
      "19:\tlearn: 0.1866513\ttotal: 264ms\tremaining: 13s\n",
      "20:\tlearn: 0.1858205\ttotal: 276ms\tremaining: 12.9s\n",
      "21:\tlearn: 0.1843972\ttotal: 290ms\tremaining: 12.9s\n",
      "22:\tlearn: 0.1842000\ttotal: 306ms\tremaining: 13s\n",
      "23:\tlearn: 0.1837915\ttotal: 317ms\tremaining: 12.9s\n",
      "24:\tlearn: 0.1759978\ttotal: 338ms\tremaining: 13.2s\n",
      "25:\tlearn: 0.1751003\ttotal: 359ms\tremaining: 13.4s\n",
      "26:\tlearn: 0.1748597\ttotal: 375ms\tremaining: 13.5s\n",
      "27:\tlearn: 0.1744824\ttotal: 394ms\tremaining: 13.7s\n",
      "28:\tlearn: 0.1739431\ttotal: 402ms\tremaining: 13.5s\n",
      "29:\tlearn: 0.1738256\ttotal: 411ms\tremaining: 13.3s\n",
      "30:\tlearn: 0.1735461\ttotal: 427ms\tremaining: 13.4s\n",
      "31:\tlearn: 0.1731771\ttotal: 439ms\tremaining: 13.3s\n",
      "32:\tlearn: 0.1731549\ttotal: 447ms\tremaining: 13.1s\n",
      "33:\tlearn: 0.1728371\ttotal: 462ms\tremaining: 13.1s\n",
      "34:\tlearn: 0.1727127\ttotal: 471ms\tremaining: 13s\n",
      "35:\tlearn: 0.1713328\ttotal: 486ms\tremaining: 13s\n",
      "36:\tlearn: 0.1712330\ttotal: 497ms\tremaining: 12.9s\n",
      "37:\tlearn: 0.1711584\ttotal: 518ms\tremaining: 13.1s\n",
      "38:\tlearn: 0.1650442\ttotal: 541ms\tremaining: 13.3s\n",
      "39:\tlearn: 0.1648862\ttotal: 564ms\tremaining: 13.5s\n",
      "40:\tlearn: 0.1646690\ttotal: 582ms\tremaining: 13.6s\n",
      "41:\tlearn: 0.1646127\ttotal: 608ms\tremaining: 13.9s\n",
      "42:\tlearn: 0.1640669\ttotal: 627ms\tremaining: 14s\n",
      "43:\tlearn: 0.1630012\ttotal: 646ms\tremaining: 14s\n",
      "44:\tlearn: 0.1625372\ttotal: 656ms\tremaining: 13.9s\n",
      "45:\tlearn: 0.1611873\ttotal: 669ms\tremaining: 13.9s\n",
      "46:\tlearn: 0.1607064\ttotal: 678ms\tremaining: 13.7s\n",
      "47:\tlearn: 0.1604845\ttotal: 692ms\tremaining: 13.7s\n",
      "48:\tlearn: 0.1602424\ttotal: 703ms\tremaining: 13.6s\n",
      "49:\tlearn: 0.1601477\ttotal: 716ms\tremaining: 13.6s\n",
      "50:\tlearn: 0.1598018\ttotal: 725ms\tremaining: 13.5s\n",
      "51:\tlearn: 0.1597470\ttotal: 737ms\tremaining: 13.4s\n",
      "52:\tlearn: 0.1555666\ttotal: 746ms\tremaining: 13.3s\n",
      "53:\tlearn: 0.1553728\ttotal: 758ms\tremaining: 13.3s\n",
      "54:\tlearn: 0.1553078\ttotal: 771ms\tremaining: 13.2s\n",
      "55:\tlearn: 0.1550158\ttotal: 788ms\tremaining: 13.3s\n",
      "56:\tlearn: 0.1547517\ttotal: 804ms\tremaining: 13.3s\n",
      "57:\tlearn: 0.1544737\ttotal: 813ms\tremaining: 13.2s\n",
      "58:\tlearn: 0.1543749\ttotal: 827ms\tremaining: 13.2s\n",
      "59:\tlearn: 0.1516291\ttotal: 836ms\tremaining: 13.1s\n",
      "60:\tlearn: 0.1461662\ttotal: 849ms\tremaining: 13.1s\n",
      "61:\tlearn: 0.1460020\ttotal: 865ms\tremaining: 13.1s\n",
      "62:\tlearn: 0.1458905\ttotal: 877ms\tremaining: 13s\n",
      "63:\tlearn: 0.1446056\ttotal: 895ms\tremaining: 13.1s\n",
      "64:\tlearn: 0.1443290\ttotal: 903ms\tremaining: 13s\n",
      "65:\tlearn: 0.1441268\ttotal: 915ms\tremaining: 12.9s\n",
      "66:\tlearn: 0.1407958\ttotal: 926ms\tremaining: 12.9s\n",
      "67:\tlearn: 0.1406901\ttotal: 936ms\tremaining: 12.8s\n",
      "68:\tlearn: 0.1406229\ttotal: 948ms\tremaining: 12.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69:\tlearn: 0.1398506\ttotal: 959ms\tremaining: 12.7s\n",
      "70:\tlearn: 0.1363405\ttotal: 968ms\tremaining: 12.7s\n",
      "71:\tlearn: 0.1357639\ttotal: 983ms\tremaining: 12.7s\n",
      "72:\tlearn: 0.1333160\ttotal: 995ms\tremaining: 12.6s\n",
      "73:\tlearn: 0.1323531\ttotal: 1s\tremaining: 12.6s\n",
      "74:\tlearn: 0.1297694\ttotal: 1.01s\tremaining: 12.5s\n",
      "75:\tlearn: 0.1282096\ttotal: 1.02s\tremaining: 12.4s\n",
      "76:\tlearn: 0.1271614\ttotal: 1.04s\tremaining: 12.4s\n",
      "77:\tlearn: 0.1252990\ttotal: 1.05s\tremaining: 12.4s\n",
      "78:\tlearn: 0.1228615\ttotal: 1.06s\tremaining: 12.3s\n",
      "79:\tlearn: 0.1218948\ttotal: 1.07s\tremaining: 12.3s\n",
      "80:\tlearn: 0.1199620\ttotal: 1.08s\tremaining: 12.2s\n",
      "81:\tlearn: 0.1184282\ttotal: 1.09s\tremaining: 12.2s\n",
      "82:\tlearn: 0.1176578\ttotal: 1.1s\tremaining: 12.1s\n",
      "83:\tlearn: 0.1165270\ttotal: 1.11s\tremaining: 12.1s\n",
      "84:\tlearn: 0.1145732\ttotal: 1.12s\tremaining: 12s\n",
      "85:\tlearn: 0.1131463\ttotal: 1.13s\tremaining: 12s\n",
      "86:\tlearn: 0.1125889\ttotal: 1.14s\tremaining: 12s\n",
      "87:\tlearn: 0.1117875\ttotal: 1.15s\tremaining: 12s\n",
      "88:\tlearn: 0.1110293\ttotal: 1.16s\tremaining: 11.9s\n",
      "89:\tlearn: 0.1101636\ttotal: 1.18s\tremaining: 11.9s\n",
      "90:\tlearn: 0.1096858\ttotal: 1.19s\tremaining: 11.9s\n",
      "91:\tlearn: 0.1090066\ttotal: 1.2s\tremaining: 11.8s\n",
      "92:\tlearn: 0.1085782\ttotal: 1.21s\tremaining: 11.8s\n",
      "93:\tlearn: 0.1079453\ttotal: 1.22s\tremaining: 11.8s\n",
      "94:\tlearn: 0.1076115\ttotal: 1.23s\tremaining: 11.7s\n",
      "95:\tlearn: 0.1071936\ttotal: 1.24s\tremaining: 11.7s\n",
      "96:\tlearn: 0.1069233\ttotal: 1.25s\tremaining: 11.6s\n",
      "97:\tlearn: 0.1064278\ttotal: 1.26s\tremaining: 11.6s\n",
      "98:\tlearn: 0.1058737\ttotal: 1.27s\tremaining: 11.6s\n",
      "99:\tlearn: 0.1051902\ttotal: 1.28s\tremaining: 11.6s\n",
      "100:\tlearn: 0.1046755\ttotal: 1.29s\tremaining: 11.5s\n",
      "101:\tlearn: 0.1043009\ttotal: 1.31s\tremaining: 11.5s\n",
      "102:\tlearn: 0.1038954\ttotal: 1.33s\tremaining: 11.6s\n",
      "103:\tlearn: 0.1036980\ttotal: 1.34s\tremaining: 11.5s\n",
      "104:\tlearn: 0.1026986\ttotal: 1.35s\tremaining: 11.5s\n",
      "105:\tlearn: 0.1023987\ttotal: 1.36s\tremaining: 11.5s\n",
      "106:\tlearn: 0.1020842\ttotal: 1.38s\tremaining: 11.5s\n",
      "107:\tlearn: 0.1016527\ttotal: 1.39s\tremaining: 11.5s\n",
      "108:\tlearn: 0.1012908\ttotal: 1.4s\tremaining: 11.5s\n",
      "109:\tlearn: 0.1005838\ttotal: 1.41s\tremaining: 11.4s\n",
      "110:\tlearn: 0.1004158\ttotal: 1.42s\tremaining: 11.4s\n",
      "111:\tlearn: 0.1002460\ttotal: 1.44s\tremaining: 11.4s\n",
      "112:\tlearn: 0.0999108\ttotal: 1.45s\tremaining: 11.4s\n",
      "113:\tlearn: 0.0996635\ttotal: 1.47s\tremaining: 11.4s\n",
      "114:\tlearn: 0.0993195\ttotal: 1.49s\tremaining: 11.4s\n",
      "115:\tlearn: 0.0991248\ttotal: 1.5s\tremaining: 11.4s\n",
      "116:\tlearn: 0.0988742\ttotal: 1.51s\tremaining: 11.4s\n",
      "117:\tlearn: 0.0984492\ttotal: 1.51s\tremaining: 11.3s\n",
      "118:\tlearn: 0.0982983\ttotal: 1.53s\tremaining: 11.3s\n",
      "119:\tlearn: 0.0980812\ttotal: 1.56s\tremaining: 11.4s\n",
      "120:\tlearn: 0.0979413\ttotal: 1.61s\tremaining: 11.7s\n",
      "121:\tlearn: 0.0977828\ttotal: 1.64s\tremaining: 11.8s\n",
      "122:\tlearn: 0.0975838\ttotal: 1.67s\tremaining: 11.9s\n",
      "123:\tlearn: 0.0971815\ttotal: 1.68s\tremaining: 11.9s\n",
      "124:\tlearn: 0.0970212\ttotal: 1.69s\tremaining: 11.9s\n",
      "125:\tlearn: 0.0968405\ttotal: 1.71s\tremaining: 11.8s\n",
      "126:\tlearn: 0.0964898\ttotal: 1.72s\tremaining: 11.8s\n",
      "127:\tlearn: 0.0963593\ttotal: 1.73s\tremaining: 11.8s\n",
      "128:\tlearn: 0.0960993\ttotal: 1.74s\tremaining: 11.7s\n",
      "129:\tlearn: 0.0958533\ttotal: 1.75s\tremaining: 11.7s\n",
      "130:\tlearn: 0.0956940\ttotal: 1.75s\tremaining: 11.6s\n",
      "131:\tlearn: 0.0954672\ttotal: 1.77s\tremaining: 11.6s\n",
      "132:\tlearn: 0.0952828\ttotal: 1.77s\tremaining: 11.6s\n",
      "133:\tlearn: 0.0951558\ttotal: 1.79s\tremaining: 11.6s\n",
      "134:\tlearn: 0.0950155\ttotal: 1.79s\tremaining: 11.5s\n",
      "135:\tlearn: 0.0947266\ttotal: 1.81s\tremaining: 11.5s\n",
      "136:\tlearn: 0.0945599\ttotal: 1.81s\tremaining: 11.4s\n",
      "137:\tlearn: 0.0944377\ttotal: 1.83s\tremaining: 11.4s\n",
      "138:\tlearn: 0.0941608\ttotal: 1.83s\tremaining: 11.4s\n",
      "139:\tlearn: 0.0940454\ttotal: 1.86s\tremaining: 11.4s\n",
      "140:\tlearn: 0.0938503\ttotal: 1.86s\tremaining: 11.4s\n",
      "141:\tlearn: 0.0937565\ttotal: 1.88s\tremaining: 11.3s\n",
      "142:\tlearn: 0.0935454\ttotal: 1.89s\tremaining: 11.3s\n",
      "143:\tlearn: 0.0934906\ttotal: 1.9s\tremaining: 11.3s\n",
      "144:\tlearn: 0.0933942\ttotal: 1.91s\tremaining: 11.3s\n",
      "145:\tlearn: 0.0932195\ttotal: 1.93s\tremaining: 11.3s\n",
      "146:\tlearn: 0.0931263\ttotal: 1.94s\tremaining: 11.3s\n",
      "147:\tlearn: 0.0930561\ttotal: 1.95s\tremaining: 11.2s\n",
      "148:\tlearn: 0.0929393\ttotal: 1.96s\tremaining: 11.2s\n",
      "149:\tlearn: 0.0928151\ttotal: 1.97s\tremaining: 11.2s\n",
      "150:\tlearn: 0.0925438\ttotal: 1.99s\tremaining: 11.2s\n",
      "151:\tlearn: 0.0923881\ttotal: 2s\tremaining: 11.2s\n",
      "152:\tlearn: 0.0921274\ttotal: 2.01s\tremaining: 11.1s\n",
      "153:\tlearn: 0.0919706\ttotal: 2.02s\tremaining: 11.1s\n",
      "154:\tlearn: 0.0918975\ttotal: 2.03s\tremaining: 11.1s\n",
      "155:\tlearn: 0.0917377\ttotal: 2.04s\tremaining: 11s\n",
      "156:\tlearn: 0.0916801\ttotal: 2.05s\tremaining: 11s\n",
      "157:\tlearn: 0.0915269\ttotal: 2.06s\tremaining: 11s\n",
      "158:\tlearn: 0.0914002\ttotal: 2.07s\tremaining: 10.9s\n",
      "159:\tlearn: 0.0913009\ttotal: 2.08s\tremaining: 10.9s\n",
      "160:\tlearn: 0.0912239\ttotal: 2.08s\tremaining: 10.9s\n",
      "161:\tlearn: 0.0910573\ttotal: 2.1s\tremaining: 10.8s\n",
      "162:\tlearn: 0.0908623\ttotal: 2.1s\tremaining: 10.8s\n",
      "163:\tlearn: 0.0907394\ttotal: 2.11s\tremaining: 10.8s\n",
      "164:\tlearn: 0.0905197\ttotal: 2.12s\tremaining: 10.7s\n",
      "165:\tlearn: 0.0904685\ttotal: 2.14s\tremaining: 10.7s\n",
      "166:\tlearn: 0.0902975\ttotal: 2.14s\tremaining: 10.7s\n",
      "167:\tlearn: 0.0902324\ttotal: 2.15s\tremaining: 10.7s\n",
      "168:\tlearn: 0.0899796\ttotal: 2.16s\tremaining: 10.6s\n",
      "169:\tlearn: 0.0898994\ttotal: 2.17s\tremaining: 10.6s\n",
      "170:\tlearn: 0.0897544\ttotal: 2.18s\tremaining: 10.6s\n",
      "171:\tlearn: 0.0894085\ttotal: 2.19s\tremaining: 10.5s\n",
      "172:\tlearn: 0.0892287\ttotal: 2.2s\tremaining: 10.5s\n",
      "173:\tlearn: 0.0891154\ttotal: 2.21s\tremaining: 10.5s\n",
      "174:\tlearn: 0.0889594\ttotal: 2.22s\tremaining: 10.5s\n",
      "175:\tlearn: 0.0888347\ttotal: 2.23s\tremaining: 10.4s\n",
      "176:\tlearn: 0.0886227\ttotal: 2.24s\tremaining: 10.4s\n",
      "177:\tlearn: 0.0885312\ttotal: 2.25s\tremaining: 10.4s\n",
      "178:\tlearn: 0.0884381\ttotal: 2.26s\tremaining: 10.4s\n",
      "179:\tlearn: 0.0883536\ttotal: 2.27s\tremaining: 10.3s\n",
      "180:\tlearn: 0.0882457\ttotal: 2.28s\tremaining: 10.3s\n",
      "181:\tlearn: 0.0881664\ttotal: 2.28s\tremaining: 10.3s\n",
      "182:\tlearn: 0.0880677\ttotal: 2.3s\tremaining: 10.3s\n",
      "183:\tlearn: 0.0880355\ttotal: 2.31s\tremaining: 10.2s\n",
      "184:\tlearn: 0.0879827\ttotal: 2.32s\tremaining: 10.2s\n",
      "185:\tlearn: 0.0878712\ttotal: 2.33s\tremaining: 10.2s\n",
      "186:\tlearn: 0.0878166\ttotal: 2.34s\tremaining: 10.2s\n",
      "187:\tlearn: 0.0877542\ttotal: 2.35s\tremaining: 10.2s\n",
      "188:\tlearn: 0.0876611\ttotal: 2.36s\tremaining: 10.1s\n",
      "189:\tlearn: 0.0874906\ttotal: 2.37s\tremaining: 10.1s\n",
      "190:\tlearn: 0.0874320\ttotal: 2.38s\tremaining: 10.1s\n",
      "191:\tlearn: 0.0872935\ttotal: 2.39s\tremaining: 10.1s\n",
      "192:\tlearn: 0.0872204\ttotal: 2.4s\tremaining: 10s\n",
      "193:\tlearn: 0.0870980\ttotal: 2.41s\tremaining: 10s\n",
      "194:\tlearn: 0.0870317\ttotal: 2.42s\tremaining: 9.99s\n",
      "195:\tlearn: 0.0869595\ttotal: 2.43s\tremaining: 9.98s\n",
      "196:\tlearn: 0.0869036\ttotal: 2.44s\tremaining: 9.96s\n",
      "197:\tlearn: 0.0868134\ttotal: 2.45s\tremaining: 9.94s\n",
      "198:\tlearn: 0.0867622\ttotal: 2.47s\tremaining: 9.95s\n",
      "199:\tlearn: 0.0865095\ttotal: 2.48s\tremaining: 9.92s\n",
      "200:\tlearn: 0.0863681\ttotal: 2.49s\tremaining: 9.9s\n",
      "201:\tlearn: 0.0863249\ttotal: 2.5s\tremaining: 9.87s\n",
      "202:\tlearn: 0.0862315\ttotal: 2.51s\tremaining: 9.84s\n",
      "203:\tlearn: 0.0861442\ttotal: 2.52s\tremaining: 9.83s\n",
      "204:\tlearn: 0.0860093\ttotal: 2.53s\tremaining: 9.8s\n",
      "205:\tlearn: 0.0859566\ttotal: 2.54s\tremaining: 9.77s\n",
      "206:\tlearn: 0.0858842\ttotal: 2.54s\tremaining: 9.75s\n",
      "207:\tlearn: 0.0858241\ttotal: 2.56s\tremaining: 9.73s\n",
      "208:\tlearn: 0.0857296\ttotal: 2.58s\tremaining: 9.77s\n",
      "209:\tlearn: 0.0856384\ttotal: 2.59s\tremaining: 9.76s\n",
      "210:\tlearn: 0.0855849\ttotal: 2.62s\tremaining: 9.81s\n",
      "211:\tlearn: 0.0855009\ttotal: 2.64s\tremaining: 9.81s\n",
      "212:\tlearn: 0.0854784\ttotal: 2.65s\tremaining: 9.81s\n",
      "213:\tlearn: 0.0854198\ttotal: 2.67s\tremaining: 9.82s\n",
      "214:\tlearn: 0.0853664\ttotal: 2.69s\tremaining: 9.83s\n",
      "215:\tlearn: 0.0853206\ttotal: 2.7s\tremaining: 9.82s\n",
      "216:\tlearn: 0.0852514\ttotal: 2.71s\tremaining: 9.79s\n",
      "217:\tlearn: 0.0852095\ttotal: 2.72s\tremaining: 9.76s\n",
      "218:\tlearn: 0.0851578\ttotal: 2.73s\tremaining: 9.74s\n",
      "219:\tlearn: 0.0850926\ttotal: 2.74s\tremaining: 9.71s\n",
      "220:\tlearn: 0.0850500\ttotal: 2.75s\tremaining: 9.69s\n",
      "221:\tlearn: 0.0850035\ttotal: 2.76s\tremaining: 9.67s\n",
      "222:\tlearn: 0.0848505\ttotal: 2.77s\tremaining: 9.63s\n",
      "223:\tlearn: 0.0847883\ttotal: 2.77s\tremaining: 9.61s\n",
      "224:\tlearn: 0.0847186\ttotal: 2.78s\tremaining: 9.59s\n",
      "225:\tlearn: 0.0846518\ttotal: 2.79s\tremaining: 9.56s\n",
      "226:\tlearn: 0.0846192\ttotal: 2.8s\tremaining: 9.54s\n",
      "227:\tlearn: 0.0845491\ttotal: 2.81s\tremaining: 9.52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228:\tlearn: 0.0845222\ttotal: 2.82s\tremaining: 9.49s\n",
      "229:\tlearn: 0.0844744\ttotal: 2.83s\tremaining: 9.47s\n",
      "230:\tlearn: 0.0844019\ttotal: 2.84s\tremaining: 9.45s\n",
      "231:\tlearn: 0.0843882\ttotal: 2.85s\tremaining: 9.42s\n",
      "232:\tlearn: 0.0843162\ttotal: 2.85s\tremaining: 9.4s\n",
      "233:\tlearn: 0.0842188\ttotal: 2.86s\tremaining: 9.38s\n",
      "234:\tlearn: 0.0841689\ttotal: 2.88s\tremaining: 9.37s\n",
      "235:\tlearn: 0.0841119\ttotal: 2.89s\tremaining: 9.35s\n",
      "236:\tlearn: 0.0840761\ttotal: 2.9s\tremaining: 9.33s\n",
      "237:\tlearn: 0.0840369\ttotal: 2.91s\tremaining: 9.31s\n",
      "238:\tlearn: 0.0839741\ttotal: 2.92s\tremaining: 9.3s\n",
      "239:\tlearn: 0.0838935\ttotal: 2.93s\tremaining: 9.28s\n",
      "240:\tlearn: 0.0838221\ttotal: 2.94s\tremaining: 9.26s\n",
      "241:\tlearn: 0.0837810\ttotal: 2.95s\tremaining: 9.23s\n",
      "242:\tlearn: 0.0837404\ttotal: 2.96s\tremaining: 9.22s\n",
      "243:\tlearn: 0.0837019\ttotal: 2.97s\tremaining: 9.2s\n",
      "244:\tlearn: 0.0836590\ttotal: 2.98s\tremaining: 9.18s\n",
      "245:\tlearn: 0.0836268\ttotal: 2.99s\tremaining: 9.16s\n",
      "246:\tlearn: 0.0835515\ttotal: 3s\tremaining: 9.13s\n",
      "247:\tlearn: 0.0833979\ttotal: 3.01s\tremaining: 9.12s\n",
      "248:\tlearn: 0.0833534\ttotal: 3.02s\tremaining: 9.1s\n",
      "249:\tlearn: 0.0833154\ttotal: 3.03s\tremaining: 9.08s\n",
      "250:\tlearn: 0.0832144\ttotal: 3.04s\tremaining: 9.06s\n",
      "251:\tlearn: 0.0831741\ttotal: 3.05s\tremaining: 9.04s\n",
      "252:\tlearn: 0.0831310\ttotal: 3.06s\tremaining: 9.03s\n",
      "253:\tlearn: 0.0830741\ttotal: 3.06s\tremaining: 9s\n",
      "254:\tlearn: 0.0829633\ttotal: 3.08s\tremaining: 8.99s\n",
      "255:\tlearn: 0.0829042\ttotal: 3.08s\tremaining: 8.96s\n",
      "256:\tlearn: 0.0828509\ttotal: 3.1s\tremaining: 8.96s\n",
      "257:\tlearn: 0.0827997\ttotal: 3.11s\tremaining: 8.94s\n",
      "258:\tlearn: 0.0827385\ttotal: 3.12s\tremaining: 8.92s\n",
      "259:\tlearn: 0.0826610\ttotal: 3.13s\tremaining: 8.9s\n",
      "260:\tlearn: 0.0826131\ttotal: 3.14s\tremaining: 8.88s\n",
      "261:\tlearn: 0.0825992\ttotal: 3.15s\tremaining: 8.87s\n",
      "262:\tlearn: 0.0825714\ttotal: 3.15s\tremaining: 8.84s\n",
      "263:\tlearn: 0.0825505\ttotal: 3.17s\tremaining: 8.83s\n",
      "264:\tlearn: 0.0825078\ttotal: 3.17s\tremaining: 8.8s\n",
      "265:\tlearn: 0.0824283\ttotal: 3.19s\tremaining: 8.8s\n",
      "266:\tlearn: 0.0823847\ttotal: 3.2s\tremaining: 8.78s\n",
      "267:\tlearn: 0.0823495\ttotal: 3.21s\tremaining: 8.77s\n",
      "268:\tlearn: 0.0822545\ttotal: 3.22s\tremaining: 8.74s\n",
      "269:\tlearn: 0.0822201\ttotal: 3.23s\tremaining: 8.73s\n",
      "270:\tlearn: 0.0821766\ttotal: 3.24s\tremaining: 8.72s\n",
      "271:\tlearn: 0.0821207\ttotal: 3.25s\tremaining: 8.69s\n",
      "272:\tlearn: 0.0820941\ttotal: 3.26s\tremaining: 8.68s\n",
      "273:\tlearn: 0.0820506\ttotal: 3.27s\tremaining: 8.66s\n",
      "274:\tlearn: 0.0819694\ttotal: 3.28s\tremaining: 8.65s\n",
      "275:\tlearn: 0.0819328\ttotal: 3.29s\tremaining: 8.62s\n",
      "276:\tlearn: 0.0818848\ttotal: 3.3s\tremaining: 8.61s\n",
      "277:\tlearn: 0.0818405\ttotal: 3.31s\tremaining: 8.59s\n",
      "278:\tlearn: 0.0818140\ttotal: 3.32s\tremaining: 8.58s\n",
      "279:\tlearn: 0.0817852\ttotal: 3.33s\tremaining: 8.56s\n",
      "280:\tlearn: 0.0817583\ttotal: 3.34s\tremaining: 8.54s\n",
      "281:\tlearn: 0.0816976\ttotal: 3.35s\tremaining: 8.52s\n",
      "282:\tlearn: 0.0816732\ttotal: 3.36s\tremaining: 8.51s\n",
      "283:\tlearn: 0.0816110\ttotal: 3.37s\tremaining: 8.48s\n",
      "284:\tlearn: 0.0815805\ttotal: 3.38s\tremaining: 8.47s\n",
      "285:\tlearn: 0.0815392\ttotal: 3.39s\tremaining: 8.45s\n",
      "286:\tlearn: 0.0814970\ttotal: 3.4s\tremaining: 8.44s\n",
      "287:\tlearn: 0.0813240\ttotal: 3.41s\tremaining: 8.42s\n",
      "288:\tlearn: 0.0813025\ttotal: 3.42s\tremaining: 8.41s\n",
      "289:\tlearn: 0.0812762\ttotal: 3.42s\tremaining: 8.39s\n",
      "290:\tlearn: 0.0812507\ttotal: 3.44s\tremaining: 8.37s\n",
      "291:\tlearn: 0.0811720\ttotal: 3.44s\tremaining: 8.35s\n",
      "292:\tlearn: 0.0811295\ttotal: 3.46s\tremaining: 8.34s\n",
      "293:\tlearn: 0.0810678\ttotal: 3.46s\tremaining: 8.32s\n",
      "294:\tlearn: 0.0810284\ttotal: 3.48s\tremaining: 8.31s\n",
      "295:\tlearn: 0.0809588\ttotal: 3.49s\tremaining: 8.29s\n",
      "296:\tlearn: 0.0809318\ttotal: 3.5s\tremaining: 8.28s\n",
      "297:\tlearn: 0.0808242\ttotal: 3.51s\tremaining: 8.26s\n",
      "298:\tlearn: 0.0807795\ttotal: 3.52s\tremaining: 8.25s\n",
      "299:\tlearn: 0.0807436\ttotal: 3.52s\tremaining: 8.22s\n",
      "300:\tlearn: 0.0807078\ttotal: 3.54s\tremaining: 8.22s\n",
      "301:\tlearn: 0.0806535\ttotal: 3.55s\tremaining: 8.2s\n",
      "302:\tlearn: 0.0806324\ttotal: 3.56s\tremaining: 8.18s\n",
      "303:\tlearn: 0.0805999\ttotal: 3.57s\tremaining: 8.17s\n",
      "304:\tlearn: 0.0805521\ttotal: 3.59s\tremaining: 8.18s\n",
      "305:\tlearn: 0.0804790\ttotal: 3.6s\tremaining: 8.18s\n",
      "306:\tlearn: 0.0804373\ttotal: 3.63s\tremaining: 8.2s\n",
      "307:\tlearn: 0.0803841\ttotal: 3.65s\tremaining: 8.19s\n",
      "308:\tlearn: 0.0802870\ttotal: 3.69s\tremaining: 8.24s\n",
      "309:\tlearn: 0.0802546\ttotal: 3.71s\tremaining: 8.25s\n",
      "310:\tlearn: 0.0802350\ttotal: 3.72s\tremaining: 8.24s\n",
      "311:\tlearn: 0.0801665\ttotal: 3.73s\tremaining: 8.22s\n",
      "312:\tlearn: 0.0801393\ttotal: 3.74s\tremaining: 8.2s\n",
      "313:\tlearn: 0.0800940\ttotal: 3.75s\tremaining: 8.18s\n",
      "314:\tlearn: 0.0800233\ttotal: 3.76s\tremaining: 8.17s\n",
      "315:\tlearn: 0.0799292\ttotal: 3.77s\tremaining: 8.16s\n",
      "316:\tlearn: 0.0798686\ttotal: 3.78s\tremaining: 8.14s\n",
      "317:\tlearn: 0.0798295\ttotal: 3.79s\tremaining: 8.12s\n",
      "318:\tlearn: 0.0798088\ttotal: 3.79s\tremaining: 8.1s\n",
      "319:\tlearn: 0.0797455\ttotal: 3.81s\tremaining: 8.09s\n",
      "320:\tlearn: 0.0797069\ttotal: 3.82s\tremaining: 8.07s\n",
      "321:\tlearn: 0.0796625\ttotal: 3.83s\tremaining: 8.07s\n",
      "322:\tlearn: 0.0796232\ttotal: 3.84s\tremaining: 8.06s\n",
      "323:\tlearn: 0.0795647\ttotal: 3.86s\tremaining: 8.04s\n",
      "324:\tlearn: 0.0794978\ttotal: 3.86s\tremaining: 8.03s\n",
      "325:\tlearn: 0.0794642\ttotal: 3.87s\tremaining: 8.01s\n",
      "326:\tlearn: 0.0794171\ttotal: 3.88s\tremaining: 7.99s\n",
      "327:\tlearn: 0.0793677\ttotal: 3.89s\tremaining: 7.98s\n",
      "328:\tlearn: 0.0793396\ttotal: 3.9s\tremaining: 7.96s\n",
      "329:\tlearn: 0.0793015\ttotal: 3.91s\tremaining: 7.94s\n",
      "330:\tlearn: 0.0792600\ttotal: 3.92s\tremaining: 7.93s\n",
      "331:\tlearn: 0.0792200\ttotal: 3.93s\tremaining: 7.91s\n",
      "332:\tlearn: 0.0791780\ttotal: 3.94s\tremaining: 7.9s\n",
      "333:\tlearn: 0.0791430\ttotal: 3.95s\tremaining: 7.88s\n",
      "334:\tlearn: 0.0791112\ttotal: 3.96s\tremaining: 7.86s\n",
      "335:\tlearn: 0.0790830\ttotal: 3.97s\tremaining: 7.85s\n",
      "336:\tlearn: 0.0790604\ttotal: 3.98s\tremaining: 7.83s\n",
      "337:\tlearn: 0.0790325\ttotal: 3.99s\tremaining: 7.81s\n",
      "338:\tlearn: 0.0789955\ttotal: 4s\tremaining: 7.8s\n",
      "339:\tlearn: 0.0789810\ttotal: 4.01s\tremaining: 7.78s\n",
      "340:\tlearn: 0.0789549\ttotal: 4.02s\tremaining: 7.76s\n",
      "341:\tlearn: 0.0789240\ttotal: 4.03s\tremaining: 7.75s\n",
      "342:\tlearn: 0.0789006\ttotal: 4.04s\tremaining: 7.73s\n",
      "343:\tlearn: 0.0788463\ttotal: 4.04s\tremaining: 7.71s\n",
      "344:\tlearn: 0.0788129\ttotal: 4.05s\tremaining: 7.69s\n",
      "345:\tlearn: 0.0787659\ttotal: 4.06s\tremaining: 7.67s\n",
      "346:\tlearn: 0.0787199\ttotal: 4.07s\tremaining: 7.65s\n",
      "347:\tlearn: 0.0786951\ttotal: 4.08s\tremaining: 7.64s\n",
      "348:\tlearn: 0.0786402\ttotal: 4.09s\tremaining: 7.63s\n",
      "349:\tlearn: 0.0786166\ttotal: 4.1s\tremaining: 7.61s\n",
      "350:\tlearn: 0.0785756\ttotal: 4.11s\tremaining: 7.59s\n",
      "351:\tlearn: 0.0784852\ttotal: 4.12s\tremaining: 7.58s\n",
      "352:\tlearn: 0.0784274\ttotal: 4.13s\tremaining: 7.56s\n",
      "353:\tlearn: 0.0783984\ttotal: 4.13s\tremaining: 7.54s\n",
      "354:\tlearn: 0.0783884\ttotal: 4.14s\tremaining: 7.53s\n",
      "355:\tlearn: 0.0783196\ttotal: 4.16s\tremaining: 7.52s\n",
      "356:\tlearn: 0.0782820\ttotal: 4.16s\tremaining: 7.5s\n",
      "357:\tlearn: 0.0782120\ttotal: 4.17s\tremaining: 7.48s\n",
      "358:\tlearn: 0.0781687\ttotal: 4.18s\tremaining: 7.47s\n",
      "359:\tlearn: 0.0781246\ttotal: 4.19s\tremaining: 7.45s\n",
      "360:\tlearn: 0.0781076\ttotal: 4.2s\tremaining: 7.44s\n",
      "361:\tlearn: 0.0780761\ttotal: 4.21s\tremaining: 7.42s\n",
      "362:\tlearn: 0.0780364\ttotal: 4.22s\tremaining: 7.41s\n",
      "363:\tlearn: 0.0780081\ttotal: 4.23s\tremaining: 7.39s\n",
      "364:\tlearn: 0.0779823\ttotal: 4.24s\tremaining: 7.37s\n",
      "365:\tlearn: 0.0779460\ttotal: 4.25s\tremaining: 7.36s\n",
      "366:\tlearn: 0.0779009\ttotal: 4.25s\tremaining: 7.34s\n",
      "367:\tlearn: 0.0778826\ttotal: 4.26s\tremaining: 7.32s\n",
      "368:\tlearn: 0.0778609\ttotal: 4.27s\tremaining: 7.31s\n",
      "369:\tlearn: 0.0778186\ttotal: 4.28s\tremaining: 7.29s\n",
      "370:\tlearn: 0.0777918\ttotal: 4.29s\tremaining: 7.28s\n",
      "371:\tlearn: 0.0777777\ttotal: 4.3s\tremaining: 7.26s\n",
      "372:\tlearn: 0.0777432\ttotal: 4.31s\tremaining: 7.24s\n",
      "373:\tlearn: 0.0777285\ttotal: 4.32s\tremaining: 7.22s\n",
      "374:\tlearn: 0.0776927\ttotal: 4.32s\tremaining: 7.21s\n",
      "375:\tlearn: 0.0776498\ttotal: 4.33s\tremaining: 7.19s\n",
      "376:\tlearn: 0.0776283\ttotal: 4.35s\tremaining: 7.18s\n",
      "377:\tlearn: 0.0775983\ttotal: 4.35s\tremaining: 7.16s\n",
      "378:\tlearn: 0.0775760\ttotal: 4.36s\tremaining: 7.15s\n",
      "379:\tlearn: 0.0775492\ttotal: 4.37s\tremaining: 7.13s\n",
      "380:\tlearn: 0.0775256\ttotal: 4.38s\tremaining: 7.12s\n",
      "381:\tlearn: 0.0775052\ttotal: 4.39s\tremaining: 7.1s\n",
      "382:\tlearn: 0.0774730\ttotal: 4.4s\tremaining: 7.09s\n",
      "383:\tlearn: 0.0774101\ttotal: 4.41s\tremaining: 7.07s\n",
      "384:\tlearn: 0.0773998\ttotal: 4.42s\tremaining: 7.05s\n",
      "385:\tlearn: 0.0773778\ttotal: 4.42s\tremaining: 7.04s\n",
      "386:\tlearn: 0.0773407\ttotal: 4.43s\tremaining: 7.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387:\tlearn: 0.0773069\ttotal: 4.44s\tremaining: 7.01s\n",
      "388:\tlearn: 0.0772739\ttotal: 4.45s\tremaining: 6.99s\n",
      "389:\tlearn: 0.0772468\ttotal: 4.46s\tremaining: 6.97s\n",
      "390:\tlearn: 0.0772144\ttotal: 4.47s\tremaining: 6.96s\n",
      "391:\tlearn: 0.0771748\ttotal: 4.48s\tremaining: 6.95s\n",
      "392:\tlearn: 0.0771397\ttotal: 4.49s\tremaining: 6.93s\n",
      "393:\tlearn: 0.0771066\ttotal: 4.5s\tremaining: 6.91s\n",
      "394:\tlearn: 0.0770717\ttotal: 4.51s\tremaining: 6.9s\n",
      "395:\tlearn: 0.0770574\ttotal: 4.52s\tremaining: 6.89s\n",
      "396:\tlearn: 0.0770294\ttotal: 4.52s\tremaining: 6.87s\n",
      "397:\tlearn: 0.0770084\ttotal: 4.54s\tremaining: 6.86s\n",
      "398:\tlearn: 0.0769933\ttotal: 4.54s\tremaining: 6.84s\n",
      "399:\tlearn: 0.0769443\ttotal: 4.55s\tremaining: 6.83s\n",
      "400:\tlearn: 0.0769020\ttotal: 4.57s\tremaining: 6.82s\n",
      "401:\tlearn: 0.0768703\ttotal: 4.58s\tremaining: 6.8s\n",
      "402:\tlearn: 0.0768487\ttotal: 4.59s\tremaining: 6.79s\n",
      "403:\tlearn: 0.0767351\ttotal: 4.62s\tremaining: 6.82s\n",
      "404:\tlearn: 0.0767053\ttotal: 4.64s\tremaining: 6.82s\n",
      "405:\tlearn: 0.0766777\ttotal: 4.66s\tremaining: 6.82s\n",
      "406:\tlearn: 0.0766334\ttotal: 4.68s\tremaining: 6.82s\n",
      "407:\tlearn: 0.0766013\ttotal: 4.71s\tremaining: 6.83s\n",
      "408:\tlearn: 0.0765608\ttotal: 4.72s\tremaining: 6.82s\n",
      "409:\tlearn: 0.0765273\ttotal: 4.73s\tremaining: 6.81s\n",
      "410:\tlearn: 0.0764969\ttotal: 4.74s\tremaining: 6.8s\n",
      "411:\tlearn: 0.0764727\ttotal: 4.75s\tremaining: 6.78s\n",
      "412:\tlearn: 0.0764376\ttotal: 4.76s\tremaining: 6.77s\n",
      "413:\tlearn: 0.0764131\ttotal: 4.77s\tremaining: 6.76s\n",
      "414:\tlearn: 0.0763814\ttotal: 4.79s\tremaining: 6.75s\n",
      "415:\tlearn: 0.0763365\ttotal: 4.79s\tremaining: 6.73s\n",
      "416:\tlearn: 0.0762994\ttotal: 4.81s\tremaining: 6.72s\n",
      "417:\tlearn: 0.0762720\ttotal: 4.82s\tremaining: 6.71s\n",
      "418:\tlearn: 0.0762413\ttotal: 4.83s\tremaining: 6.69s\n",
      "419:\tlearn: 0.0762160\ttotal: 4.84s\tremaining: 6.68s\n",
      "420:\tlearn: 0.0761949\ttotal: 4.85s\tremaining: 6.67s\n",
      "421:\tlearn: 0.0761832\ttotal: 4.86s\tremaining: 6.66s\n",
      "422:\tlearn: 0.0761531\ttotal: 4.87s\tremaining: 6.64s\n",
      "423:\tlearn: 0.0761283\ttotal: 4.88s\tremaining: 6.63s\n",
      "424:\tlearn: 0.0760904\ttotal: 4.89s\tremaining: 6.62s\n",
      "425:\tlearn: 0.0760591\ttotal: 4.91s\tremaining: 6.61s\n",
      "426:\tlearn: 0.0760384\ttotal: 4.92s\tremaining: 6.6s\n",
      "427:\tlearn: 0.0760189\ttotal: 4.93s\tremaining: 6.59s\n",
      "428:\tlearn: 0.0759907\ttotal: 4.94s\tremaining: 6.57s\n",
      "429:\tlearn: 0.0759632\ttotal: 4.95s\tremaining: 6.56s\n",
      "430:\tlearn: 0.0759280\ttotal: 4.96s\tremaining: 6.55s\n",
      "431:\tlearn: 0.0758932\ttotal: 4.97s\tremaining: 6.54s\n",
      "432:\tlearn: 0.0758612\ttotal: 4.99s\tremaining: 6.53s\n",
      "433:\tlearn: 0.0758339\ttotal: 5s\tremaining: 6.52s\n",
      "434:\tlearn: 0.0757976\ttotal: 5.01s\tremaining: 6.5s\n",
      "435:\tlearn: 0.0757790\ttotal: 5.02s\tremaining: 6.49s\n",
      "436:\tlearn: 0.0757531\ttotal: 5.03s\tremaining: 6.47s\n",
      "437:\tlearn: 0.0757284\ttotal: 5.03s\tremaining: 6.46s\n",
      "438:\tlearn: 0.0757141\ttotal: 5.04s\tremaining: 6.45s\n",
      "439:\tlearn: 0.0756869\ttotal: 5.05s\tremaining: 6.43s\n",
      "440:\tlearn: 0.0756703\ttotal: 5.07s\tremaining: 6.43s\n",
      "441:\tlearn: 0.0756482\ttotal: 5.08s\tremaining: 6.41s\n",
      "442:\tlearn: 0.0755863\ttotal: 5.09s\tremaining: 6.41s\n",
      "443:\tlearn: 0.0755607\ttotal: 5.1s\tremaining: 6.39s\n",
      "444:\tlearn: 0.0755036\ttotal: 5.12s\tremaining: 6.38s\n",
      "445:\tlearn: 0.0754865\ttotal: 5.12s\tremaining: 6.37s\n",
      "446:\tlearn: 0.0754146\ttotal: 5.14s\tremaining: 6.36s\n",
      "447:\tlearn: 0.0753985\ttotal: 5.15s\tremaining: 6.34s\n",
      "448:\tlearn: 0.0753672\ttotal: 5.16s\tremaining: 6.33s\n",
      "449:\tlearn: 0.0753355\ttotal: 5.17s\tremaining: 6.32s\n",
      "450:\tlearn: 0.0753125\ttotal: 5.18s\tremaining: 6.31s\n",
      "451:\tlearn: 0.0752921\ttotal: 5.19s\tremaining: 6.29s\n",
      "452:\tlearn: 0.0752533\ttotal: 5.2s\tremaining: 6.28s\n",
      "453:\tlearn: 0.0752058\ttotal: 5.22s\tremaining: 6.27s\n",
      "454:\tlearn: 0.0751827\ttotal: 5.22s\tremaining: 6.26s\n",
      "455:\tlearn: 0.0751649\ttotal: 5.23s\tremaining: 6.24s\n",
      "456:\tlearn: 0.0751324\ttotal: 5.24s\tremaining: 6.23s\n",
      "457:\tlearn: 0.0751067\ttotal: 5.25s\tremaining: 6.22s\n",
      "458:\tlearn: 0.0750771\ttotal: 5.26s\tremaining: 6.2s\n",
      "459:\tlearn: 0.0750612\ttotal: 5.28s\tremaining: 6.19s\n",
      "460:\tlearn: 0.0750367\ttotal: 5.28s\tremaining: 6.18s\n",
      "461:\tlearn: 0.0750049\ttotal: 5.3s\tremaining: 6.17s\n",
      "462:\tlearn: 0.0749665\ttotal: 5.3s\tremaining: 6.15s\n",
      "463:\tlearn: 0.0749387\ttotal: 5.31s\tremaining: 6.14s\n",
      "464:\tlearn: 0.0749033\ttotal: 5.32s\tremaining: 6.12s\n",
      "465:\tlearn: 0.0748665\ttotal: 5.33s\tremaining: 6.11s\n",
      "466:\tlearn: 0.0748506\ttotal: 5.34s\tremaining: 6.1s\n",
      "467:\tlearn: 0.0748357\ttotal: 5.35s\tremaining: 6.08s\n",
      "468:\tlearn: 0.0748007\ttotal: 5.36s\tremaining: 6.07s\n",
      "469:\tlearn: 0.0747540\ttotal: 5.37s\tremaining: 6.06s\n",
      "470:\tlearn: 0.0747441\ttotal: 5.39s\tremaining: 6.05s\n",
      "471:\tlearn: 0.0747096\ttotal: 5.4s\tremaining: 6.04s\n",
      "472:\tlearn: 0.0746940\ttotal: 5.41s\tremaining: 6.03s\n",
      "473:\tlearn: 0.0746813\ttotal: 5.42s\tremaining: 6.01s\n",
      "474:\tlearn: 0.0746547\ttotal: 5.43s\tremaining: 6s\n",
      "475:\tlearn: 0.0746403\ttotal: 5.44s\tremaining: 5.99s\n",
      "476:\tlearn: 0.0746086\ttotal: 5.46s\tremaining: 5.98s\n",
      "477:\tlearn: 0.0745790\ttotal: 5.46s\tremaining: 5.97s\n",
      "478:\tlearn: 0.0745676\ttotal: 5.48s\tremaining: 5.96s\n",
      "479:\tlearn: 0.0745247\ttotal: 5.49s\tremaining: 5.95s\n",
      "480:\tlearn: 0.0744957\ttotal: 5.5s\tremaining: 5.94s\n",
      "481:\tlearn: 0.0744792\ttotal: 5.52s\tremaining: 5.93s\n",
      "482:\tlearn: 0.0744285\ttotal: 5.53s\tremaining: 5.91s\n",
      "483:\tlearn: 0.0743893\ttotal: 5.54s\tremaining: 5.9s\n",
      "484:\tlearn: 0.0743579\ttotal: 5.55s\tremaining: 5.89s\n",
      "485:\tlearn: 0.0743196\ttotal: 5.56s\tremaining: 5.88s\n",
      "486:\tlearn: 0.0742988\ttotal: 5.57s\tremaining: 5.87s\n",
      "487:\tlearn: 0.0742780\ttotal: 5.58s\tremaining: 5.86s\n",
      "488:\tlearn: 0.0742670\ttotal: 5.59s\tremaining: 5.84s\n",
      "489:\tlearn: 0.0742514\ttotal: 5.6s\tremaining: 5.83s\n",
      "490:\tlearn: 0.0742255\ttotal: 5.62s\tremaining: 5.82s\n",
      "491:\tlearn: 0.0741970\ttotal: 5.63s\tremaining: 5.82s\n",
      "492:\tlearn: 0.0741700\ttotal: 5.65s\tremaining: 5.81s\n",
      "493:\tlearn: 0.0741628\ttotal: 5.67s\tremaining: 5.81s\n",
      "494:\tlearn: 0.0741386\ttotal: 5.69s\tremaining: 5.8s\n",
      "495:\tlearn: 0.0741234\ttotal: 5.7s\tremaining: 5.8s\n",
      "496:\tlearn: 0.0740706\ttotal: 5.72s\tremaining: 5.79s\n",
      "497:\tlearn: 0.0740367\ttotal: 5.74s\tremaining: 5.79s\n",
      "498:\tlearn: 0.0740114\ttotal: 5.76s\tremaining: 5.78s\n",
      "499:\tlearn: 0.0739982\ttotal: 5.77s\tremaining: 5.77s\n",
      "500:\tlearn: 0.0739723\ttotal: 5.78s\tremaining: 5.76s\n",
      "501:\tlearn: 0.0739490\ttotal: 5.79s\tremaining: 5.75s\n",
      "502:\tlearn: 0.0739048\ttotal: 5.8s\tremaining: 5.73s\n",
      "503:\tlearn: 0.0738723\ttotal: 5.81s\tremaining: 5.72s\n",
      "504:\tlearn: 0.0738465\ttotal: 5.82s\tremaining: 5.71s\n",
      "505:\tlearn: 0.0738352\ttotal: 5.83s\tremaining: 5.69s\n",
      "506:\tlearn: 0.0737955\ttotal: 5.84s\tremaining: 5.68s\n",
      "507:\tlearn: 0.0737669\ttotal: 5.86s\tremaining: 5.67s\n",
      "508:\tlearn: 0.0737470\ttotal: 5.86s\tremaining: 5.66s\n",
      "509:\tlearn: 0.0737143\ttotal: 5.87s\tremaining: 5.64s\n",
      "510:\tlearn: 0.0736941\ttotal: 5.88s\tremaining: 5.63s\n",
      "511:\tlearn: 0.0736695\ttotal: 5.89s\tremaining: 5.62s\n",
      "512:\tlearn: 0.0736506\ttotal: 5.9s\tremaining: 5.6s\n",
      "513:\tlearn: 0.0736304\ttotal: 5.91s\tremaining: 5.59s\n",
      "514:\tlearn: 0.0735955\ttotal: 5.92s\tremaining: 5.58s\n",
      "515:\tlearn: 0.0735733\ttotal: 5.93s\tremaining: 5.56s\n",
      "516:\tlearn: 0.0735394\ttotal: 5.94s\tremaining: 5.55s\n",
      "517:\tlearn: 0.0735294\ttotal: 5.95s\tremaining: 5.54s\n",
      "518:\tlearn: 0.0735093\ttotal: 5.96s\tremaining: 5.53s\n",
      "519:\tlearn: 0.0734898\ttotal: 5.98s\tremaining: 5.52s\n",
      "520:\tlearn: 0.0734786\ttotal: 5.99s\tremaining: 5.5s\n",
      "521:\tlearn: 0.0734512\ttotal: 6s\tremaining: 5.49s\n",
      "522:\tlearn: 0.0734209\ttotal: 6.01s\tremaining: 5.48s\n",
      "523:\tlearn: 0.0733992\ttotal: 6.01s\tremaining: 5.46s\n",
      "524:\tlearn: 0.0733628\ttotal: 6.03s\tremaining: 5.45s\n",
      "525:\tlearn: 0.0733432\ttotal: 6.04s\tremaining: 5.44s\n",
      "526:\tlearn: 0.0732706\ttotal: 6.04s\tremaining: 5.43s\n",
      "527:\tlearn: 0.0732265\ttotal: 6.05s\tremaining: 5.41s\n",
      "528:\tlearn: 0.0731995\ttotal: 6.06s\tremaining: 5.4s\n",
      "529:\tlearn: 0.0731739\ttotal: 6.08s\tremaining: 5.39s\n",
      "530:\tlearn: 0.0731556\ttotal: 6.09s\tremaining: 5.38s\n",
      "531:\tlearn: 0.0731403\ttotal: 6.11s\tremaining: 5.37s\n",
      "532:\tlearn: 0.0731074\ttotal: 6.12s\tremaining: 5.36s\n",
      "533:\tlearn: 0.0730796\ttotal: 6.13s\tremaining: 5.35s\n",
      "534:\tlearn: 0.0730570\ttotal: 6.14s\tremaining: 5.34s\n",
      "535:\tlearn: 0.0730335\ttotal: 6.15s\tremaining: 5.33s\n",
      "536:\tlearn: 0.0730148\ttotal: 6.16s\tremaining: 5.31s\n",
      "537:\tlearn: 0.0730011\ttotal: 6.17s\tremaining: 5.3s\n",
      "538:\tlearn: 0.0729801\ttotal: 6.18s\tremaining: 5.29s\n",
      "539:\tlearn: 0.0729510\ttotal: 6.19s\tremaining: 5.27s\n",
      "540:\tlearn: 0.0729187\ttotal: 6.2s\tremaining: 5.26s\n",
      "541:\tlearn: 0.0729032\ttotal: 6.21s\tremaining: 5.25s\n",
      "542:\tlearn: 0.0728842\ttotal: 6.22s\tremaining: 5.24s\n",
      "543:\tlearn: 0.0728699\ttotal: 6.23s\tremaining: 5.22s\n",
      "544:\tlearn: 0.0728528\ttotal: 6.24s\tremaining: 5.21s\n",
      "545:\tlearn: 0.0728249\ttotal: 6.25s\tremaining: 5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546:\tlearn: 0.0727851\ttotal: 6.26s\tremaining: 5.18s\n",
      "547:\tlearn: 0.0727619\ttotal: 6.27s\tremaining: 5.17s\n",
      "548:\tlearn: 0.0727467\ttotal: 6.28s\tremaining: 5.16s\n",
      "549:\tlearn: 0.0727281\ttotal: 6.29s\tremaining: 5.15s\n",
      "550:\tlearn: 0.0727085\ttotal: 6.3s\tremaining: 5.13s\n",
      "551:\tlearn: 0.0726728\ttotal: 6.31s\tremaining: 5.12s\n",
      "552:\tlearn: 0.0726466\ttotal: 6.32s\tremaining: 5.11s\n",
      "553:\tlearn: 0.0726158\ttotal: 6.34s\tremaining: 5.1s\n",
      "554:\tlearn: 0.0725953\ttotal: 6.35s\tremaining: 5.09s\n",
      "555:\tlearn: 0.0725719\ttotal: 6.36s\tremaining: 5.08s\n",
      "556:\tlearn: 0.0725366\ttotal: 6.37s\tremaining: 5.07s\n",
      "557:\tlearn: 0.0725122\ttotal: 6.38s\tremaining: 5.05s\n",
      "558:\tlearn: 0.0724920\ttotal: 6.39s\tremaining: 5.04s\n",
      "559:\tlearn: 0.0724692\ttotal: 6.4s\tremaining: 5.03s\n",
      "560:\tlearn: 0.0724517\ttotal: 6.41s\tremaining: 5.02s\n",
      "561:\tlearn: 0.0724146\ttotal: 6.42s\tremaining: 5s\n",
      "562:\tlearn: 0.0723679\ttotal: 6.43s\tremaining: 4.99s\n",
      "563:\tlearn: 0.0723526\ttotal: 6.44s\tremaining: 4.98s\n",
      "564:\tlearn: 0.0723319\ttotal: 6.46s\tremaining: 4.97s\n",
      "565:\tlearn: 0.0723098\ttotal: 6.48s\tremaining: 4.97s\n",
      "566:\tlearn: 0.0722708\ttotal: 6.49s\tremaining: 4.96s\n",
      "567:\tlearn: 0.0722387\ttotal: 6.51s\tremaining: 4.95s\n",
      "568:\tlearn: 0.0722233\ttotal: 6.53s\tremaining: 4.95s\n",
      "569:\tlearn: 0.0722062\ttotal: 6.54s\tremaining: 4.93s\n",
      "570:\tlearn: 0.0721756\ttotal: 6.56s\tremaining: 4.92s\n",
      "571:\tlearn: 0.0721364\ttotal: 6.57s\tremaining: 4.92s\n",
      "572:\tlearn: 0.0721140\ttotal: 6.58s\tremaining: 4.91s\n",
      "573:\tlearn: 0.0720898\ttotal: 6.59s\tremaining: 4.89s\n",
      "574:\tlearn: 0.0720731\ttotal: 6.61s\tremaining: 4.88s\n",
      "575:\tlearn: 0.0720604\ttotal: 6.62s\tremaining: 4.87s\n",
      "576:\tlearn: 0.0720288\ttotal: 6.64s\tremaining: 4.87s\n",
      "577:\tlearn: 0.0720169\ttotal: 6.67s\tremaining: 4.87s\n",
      "578:\tlearn: 0.0719897\ttotal: 6.69s\tremaining: 4.86s\n",
      "579:\tlearn: 0.0719565\ttotal: 6.7s\tremaining: 4.85s\n",
      "580:\tlearn: 0.0719319\ttotal: 6.72s\tremaining: 4.85s\n",
      "581:\tlearn: 0.0718988\ttotal: 6.74s\tremaining: 4.84s\n",
      "582:\tlearn: 0.0718662\ttotal: 6.76s\tremaining: 4.84s\n",
      "583:\tlearn: 0.0718533\ttotal: 6.78s\tremaining: 4.83s\n",
      "584:\tlearn: 0.0718342\ttotal: 6.79s\tremaining: 4.82s\n",
      "585:\tlearn: 0.0718180\ttotal: 6.8s\tremaining: 4.8s\n",
      "586:\tlearn: 0.0718011\ttotal: 6.81s\tremaining: 4.79s\n",
      "587:\tlearn: 0.0717712\ttotal: 6.82s\tremaining: 4.78s\n",
      "588:\tlearn: 0.0717371\ttotal: 6.83s\tremaining: 4.77s\n",
      "589:\tlearn: 0.0717273\ttotal: 6.85s\tremaining: 4.76s\n",
      "590:\tlearn: 0.0716944\ttotal: 6.86s\tremaining: 4.74s\n",
      "591:\tlearn: 0.0716782\ttotal: 6.87s\tremaining: 4.73s\n",
      "592:\tlearn: 0.0716591\ttotal: 6.88s\tremaining: 4.72s\n",
      "593:\tlearn: 0.0716483\ttotal: 6.89s\tremaining: 4.71s\n",
      "594:\tlearn: 0.0716382\ttotal: 6.9s\tremaining: 4.7s\n",
      "595:\tlearn: 0.0716292\ttotal: 6.91s\tremaining: 4.68s\n",
      "596:\tlearn: 0.0716008\ttotal: 6.92s\tremaining: 4.67s\n",
      "597:\tlearn: 0.0715848\ttotal: 6.93s\tremaining: 4.66s\n",
      "598:\tlearn: 0.0715726\ttotal: 6.94s\tremaining: 4.65s\n",
      "599:\tlearn: 0.0715296\ttotal: 6.95s\tremaining: 4.63s\n",
      "600:\tlearn: 0.0714777\ttotal: 6.96s\tremaining: 4.62s\n",
      "601:\tlearn: 0.0714482\ttotal: 6.98s\tremaining: 4.61s\n",
      "602:\tlearn: 0.0714281\ttotal: 6.99s\tremaining: 4.6s\n",
      "603:\tlearn: 0.0714033\ttotal: 7s\tremaining: 4.59s\n",
      "604:\tlearn: 0.0713959\ttotal: 7.01s\tremaining: 4.58s\n",
      "605:\tlearn: 0.0713682\ttotal: 7.02s\tremaining: 4.57s\n",
      "606:\tlearn: 0.0713346\ttotal: 7.03s\tremaining: 4.55s\n",
      "607:\tlearn: 0.0713194\ttotal: 7.05s\tremaining: 4.54s\n",
      "608:\tlearn: 0.0712967\ttotal: 7.06s\tremaining: 4.53s\n",
      "609:\tlearn: 0.0712797\ttotal: 7.07s\tremaining: 4.52s\n",
      "610:\tlearn: 0.0712616\ttotal: 7.08s\tremaining: 4.51s\n",
      "611:\tlearn: 0.0712491\ttotal: 7.09s\tremaining: 4.49s\n",
      "612:\tlearn: 0.0712222\ttotal: 7.1s\tremaining: 4.48s\n",
      "613:\tlearn: 0.0712069\ttotal: 7.11s\tremaining: 4.47s\n",
      "614:\tlearn: 0.0711799\ttotal: 7.13s\tremaining: 4.46s\n",
      "615:\tlearn: 0.0711569\ttotal: 7.14s\tremaining: 4.45s\n",
      "616:\tlearn: 0.0711432\ttotal: 7.15s\tremaining: 4.44s\n",
      "617:\tlearn: 0.0711275\ttotal: 7.16s\tremaining: 4.43s\n",
      "618:\tlearn: 0.0711052\ttotal: 7.17s\tremaining: 4.41s\n",
      "619:\tlearn: 0.0710722\ttotal: 7.18s\tremaining: 4.4s\n",
      "620:\tlearn: 0.0710486\ttotal: 7.2s\tremaining: 4.39s\n",
      "621:\tlearn: 0.0710236\ttotal: 7.21s\tremaining: 4.38s\n",
      "622:\tlearn: 0.0710029\ttotal: 7.22s\tremaining: 4.37s\n",
      "623:\tlearn: 0.0709868\ttotal: 7.23s\tremaining: 4.36s\n",
      "624:\tlearn: 0.0709681\ttotal: 7.24s\tremaining: 4.34s\n",
      "625:\tlearn: 0.0709555\ttotal: 7.25s\tremaining: 4.33s\n",
      "626:\tlearn: 0.0709410\ttotal: 7.26s\tremaining: 4.32s\n",
      "627:\tlearn: 0.0709214\ttotal: 7.27s\tremaining: 4.31s\n",
      "628:\tlearn: 0.0708936\ttotal: 7.29s\tremaining: 4.3s\n",
      "629:\tlearn: 0.0708799\ttotal: 7.3s\tremaining: 4.29s\n",
      "630:\tlearn: 0.0708661\ttotal: 7.31s\tremaining: 4.27s\n",
      "631:\tlearn: 0.0708421\ttotal: 7.32s\tremaining: 4.26s\n",
      "632:\tlearn: 0.0708333\ttotal: 7.33s\tremaining: 4.25s\n",
      "633:\tlearn: 0.0708237\ttotal: 7.34s\tremaining: 4.24s\n",
      "634:\tlearn: 0.0708156\ttotal: 7.35s\tremaining: 4.23s\n",
      "635:\tlearn: 0.0707971\ttotal: 7.37s\tremaining: 4.21s\n",
      "636:\tlearn: 0.0707828\ttotal: 7.38s\tremaining: 4.21s\n",
      "637:\tlearn: 0.0707743\ttotal: 7.39s\tremaining: 4.19s\n",
      "638:\tlearn: 0.0707440\ttotal: 7.4s\tremaining: 4.18s\n",
      "639:\tlearn: 0.0707296\ttotal: 7.41s\tremaining: 4.17s\n",
      "640:\tlearn: 0.0706991\ttotal: 7.42s\tremaining: 4.16s\n",
      "641:\tlearn: 0.0706807\ttotal: 7.44s\tremaining: 4.15s\n",
      "642:\tlearn: 0.0706665\ttotal: 7.44s\tremaining: 4.13s\n",
      "643:\tlearn: 0.0706355\ttotal: 7.46s\tremaining: 4.12s\n",
      "644:\tlearn: 0.0706136\ttotal: 7.46s\tremaining: 4.11s\n",
      "645:\tlearn: 0.0705899\ttotal: 7.47s\tremaining: 4.09s\n",
      "646:\tlearn: 0.0705579\ttotal: 7.49s\tremaining: 4.08s\n",
      "647:\tlearn: 0.0705155\ttotal: 7.5s\tremaining: 4.07s\n",
      "648:\tlearn: 0.0705002\ttotal: 7.51s\tremaining: 4.06s\n",
      "649:\tlearn: 0.0704862\ttotal: 7.52s\tremaining: 4.05s\n",
      "650:\tlearn: 0.0704562\ttotal: 7.54s\tremaining: 4.04s\n",
      "651:\tlearn: 0.0704370\ttotal: 7.55s\tremaining: 4.03s\n",
      "652:\tlearn: 0.0704247\ttotal: 7.56s\tremaining: 4.02s\n",
      "653:\tlearn: 0.0704166\ttotal: 7.57s\tremaining: 4.01s\n",
      "654:\tlearn: 0.0703876\ttotal: 7.58s\tremaining: 3.99s\n",
      "655:\tlearn: 0.0703671\ttotal: 7.6s\tremaining: 3.98s\n",
      "656:\tlearn: 0.0703544\ttotal: 7.61s\tremaining: 3.97s\n",
      "657:\tlearn: 0.0702984\ttotal: 7.63s\tremaining: 3.96s\n",
      "658:\tlearn: 0.0702775\ttotal: 7.64s\tremaining: 3.95s\n",
      "659:\tlearn: 0.0702592\ttotal: 7.65s\tremaining: 3.94s\n",
      "660:\tlearn: 0.0702435\ttotal: 7.67s\tremaining: 3.93s\n",
      "661:\tlearn: 0.0702315\ttotal: 7.7s\tremaining: 3.93s\n",
      "662:\tlearn: 0.0702175\ttotal: 7.73s\tremaining: 3.93s\n",
      "663:\tlearn: 0.0702034\ttotal: 7.75s\tremaining: 3.92s\n",
      "664:\tlearn: 0.0701866\ttotal: 7.78s\tremaining: 3.92s\n",
      "665:\tlearn: 0.0701624\ttotal: 7.8s\tremaining: 3.91s\n",
      "666:\tlearn: 0.0701429\ttotal: 7.82s\tremaining: 3.9s\n",
      "667:\tlearn: 0.0701239\ttotal: 7.83s\tremaining: 3.89s\n",
      "668:\tlearn: 0.0700959\ttotal: 7.85s\tremaining: 3.88s\n",
      "669:\tlearn: 0.0700657\ttotal: 7.86s\tremaining: 3.87s\n",
      "670:\tlearn: 0.0700551\ttotal: 7.87s\tremaining: 3.86s\n",
      "671:\tlearn: 0.0700275\ttotal: 7.88s\tremaining: 3.85s\n",
      "672:\tlearn: 0.0700064\ttotal: 7.89s\tremaining: 3.83s\n",
      "673:\tlearn: 0.0699639\ttotal: 7.9s\tremaining: 3.82s\n",
      "674:\tlearn: 0.0699392\ttotal: 7.91s\tremaining: 3.81s\n",
      "675:\tlearn: 0.0699186\ttotal: 7.92s\tremaining: 3.79s\n",
      "676:\tlearn: 0.0698966\ttotal: 7.93s\tremaining: 3.78s\n",
      "677:\tlearn: 0.0698776\ttotal: 7.94s\tremaining: 3.77s\n",
      "678:\tlearn: 0.0698462\ttotal: 7.95s\tremaining: 3.76s\n",
      "679:\tlearn: 0.0698241\ttotal: 7.96s\tremaining: 3.75s\n",
      "680:\tlearn: 0.0697879\ttotal: 7.97s\tremaining: 3.73s\n",
      "681:\tlearn: 0.0697751\ttotal: 7.98s\tremaining: 3.72s\n",
      "682:\tlearn: 0.0697558\ttotal: 7.99s\tremaining: 3.71s\n",
      "683:\tlearn: 0.0697318\ttotal: 8.01s\tremaining: 3.7s\n",
      "684:\tlearn: 0.0697242\ttotal: 8.02s\tremaining: 3.69s\n",
      "685:\tlearn: 0.0696869\ttotal: 8.03s\tremaining: 3.68s\n",
      "686:\tlearn: 0.0696752\ttotal: 8.04s\tremaining: 3.66s\n",
      "687:\tlearn: 0.0696595\ttotal: 8.05s\tremaining: 3.65s\n",
      "688:\tlearn: 0.0696360\ttotal: 8.07s\tremaining: 3.64s\n",
      "689:\tlearn: 0.0696174\ttotal: 8.08s\tremaining: 3.63s\n",
      "690:\tlearn: 0.0695932\ttotal: 8.09s\tremaining: 3.62s\n",
      "691:\tlearn: 0.0695704\ttotal: 8.1s\tremaining: 3.6s\n",
      "692:\tlearn: 0.0695507\ttotal: 8.11s\tremaining: 3.59s\n",
      "693:\tlearn: 0.0695319\ttotal: 8.12s\tremaining: 3.58s\n",
      "694:\tlearn: 0.0695127\ttotal: 8.13s\tremaining: 3.57s\n",
      "695:\tlearn: 0.0694922\ttotal: 8.14s\tremaining: 3.56s\n",
      "696:\tlearn: 0.0694693\ttotal: 8.16s\tremaining: 3.55s\n",
      "697:\tlearn: 0.0694370\ttotal: 8.17s\tremaining: 3.53s\n",
      "698:\tlearn: 0.0694189\ttotal: 8.18s\tremaining: 3.52s\n",
      "699:\tlearn: 0.0694064\ttotal: 8.19s\tremaining: 3.51s\n",
      "700:\tlearn: 0.0693900\ttotal: 8.2s\tremaining: 3.5s\n",
      "701:\tlearn: 0.0693624\ttotal: 8.21s\tremaining: 3.48s\n",
      "702:\tlearn: 0.0693357\ttotal: 8.22s\tremaining: 3.47s\n",
      "703:\tlearn: 0.0693223\ttotal: 8.23s\tremaining: 3.46s\n",
      "704:\tlearn: 0.0693013\ttotal: 8.24s\tremaining: 3.45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705:\tlearn: 0.0692741\ttotal: 8.25s\tremaining: 3.44s\n",
      "706:\tlearn: 0.0692677\ttotal: 8.26s\tremaining: 3.42s\n",
      "707:\tlearn: 0.0692294\ttotal: 8.27s\tremaining: 3.41s\n",
      "708:\tlearn: 0.0692030\ttotal: 8.28s\tremaining: 3.4s\n",
      "709:\tlearn: 0.0691822\ttotal: 8.29s\tremaining: 3.39s\n",
      "710:\tlearn: 0.0691734\ttotal: 8.3s\tremaining: 3.37s\n",
      "711:\tlearn: 0.0691481\ttotal: 8.31s\tremaining: 3.36s\n",
      "712:\tlearn: 0.0691265\ttotal: 8.33s\tremaining: 3.35s\n",
      "713:\tlearn: 0.0691076\ttotal: 8.34s\tremaining: 3.34s\n",
      "714:\tlearn: 0.0690791\ttotal: 8.35s\tremaining: 3.33s\n",
      "715:\tlearn: 0.0690667\ttotal: 8.36s\tremaining: 3.32s\n",
      "716:\tlearn: 0.0690320\ttotal: 8.37s\tremaining: 3.31s\n",
      "717:\tlearn: 0.0690195\ttotal: 8.38s\tremaining: 3.29s\n",
      "718:\tlearn: 0.0689989\ttotal: 8.4s\tremaining: 3.28s\n",
      "719:\tlearn: 0.0689781\ttotal: 8.41s\tremaining: 3.27s\n",
      "720:\tlearn: 0.0689666\ttotal: 8.42s\tremaining: 3.26s\n",
      "721:\tlearn: 0.0689270\ttotal: 8.43s\tremaining: 3.25s\n",
      "722:\tlearn: 0.0689027\ttotal: 8.44s\tremaining: 3.23s\n",
      "723:\tlearn: 0.0688833\ttotal: 8.45s\tremaining: 3.22s\n",
      "724:\tlearn: 0.0688597\ttotal: 8.47s\tremaining: 3.21s\n",
      "725:\tlearn: 0.0688452\ttotal: 8.48s\tremaining: 3.2s\n",
      "726:\tlearn: 0.0687873\ttotal: 8.49s\tremaining: 3.19s\n",
      "727:\tlearn: 0.0687689\ttotal: 8.5s\tremaining: 3.18s\n",
      "728:\tlearn: 0.0687574\ttotal: 8.51s\tremaining: 3.16s\n",
      "729:\tlearn: 0.0687444\ttotal: 8.53s\tremaining: 3.15s\n",
      "730:\tlearn: 0.0687332\ttotal: 8.54s\tremaining: 3.14s\n",
      "731:\tlearn: 0.0687179\ttotal: 8.55s\tremaining: 3.13s\n",
      "732:\tlearn: 0.0686925\ttotal: 8.56s\tremaining: 3.12s\n",
      "733:\tlearn: 0.0686787\ttotal: 8.57s\tremaining: 3.11s\n",
      "734:\tlearn: 0.0686511\ttotal: 8.59s\tremaining: 3.1s\n",
      "735:\tlearn: 0.0686413\ttotal: 8.6s\tremaining: 3.08s\n",
      "736:\tlearn: 0.0686308\ttotal: 8.61s\tremaining: 3.07s\n",
      "737:\tlearn: 0.0686176\ttotal: 8.62s\tremaining: 3.06s\n",
      "738:\tlearn: 0.0685996\ttotal: 8.63s\tremaining: 3.05s\n",
      "739:\tlearn: 0.0685783\ttotal: 8.64s\tremaining: 3.04s\n",
      "740:\tlearn: 0.0685671\ttotal: 8.65s\tremaining: 3.02s\n",
      "741:\tlearn: 0.0685454\ttotal: 8.67s\tremaining: 3.01s\n",
      "742:\tlearn: 0.0685280\ttotal: 8.68s\tremaining: 3s\n",
      "743:\tlearn: 0.0685035\ttotal: 8.69s\tremaining: 2.99s\n",
      "744:\tlearn: 0.0684873\ttotal: 8.71s\tremaining: 2.98s\n",
      "745:\tlearn: 0.0684753\ttotal: 8.72s\tremaining: 2.97s\n",
      "746:\tlearn: 0.0684689\ttotal: 8.75s\tremaining: 2.96s\n",
      "747:\tlearn: 0.0684466\ttotal: 8.78s\tremaining: 2.96s\n",
      "748:\tlearn: 0.0684254\ttotal: 8.8s\tremaining: 2.95s\n",
      "749:\tlearn: 0.0684070\ttotal: 8.83s\tremaining: 2.94s\n",
      "750:\tlearn: 0.0683868\ttotal: 8.85s\tremaining: 2.93s\n",
      "751:\tlearn: 0.0683758\ttotal: 8.86s\tremaining: 2.92s\n",
      "752:\tlearn: 0.0683543\ttotal: 8.87s\tremaining: 2.91s\n",
      "753:\tlearn: 0.0683334\ttotal: 8.88s\tremaining: 2.9s\n",
      "754:\tlearn: 0.0682972\ttotal: 8.89s\tremaining: 2.88s\n",
      "755:\tlearn: 0.0682824\ttotal: 8.9s\tremaining: 2.87s\n",
      "756:\tlearn: 0.0682642\ttotal: 8.92s\tremaining: 2.86s\n",
      "757:\tlearn: 0.0682555\ttotal: 8.94s\tremaining: 2.85s\n",
      "758:\tlearn: 0.0682369\ttotal: 8.95s\tremaining: 2.84s\n",
      "759:\tlearn: 0.0682051\ttotal: 8.96s\tremaining: 2.83s\n",
      "760:\tlearn: 0.0681891\ttotal: 8.97s\tremaining: 2.82s\n",
      "761:\tlearn: 0.0681686\ttotal: 8.99s\tremaining: 2.81s\n",
      "762:\tlearn: 0.0681418\ttotal: 8.99s\tremaining: 2.79s\n",
      "763:\tlearn: 0.0681115\ttotal: 9.01s\tremaining: 2.78s\n",
      "764:\tlearn: 0.0681010\ttotal: 9.02s\tremaining: 2.77s\n",
      "765:\tlearn: 0.0680853\ttotal: 9.03s\tremaining: 2.76s\n",
      "766:\tlearn: 0.0680576\ttotal: 9.04s\tremaining: 2.75s\n",
      "767:\tlearn: 0.0680355\ttotal: 9.05s\tremaining: 2.73s\n",
      "768:\tlearn: 0.0680246\ttotal: 9.07s\tremaining: 2.72s\n",
      "769:\tlearn: 0.0679926\ttotal: 9.08s\tremaining: 2.71s\n",
      "770:\tlearn: 0.0679625\ttotal: 9.1s\tremaining: 2.7s\n",
      "771:\tlearn: 0.0679247\ttotal: 9.11s\tremaining: 2.69s\n",
      "772:\tlearn: 0.0679127\ttotal: 9.12s\tremaining: 2.68s\n",
      "773:\tlearn: 0.0678989\ttotal: 9.13s\tremaining: 2.67s\n",
      "774:\tlearn: 0.0678857\ttotal: 9.14s\tremaining: 2.65s\n",
      "775:\tlearn: 0.0678739\ttotal: 9.15s\tremaining: 2.64s\n",
      "776:\tlearn: 0.0678542\ttotal: 9.16s\tremaining: 2.63s\n",
      "777:\tlearn: 0.0678477\ttotal: 9.17s\tremaining: 2.62s\n",
      "778:\tlearn: 0.0678267\ttotal: 9.19s\tremaining: 2.61s\n",
      "779:\tlearn: 0.0678077\ttotal: 9.2s\tremaining: 2.59s\n",
      "780:\tlearn: 0.0677856\ttotal: 9.21s\tremaining: 2.58s\n",
      "781:\tlearn: 0.0677765\ttotal: 9.22s\tremaining: 2.57s\n",
      "782:\tlearn: 0.0677523\ttotal: 9.23s\tremaining: 2.56s\n",
      "783:\tlearn: 0.0677379\ttotal: 9.24s\tremaining: 2.55s\n",
      "784:\tlearn: 0.0677065\ttotal: 9.26s\tremaining: 2.54s\n",
      "785:\tlearn: 0.0676970\ttotal: 9.27s\tremaining: 2.52s\n",
      "786:\tlearn: 0.0676878\ttotal: 9.28s\tremaining: 2.51s\n",
      "787:\tlearn: 0.0676702\ttotal: 9.29s\tremaining: 2.5s\n",
      "788:\tlearn: 0.0676590\ttotal: 9.31s\tremaining: 2.49s\n",
      "789:\tlearn: 0.0676254\ttotal: 9.32s\tremaining: 2.48s\n",
      "790:\tlearn: 0.0676132\ttotal: 9.34s\tremaining: 2.47s\n",
      "791:\tlearn: 0.0675948\ttotal: 9.35s\tremaining: 2.46s\n",
      "792:\tlearn: 0.0675858\ttotal: 9.36s\tremaining: 2.44s\n",
      "793:\tlearn: 0.0675718\ttotal: 9.37s\tremaining: 2.43s\n",
      "794:\tlearn: 0.0675560\ttotal: 9.38s\tremaining: 2.42s\n",
      "795:\tlearn: 0.0675390\ttotal: 9.39s\tremaining: 2.41s\n",
      "796:\tlearn: 0.0675196\ttotal: 9.41s\tremaining: 2.4s\n",
      "797:\tlearn: 0.0675127\ttotal: 9.42s\tremaining: 2.38s\n",
      "798:\tlearn: 0.0674822\ttotal: 9.43s\tremaining: 2.37s\n",
      "799:\tlearn: 0.0674666\ttotal: 9.45s\tremaining: 2.36s\n",
      "800:\tlearn: 0.0674475\ttotal: 9.46s\tremaining: 2.35s\n",
      "801:\tlearn: 0.0674355\ttotal: 9.48s\tremaining: 2.34s\n",
      "802:\tlearn: 0.0674193\ttotal: 9.49s\tremaining: 2.33s\n",
      "803:\tlearn: 0.0673995\ttotal: 9.51s\tremaining: 2.32s\n",
      "804:\tlearn: 0.0673908\ttotal: 9.52s\tremaining: 2.31s\n",
      "805:\tlearn: 0.0673739\ttotal: 9.52s\tremaining: 2.29s\n",
      "806:\tlearn: 0.0673503\ttotal: 9.54s\tremaining: 2.28s\n",
      "807:\tlearn: 0.0673426\ttotal: 9.54s\tremaining: 2.27s\n",
      "808:\tlearn: 0.0673216\ttotal: 9.56s\tremaining: 2.26s\n",
      "809:\tlearn: 0.0673123\ttotal: 9.57s\tremaining: 2.24s\n",
      "810:\tlearn: 0.0672955\ttotal: 9.58s\tremaining: 2.23s\n",
      "811:\tlearn: 0.0672860\ttotal: 9.59s\tremaining: 2.22s\n",
      "812:\tlearn: 0.0672646\ttotal: 9.61s\tremaining: 2.21s\n",
      "813:\tlearn: 0.0672447\ttotal: 9.62s\tremaining: 2.2s\n",
      "814:\tlearn: 0.0672348\ttotal: 9.63s\tremaining: 2.19s\n",
      "815:\tlearn: 0.0672095\ttotal: 9.64s\tremaining: 2.17s\n",
      "816:\tlearn: 0.0671940\ttotal: 9.65s\tremaining: 2.16s\n",
      "817:\tlearn: 0.0671591\ttotal: 9.66s\tremaining: 2.15s\n",
      "818:\tlearn: 0.0671419\ttotal: 9.68s\tremaining: 2.14s\n",
      "819:\tlearn: 0.0671048\ttotal: 9.69s\tremaining: 2.13s\n",
      "820:\tlearn: 0.0670865\ttotal: 9.7s\tremaining: 2.12s\n",
      "821:\tlearn: 0.0670661\ttotal: 9.71s\tremaining: 2.1s\n",
      "822:\tlearn: 0.0670587\ttotal: 9.72s\tremaining: 2.09s\n",
      "823:\tlearn: 0.0670456\ttotal: 9.74s\tremaining: 2.08s\n",
      "824:\tlearn: 0.0670223\ttotal: 9.76s\tremaining: 2.07s\n",
      "825:\tlearn: 0.0670071\ttotal: 9.79s\tremaining: 2.06s\n",
      "826:\tlearn: 0.0669833\ttotal: 9.8s\tremaining: 2.05s\n",
      "827:\tlearn: 0.0669547\ttotal: 9.83s\tremaining: 2.04s\n",
      "828:\tlearn: 0.0669280\ttotal: 9.84s\tremaining: 2.03s\n",
      "829:\tlearn: 0.0669193\ttotal: 9.86s\tremaining: 2.02s\n",
      "830:\tlearn: 0.0669085\ttotal: 9.88s\tremaining: 2.01s\n",
      "831:\tlearn: 0.0668638\ttotal: 9.89s\tremaining: 2s\n",
      "832:\tlearn: 0.0668523\ttotal: 9.9s\tremaining: 1.99s\n",
      "833:\tlearn: 0.0668328\ttotal: 9.91s\tremaining: 1.97s\n",
      "834:\tlearn: 0.0668250\ttotal: 9.92s\tremaining: 1.96s\n",
      "835:\tlearn: 0.0668046\ttotal: 9.93s\tremaining: 1.95s\n",
      "836:\tlearn: 0.0667879\ttotal: 9.95s\tremaining: 1.94s\n",
      "837:\tlearn: 0.0667664\ttotal: 9.96s\tremaining: 1.93s\n",
      "838:\tlearn: 0.0667573\ttotal: 9.97s\tremaining: 1.91s\n",
      "839:\tlearn: 0.0667496\ttotal: 9.98s\tremaining: 1.9s\n",
      "840:\tlearn: 0.0667339\ttotal: 9.99s\tremaining: 1.89s\n",
      "841:\tlearn: 0.0667139\ttotal: 10s\tremaining: 1.88s\n",
      "842:\tlearn: 0.0666984\ttotal: 10s\tremaining: 1.86s\n",
      "843:\tlearn: 0.0666618\ttotal: 10s\tremaining: 1.85s\n",
      "844:\tlearn: 0.0666500\ttotal: 10s\tremaining: 1.84s\n",
      "845:\tlearn: 0.0666246\ttotal: 10.1s\tremaining: 1.83s\n",
      "846:\tlearn: 0.0666162\ttotal: 10.1s\tremaining: 1.82s\n",
      "847:\tlearn: 0.0666020\ttotal: 10.1s\tremaining: 1.81s\n",
      "848:\tlearn: 0.0665880\ttotal: 10.1s\tremaining: 1.79s\n",
      "849:\tlearn: 0.0665809\ttotal: 10.1s\tremaining: 1.78s\n",
      "850:\tlearn: 0.0665611\ttotal: 10.1s\tremaining: 1.77s\n",
      "851:\tlearn: 0.0665406\ttotal: 10.1s\tremaining: 1.76s\n",
      "852:\tlearn: 0.0664956\ttotal: 10.1s\tremaining: 1.75s\n",
      "853:\tlearn: 0.0664796\ttotal: 10.1s\tremaining: 1.74s\n",
      "854:\tlearn: 0.0664639\ttotal: 10.2s\tremaining: 1.72s\n",
      "855:\tlearn: 0.0664376\ttotal: 10.2s\tremaining: 1.71s\n",
      "856:\tlearn: 0.0664285\ttotal: 10.2s\tremaining: 1.7s\n",
      "857:\tlearn: 0.0664176\ttotal: 10.2s\tremaining: 1.69s\n",
      "858:\tlearn: 0.0664053\ttotal: 10.2s\tremaining: 1.67s\n",
      "859:\tlearn: 0.0663790\ttotal: 10.2s\tremaining: 1.66s\n",
      "860:\tlearn: 0.0663525\ttotal: 10.2s\tremaining: 1.65s\n",
      "861:\tlearn: 0.0663376\ttotal: 10.2s\tremaining: 1.64s\n",
      "862:\tlearn: 0.0663235\ttotal: 10.3s\tremaining: 1.63s\n",
      "863:\tlearn: 0.0663081\ttotal: 10.3s\tremaining: 1.61s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864:\tlearn: 0.0663007\ttotal: 10.3s\tremaining: 1.6s\n",
      "865:\tlearn: 0.0662862\ttotal: 10.3s\tremaining: 1.59s\n",
      "866:\tlearn: 0.0662761\ttotal: 10.3s\tremaining: 1.58s\n",
      "867:\tlearn: 0.0662597\ttotal: 10.3s\tremaining: 1.57s\n",
      "868:\tlearn: 0.0662421\ttotal: 10.3s\tremaining: 1.55s\n",
      "869:\tlearn: 0.0662172\ttotal: 10.3s\tremaining: 1.54s\n",
      "870:\tlearn: 0.0661892\ttotal: 10.3s\tremaining: 1.53s\n",
      "871:\tlearn: 0.0661755\ttotal: 10.4s\tremaining: 1.52s\n",
      "872:\tlearn: 0.0661623\ttotal: 10.4s\tremaining: 1.51s\n",
      "873:\tlearn: 0.0661366\ttotal: 10.4s\tremaining: 1.5s\n",
      "874:\tlearn: 0.0661283\ttotal: 10.4s\tremaining: 1.48s\n",
      "875:\tlearn: 0.0661194\ttotal: 10.4s\tremaining: 1.47s\n",
      "876:\tlearn: 0.0661049\ttotal: 10.4s\tremaining: 1.46s\n",
      "877:\tlearn: 0.0660849\ttotal: 10.4s\tremaining: 1.45s\n",
      "878:\tlearn: 0.0660698\ttotal: 10.4s\tremaining: 1.44s\n",
      "879:\tlearn: 0.0660541\ttotal: 10.4s\tremaining: 1.42s\n",
      "880:\tlearn: 0.0660229\ttotal: 10.5s\tremaining: 1.41s\n",
      "881:\tlearn: 0.0660070\ttotal: 10.5s\tremaining: 1.4s\n",
      "882:\tlearn: 0.0659979\ttotal: 10.5s\tremaining: 1.39s\n",
      "883:\tlearn: 0.0659859\ttotal: 10.5s\tremaining: 1.38s\n",
      "884:\tlearn: 0.0659648\ttotal: 10.5s\tremaining: 1.36s\n",
      "885:\tlearn: 0.0659475\ttotal: 10.5s\tremaining: 1.35s\n",
      "886:\tlearn: 0.0659375\ttotal: 10.5s\tremaining: 1.34s\n",
      "887:\tlearn: 0.0659220\ttotal: 10.5s\tremaining: 1.33s\n",
      "888:\tlearn: 0.0658734\ttotal: 10.5s\tremaining: 1.32s\n",
      "889:\tlearn: 0.0658540\ttotal: 10.6s\tremaining: 1.3s\n",
      "890:\tlearn: 0.0658248\ttotal: 10.6s\tremaining: 1.29s\n",
      "891:\tlearn: 0.0658166\ttotal: 10.6s\tremaining: 1.28s\n",
      "892:\tlearn: 0.0658049\ttotal: 10.6s\tremaining: 1.27s\n",
      "893:\tlearn: 0.0657955\ttotal: 10.6s\tremaining: 1.26s\n",
      "894:\tlearn: 0.0657791\ttotal: 10.6s\tremaining: 1.25s\n",
      "895:\tlearn: 0.0657749\ttotal: 10.6s\tremaining: 1.23s\n",
      "896:\tlearn: 0.0657692\ttotal: 10.6s\tremaining: 1.22s\n",
      "897:\tlearn: 0.0657402\ttotal: 10.7s\tremaining: 1.21s\n",
      "898:\tlearn: 0.0657162\ttotal: 10.7s\tremaining: 1.2s\n",
      "899:\tlearn: 0.0657071\ttotal: 10.7s\tremaining: 1.19s\n",
      "900:\tlearn: 0.0656990\ttotal: 10.7s\tremaining: 1.17s\n",
      "901:\tlearn: 0.0656878\ttotal: 10.7s\tremaining: 1.16s\n",
      "902:\tlearn: 0.0656728\ttotal: 10.7s\tremaining: 1.15s\n",
      "903:\tlearn: 0.0656540\ttotal: 10.7s\tremaining: 1.14s\n",
      "904:\tlearn: 0.0656430\ttotal: 10.7s\tremaining: 1.13s\n",
      "905:\tlearn: 0.0656367\ttotal: 10.8s\tremaining: 1.11s\n",
      "906:\tlearn: 0.0656244\ttotal: 10.8s\tremaining: 1.1s\n",
      "907:\tlearn: 0.0656115\ttotal: 10.8s\tremaining: 1.09s\n",
      "908:\tlearn: 0.0655758\ttotal: 10.8s\tremaining: 1.08s\n",
      "909:\tlearn: 0.0655645\ttotal: 10.8s\tremaining: 1.07s\n",
      "910:\tlearn: 0.0655518\ttotal: 10.9s\tremaining: 1.06s\n",
      "911:\tlearn: 0.0655315\ttotal: 10.9s\tremaining: 1.05s\n",
      "912:\tlearn: 0.0655212\ttotal: 10.9s\tremaining: 1.04s\n",
      "913:\tlearn: 0.0654972\ttotal: 10.9s\tremaining: 1.03s\n",
      "914:\tlearn: 0.0654739\ttotal: 10.9s\tremaining: 1.01s\n",
      "915:\tlearn: 0.0654663\ttotal: 10.9s\tremaining: 1s\n",
      "916:\tlearn: 0.0654575\ttotal: 10.9s\tremaining: 991ms\n",
      "917:\tlearn: 0.0654392\ttotal: 11s\tremaining: 979ms\n",
      "918:\tlearn: 0.0654114\ttotal: 11s\tremaining: 967ms\n",
      "919:\tlearn: 0.0653950\ttotal: 11s\tremaining: 955ms\n",
      "920:\tlearn: 0.0653762\ttotal: 11s\tremaining: 943ms\n",
      "921:\tlearn: 0.0653669\ttotal: 11s\tremaining: 931ms\n",
      "922:\tlearn: 0.0653466\ttotal: 11s\tremaining: 919ms\n",
      "923:\tlearn: 0.0653221\ttotal: 11s\tremaining: 907ms\n",
      "924:\tlearn: 0.0652901\ttotal: 11s\tremaining: 895ms\n",
      "925:\tlearn: 0.0652719\ttotal: 11.1s\tremaining: 883ms\n",
      "926:\tlearn: 0.0652593\ttotal: 11.1s\tremaining: 871ms\n",
      "927:\tlearn: 0.0652332\ttotal: 11.1s\tremaining: 859ms\n",
      "928:\tlearn: 0.0652156\ttotal: 11.1s\tremaining: 847ms\n",
      "929:\tlearn: 0.0652096\ttotal: 11.1s\tremaining: 835ms\n",
      "930:\tlearn: 0.0651948\ttotal: 11.1s\tremaining: 823ms\n",
      "931:\tlearn: 0.0651809\ttotal: 11.1s\tremaining: 811ms\n",
      "932:\tlearn: 0.0651710\ttotal: 11.1s\tremaining: 799ms\n",
      "933:\tlearn: 0.0651573\ttotal: 11.1s\tremaining: 787ms\n",
      "934:\tlearn: 0.0651389\ttotal: 11.1s\tremaining: 775ms\n",
      "935:\tlearn: 0.0651110\ttotal: 11.2s\tremaining: 763ms\n",
      "936:\tlearn: 0.0651016\ttotal: 11.2s\tremaining: 751ms\n",
      "937:\tlearn: 0.0650922\ttotal: 11.2s\tremaining: 739ms\n",
      "938:\tlearn: 0.0650693\ttotal: 11.2s\tremaining: 727ms\n",
      "939:\tlearn: 0.0650572\ttotal: 11.2s\tremaining: 715ms\n",
      "940:\tlearn: 0.0650377\ttotal: 11.2s\tremaining: 703ms\n",
      "941:\tlearn: 0.0650270\ttotal: 11.2s\tremaining: 691ms\n",
      "942:\tlearn: 0.0650088\ttotal: 11.2s\tremaining: 679ms\n",
      "943:\tlearn: 0.0650042\ttotal: 11.3s\tremaining: 667ms\n",
      "944:\tlearn: 0.0649788\ttotal: 11.3s\tremaining: 655ms\n",
      "945:\tlearn: 0.0649734\ttotal: 11.3s\tremaining: 644ms\n",
      "946:\tlearn: 0.0649650\ttotal: 11.3s\tremaining: 632ms\n",
      "947:\tlearn: 0.0649539\ttotal: 11.3s\tremaining: 620ms\n",
      "948:\tlearn: 0.0649439\ttotal: 11.3s\tremaining: 608ms\n",
      "949:\tlearn: 0.0649309\ttotal: 11.3s\tremaining: 596ms\n",
      "950:\tlearn: 0.0649250\ttotal: 11.3s\tremaining: 584ms\n",
      "951:\tlearn: 0.0649116\ttotal: 11.3s\tremaining: 572ms\n",
      "952:\tlearn: 0.0648970\ttotal: 11.4s\tremaining: 560ms\n",
      "953:\tlearn: 0.0648727\ttotal: 11.4s\tremaining: 548ms\n",
      "954:\tlearn: 0.0648560\ttotal: 11.4s\tremaining: 536ms\n",
      "955:\tlearn: 0.0648507\ttotal: 11.4s\tremaining: 524ms\n",
      "956:\tlearn: 0.0648360\ttotal: 11.4s\tremaining: 512ms\n",
      "957:\tlearn: 0.0648268\ttotal: 11.4s\tremaining: 500ms\n",
      "958:\tlearn: 0.0648232\ttotal: 11.4s\tremaining: 488ms\n",
      "959:\tlearn: 0.0648017\ttotal: 11.4s\tremaining: 476ms\n",
      "960:\tlearn: 0.0647861\ttotal: 11.4s\tremaining: 464ms\n",
      "961:\tlearn: 0.0647694\ttotal: 11.5s\tremaining: 452ms\n",
      "962:\tlearn: 0.0647496\ttotal: 11.5s\tremaining: 440ms\n",
      "963:\tlearn: 0.0647249\ttotal: 11.5s\tremaining: 429ms\n",
      "964:\tlearn: 0.0647144\ttotal: 11.5s\tremaining: 416ms\n",
      "965:\tlearn: 0.0646956\ttotal: 11.5s\tremaining: 405ms\n",
      "966:\tlearn: 0.0646831\ttotal: 11.5s\tremaining: 393ms\n",
      "967:\tlearn: 0.0646508\ttotal: 11.5s\tremaining: 381ms\n",
      "968:\tlearn: 0.0646408\ttotal: 11.5s\tremaining: 369ms\n",
      "969:\tlearn: 0.0646336\ttotal: 11.6s\tremaining: 357ms\n",
      "970:\tlearn: 0.0646126\ttotal: 11.6s\tremaining: 345ms\n",
      "971:\tlearn: 0.0645922\ttotal: 11.6s\tremaining: 333ms\n",
      "972:\tlearn: 0.0645814\ttotal: 11.6s\tremaining: 322ms\n",
      "973:\tlearn: 0.0645706\ttotal: 11.6s\tremaining: 310ms\n",
      "974:\tlearn: 0.0645583\ttotal: 11.6s\tremaining: 298ms\n",
      "975:\tlearn: 0.0645403\ttotal: 11.6s\tremaining: 286ms\n",
      "976:\tlearn: 0.0645159\ttotal: 11.6s\tremaining: 274ms\n",
      "977:\tlearn: 0.0645025\ttotal: 11.6s\tremaining: 262ms\n",
      "978:\tlearn: 0.0644932\ttotal: 11.6s\tremaining: 250ms\n",
      "979:\tlearn: 0.0644739\ttotal: 11.7s\tremaining: 238ms\n",
      "980:\tlearn: 0.0644640\ttotal: 11.7s\tremaining: 226ms\n",
      "981:\tlearn: 0.0644244\ttotal: 11.7s\tremaining: 214ms\n",
      "982:\tlearn: 0.0644145\ttotal: 11.7s\tremaining: 202ms\n",
      "983:\tlearn: 0.0644035\ttotal: 11.7s\tremaining: 190ms\n",
      "984:\tlearn: 0.0643920\ttotal: 11.7s\tremaining: 178ms\n",
      "985:\tlearn: 0.0643797\ttotal: 11.7s\tremaining: 166ms\n",
      "986:\tlearn: 0.0643520\ttotal: 11.7s\tremaining: 155ms\n",
      "987:\tlearn: 0.0643402\ttotal: 11.7s\tremaining: 143ms\n",
      "988:\tlearn: 0.0643287\ttotal: 11.8s\tremaining: 131ms\n",
      "989:\tlearn: 0.0643156\ttotal: 11.8s\tremaining: 119ms\n",
      "990:\tlearn: 0.0643064\ttotal: 11.8s\tremaining: 107ms\n",
      "991:\tlearn: 0.0642957\ttotal: 11.8s\tremaining: 95.2ms\n",
      "992:\tlearn: 0.0642854\ttotal: 11.8s\tremaining: 83.3ms\n",
      "993:\tlearn: 0.0642730\ttotal: 11.8s\tremaining: 71.4ms\n",
      "994:\tlearn: 0.0642662\ttotal: 11.9s\tremaining: 59.6ms\n",
      "995:\tlearn: 0.0642611\ttotal: 11.9s\tremaining: 47.7ms\n",
      "996:\tlearn: 0.0642374\ttotal: 11.9s\tremaining: 35.8ms\n",
      "997:\tlearn: 0.0642216\ttotal: 11.9s\tremaining: 23.9ms\n",
      "998:\tlearn: 0.0642152\ttotal: 11.9s\tremaining: 11.9ms\n",
      "999:\tlearn: 0.0641982\ttotal: 11.9s\tremaining: 0us\n",
      "0:\tlearn: 0.4007122\ttotal: 17.7ms\tremaining: 17.7s\n",
      "1:\tlearn: 0.3623709\ttotal: 30.7ms\tremaining: 15.3s\n",
      "2:\tlearn: 0.3349015\ttotal: 45.6ms\tremaining: 15.2s\n",
      "3:\tlearn: 0.3107726\ttotal: 55.4ms\tremaining: 13.8s\n",
      "4:\tlearn: 0.2918783\ttotal: 67.4ms\tremaining: 13.4s\n",
      "5:\tlearn: 0.2779789\ttotal: 75.5ms\tremaining: 12.5s\n",
      "6:\tlearn: 0.2643344\ttotal: 90.8ms\tremaining: 12.9s\n",
      "7:\tlearn: 0.2559071\ttotal: 98.6ms\tremaining: 12.2s\n",
      "8:\tlearn: 0.2453019\ttotal: 112ms\tremaining: 12.3s\n",
      "9:\tlearn: 0.2333089\ttotal: 123ms\tremaining: 12.2s\n",
      "10:\tlearn: 0.2215604\ttotal: 134ms\tremaining: 12s\n",
      "11:\tlearn: 0.2077108\ttotal: 144ms\tremaining: 11.8s\n",
      "12:\tlearn: 0.2012111\ttotal: 155ms\tremaining: 11.8s\n",
      "13:\tlearn: 0.1970631\ttotal: 165ms\tremaining: 11.6s\n",
      "14:\tlearn: 0.1945278\ttotal: 178ms\tremaining: 11.7s\n",
      "15:\tlearn: 0.1917026\ttotal: 189ms\tremaining: 11.6s\n",
      "16:\tlearn: 0.1887336\ttotal: 200ms\tremaining: 11.6s\n",
      "17:\tlearn: 0.1849692\ttotal: 211ms\tremaining: 11.5s\n",
      "18:\tlearn: 0.1815343\ttotal: 223ms\tremaining: 11.5s\n",
      "19:\tlearn: 0.1802542\ttotal: 237ms\tremaining: 11.6s\n",
      "20:\tlearn: 0.1794244\ttotal: 247ms\tremaining: 11.5s\n",
      "21:\tlearn: 0.1780676\ttotal: 262ms\tremaining: 11.6s\n",
      "22:\tlearn: 0.1779088\ttotal: 269ms\tremaining: 11.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:\tlearn: 0.1775700\ttotal: 284ms\tremaining: 11.5s\n",
      "24:\tlearn: 0.1700113\ttotal: 293ms\tremaining: 11.4s\n",
      "25:\tlearn: 0.1690773\ttotal: 311ms\tremaining: 11.6s\n",
      "26:\tlearn: 0.1688258\ttotal: 326ms\tremaining: 11.8s\n",
      "27:\tlearn: 0.1684349\ttotal: 336ms\tremaining: 11.7s\n",
      "28:\tlearn: 0.1678890\ttotal: 353ms\tremaining: 11.8s\n",
      "29:\tlearn: 0.1677624\ttotal: 362ms\tremaining: 11.7s\n",
      "30:\tlearn: 0.1674600\ttotal: 376ms\tremaining: 11.7s\n",
      "31:\tlearn: 0.1671059\ttotal: 387ms\tremaining: 11.7s\n",
      "32:\tlearn: 0.1670821\ttotal: 400ms\tremaining: 11.7s\n",
      "33:\tlearn: 0.1669657\ttotal: 408ms\tremaining: 11.6s\n",
      "34:\tlearn: 0.1666922\ttotal: 421ms\tremaining: 11.6s\n",
      "35:\tlearn: 0.1663787\ttotal: 430ms\tremaining: 11.5s\n",
      "36:\tlearn: 0.1658253\ttotal: 444ms\tremaining: 11.6s\n",
      "37:\tlearn: 0.1653139\ttotal: 455ms\tremaining: 11.5s\n",
      "38:\tlearn: 0.1645261\ttotal: 466ms\tremaining: 11.5s\n",
      "39:\tlearn: 0.1643665\ttotal: 477ms\tremaining: 11.4s\n",
      "40:\tlearn: 0.1642770\ttotal: 496ms\tremaining: 11.6s\n",
      "41:\tlearn: 0.1638012\ttotal: 509ms\tremaining: 11.6s\n",
      "42:\tlearn: 0.1637227\ttotal: 521ms\tremaining: 11.6s\n",
      "43:\tlearn: 0.1635467\ttotal: 536ms\tremaining: 11.6s\n",
      "44:\tlearn: 0.1623896\ttotal: 544ms\tremaining: 11.5s\n",
      "45:\tlearn: 0.1601522\ttotal: 557ms\tremaining: 11.6s\n",
      "46:\tlearn: 0.1600698\ttotal: 572ms\tremaining: 11.6s\n",
      "47:\tlearn: 0.1600645\ttotal: 578ms\tremaining: 11.5s\n",
      "48:\tlearn: 0.1598907\ttotal: 589ms\tremaining: 11.4s\n",
      "49:\tlearn: 0.1594096\ttotal: 602ms\tremaining: 11.4s\n",
      "50:\tlearn: 0.1593666\ttotal: 616ms\tremaining: 11.5s\n",
      "51:\tlearn: 0.1593415\ttotal: 622ms\tremaining: 11.3s\n",
      "52:\tlearn: 0.1576723\ttotal: 633ms\tremaining: 11.3s\n",
      "53:\tlearn: 0.1575864\ttotal: 642ms\tremaining: 11.3s\n",
      "54:\tlearn: 0.1575392\ttotal: 658ms\tremaining: 11.3s\n",
      "55:\tlearn: 0.1574674\ttotal: 667ms\tremaining: 11.2s\n",
      "56:\tlearn: 0.1573153\ttotal: 681ms\tremaining: 11.3s\n",
      "57:\tlearn: 0.1572186\ttotal: 695ms\tremaining: 11.3s\n",
      "58:\tlearn: 0.1571285\ttotal: 705ms\tremaining: 11.2s\n",
      "59:\tlearn: 0.1568862\ttotal: 716ms\tremaining: 11.2s\n",
      "60:\tlearn: 0.1565640\ttotal: 728ms\tremaining: 11.2s\n",
      "61:\tlearn: 0.1559204\ttotal: 749ms\tremaining: 11.3s\n",
      "62:\tlearn: 0.1539511\ttotal: 766ms\tremaining: 11.4s\n",
      "63:\tlearn: 0.1520436\ttotal: 789ms\tremaining: 11.5s\n",
      "64:\tlearn: 0.1510616\ttotal: 820ms\tremaining: 11.8s\n",
      "65:\tlearn: 0.1481651\ttotal: 836ms\tremaining: 11.8s\n",
      "66:\tlearn: 0.1458443\ttotal: 855ms\tremaining: 11.9s\n",
      "67:\tlearn: 0.1453938\ttotal: 875ms\tremaining: 12s\n",
      "68:\tlearn: 0.1434376\ttotal: 886ms\tremaining: 12s\n",
      "69:\tlearn: 0.1416672\ttotal: 900ms\tremaining: 12s\n",
      "70:\tlearn: 0.1385768\ttotal: 913ms\tremaining: 11.9s\n",
      "71:\tlearn: 0.1384839\ttotal: 925ms\tremaining: 11.9s\n",
      "72:\tlearn: 0.1372438\ttotal: 937ms\tremaining: 11.9s\n",
      "73:\tlearn: 0.1337331\ttotal: 954ms\tremaining: 11.9s\n",
      "74:\tlearn: 0.1308318\ttotal: 962ms\tremaining: 11.9s\n",
      "75:\tlearn: 0.1301385\ttotal: 978ms\tremaining: 11.9s\n",
      "76:\tlearn: 0.1281073\ttotal: 988ms\tremaining: 11.8s\n",
      "77:\tlearn: 0.1267059\ttotal: 1s\tremaining: 11.8s\n",
      "78:\tlearn: 0.1248598\ttotal: 1.01s\tremaining: 11.8s\n",
      "79:\tlearn: 0.1235445\ttotal: 1.03s\tremaining: 11.8s\n",
      "80:\tlearn: 0.1224818\ttotal: 1.04s\tremaining: 11.8s\n",
      "81:\tlearn: 0.1207907\ttotal: 1.05s\tremaining: 11.8s\n",
      "82:\tlearn: 0.1192580\ttotal: 1.07s\tremaining: 11.8s\n",
      "83:\tlearn: 0.1183001\ttotal: 1.09s\tremaining: 11.9s\n",
      "84:\tlearn: 0.1171437\ttotal: 1.1s\tremaining: 11.9s\n",
      "85:\tlearn: 0.1158975\ttotal: 1.12s\tremaining: 11.9s\n",
      "86:\tlearn: 0.1142834\ttotal: 1.13s\tremaining: 11.9s\n",
      "87:\tlearn: 0.1133152\ttotal: 1.14s\tremaining: 11.8s\n",
      "88:\tlearn: 0.1123060\ttotal: 1.16s\tremaining: 11.9s\n",
      "89:\tlearn: 0.1117361\ttotal: 1.17s\tremaining: 11.8s\n",
      "90:\tlearn: 0.1110327\ttotal: 1.19s\tremaining: 11.8s\n",
      "91:\tlearn: 0.1105879\ttotal: 1.2s\tremaining: 11.8s\n",
      "92:\tlearn: 0.1096418\ttotal: 1.21s\tremaining: 11.8s\n",
      "93:\tlearn: 0.1088915\ttotal: 1.22s\tremaining: 11.8s\n",
      "94:\tlearn: 0.1085827\ttotal: 1.23s\tremaining: 11.8s\n",
      "95:\tlearn: 0.1077225\ttotal: 1.24s\tremaining: 11.7s\n",
      "96:\tlearn: 0.1067853\ttotal: 1.25s\tremaining: 11.7s\n",
      "97:\tlearn: 0.1060693\ttotal: 1.26s\tremaining: 11.6s\n",
      "98:\tlearn: 0.1056633\ttotal: 1.27s\tremaining: 11.6s\n",
      "99:\tlearn: 0.1050994\ttotal: 1.28s\tremaining: 11.6s\n",
      "100:\tlearn: 0.1047524\ttotal: 1.29s\tremaining: 11.5s\n",
      "101:\tlearn: 0.1044336\ttotal: 1.3s\tremaining: 11.5s\n",
      "102:\tlearn: 0.1038653\ttotal: 1.32s\tremaining: 11.5s\n",
      "103:\tlearn: 0.1035560\ttotal: 1.33s\tremaining: 11.4s\n",
      "104:\tlearn: 0.1031220\ttotal: 1.34s\tremaining: 11.4s\n",
      "105:\tlearn: 0.1028489\ttotal: 1.35s\tremaining: 11.4s\n",
      "106:\tlearn: 0.1022956\ttotal: 1.36s\tremaining: 11.4s\n",
      "107:\tlearn: 0.1020541\ttotal: 1.37s\tremaining: 11.3s\n",
      "108:\tlearn: 0.1017233\ttotal: 1.38s\tremaining: 11.3s\n",
      "109:\tlearn: 0.1014320\ttotal: 1.39s\tremaining: 11.3s\n",
      "110:\tlearn: 0.1008449\ttotal: 1.4s\tremaining: 11.2s\n",
      "111:\tlearn: 0.1005721\ttotal: 1.41s\tremaining: 11.2s\n",
      "112:\tlearn: 0.1002300\ttotal: 1.42s\tremaining: 11.2s\n",
      "113:\tlearn: 0.0999948\ttotal: 1.43s\tremaining: 11.1s\n",
      "114:\tlearn: 0.0995551\ttotal: 1.44s\tremaining: 11.1s\n",
      "115:\tlearn: 0.0993128\ttotal: 1.46s\tremaining: 11.1s\n",
      "116:\tlearn: 0.0988711\ttotal: 1.47s\tremaining: 11.1s\n",
      "117:\tlearn: 0.0983113\ttotal: 1.48s\tremaining: 11s\n",
      "118:\tlearn: 0.0980515\ttotal: 1.49s\tremaining: 11s\n",
      "119:\tlearn: 0.0978420\ttotal: 1.5s\tremaining: 11s\n",
      "120:\tlearn: 0.0975102\ttotal: 1.51s\tremaining: 11s\n",
      "121:\tlearn: 0.0972753\ttotal: 1.52s\tremaining: 10.9s\n",
      "122:\tlearn: 0.0971056\ttotal: 1.52s\tremaining: 10.9s\n",
      "123:\tlearn: 0.0968971\ttotal: 1.54s\tremaining: 10.9s\n",
      "124:\tlearn: 0.0966014\ttotal: 1.55s\tremaining: 10.8s\n",
      "125:\tlearn: 0.0964790\ttotal: 1.56s\tremaining: 10.8s\n",
      "126:\tlearn: 0.0961503\ttotal: 1.57s\tremaining: 10.8s\n",
      "127:\tlearn: 0.0960130\ttotal: 1.58s\tremaining: 10.8s\n",
      "128:\tlearn: 0.0957547\ttotal: 1.59s\tremaining: 10.8s\n",
      "129:\tlearn: 0.0955745\ttotal: 1.6s\tremaining: 10.7s\n",
      "130:\tlearn: 0.0953919\ttotal: 1.62s\tremaining: 10.7s\n",
      "131:\tlearn: 0.0952915\ttotal: 1.63s\tremaining: 10.7s\n",
      "132:\tlearn: 0.0950284\ttotal: 1.64s\tremaining: 10.7s\n",
      "133:\tlearn: 0.0949455\ttotal: 1.65s\tremaining: 10.7s\n",
      "134:\tlearn: 0.0947780\ttotal: 1.66s\tremaining: 10.6s\n",
      "135:\tlearn: 0.0946499\ttotal: 1.67s\tremaining: 10.6s\n",
      "136:\tlearn: 0.0945608\ttotal: 1.68s\tremaining: 10.6s\n",
      "137:\tlearn: 0.0942842\ttotal: 1.69s\tremaining: 10.5s\n",
      "138:\tlearn: 0.0940672\ttotal: 1.7s\tremaining: 10.5s\n",
      "139:\tlearn: 0.0938950\ttotal: 1.71s\tremaining: 10.5s\n",
      "140:\tlearn: 0.0937581\ttotal: 1.72s\tremaining: 10.5s\n",
      "141:\tlearn: 0.0936136\ttotal: 1.73s\tremaining: 10.5s\n",
      "142:\tlearn: 0.0935030\ttotal: 1.75s\tremaining: 10.5s\n",
      "143:\tlearn: 0.0933926\ttotal: 1.76s\tremaining: 10.5s\n",
      "144:\tlearn: 0.0931942\ttotal: 1.8s\tremaining: 10.6s\n",
      "145:\tlearn: 0.0929552\ttotal: 1.81s\tremaining: 10.6s\n",
      "146:\tlearn: 0.0928352\ttotal: 1.84s\tremaining: 10.7s\n",
      "147:\tlearn: 0.0927355\ttotal: 1.86s\tremaining: 10.7s\n",
      "148:\tlearn: 0.0925315\ttotal: 1.88s\tremaining: 10.7s\n",
      "149:\tlearn: 0.0924160\ttotal: 1.9s\tremaining: 10.8s\n",
      "150:\tlearn: 0.0923287\ttotal: 1.91s\tremaining: 10.7s\n",
      "151:\tlearn: 0.0921557\ttotal: 1.92s\tremaining: 10.7s\n",
      "152:\tlearn: 0.0920898\ttotal: 1.93s\tremaining: 10.7s\n",
      "153:\tlearn: 0.0920286\ttotal: 1.94s\tremaining: 10.7s\n",
      "154:\tlearn: 0.0918364\ttotal: 1.95s\tremaining: 10.6s\n",
      "155:\tlearn: 0.0915689\ttotal: 1.96s\tremaining: 10.6s\n",
      "156:\tlearn: 0.0915184\ttotal: 1.97s\tremaining: 10.6s\n",
      "157:\tlearn: 0.0913908\ttotal: 1.98s\tremaining: 10.6s\n",
      "158:\tlearn: 0.0912203\ttotal: 2s\tremaining: 10.6s\n",
      "159:\tlearn: 0.0911408\ttotal: 2s\tremaining: 10.5s\n",
      "160:\tlearn: 0.0909255\ttotal: 2.01s\tremaining: 10.5s\n",
      "161:\tlearn: 0.0907579\ttotal: 2.03s\tremaining: 10.5s\n",
      "162:\tlearn: 0.0906377\ttotal: 2.03s\tremaining: 10.4s\n",
      "163:\tlearn: 0.0905507\ttotal: 2.05s\tremaining: 10.4s\n",
      "164:\tlearn: 0.0904737\ttotal: 2.05s\tremaining: 10.4s\n",
      "165:\tlearn: 0.0903671\ttotal: 2.07s\tremaining: 10.4s\n",
      "166:\tlearn: 0.0902389\ttotal: 2.08s\tremaining: 10.4s\n",
      "167:\tlearn: 0.0901772\ttotal: 2.09s\tremaining: 10.4s\n",
      "168:\tlearn: 0.0900657\ttotal: 2.1s\tremaining: 10.3s\n",
      "169:\tlearn: 0.0899488\ttotal: 2.11s\tremaining: 10.3s\n",
      "170:\tlearn: 0.0896855\ttotal: 2.13s\tremaining: 10.3s\n",
      "171:\tlearn: 0.0896365\ttotal: 2.14s\tremaining: 10.3s\n",
      "172:\tlearn: 0.0895440\ttotal: 2.15s\tremaining: 10.3s\n",
      "173:\tlearn: 0.0894681\ttotal: 2.17s\tremaining: 10.3s\n",
      "174:\tlearn: 0.0894008\ttotal: 2.17s\tremaining: 10.3s\n",
      "175:\tlearn: 0.0892421\ttotal: 2.19s\tremaining: 10.2s\n",
      "176:\tlearn: 0.0891290\ttotal: 2.2s\tremaining: 10.2s\n",
      "177:\tlearn: 0.0890464\ttotal: 2.22s\tremaining: 10.2s\n",
      "178:\tlearn: 0.0889798\ttotal: 2.23s\tremaining: 10.2s\n",
      "179:\tlearn: 0.0888796\ttotal: 2.24s\tremaining: 10.2s\n",
      "180:\tlearn: 0.0887007\ttotal: 2.25s\tremaining: 10.2s\n",
      "181:\tlearn: 0.0885962\ttotal: 2.26s\tremaining: 10.2s\n",
      "182:\tlearn: 0.0884873\ttotal: 2.27s\tremaining: 10.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183:\tlearn: 0.0884152\ttotal: 2.29s\tremaining: 10.1s\n",
      "184:\tlearn: 0.0883303\ttotal: 2.3s\tremaining: 10.1s\n",
      "185:\tlearn: 0.0882662\ttotal: 2.3s\tremaining: 10.1s\n",
      "186:\tlearn: 0.0881796\ttotal: 2.31s\tremaining: 10.1s\n",
      "187:\tlearn: 0.0880987\ttotal: 2.32s\tremaining: 10s\n",
      "188:\tlearn: 0.0880300\ttotal: 2.33s\tremaining: 10s\n",
      "189:\tlearn: 0.0879774\ttotal: 2.35s\tremaining: 10s\n",
      "190:\tlearn: 0.0879146\ttotal: 2.36s\tremaining: 10s\n",
      "191:\tlearn: 0.0878217\ttotal: 2.37s\tremaining: 9.99s\n",
      "192:\tlearn: 0.0877591\ttotal: 2.38s\tremaining: 9.96s\n",
      "193:\tlearn: 0.0876519\ttotal: 2.39s\tremaining: 9.94s\n",
      "194:\tlearn: 0.0875871\ttotal: 2.4s\tremaining: 9.92s\n",
      "195:\tlearn: 0.0875415\ttotal: 2.42s\tremaining: 9.93s\n",
      "196:\tlearn: 0.0874452\ttotal: 2.43s\tremaining: 9.9s\n",
      "197:\tlearn: 0.0873879\ttotal: 2.44s\tremaining: 9.89s\n",
      "198:\tlearn: 0.0873249\ttotal: 2.45s\tremaining: 9.87s\n",
      "199:\tlearn: 0.0872870\ttotal: 2.46s\tremaining: 9.85s\n",
      "200:\tlearn: 0.0872084\ttotal: 2.48s\tremaining: 9.84s\n",
      "201:\tlearn: 0.0871575\ttotal: 2.49s\tremaining: 9.82s\n",
      "202:\tlearn: 0.0870628\ttotal: 2.5s\tremaining: 9.8s\n",
      "203:\tlearn: 0.0870165\ttotal: 2.51s\tremaining: 9.8s\n",
      "204:\tlearn: 0.0869812\ttotal: 2.52s\tremaining: 9.78s\n",
      "205:\tlearn: 0.0869270\ttotal: 2.53s\tremaining: 9.76s\n",
      "206:\tlearn: 0.0868605\ttotal: 2.54s\tremaining: 9.74s\n",
      "207:\tlearn: 0.0867797\ttotal: 2.55s\tremaining: 9.73s\n",
      "208:\tlearn: 0.0867213\ttotal: 2.57s\tremaining: 9.72s\n",
      "209:\tlearn: 0.0866434\ttotal: 2.58s\tremaining: 9.71s\n",
      "210:\tlearn: 0.0866053\ttotal: 2.59s\tremaining: 9.69s\n",
      "211:\tlearn: 0.0865514\ttotal: 2.6s\tremaining: 9.67s\n",
      "212:\tlearn: 0.0864956\ttotal: 2.61s\tremaining: 9.66s\n",
      "213:\tlearn: 0.0864199\ttotal: 2.63s\tremaining: 9.64s\n",
      "214:\tlearn: 0.0863302\ttotal: 2.63s\tremaining: 9.62s\n",
      "215:\tlearn: 0.0862423\ttotal: 2.65s\tremaining: 9.61s\n",
      "216:\tlearn: 0.0861632\ttotal: 2.66s\tremaining: 9.59s\n",
      "217:\tlearn: 0.0860945\ttotal: 2.67s\tremaining: 9.59s\n",
      "218:\tlearn: 0.0859524\ttotal: 2.68s\tremaining: 9.57s\n",
      "219:\tlearn: 0.0858958\ttotal: 2.69s\tremaining: 9.54s\n",
      "220:\tlearn: 0.0858666\ttotal: 2.7s\tremaining: 9.52s\n",
      "221:\tlearn: 0.0858268\ttotal: 2.71s\tremaining: 9.51s\n",
      "222:\tlearn: 0.0857940\ttotal: 2.72s\tremaining: 9.48s\n",
      "223:\tlearn: 0.0855885\ttotal: 2.73s\tremaining: 9.47s\n",
      "224:\tlearn: 0.0855347\ttotal: 2.74s\tremaining: 9.45s\n",
      "225:\tlearn: 0.0854618\ttotal: 2.76s\tremaining: 9.44s\n",
      "226:\tlearn: 0.0854033\ttotal: 2.77s\tremaining: 9.44s\n",
      "227:\tlearn: 0.0852473\ttotal: 2.78s\tremaining: 9.42s\n",
      "228:\tlearn: 0.0851286\ttotal: 2.81s\tremaining: 9.46s\n",
      "229:\tlearn: 0.0850746\ttotal: 2.83s\tremaining: 9.46s\n",
      "230:\tlearn: 0.0850255\ttotal: 2.85s\tremaining: 9.49s\n",
      "231:\tlearn: 0.0849665\ttotal: 2.87s\tremaining: 9.5s\n",
      "232:\tlearn: 0.0848000\ttotal: 2.89s\tremaining: 9.53s\n",
      "233:\tlearn: 0.0847438\ttotal: 2.91s\tremaining: 9.53s\n",
      "234:\tlearn: 0.0847119\ttotal: 2.92s\tremaining: 9.51s\n",
      "235:\tlearn: 0.0846026\ttotal: 2.93s\tremaining: 9.48s\n",
      "236:\tlearn: 0.0844918\ttotal: 2.94s\tremaining: 9.46s\n",
      "237:\tlearn: 0.0844405\ttotal: 2.95s\tremaining: 9.44s\n",
      "238:\tlearn: 0.0843567\ttotal: 2.96s\tremaining: 9.43s\n",
      "239:\tlearn: 0.0843141\ttotal: 2.97s\tremaining: 9.41s\n",
      "240:\tlearn: 0.0842680\ttotal: 2.98s\tremaining: 9.39s\n",
      "241:\tlearn: 0.0842173\ttotal: 2.99s\tremaining: 9.36s\n",
      "242:\tlearn: 0.0841877\ttotal: 3s\tremaining: 9.35s\n",
      "243:\tlearn: 0.0841519\ttotal: 3.01s\tremaining: 9.33s\n",
      "244:\tlearn: 0.0841119\ttotal: 3.03s\tremaining: 9.33s\n",
      "245:\tlearn: 0.0840528\ttotal: 3.04s\tremaining: 9.31s\n",
      "246:\tlearn: 0.0840073\ttotal: 3.05s\tremaining: 9.29s\n",
      "247:\tlearn: 0.0839284\ttotal: 3.06s\tremaining: 9.27s\n",
      "248:\tlearn: 0.0838513\ttotal: 3.07s\tremaining: 9.25s\n",
      "249:\tlearn: 0.0837744\ttotal: 3.08s\tremaining: 9.23s\n",
      "250:\tlearn: 0.0837399\ttotal: 3.09s\tremaining: 9.22s\n",
      "251:\tlearn: 0.0836740\ttotal: 3.1s\tremaining: 9.21s\n",
      "252:\tlearn: 0.0836341\ttotal: 3.12s\tremaining: 9.2s\n",
      "253:\tlearn: 0.0835917\ttotal: 3.13s\tremaining: 9.18s\n",
      "254:\tlearn: 0.0835182\ttotal: 3.13s\tremaining: 9.16s\n",
      "255:\tlearn: 0.0834787\ttotal: 3.15s\tremaining: 9.16s\n",
      "256:\tlearn: 0.0834026\ttotal: 3.16s\tremaining: 9.14s\n",
      "257:\tlearn: 0.0833617\ttotal: 3.17s\tremaining: 9.11s\n",
      "258:\tlearn: 0.0833021\ttotal: 3.18s\tremaining: 9.11s\n",
      "259:\tlearn: 0.0832570\ttotal: 3.19s\tremaining: 9.09s\n",
      "260:\tlearn: 0.0832226\ttotal: 3.2s\tremaining: 9.07s\n",
      "261:\tlearn: 0.0831861\ttotal: 3.21s\tremaining: 9.05s\n",
      "262:\tlearn: 0.0831557\ttotal: 3.23s\tremaining: 9.04s\n",
      "263:\tlearn: 0.0831166\ttotal: 3.24s\tremaining: 9.03s\n",
      "264:\tlearn: 0.0830689\ttotal: 3.25s\tremaining: 9.01s\n",
      "265:\tlearn: 0.0830392\ttotal: 3.26s\tremaining: 8.99s\n",
      "266:\tlearn: 0.0829721\ttotal: 3.27s\tremaining: 8.96s\n",
      "267:\tlearn: 0.0829492\ttotal: 3.28s\tremaining: 8.95s\n",
      "268:\tlearn: 0.0828994\ttotal: 3.28s\tremaining: 8.93s\n",
      "269:\tlearn: 0.0828130\ttotal: 3.3s\tremaining: 8.91s\n",
      "270:\tlearn: 0.0827399\ttotal: 3.31s\tremaining: 8.9s\n",
      "271:\tlearn: 0.0826594\ttotal: 3.32s\tremaining: 8.88s\n",
      "272:\tlearn: 0.0826207\ttotal: 3.33s\tremaining: 8.86s\n",
      "273:\tlearn: 0.0825738\ttotal: 3.34s\tremaining: 8.86s\n",
      "274:\tlearn: 0.0825174\ttotal: 3.35s\tremaining: 8.84s\n",
      "275:\tlearn: 0.0824860\ttotal: 3.37s\tremaining: 8.83s\n",
      "276:\tlearn: 0.0824318\ttotal: 3.38s\tremaining: 8.82s\n",
      "277:\tlearn: 0.0824074\ttotal: 3.38s\tremaining: 8.79s\n",
      "278:\tlearn: 0.0823614\ttotal: 3.4s\tremaining: 8.78s\n",
      "279:\tlearn: 0.0822699\ttotal: 3.41s\tremaining: 8.77s\n",
      "280:\tlearn: 0.0822396\ttotal: 3.42s\tremaining: 8.75s\n",
      "281:\tlearn: 0.0822221\ttotal: 3.43s\tremaining: 8.73s\n",
      "282:\tlearn: 0.0821618\ttotal: 3.44s\tremaining: 8.73s\n",
      "283:\tlearn: 0.0821381\ttotal: 3.46s\tremaining: 8.73s\n",
      "284:\tlearn: 0.0820404\ttotal: 3.47s\tremaining: 8.71s\n",
      "285:\tlearn: 0.0820027\ttotal: 3.48s\tremaining: 8.69s\n",
      "286:\tlearn: 0.0819676\ttotal: 3.49s\tremaining: 8.67s\n",
      "287:\tlearn: 0.0819285\ttotal: 3.51s\tremaining: 8.67s\n",
      "288:\tlearn: 0.0818691\ttotal: 3.52s\tremaining: 8.65s\n",
      "289:\tlearn: 0.0818249\ttotal: 3.53s\tremaining: 8.63s\n",
      "290:\tlearn: 0.0817956\ttotal: 3.54s\tremaining: 8.62s\n",
      "291:\tlearn: 0.0817501\ttotal: 3.55s\tremaining: 8.6s\n",
      "292:\tlearn: 0.0817033\ttotal: 3.56s\tremaining: 8.58s\n",
      "293:\tlearn: 0.0816664\ttotal: 3.57s\tremaining: 8.58s\n",
      "294:\tlearn: 0.0814731\ttotal: 3.58s\tremaining: 8.56s\n",
      "295:\tlearn: 0.0814418\ttotal: 3.6s\tremaining: 8.55s\n",
      "296:\tlearn: 0.0813688\ttotal: 3.6s\tremaining: 8.53s\n",
      "297:\tlearn: 0.0813141\ttotal: 3.61s\tremaining: 8.51s\n",
      "298:\tlearn: 0.0812709\ttotal: 3.62s\tremaining: 8.5s\n",
      "299:\tlearn: 0.0812275\ttotal: 3.63s\tremaining: 8.47s\n",
      "300:\tlearn: 0.0811791\ttotal: 3.64s\tremaining: 8.46s\n",
      "301:\tlearn: 0.0811422\ttotal: 3.65s\tremaining: 8.45s\n",
      "302:\tlearn: 0.0810924\ttotal: 3.67s\tremaining: 8.44s\n",
      "303:\tlearn: 0.0810312\ttotal: 3.68s\tremaining: 8.42s\n",
      "304:\tlearn: 0.0809895\ttotal: 3.69s\tremaining: 8.41s\n",
      "305:\tlearn: 0.0809615\ttotal: 3.7s\tremaining: 8.4s\n",
      "306:\tlearn: 0.0809199\ttotal: 3.71s\tremaining: 8.39s\n",
      "307:\tlearn: 0.0808768\ttotal: 3.73s\tremaining: 8.37s\n",
      "308:\tlearn: 0.0808394\ttotal: 3.74s\tremaining: 8.36s\n",
      "309:\tlearn: 0.0807955\ttotal: 3.75s\tremaining: 8.35s\n",
      "310:\tlearn: 0.0807435\ttotal: 3.76s\tremaining: 8.33s\n",
      "311:\tlearn: 0.0807044\ttotal: 3.77s\tremaining: 8.32s\n",
      "312:\tlearn: 0.0806760\ttotal: 3.78s\tremaining: 8.3s\n",
      "313:\tlearn: 0.0806434\ttotal: 3.79s\tremaining: 8.28s\n",
      "314:\tlearn: 0.0806170\ttotal: 3.81s\tremaining: 8.29s\n",
      "315:\tlearn: 0.0805828\ttotal: 3.82s\tremaining: 8.28s\n",
      "316:\tlearn: 0.0804695\ttotal: 3.85s\tremaining: 8.3s\n",
      "317:\tlearn: 0.0804167\ttotal: 3.87s\tremaining: 8.29s\n",
      "318:\tlearn: 0.0803679\ttotal: 3.88s\tremaining: 8.29s\n",
      "319:\tlearn: 0.0803325\ttotal: 3.91s\tremaining: 8.3s\n",
      "320:\tlearn: 0.0803145\ttotal: 3.92s\tremaining: 8.29s\n",
      "321:\tlearn: 0.0802351\ttotal: 3.93s\tremaining: 8.27s\n",
      "322:\tlearn: 0.0802076\ttotal: 3.94s\tremaining: 8.26s\n",
      "323:\tlearn: 0.0801921\ttotal: 3.95s\tremaining: 8.24s\n",
      "324:\tlearn: 0.0801557\ttotal: 3.96s\tremaining: 8.22s\n",
      "325:\tlearn: 0.0801219\ttotal: 3.97s\tremaining: 8.2s\n",
      "326:\tlearn: 0.0800884\ttotal: 3.98s\tremaining: 8.19s\n",
      "327:\tlearn: 0.0800433\ttotal: 3.99s\tremaining: 8.18s\n",
      "328:\tlearn: 0.0800082\ttotal: 4s\tremaining: 8.16s\n",
      "329:\tlearn: 0.0799819\ttotal: 4.01s\tremaining: 8.14s\n",
      "330:\tlearn: 0.0799634\ttotal: 4.02s\tremaining: 8.13s\n",
      "331:\tlearn: 0.0799188\ttotal: 4.03s\tremaining: 8.11s\n",
      "332:\tlearn: 0.0798917\ttotal: 4.04s\tremaining: 8.09s\n",
      "333:\tlearn: 0.0798442\ttotal: 4.05s\tremaining: 8.08s\n",
      "334:\tlearn: 0.0798156\ttotal: 4.06s\tremaining: 8.07s\n",
      "335:\tlearn: 0.0797925\ttotal: 4.07s\tremaining: 8.05s\n",
      "336:\tlearn: 0.0797484\ttotal: 4.09s\tremaining: 8.04s\n",
      "337:\tlearn: 0.0797195\ttotal: 4.1s\tremaining: 8.02s\n",
      "338:\tlearn: 0.0796640\ttotal: 4.11s\tremaining: 8.01s\n",
      "339:\tlearn: 0.0796377\ttotal: 4.12s\tremaining: 8s\n",
      "340:\tlearn: 0.0795808\ttotal: 4.13s\tremaining: 7.98s\n",
      "341:\tlearn: 0.0795544\ttotal: 4.14s\tremaining: 7.96s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342:\tlearn: 0.0795202\ttotal: 4.15s\tremaining: 7.95s\n",
      "343:\tlearn: 0.0794672\ttotal: 4.17s\tremaining: 7.94s\n",
      "344:\tlearn: 0.0794369\ttotal: 4.18s\tremaining: 7.93s\n",
      "345:\tlearn: 0.0793965\ttotal: 4.19s\tremaining: 7.92s\n",
      "346:\tlearn: 0.0793633\ttotal: 4.2s\tremaining: 7.91s\n",
      "347:\tlearn: 0.0793408\ttotal: 4.21s\tremaining: 7.9s\n",
      "348:\tlearn: 0.0792792\ttotal: 4.23s\tremaining: 7.88s\n",
      "349:\tlearn: 0.0792514\ttotal: 4.24s\tremaining: 7.87s\n",
      "350:\tlearn: 0.0792337\ttotal: 4.25s\tremaining: 7.85s\n",
      "351:\tlearn: 0.0792146\ttotal: 4.26s\tremaining: 7.84s\n",
      "352:\tlearn: 0.0791924\ttotal: 4.27s\tremaining: 7.83s\n",
      "353:\tlearn: 0.0791488\ttotal: 4.28s\tremaining: 7.82s\n",
      "354:\tlearn: 0.0791194\ttotal: 4.29s\tremaining: 7.8s\n",
      "355:\tlearn: 0.0790848\ttotal: 4.3s\tremaining: 7.78s\n",
      "356:\tlearn: 0.0790523\ttotal: 4.31s\tremaining: 7.77s\n",
      "357:\tlearn: 0.0790330\ttotal: 4.32s\tremaining: 7.75s\n",
      "358:\tlearn: 0.0789821\ttotal: 4.34s\tremaining: 7.74s\n",
      "359:\tlearn: 0.0789609\ttotal: 4.34s\tremaining: 7.72s\n",
      "360:\tlearn: 0.0789340\ttotal: 4.36s\tremaining: 7.71s\n",
      "361:\tlearn: 0.0789060\ttotal: 4.37s\tremaining: 7.69s\n",
      "362:\tlearn: 0.0788654\ttotal: 4.38s\tremaining: 7.68s\n",
      "363:\tlearn: 0.0787992\ttotal: 4.39s\tremaining: 7.67s\n",
      "364:\tlearn: 0.0787745\ttotal: 4.39s\tremaining: 7.65s\n",
      "365:\tlearn: 0.0787457\ttotal: 4.41s\tremaining: 7.64s\n",
      "366:\tlearn: 0.0787231\ttotal: 4.42s\tremaining: 7.62s\n",
      "367:\tlearn: 0.0786601\ttotal: 4.43s\tremaining: 7.61s\n",
      "368:\tlearn: 0.0786396\ttotal: 4.44s\tremaining: 7.59s\n",
      "369:\tlearn: 0.0785949\ttotal: 4.46s\tremaining: 7.58s\n",
      "370:\tlearn: 0.0785840\ttotal: 4.48s\tremaining: 7.6s\n",
      "371:\tlearn: 0.0785557\ttotal: 4.5s\tremaining: 7.59s\n",
      "372:\tlearn: 0.0785350\ttotal: 4.5s\tremaining: 7.57s\n",
      "373:\tlearn: 0.0785159\ttotal: 4.52s\tremaining: 7.56s\n",
      "374:\tlearn: 0.0784654\ttotal: 4.53s\tremaining: 7.54s\n",
      "375:\tlearn: 0.0784415\ttotal: 4.54s\tremaining: 7.53s\n",
      "376:\tlearn: 0.0784205\ttotal: 4.54s\tremaining: 7.51s\n",
      "377:\tlearn: 0.0783787\ttotal: 4.56s\tremaining: 7.5s\n",
      "378:\tlearn: 0.0782839\ttotal: 4.57s\tremaining: 7.48s\n",
      "379:\tlearn: 0.0782557\ttotal: 4.58s\tremaining: 7.47s\n",
      "380:\tlearn: 0.0782030\ttotal: 4.59s\tremaining: 7.45s\n",
      "381:\tlearn: 0.0781343\ttotal: 4.6s\tremaining: 7.44s\n",
      "382:\tlearn: 0.0781018\ttotal: 4.62s\tremaining: 7.45s\n",
      "383:\tlearn: 0.0780701\ttotal: 4.63s\tremaining: 7.43s\n",
      "384:\tlearn: 0.0780358\ttotal: 4.65s\tremaining: 7.42s\n",
      "385:\tlearn: 0.0780141\ttotal: 4.66s\tremaining: 7.41s\n",
      "386:\tlearn: 0.0779839\ttotal: 4.67s\tremaining: 7.4s\n",
      "387:\tlearn: 0.0779582\ttotal: 4.68s\tremaining: 7.38s\n",
      "388:\tlearn: 0.0779287\ttotal: 4.69s\tremaining: 7.37s\n",
      "389:\tlearn: 0.0779026\ttotal: 4.7s\tremaining: 7.35s\n",
      "390:\tlearn: 0.0778694\ttotal: 4.71s\tremaining: 7.34s\n",
      "391:\tlearn: 0.0778378\ttotal: 4.72s\tremaining: 7.33s\n",
      "392:\tlearn: 0.0778025\ttotal: 4.74s\tremaining: 7.32s\n",
      "393:\tlearn: 0.0777016\ttotal: 4.75s\tremaining: 7.31s\n",
      "394:\tlearn: 0.0776455\ttotal: 4.76s\tremaining: 7.29s\n",
      "395:\tlearn: 0.0776141\ttotal: 4.78s\tremaining: 7.29s\n",
      "396:\tlearn: 0.0775574\ttotal: 4.79s\tremaining: 7.27s\n",
      "397:\tlearn: 0.0775147\ttotal: 4.8s\tremaining: 7.26s\n",
      "398:\tlearn: 0.0774626\ttotal: 4.81s\tremaining: 7.25s\n",
      "399:\tlearn: 0.0774448\ttotal: 4.83s\tremaining: 7.25s\n",
      "400:\tlearn: 0.0774332\ttotal: 4.85s\tremaining: 7.24s\n",
      "401:\tlearn: 0.0774152\ttotal: 4.87s\tremaining: 7.24s\n",
      "402:\tlearn: 0.0773628\ttotal: 4.89s\tremaining: 7.24s\n",
      "403:\tlearn: 0.0773274\ttotal: 4.9s\tremaining: 7.24s\n",
      "404:\tlearn: 0.0773072\ttotal: 4.93s\tremaining: 7.24s\n",
      "405:\tlearn: 0.0772559\ttotal: 4.94s\tremaining: 7.23s\n",
      "406:\tlearn: 0.0772211\ttotal: 4.95s\tremaining: 7.21s\n",
      "407:\tlearn: 0.0772011\ttotal: 4.96s\tremaining: 7.2s\n",
      "408:\tlearn: 0.0771478\ttotal: 4.97s\tremaining: 7.19s\n",
      "409:\tlearn: 0.0771279\ttotal: 4.99s\tremaining: 7.17s\n",
      "410:\tlearn: 0.0770977\ttotal: 5s\tremaining: 7.16s\n",
      "411:\tlearn: 0.0770264\ttotal: 5s\tremaining: 7.14s\n",
      "412:\tlearn: 0.0770023\ttotal: 5.02s\tremaining: 7.13s\n",
      "413:\tlearn: 0.0769808\ttotal: 5.03s\tremaining: 7.12s\n",
      "414:\tlearn: 0.0769662\ttotal: 5.04s\tremaining: 7.1s\n",
      "415:\tlearn: 0.0769312\ttotal: 5.05s\tremaining: 7.09s\n",
      "416:\tlearn: 0.0768668\ttotal: 5.05s\tremaining: 7.07s\n",
      "417:\tlearn: 0.0768423\ttotal: 5.07s\tremaining: 7.05s\n",
      "418:\tlearn: 0.0767976\ttotal: 5.08s\tremaining: 7.04s\n",
      "419:\tlearn: 0.0767819\ttotal: 5.09s\tremaining: 7.02s\n",
      "420:\tlearn: 0.0767454\ttotal: 5.09s\tremaining: 7.01s\n",
      "421:\tlearn: 0.0767152\ttotal: 5.11s\tremaining: 7s\n",
      "422:\tlearn: 0.0766943\ttotal: 5.12s\tremaining: 6.98s\n",
      "423:\tlearn: 0.0766588\ttotal: 5.13s\tremaining: 6.97s\n",
      "424:\tlearn: 0.0766082\ttotal: 5.14s\tremaining: 6.96s\n",
      "425:\tlearn: 0.0765361\ttotal: 5.15s\tremaining: 6.94s\n",
      "426:\tlearn: 0.0765157\ttotal: 5.16s\tremaining: 6.93s\n",
      "427:\tlearn: 0.0764781\ttotal: 5.17s\tremaining: 6.91s\n",
      "428:\tlearn: 0.0764560\ttotal: 5.18s\tremaining: 6.9s\n",
      "429:\tlearn: 0.0764347\ttotal: 5.19s\tremaining: 6.88s\n",
      "430:\tlearn: 0.0764110\ttotal: 5.2s\tremaining: 6.87s\n",
      "431:\tlearn: 0.0763955\ttotal: 5.21s\tremaining: 6.86s\n",
      "432:\tlearn: 0.0763666\ttotal: 5.22s\tremaining: 6.84s\n",
      "433:\tlearn: 0.0763303\ttotal: 5.24s\tremaining: 6.83s\n",
      "434:\tlearn: 0.0763049\ttotal: 5.25s\tremaining: 6.82s\n",
      "435:\tlearn: 0.0762791\ttotal: 5.26s\tremaining: 6.8s\n",
      "436:\tlearn: 0.0762530\ttotal: 5.27s\tremaining: 6.79s\n",
      "437:\tlearn: 0.0762099\ttotal: 5.28s\tremaining: 6.78s\n",
      "438:\tlearn: 0.0761865\ttotal: 5.29s\tremaining: 6.76s\n",
      "439:\tlearn: 0.0761354\ttotal: 5.3s\tremaining: 6.75s\n",
      "440:\tlearn: 0.0761036\ttotal: 5.31s\tremaining: 6.73s\n",
      "441:\tlearn: 0.0760696\ttotal: 5.32s\tremaining: 6.72s\n",
      "442:\tlearn: 0.0760471\ttotal: 5.33s\tremaining: 6.7s\n",
      "443:\tlearn: 0.0760207\ttotal: 5.34s\tremaining: 6.69s\n",
      "444:\tlearn: 0.0759994\ttotal: 5.35s\tremaining: 6.68s\n",
      "445:\tlearn: 0.0759838\ttotal: 5.36s\tremaining: 6.66s\n",
      "446:\tlearn: 0.0759596\ttotal: 5.37s\tremaining: 6.64s\n",
      "447:\tlearn: 0.0759411\ttotal: 5.38s\tremaining: 6.63s\n",
      "448:\tlearn: 0.0758988\ttotal: 5.39s\tremaining: 6.62s\n",
      "449:\tlearn: 0.0758763\ttotal: 5.4s\tremaining: 6.6s\n",
      "450:\tlearn: 0.0758489\ttotal: 5.42s\tremaining: 6.59s\n",
      "451:\tlearn: 0.0758204\ttotal: 5.43s\tremaining: 6.58s\n",
      "452:\tlearn: 0.0757984\ttotal: 5.44s\tremaining: 6.56s\n",
      "453:\tlearn: 0.0757840\ttotal: 5.45s\tremaining: 6.55s\n",
      "454:\tlearn: 0.0757602\ttotal: 5.46s\tremaining: 6.54s\n",
      "455:\tlearn: 0.0757137\ttotal: 5.47s\tremaining: 6.53s\n",
      "456:\tlearn: 0.0756998\ttotal: 5.48s\tremaining: 6.51s\n",
      "457:\tlearn: 0.0756673\ttotal: 5.49s\tremaining: 6.5s\n",
      "458:\tlearn: 0.0756400\ttotal: 5.5s\tremaining: 6.49s\n",
      "459:\tlearn: 0.0756236\ttotal: 5.51s\tremaining: 6.47s\n",
      "460:\tlearn: 0.0756053\ttotal: 5.52s\tremaining: 6.45s\n",
      "461:\tlearn: 0.0755749\ttotal: 5.53s\tremaining: 6.44s\n",
      "462:\tlearn: 0.0755401\ttotal: 5.54s\tremaining: 6.43s\n",
      "463:\tlearn: 0.0755065\ttotal: 5.56s\tremaining: 6.42s\n",
      "464:\tlearn: 0.0754886\ttotal: 5.56s\tremaining: 6.4s\n",
      "465:\tlearn: 0.0754459\ttotal: 5.57s\tremaining: 6.39s\n",
      "466:\tlearn: 0.0754382\ttotal: 5.58s\tremaining: 6.37s\n",
      "467:\tlearn: 0.0754116\ttotal: 5.59s\tremaining: 6.36s\n",
      "468:\tlearn: 0.0753930\ttotal: 5.6s\tremaining: 6.34s\n",
      "469:\tlearn: 0.0753739\ttotal: 5.61s\tremaining: 6.33s\n",
      "470:\tlearn: 0.0753529\ttotal: 5.62s\tremaining: 6.31s\n",
      "471:\tlearn: 0.0753325\ttotal: 5.63s\tremaining: 6.3s\n",
      "472:\tlearn: 0.0753063\ttotal: 5.64s\tremaining: 6.28s\n",
      "473:\tlearn: 0.0752744\ttotal: 5.65s\tremaining: 6.27s\n",
      "474:\tlearn: 0.0752626\ttotal: 5.67s\tremaining: 6.26s\n",
      "475:\tlearn: 0.0752038\ttotal: 5.67s\tremaining: 6.25s\n",
      "476:\tlearn: 0.0751804\ttotal: 5.68s\tremaining: 6.23s\n",
      "477:\tlearn: 0.0751617\ttotal: 5.69s\tremaining: 6.22s\n",
      "478:\tlearn: 0.0751333\ttotal: 5.7s\tremaining: 6.2s\n",
      "479:\tlearn: 0.0750877\ttotal: 5.71s\tremaining: 6.19s\n",
      "480:\tlearn: 0.0750639\ttotal: 5.72s\tremaining: 6.17s\n",
      "481:\tlearn: 0.0750263\ttotal: 5.73s\tremaining: 6.16s\n",
      "482:\tlearn: 0.0749838\ttotal: 5.74s\tremaining: 6.15s\n",
      "483:\tlearn: 0.0748878\ttotal: 5.75s\tremaining: 6.13s\n",
      "484:\tlearn: 0.0748789\ttotal: 5.76s\tremaining: 6.12s\n",
      "485:\tlearn: 0.0748650\ttotal: 5.77s\tremaining: 6.1s\n",
      "486:\tlearn: 0.0748309\ttotal: 5.78s\tremaining: 6.09s\n",
      "487:\tlearn: 0.0747908\ttotal: 5.79s\tremaining: 6.08s\n",
      "488:\tlearn: 0.0747624\ttotal: 5.8s\tremaining: 6.06s\n",
      "489:\tlearn: 0.0747478\ttotal: 5.81s\tremaining: 6.05s\n",
      "490:\tlearn: 0.0747349\ttotal: 5.82s\tremaining: 6.04s\n",
      "491:\tlearn: 0.0747206\ttotal: 5.85s\tremaining: 6.04s\n",
      "492:\tlearn: 0.0747068\ttotal: 5.87s\tremaining: 6.03s\n",
      "493:\tlearn: 0.0746817\ttotal: 5.88s\tremaining: 6.03s\n",
      "494:\tlearn: 0.0746656\ttotal: 5.91s\tremaining: 6.03s\n",
      "495:\tlearn: 0.0746404\ttotal: 5.93s\tremaining: 6.03s\n",
      "496:\tlearn: 0.0746209\ttotal: 5.95s\tremaining: 6.02s\n",
      "497:\tlearn: 0.0745844\ttotal: 5.96s\tremaining: 6.01s\n",
      "498:\tlearn: 0.0745582\ttotal: 5.97s\tremaining: 6s\n",
      "499:\tlearn: 0.0745180\ttotal: 5.98s\tremaining: 5.98s\n",
      "500:\tlearn: 0.0744901\ttotal: 5.99s\tremaining: 5.97s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501:\tlearn: 0.0744698\ttotal: 6.01s\tremaining: 5.96s\n",
      "502:\tlearn: 0.0744584\ttotal: 6.02s\tremaining: 5.95s\n",
      "503:\tlearn: 0.0744109\ttotal: 6.03s\tremaining: 5.93s\n",
      "504:\tlearn: 0.0743749\ttotal: 6.04s\tremaining: 5.92s\n",
      "505:\tlearn: 0.0743469\ttotal: 6.05s\tremaining: 5.91s\n",
      "506:\tlearn: 0.0743282\ttotal: 6.06s\tremaining: 5.89s\n",
      "507:\tlearn: 0.0742876\ttotal: 6.07s\tremaining: 5.88s\n",
      "508:\tlearn: 0.0742371\ttotal: 6.08s\tremaining: 5.87s\n",
      "509:\tlearn: 0.0742175\ttotal: 6.09s\tremaining: 5.86s\n",
      "510:\tlearn: 0.0741979\ttotal: 6.11s\tremaining: 5.85s\n",
      "511:\tlearn: 0.0741654\ttotal: 6.12s\tremaining: 5.84s\n",
      "512:\tlearn: 0.0741383\ttotal: 6.13s\tremaining: 5.82s\n",
      "513:\tlearn: 0.0741132\ttotal: 6.14s\tremaining: 5.81s\n",
      "514:\tlearn: 0.0740981\ttotal: 6.15s\tremaining: 5.79s\n",
      "515:\tlearn: 0.0740656\ttotal: 6.17s\tremaining: 5.79s\n",
      "516:\tlearn: 0.0740379\ttotal: 6.18s\tremaining: 5.77s\n",
      "517:\tlearn: 0.0739961\ttotal: 6.19s\tremaining: 5.76s\n",
      "518:\tlearn: 0.0739789\ttotal: 6.2s\tremaining: 5.75s\n",
      "519:\tlearn: 0.0739516\ttotal: 6.21s\tremaining: 5.73s\n",
      "520:\tlearn: 0.0739067\ttotal: 6.22s\tremaining: 5.72s\n",
      "521:\tlearn: 0.0738865\ttotal: 6.23s\tremaining: 5.7s\n",
      "522:\tlearn: 0.0738414\ttotal: 6.24s\tremaining: 5.69s\n",
      "523:\tlearn: 0.0738119\ttotal: 6.25s\tremaining: 5.68s\n",
      "524:\tlearn: 0.0738041\ttotal: 6.26s\tremaining: 5.67s\n",
      "525:\tlearn: 0.0737689\ttotal: 6.27s\tremaining: 5.65s\n",
      "526:\tlearn: 0.0737252\ttotal: 6.28s\tremaining: 5.64s\n",
      "527:\tlearn: 0.0736889\ttotal: 6.29s\tremaining: 5.63s\n",
      "528:\tlearn: 0.0736721\ttotal: 6.3s\tremaining: 5.61s\n",
      "529:\tlearn: 0.0736572\ttotal: 6.32s\tremaining: 5.6s\n",
      "530:\tlearn: 0.0736448\ttotal: 6.33s\tremaining: 5.59s\n",
      "531:\tlearn: 0.0736244\ttotal: 6.33s\tremaining: 5.57s\n",
      "532:\tlearn: 0.0736054\ttotal: 6.35s\tremaining: 5.56s\n",
      "533:\tlearn: 0.0735646\ttotal: 6.36s\tremaining: 5.55s\n",
      "534:\tlearn: 0.0735390\ttotal: 6.37s\tremaining: 5.54s\n",
      "535:\tlearn: 0.0735242\ttotal: 6.38s\tremaining: 5.53s\n",
      "536:\tlearn: 0.0734885\ttotal: 6.39s\tremaining: 5.51s\n",
      "537:\tlearn: 0.0734644\ttotal: 6.41s\tremaining: 5.5s\n",
      "538:\tlearn: 0.0734433\ttotal: 6.41s\tremaining: 5.49s\n",
      "539:\tlearn: 0.0734061\ttotal: 6.44s\tremaining: 5.49s\n",
      "540:\tlearn: 0.0733882\ttotal: 6.45s\tremaining: 5.47s\n",
      "541:\tlearn: 0.0733486\ttotal: 6.46s\tremaining: 5.46s\n",
      "542:\tlearn: 0.0733213\ttotal: 6.48s\tremaining: 5.46s\n",
      "543:\tlearn: 0.0732965\ttotal: 6.49s\tremaining: 5.44s\n",
      "544:\tlearn: 0.0732679\ttotal: 6.51s\tremaining: 5.43s\n",
      "545:\tlearn: 0.0732483\ttotal: 6.52s\tremaining: 5.42s\n",
      "546:\tlearn: 0.0732173\ttotal: 6.53s\tremaining: 5.41s\n",
      "547:\tlearn: 0.0731973\ttotal: 6.54s\tremaining: 5.39s\n",
      "548:\tlearn: 0.0731640\ttotal: 6.55s\tremaining: 5.38s\n",
      "549:\tlearn: 0.0731268\ttotal: 6.56s\tremaining: 5.37s\n",
      "550:\tlearn: 0.0731035\ttotal: 6.57s\tremaining: 5.36s\n",
      "551:\tlearn: 0.0730859\ttotal: 6.58s\tremaining: 5.34s\n",
      "552:\tlearn: 0.0730728\ttotal: 6.6s\tremaining: 5.34s\n",
      "553:\tlearn: 0.0730364\ttotal: 6.61s\tremaining: 5.32s\n",
      "554:\tlearn: 0.0730129\ttotal: 6.62s\tremaining: 5.31s\n",
      "555:\tlearn: 0.0729849\ttotal: 6.63s\tremaining: 5.3s\n",
      "556:\tlearn: 0.0729579\ttotal: 6.64s\tremaining: 5.29s\n",
      "557:\tlearn: 0.0729444\ttotal: 6.66s\tremaining: 5.28s\n",
      "558:\tlearn: 0.0729094\ttotal: 6.67s\tremaining: 5.26s\n",
      "559:\tlearn: 0.0728827\ttotal: 6.68s\tremaining: 5.25s\n",
      "560:\tlearn: 0.0728604\ttotal: 6.69s\tremaining: 5.24s\n",
      "561:\tlearn: 0.0728426\ttotal: 6.71s\tremaining: 5.22s\n",
      "562:\tlearn: 0.0728047\ttotal: 6.71s\tremaining: 5.21s\n",
      "563:\tlearn: 0.0727885\ttotal: 6.72s\tremaining: 5.2s\n",
      "564:\tlearn: 0.0727716\ttotal: 6.74s\tremaining: 5.18s\n",
      "565:\tlearn: 0.0727581\ttotal: 6.75s\tremaining: 5.17s\n",
      "566:\tlearn: 0.0727346\ttotal: 6.76s\tremaining: 5.16s\n",
      "567:\tlearn: 0.0727134\ttotal: 6.77s\tremaining: 5.15s\n",
      "568:\tlearn: 0.0726683\ttotal: 6.78s\tremaining: 5.13s\n",
      "569:\tlearn: 0.0726527\ttotal: 6.79s\tremaining: 5.12s\n",
      "570:\tlearn: 0.0726319\ttotal: 6.8s\tremaining: 5.11s\n",
      "571:\tlearn: 0.0725916\ttotal: 6.81s\tremaining: 5.1s\n",
      "572:\tlearn: 0.0725728\ttotal: 6.83s\tremaining: 5.09s\n",
      "573:\tlearn: 0.0725364\ttotal: 6.83s\tremaining: 5.07s\n",
      "574:\tlearn: 0.0725199\ttotal: 6.85s\tremaining: 5.07s\n",
      "575:\tlearn: 0.0725028\ttotal: 6.87s\tremaining: 5.06s\n",
      "576:\tlearn: 0.0724903\ttotal: 6.89s\tremaining: 5.05s\n",
      "577:\tlearn: 0.0724805\ttotal: 6.91s\tremaining: 5.04s\n",
      "578:\tlearn: 0.0724537\ttotal: 6.93s\tremaining: 5.04s\n",
      "579:\tlearn: 0.0724231\ttotal: 6.96s\tremaining: 5.04s\n",
      "580:\tlearn: 0.0724002\ttotal: 6.97s\tremaining: 5.03s\n",
      "581:\tlearn: 0.0723764\ttotal: 6.98s\tremaining: 5.01s\n",
      "582:\tlearn: 0.0723394\ttotal: 6.99s\tremaining: 5s\n",
      "583:\tlearn: 0.0723115\ttotal: 7s\tremaining: 4.99s\n",
      "584:\tlearn: 0.0722778\ttotal: 7.01s\tremaining: 4.97s\n",
      "585:\tlearn: 0.0722570\ttotal: 7.02s\tremaining: 4.96s\n",
      "586:\tlearn: 0.0722140\ttotal: 7.03s\tremaining: 4.95s\n",
      "587:\tlearn: 0.0721843\ttotal: 7.04s\tremaining: 4.93s\n",
      "588:\tlearn: 0.0721660\ttotal: 7.05s\tremaining: 4.92s\n",
      "589:\tlearn: 0.0721447\ttotal: 7.06s\tremaining: 4.91s\n",
      "590:\tlearn: 0.0721058\ttotal: 7.07s\tremaining: 4.89s\n",
      "591:\tlearn: 0.0720810\ttotal: 7.08s\tremaining: 4.88s\n",
      "592:\tlearn: 0.0720596\ttotal: 7.09s\tremaining: 4.87s\n",
      "593:\tlearn: 0.0720290\ttotal: 7.11s\tremaining: 4.86s\n",
      "594:\tlearn: 0.0720167\ttotal: 7.12s\tremaining: 4.85s\n",
      "595:\tlearn: 0.0720016\ttotal: 7.13s\tremaining: 4.83s\n",
      "596:\tlearn: 0.0719767\ttotal: 7.14s\tremaining: 4.82s\n",
      "597:\tlearn: 0.0719535\ttotal: 7.15s\tremaining: 4.81s\n",
      "598:\tlearn: 0.0719389\ttotal: 7.16s\tremaining: 4.8s\n",
      "599:\tlearn: 0.0719269\ttotal: 7.17s\tremaining: 4.78s\n",
      "600:\tlearn: 0.0719146\ttotal: 7.18s\tremaining: 4.77s\n",
      "601:\tlearn: 0.0718831\ttotal: 7.19s\tremaining: 4.76s\n",
      "602:\tlearn: 0.0718516\ttotal: 7.2s\tremaining: 4.74s\n",
      "603:\tlearn: 0.0718354\ttotal: 7.21s\tremaining: 4.73s\n",
      "604:\tlearn: 0.0718172\ttotal: 7.23s\tremaining: 4.72s\n",
      "605:\tlearn: 0.0717815\ttotal: 7.24s\tremaining: 4.7s\n",
      "606:\tlearn: 0.0717567\ttotal: 7.24s\tremaining: 4.69s\n",
      "607:\tlearn: 0.0717287\ttotal: 7.25s\tremaining: 4.68s\n",
      "608:\tlearn: 0.0717023\ttotal: 7.27s\tremaining: 4.67s\n",
      "609:\tlearn: 0.0716774\ttotal: 7.28s\tremaining: 4.65s\n",
      "610:\tlearn: 0.0716551\ttotal: 7.29s\tremaining: 4.64s\n",
      "611:\tlearn: 0.0716343\ttotal: 7.3s\tremaining: 4.63s\n",
      "612:\tlearn: 0.0716146\ttotal: 7.31s\tremaining: 4.62s\n",
      "613:\tlearn: 0.0715965\ttotal: 7.33s\tremaining: 4.61s\n",
      "614:\tlearn: 0.0715767\ttotal: 7.34s\tremaining: 4.59s\n",
      "615:\tlearn: 0.0715696\ttotal: 7.35s\tremaining: 4.58s\n",
      "616:\tlearn: 0.0715520\ttotal: 7.36s\tremaining: 4.57s\n",
      "617:\tlearn: 0.0715403\ttotal: 7.37s\tremaining: 4.55s\n",
      "618:\tlearn: 0.0715150\ttotal: 7.38s\tremaining: 4.54s\n",
      "619:\tlearn: 0.0715006\ttotal: 7.39s\tremaining: 4.53s\n",
      "620:\tlearn: 0.0714538\ttotal: 7.4s\tremaining: 4.51s\n",
      "621:\tlearn: 0.0714237\ttotal: 7.41s\tremaining: 4.5s\n",
      "622:\tlearn: 0.0714048\ttotal: 7.42s\tremaining: 4.49s\n",
      "623:\tlearn: 0.0713660\ttotal: 7.43s\tremaining: 4.48s\n",
      "624:\tlearn: 0.0713501\ttotal: 7.45s\tremaining: 4.47s\n",
      "625:\tlearn: 0.0713333\ttotal: 7.46s\tremaining: 4.46s\n",
      "626:\tlearn: 0.0713076\ttotal: 7.46s\tremaining: 4.44s\n",
      "627:\tlearn: 0.0712892\ttotal: 7.48s\tremaining: 4.43s\n",
      "628:\tlearn: 0.0712728\ttotal: 7.49s\tremaining: 4.42s\n",
      "629:\tlearn: 0.0712514\ttotal: 7.49s\tremaining: 4.4s\n",
      "630:\tlearn: 0.0712414\ttotal: 7.5s\tremaining: 4.39s\n",
      "631:\tlearn: 0.0712198\ttotal: 7.51s\tremaining: 4.37s\n",
      "632:\tlearn: 0.0711946\ttotal: 7.52s\tremaining: 4.36s\n",
      "633:\tlearn: 0.0711720\ttotal: 7.53s\tremaining: 4.35s\n",
      "634:\tlearn: 0.0711469\ttotal: 7.55s\tremaining: 4.34s\n",
      "635:\tlearn: 0.0711345\ttotal: 7.56s\tremaining: 4.33s\n",
      "636:\tlearn: 0.0711024\ttotal: 7.57s\tremaining: 4.31s\n",
      "637:\tlearn: 0.0710855\ttotal: 7.58s\tremaining: 4.3s\n",
      "638:\tlearn: 0.0710720\ttotal: 7.59s\tremaining: 4.29s\n",
      "639:\tlearn: 0.0710690\ttotal: 7.59s\tremaining: 4.27s\n",
      "640:\tlearn: 0.0710524\ttotal: 7.61s\tremaining: 4.26s\n",
      "641:\tlearn: 0.0709950\ttotal: 7.62s\tremaining: 4.25s\n",
      "642:\tlearn: 0.0709673\ttotal: 7.63s\tremaining: 4.24s\n",
      "643:\tlearn: 0.0709511\ttotal: 7.64s\tremaining: 4.22s\n",
      "644:\tlearn: 0.0709384\ttotal: 7.65s\tremaining: 4.21s\n",
      "645:\tlearn: 0.0709116\ttotal: 7.66s\tremaining: 4.2s\n",
      "646:\tlearn: 0.0708709\ttotal: 7.67s\tremaining: 4.19s\n",
      "647:\tlearn: 0.0708574\ttotal: 7.68s\tremaining: 4.17s\n",
      "648:\tlearn: 0.0708275\ttotal: 7.7s\tremaining: 4.16s\n",
      "649:\tlearn: 0.0708158\ttotal: 7.71s\tremaining: 4.15s\n",
      "650:\tlearn: 0.0708008\ttotal: 7.72s\tremaining: 4.14s\n",
      "651:\tlearn: 0.0707885\ttotal: 7.73s\tremaining: 4.13s\n",
      "652:\tlearn: 0.0707725\ttotal: 7.74s\tremaining: 4.11s\n",
      "653:\tlearn: 0.0707499\ttotal: 7.75s\tremaining: 4.1s\n",
      "654:\tlearn: 0.0707225\ttotal: 7.76s\tremaining: 4.09s\n",
      "655:\tlearn: 0.0707173\ttotal: 7.77s\tremaining: 4.08s\n",
      "656:\tlearn: 0.0706956\ttotal: 7.78s\tremaining: 4.06s\n",
      "657:\tlearn: 0.0706812\ttotal: 7.79s\tremaining: 4.05s\n",
      "658:\tlearn: 0.0706592\ttotal: 7.8s\tremaining: 4.04s\n",
      "659:\tlearn: 0.0706551\ttotal: 7.81s\tremaining: 4.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660:\tlearn: 0.0706409\ttotal: 7.81s\tremaining: 4.01s\n",
      "661:\tlearn: 0.0706211\ttotal: 7.83s\tremaining: 4s\n",
      "662:\tlearn: 0.0705953\ttotal: 7.84s\tremaining: 3.98s\n",
      "663:\tlearn: 0.0705653\ttotal: 7.85s\tremaining: 3.97s\n",
      "664:\tlearn: 0.0705568\ttotal: 7.87s\tremaining: 3.96s\n",
      "665:\tlearn: 0.0705306\ttotal: 7.88s\tremaining: 3.95s\n",
      "666:\tlearn: 0.0705011\ttotal: 7.9s\tremaining: 3.94s\n",
      "667:\tlearn: 0.0704702\ttotal: 7.93s\tremaining: 3.94s\n",
      "668:\tlearn: 0.0704094\ttotal: 7.95s\tremaining: 3.94s\n",
      "669:\tlearn: 0.0703831\ttotal: 7.97s\tremaining: 3.93s\n",
      "670:\tlearn: 0.0703655\ttotal: 7.98s\tremaining: 3.91s\n",
      "671:\tlearn: 0.0703396\ttotal: 7.99s\tremaining: 3.9s\n",
      "672:\tlearn: 0.0703232\ttotal: 8.01s\tremaining: 3.89s\n",
      "673:\tlearn: 0.0703120\ttotal: 8.01s\tremaining: 3.88s\n",
      "674:\tlearn: 0.0702922\ttotal: 8.03s\tremaining: 3.86s\n",
      "675:\tlearn: 0.0702840\ttotal: 8.03s\tremaining: 3.85s\n",
      "676:\tlearn: 0.0702795\ttotal: 8.04s\tremaining: 3.84s\n",
      "677:\tlearn: 0.0702664\ttotal: 8.05s\tremaining: 3.83s\n",
      "678:\tlearn: 0.0702370\ttotal: 8.06s\tremaining: 3.81s\n",
      "679:\tlearn: 0.0702074\ttotal: 8.08s\tremaining: 3.8s\n",
      "680:\tlearn: 0.0701844\ttotal: 8.09s\tremaining: 3.79s\n",
      "681:\tlearn: 0.0701555\ttotal: 8.1s\tremaining: 3.78s\n",
      "682:\tlearn: 0.0701500\ttotal: 8.11s\tremaining: 3.76s\n",
      "683:\tlearn: 0.0701394\ttotal: 8.12s\tremaining: 3.75s\n",
      "684:\tlearn: 0.0701197\ttotal: 8.13s\tremaining: 3.74s\n",
      "685:\tlearn: 0.0700969\ttotal: 8.14s\tremaining: 3.73s\n",
      "686:\tlearn: 0.0700749\ttotal: 8.15s\tremaining: 3.71s\n",
      "687:\tlearn: 0.0700563\ttotal: 8.16s\tremaining: 3.7s\n",
      "688:\tlearn: 0.0700485\ttotal: 8.17s\tremaining: 3.69s\n",
      "689:\tlearn: 0.0700373\ttotal: 8.18s\tremaining: 3.67s\n",
      "690:\tlearn: 0.0700118\ttotal: 8.19s\tremaining: 3.66s\n",
      "691:\tlearn: 0.0699959\ttotal: 8.2s\tremaining: 3.65s\n",
      "692:\tlearn: 0.0699787\ttotal: 8.21s\tremaining: 3.64s\n",
      "693:\tlearn: 0.0699592\ttotal: 8.22s\tremaining: 3.63s\n",
      "694:\tlearn: 0.0699473\ttotal: 8.23s\tremaining: 3.61s\n",
      "695:\tlearn: 0.0699401\ttotal: 8.25s\tremaining: 3.6s\n",
      "696:\tlearn: 0.0699304\ttotal: 8.27s\tremaining: 3.59s\n",
      "697:\tlearn: 0.0699129\ttotal: 8.29s\tremaining: 3.58s\n",
      "698:\tlearn: 0.0698822\ttotal: 8.3s\tremaining: 3.58s\n",
      "699:\tlearn: 0.0698665\ttotal: 8.32s\tremaining: 3.56s\n",
      "700:\tlearn: 0.0698467\ttotal: 8.34s\tremaining: 3.56s\n",
      "701:\tlearn: 0.0698258\ttotal: 8.36s\tremaining: 3.55s\n",
      "702:\tlearn: 0.0698022\ttotal: 8.37s\tremaining: 3.54s\n",
      "703:\tlearn: 0.0697972\ttotal: 8.38s\tremaining: 3.52s\n",
      "704:\tlearn: 0.0697747\ttotal: 8.4s\tremaining: 3.51s\n",
      "705:\tlearn: 0.0697430\ttotal: 8.4s\tremaining: 3.5s\n",
      "706:\tlearn: 0.0697211\ttotal: 8.42s\tremaining: 3.49s\n",
      "707:\tlearn: 0.0697059\ttotal: 8.42s\tremaining: 3.47s\n",
      "708:\tlearn: 0.0696830\ttotal: 8.44s\tremaining: 3.46s\n",
      "709:\tlearn: 0.0696586\ttotal: 8.45s\tremaining: 3.45s\n",
      "710:\tlearn: 0.0696436\ttotal: 8.47s\tremaining: 3.44s\n",
      "711:\tlearn: 0.0696245\ttotal: 8.48s\tremaining: 3.43s\n",
      "712:\tlearn: 0.0695944\ttotal: 8.49s\tremaining: 3.42s\n",
      "713:\tlearn: 0.0695719\ttotal: 8.5s\tremaining: 3.4s\n",
      "714:\tlearn: 0.0695618\ttotal: 8.51s\tremaining: 3.39s\n",
      "715:\tlearn: 0.0695392\ttotal: 8.52s\tremaining: 3.38s\n",
      "716:\tlearn: 0.0695064\ttotal: 8.53s\tremaining: 3.37s\n",
      "717:\tlearn: 0.0694879\ttotal: 8.54s\tremaining: 3.35s\n",
      "718:\tlearn: 0.0694522\ttotal: 8.56s\tremaining: 3.35s\n",
      "719:\tlearn: 0.0694467\ttotal: 8.57s\tremaining: 3.33s\n",
      "720:\tlearn: 0.0694163\ttotal: 8.58s\tremaining: 3.32s\n",
      "721:\tlearn: 0.0693879\ttotal: 8.59s\tremaining: 3.31s\n",
      "722:\tlearn: 0.0693624\ttotal: 8.6s\tremaining: 3.3s\n",
      "723:\tlearn: 0.0693485\ttotal: 8.61s\tremaining: 3.28s\n",
      "724:\tlearn: 0.0693387\ttotal: 8.62s\tremaining: 3.27s\n",
      "725:\tlearn: 0.0693337\ttotal: 8.63s\tremaining: 3.26s\n",
      "726:\tlearn: 0.0693164\ttotal: 8.64s\tremaining: 3.25s\n",
      "727:\tlearn: 0.0692886\ttotal: 8.65s\tremaining: 3.23s\n",
      "728:\tlearn: 0.0692657\ttotal: 8.66s\tremaining: 3.22s\n",
      "729:\tlearn: 0.0692508\ttotal: 8.67s\tremaining: 3.21s\n",
      "730:\tlearn: 0.0692325\ttotal: 8.69s\tremaining: 3.2s\n",
      "731:\tlearn: 0.0692184\ttotal: 8.7s\tremaining: 3.18s\n",
      "732:\tlearn: 0.0691997\ttotal: 8.71s\tremaining: 3.17s\n",
      "733:\tlearn: 0.0691818\ttotal: 8.72s\tremaining: 3.16s\n",
      "734:\tlearn: 0.0691634\ttotal: 8.73s\tremaining: 3.15s\n",
      "735:\tlearn: 0.0691471\ttotal: 8.74s\tremaining: 3.13s\n",
      "736:\tlearn: 0.0691272\ttotal: 8.75s\tremaining: 3.12s\n",
      "737:\tlearn: 0.0690955\ttotal: 8.76s\tremaining: 3.11s\n",
      "738:\tlearn: 0.0690744\ttotal: 8.77s\tremaining: 3.1s\n",
      "739:\tlearn: 0.0690435\ttotal: 8.78s\tremaining: 3.08s\n",
      "740:\tlearn: 0.0690288\ttotal: 8.79s\tremaining: 3.07s\n",
      "741:\tlearn: 0.0690134\ttotal: 8.8s\tremaining: 3.06s\n",
      "742:\tlearn: 0.0689896\ttotal: 8.81s\tremaining: 3.05s\n",
      "743:\tlearn: 0.0689789\ttotal: 8.82s\tremaining: 3.04s\n",
      "744:\tlearn: 0.0689608\ttotal: 8.83s\tremaining: 3.02s\n",
      "745:\tlearn: 0.0689471\ttotal: 8.85s\tremaining: 3.01s\n",
      "746:\tlearn: 0.0689242\ttotal: 8.85s\tremaining: 3s\n",
      "747:\tlearn: 0.0689071\ttotal: 8.87s\tremaining: 2.99s\n",
      "748:\tlearn: 0.0688910\ttotal: 8.89s\tremaining: 2.98s\n",
      "749:\tlearn: 0.0688809\ttotal: 8.91s\tremaining: 2.97s\n",
      "750:\tlearn: 0.0688715\ttotal: 8.95s\tremaining: 2.97s\n",
      "751:\tlearn: 0.0688393\ttotal: 9.03s\tremaining: 2.98s\n",
      "752:\tlearn: 0.0688082\ttotal: 9.05s\tremaining: 2.97s\n",
      "753:\tlearn: 0.0687868\ttotal: 9.06s\tremaining: 2.96s\n",
      "754:\tlearn: 0.0687678\ttotal: 9.07s\tremaining: 2.94s\n",
      "755:\tlearn: 0.0687558\ttotal: 9.08s\tremaining: 2.93s\n",
      "756:\tlearn: 0.0687399\ttotal: 9.09s\tremaining: 2.92s\n",
      "757:\tlearn: 0.0687271\ttotal: 9.11s\tremaining: 2.91s\n",
      "758:\tlearn: 0.0687167\ttotal: 9.12s\tremaining: 2.89s\n",
      "759:\tlearn: 0.0687033\ttotal: 9.12s\tremaining: 2.88s\n",
      "760:\tlearn: 0.0686794\ttotal: 9.14s\tremaining: 2.87s\n",
      "761:\tlearn: 0.0686694\ttotal: 9.15s\tremaining: 2.86s\n",
      "762:\tlearn: 0.0686583\ttotal: 9.16s\tremaining: 2.84s\n",
      "763:\tlearn: 0.0686486\ttotal: 9.17s\tremaining: 2.83s\n",
      "764:\tlearn: 0.0686355\ttotal: 9.18s\tremaining: 2.82s\n",
      "765:\tlearn: 0.0686193\ttotal: 9.19s\tremaining: 2.81s\n",
      "766:\tlearn: 0.0686094\ttotal: 9.2s\tremaining: 2.8s\n",
      "767:\tlearn: 0.0685931\ttotal: 9.21s\tremaining: 2.78s\n",
      "768:\tlearn: 0.0685749\ttotal: 9.23s\tremaining: 2.77s\n",
      "769:\tlearn: 0.0685604\ttotal: 9.24s\tremaining: 2.76s\n",
      "770:\tlearn: 0.0685320\ttotal: 9.25s\tremaining: 2.75s\n",
      "771:\tlearn: 0.0685079\ttotal: 9.26s\tremaining: 2.73s\n",
      "772:\tlearn: 0.0684746\ttotal: 9.27s\tremaining: 2.72s\n",
      "773:\tlearn: 0.0684590\ttotal: 9.28s\tremaining: 2.71s\n",
      "774:\tlearn: 0.0684393\ttotal: 9.29s\tremaining: 2.7s\n",
      "775:\tlearn: 0.0684135\ttotal: 9.3s\tremaining: 2.69s\n",
      "776:\tlearn: 0.0683810\ttotal: 9.31s\tremaining: 2.67s\n",
      "777:\tlearn: 0.0683440\ttotal: 9.33s\tremaining: 2.66s\n",
      "778:\tlearn: 0.0683298\ttotal: 9.34s\tremaining: 2.65s\n",
      "779:\tlearn: 0.0683179\ttotal: 9.35s\tremaining: 2.64s\n",
      "780:\tlearn: 0.0683036\ttotal: 9.36s\tremaining: 2.62s\n",
      "781:\tlearn: 0.0682833\ttotal: 9.37s\tremaining: 2.61s\n",
      "782:\tlearn: 0.0682711\ttotal: 9.38s\tremaining: 2.6s\n",
      "783:\tlearn: 0.0682545\ttotal: 9.39s\tremaining: 2.59s\n",
      "784:\tlearn: 0.0682429\ttotal: 9.4s\tremaining: 2.57s\n",
      "785:\tlearn: 0.0682075\ttotal: 9.41s\tremaining: 2.56s\n",
      "786:\tlearn: 0.0681877\ttotal: 9.42s\tremaining: 2.55s\n",
      "787:\tlearn: 0.0681719\ttotal: 9.43s\tremaining: 2.54s\n",
      "788:\tlearn: 0.0681510\ttotal: 9.45s\tremaining: 2.52s\n",
      "789:\tlearn: 0.0681233\ttotal: 9.45s\tremaining: 2.51s\n",
      "790:\tlearn: 0.0680737\ttotal: 9.46s\tremaining: 2.5s\n",
      "791:\tlearn: 0.0680372\ttotal: 9.47s\tremaining: 2.49s\n",
      "792:\tlearn: 0.0679973\ttotal: 9.48s\tremaining: 2.48s\n",
      "793:\tlearn: 0.0679704\ttotal: 9.49s\tremaining: 2.46s\n",
      "794:\tlearn: 0.0679441\ttotal: 9.5s\tremaining: 2.45s\n",
      "795:\tlearn: 0.0679281\ttotal: 9.52s\tremaining: 2.44s\n",
      "796:\tlearn: 0.0679131\ttotal: 9.53s\tremaining: 2.43s\n",
      "797:\tlearn: 0.0678928\ttotal: 9.54s\tremaining: 2.41s\n",
      "798:\tlearn: 0.0678632\ttotal: 9.55s\tremaining: 2.4s\n",
      "799:\tlearn: 0.0678337\ttotal: 9.56s\tremaining: 2.39s\n",
      "800:\tlearn: 0.0678196\ttotal: 9.57s\tremaining: 2.38s\n",
      "801:\tlearn: 0.0678000\ttotal: 9.58s\tremaining: 2.37s\n",
      "802:\tlearn: 0.0677805\ttotal: 9.59s\tremaining: 2.35s\n",
      "803:\tlearn: 0.0677710\ttotal: 9.6s\tremaining: 2.34s\n",
      "804:\tlearn: 0.0677574\ttotal: 9.61s\tremaining: 2.33s\n",
      "805:\tlearn: 0.0677516\ttotal: 9.62s\tremaining: 2.31s\n",
      "806:\tlearn: 0.0677358\ttotal: 9.63s\tremaining: 2.3s\n",
      "807:\tlearn: 0.0677193\ttotal: 9.64s\tremaining: 2.29s\n",
      "808:\tlearn: 0.0676962\ttotal: 9.65s\tremaining: 2.28s\n",
      "809:\tlearn: 0.0676829\ttotal: 9.66s\tremaining: 2.27s\n",
      "810:\tlearn: 0.0676641\ttotal: 9.67s\tremaining: 2.25s\n",
      "811:\tlearn: 0.0676488\ttotal: 9.69s\tremaining: 2.24s\n",
      "812:\tlearn: 0.0676209\ttotal: 9.7s\tremaining: 2.23s\n",
      "813:\tlearn: 0.0676059\ttotal: 9.71s\tremaining: 2.22s\n",
      "814:\tlearn: 0.0675891\ttotal: 9.73s\tremaining: 2.21s\n",
      "815:\tlearn: 0.0675752\ttotal: 9.74s\tremaining: 2.19s\n",
      "816:\tlearn: 0.0675465\ttotal: 9.75s\tremaining: 2.18s\n",
      "817:\tlearn: 0.0675403\ttotal: 9.76s\tremaining: 2.17s\n",
      "818:\tlearn: 0.0675195\ttotal: 9.77s\tremaining: 2.16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819:\tlearn: 0.0675128\ttotal: 9.78s\tremaining: 2.15s\n",
      "820:\tlearn: 0.0674932\ttotal: 9.79s\tremaining: 2.13s\n",
      "821:\tlearn: 0.0674715\ttotal: 9.8s\tremaining: 2.12s\n",
      "822:\tlearn: 0.0674616\ttotal: 9.82s\tremaining: 2.11s\n",
      "823:\tlearn: 0.0674565\ttotal: 9.83s\tremaining: 2.1s\n",
      "824:\tlearn: 0.0674347\ttotal: 9.84s\tremaining: 2.09s\n",
      "825:\tlearn: 0.0674145\ttotal: 9.85s\tremaining: 2.07s\n",
      "826:\tlearn: 0.0673909\ttotal: 9.86s\tremaining: 2.06s\n",
      "827:\tlearn: 0.0673764\ttotal: 9.88s\tremaining: 2.05s\n",
      "828:\tlearn: 0.0673611\ttotal: 9.89s\tremaining: 2.04s\n",
      "829:\tlearn: 0.0673570\ttotal: 9.9s\tremaining: 2.03s\n",
      "830:\tlearn: 0.0673329\ttotal: 9.91s\tremaining: 2.01s\n",
      "831:\tlearn: 0.0673083\ttotal: 9.92s\tremaining: 2s\n",
      "832:\tlearn: 0.0673016\ttotal: 9.94s\tremaining: 1.99s\n",
      "833:\tlearn: 0.0672941\ttotal: 9.96s\tremaining: 1.98s\n",
      "834:\tlearn: 0.0672796\ttotal: 9.98s\tremaining: 1.97s\n",
      "835:\tlearn: 0.0672714\ttotal: 10s\tremaining: 1.96s\n",
      "836:\tlearn: 0.0672619\ttotal: 10s\tremaining: 1.95s\n",
      "837:\tlearn: 0.0672435\ttotal: 10s\tremaining: 1.94s\n",
      "838:\tlearn: 0.0672207\ttotal: 10.1s\tremaining: 1.93s\n",
      "839:\tlearn: 0.0671981\ttotal: 10.1s\tremaining: 1.92s\n",
      "840:\tlearn: 0.0671668\ttotal: 10.1s\tremaining: 1.91s\n",
      "841:\tlearn: 0.0671337\ttotal: 10.1s\tremaining: 1.89s\n",
      "842:\tlearn: 0.0671160\ttotal: 10.1s\tremaining: 1.88s\n",
      "843:\tlearn: 0.0670962\ttotal: 10.1s\tremaining: 1.87s\n",
      "844:\tlearn: 0.0670831\ttotal: 10.1s\tremaining: 1.86s\n",
      "845:\tlearn: 0.0670539\ttotal: 10.1s\tremaining: 1.84s\n",
      "846:\tlearn: 0.0670421\ttotal: 10.1s\tremaining: 1.83s\n",
      "847:\tlearn: 0.0670261\ttotal: 10.2s\tremaining: 1.82s\n",
      "848:\tlearn: 0.0670165\ttotal: 10.2s\tremaining: 1.81s\n",
      "849:\tlearn: 0.0670042\ttotal: 10.2s\tremaining: 1.8s\n",
      "850:\tlearn: 0.0669892\ttotal: 10.2s\tremaining: 1.78s\n",
      "851:\tlearn: 0.0669610\ttotal: 10.2s\tremaining: 1.77s\n",
      "852:\tlearn: 0.0669485\ttotal: 10.2s\tremaining: 1.76s\n",
      "853:\tlearn: 0.0669195\ttotal: 10.2s\tremaining: 1.75s\n",
      "854:\tlearn: 0.0669117\ttotal: 10.2s\tremaining: 1.73s\n",
      "855:\tlearn: 0.0668892\ttotal: 10.2s\tremaining: 1.72s\n",
      "856:\tlearn: 0.0668758\ttotal: 10.2s\tremaining: 1.71s\n",
      "857:\tlearn: 0.0668440\ttotal: 10.3s\tremaining: 1.7s\n",
      "858:\tlearn: 0.0668235\ttotal: 10.3s\tremaining: 1.69s\n",
      "859:\tlearn: 0.0667951\ttotal: 10.3s\tremaining: 1.67s\n",
      "860:\tlearn: 0.0667673\ttotal: 10.3s\tremaining: 1.66s\n",
      "861:\tlearn: 0.0667538\ttotal: 10.3s\tremaining: 1.65s\n",
      "862:\tlearn: 0.0667375\ttotal: 10.3s\tremaining: 1.64s\n",
      "863:\tlearn: 0.0667221\ttotal: 10.3s\tremaining: 1.62s\n",
      "864:\tlearn: 0.0667017\ttotal: 10.3s\tremaining: 1.61s\n",
      "865:\tlearn: 0.0666857\ttotal: 10.3s\tremaining: 1.6s\n",
      "866:\tlearn: 0.0666687\ttotal: 10.4s\tremaining: 1.59s\n",
      "867:\tlearn: 0.0666471\ttotal: 10.4s\tremaining: 1.58s\n",
      "868:\tlearn: 0.0666342\ttotal: 10.4s\tremaining: 1.56s\n",
      "869:\tlearn: 0.0666247\ttotal: 10.4s\tremaining: 1.55s\n",
      "870:\tlearn: 0.0666153\ttotal: 10.4s\tremaining: 1.54s\n",
      "871:\tlearn: 0.0666053\ttotal: 10.4s\tremaining: 1.53s\n",
      "872:\tlearn: 0.0665948\ttotal: 10.4s\tremaining: 1.52s\n",
      "873:\tlearn: 0.0665778\ttotal: 10.4s\tremaining: 1.5s\n",
      "874:\tlearn: 0.0665726\ttotal: 10.4s\tremaining: 1.49s\n",
      "875:\tlearn: 0.0665513\ttotal: 10.5s\tremaining: 1.48s\n",
      "876:\tlearn: 0.0665291\ttotal: 10.5s\tremaining: 1.47s\n",
      "877:\tlearn: 0.0665045\ttotal: 10.5s\tremaining: 1.46s\n",
      "878:\tlearn: 0.0664948\ttotal: 10.5s\tremaining: 1.44s\n",
      "879:\tlearn: 0.0664831\ttotal: 10.5s\tremaining: 1.43s\n",
      "880:\tlearn: 0.0664542\ttotal: 10.5s\tremaining: 1.42s\n",
      "881:\tlearn: 0.0664378\ttotal: 10.5s\tremaining: 1.41s\n",
      "882:\tlearn: 0.0664258\ttotal: 10.5s\tremaining: 1.39s\n",
      "883:\tlearn: 0.0664222\ttotal: 10.5s\tremaining: 1.38s\n",
      "884:\tlearn: 0.0663972\ttotal: 10.5s\tremaining: 1.37s\n",
      "885:\tlearn: 0.0663832\ttotal: 10.6s\tremaining: 1.36s\n",
      "886:\tlearn: 0.0663670\ttotal: 10.6s\tremaining: 1.35s\n",
      "887:\tlearn: 0.0663536\ttotal: 10.6s\tremaining: 1.33s\n",
      "888:\tlearn: 0.0663313\ttotal: 10.6s\tremaining: 1.32s\n",
      "889:\tlearn: 0.0663067\ttotal: 10.6s\tremaining: 1.31s\n",
      "890:\tlearn: 0.0662935\ttotal: 10.6s\tremaining: 1.3s\n",
      "891:\tlearn: 0.0662825\ttotal: 10.6s\tremaining: 1.28s\n",
      "892:\tlearn: 0.0662647\ttotal: 10.6s\tremaining: 1.27s\n",
      "893:\tlearn: 0.0662451\ttotal: 10.6s\tremaining: 1.26s\n",
      "894:\tlearn: 0.0662340\ttotal: 10.6s\tremaining: 1.25s\n",
      "895:\tlearn: 0.0662181\ttotal: 10.7s\tremaining: 1.24s\n",
      "896:\tlearn: 0.0662071\ttotal: 10.7s\tremaining: 1.23s\n",
      "897:\tlearn: 0.0661913\ttotal: 10.7s\tremaining: 1.21s\n",
      "898:\tlearn: 0.0661683\ttotal: 10.7s\tremaining: 1.2s\n",
      "899:\tlearn: 0.0661403\ttotal: 10.7s\tremaining: 1.19s\n",
      "900:\tlearn: 0.0661262\ttotal: 10.7s\tremaining: 1.18s\n",
      "901:\tlearn: 0.0661246\ttotal: 10.7s\tremaining: 1.17s\n",
      "902:\tlearn: 0.0661211\ttotal: 10.7s\tremaining: 1.15s\n",
      "903:\tlearn: 0.0661029\ttotal: 10.7s\tremaining: 1.14s\n",
      "904:\tlearn: 0.0660725\ttotal: 10.8s\tremaining: 1.13s\n",
      "905:\tlearn: 0.0660173\ttotal: 10.8s\tremaining: 1.12s\n",
      "906:\tlearn: 0.0660004\ttotal: 10.8s\tremaining: 1.1s\n",
      "907:\tlearn: 0.0659871\ttotal: 10.8s\tremaining: 1.09s\n",
      "908:\tlearn: 0.0659741\ttotal: 10.8s\tremaining: 1.08s\n",
      "909:\tlearn: 0.0659658\ttotal: 10.8s\tremaining: 1.07s\n",
      "910:\tlearn: 0.0659222\ttotal: 10.8s\tremaining: 1.06s\n",
      "911:\tlearn: 0.0659133\ttotal: 10.8s\tremaining: 1.04s\n",
      "912:\tlearn: 0.0658959\ttotal: 10.8s\tremaining: 1.03s\n",
      "913:\tlearn: 0.0658851\ttotal: 10.9s\tremaining: 1.02s\n",
      "914:\tlearn: 0.0658602\ttotal: 10.9s\tremaining: 1.01s\n",
      "915:\tlearn: 0.0658374\ttotal: 10.9s\tremaining: 997ms\n",
      "916:\tlearn: 0.0658100\ttotal: 10.9s\tremaining: 985ms\n",
      "917:\tlearn: 0.0657908\ttotal: 10.9s\tremaining: 973ms\n",
      "918:\tlearn: 0.0657689\ttotal: 10.9s\tremaining: 961ms\n",
      "919:\tlearn: 0.0657553\ttotal: 10.9s\tremaining: 949ms\n",
      "920:\tlearn: 0.0657411\ttotal: 10.9s\tremaining: 937ms\n",
      "921:\tlearn: 0.0657170\ttotal: 10.9s\tremaining: 926ms\n",
      "922:\tlearn: 0.0657039\ttotal: 11s\tremaining: 914ms\n",
      "923:\tlearn: 0.0656818\ttotal: 11s\tremaining: 904ms\n",
      "924:\tlearn: 0.0656451\ttotal: 11s\tremaining: 892ms\n",
      "925:\tlearn: 0.0656222\ttotal: 11s\tremaining: 881ms\n",
      "926:\tlearn: 0.0656052\ttotal: 11s\tremaining: 869ms\n",
      "927:\tlearn: 0.0655901\ttotal: 11.1s\tremaining: 857ms\n",
      "928:\tlearn: 0.0655599\ttotal: 11.1s\tremaining: 845ms\n",
      "929:\tlearn: 0.0655477\ttotal: 11.1s\tremaining: 833ms\n",
      "930:\tlearn: 0.0655313\ttotal: 11.1s\tremaining: 821ms\n",
      "931:\tlearn: 0.0655264\ttotal: 11.1s\tremaining: 809ms\n",
      "932:\tlearn: 0.0655131\ttotal: 11.1s\tremaining: 798ms\n",
      "933:\tlearn: 0.0654971\ttotal: 11.1s\tremaining: 786ms\n",
      "934:\tlearn: 0.0654800\ttotal: 11.1s\tremaining: 773ms\n",
      "935:\tlearn: 0.0654587\ttotal: 11.1s\tremaining: 762ms\n",
      "936:\tlearn: 0.0654485\ttotal: 11.1s\tremaining: 750ms\n",
      "937:\tlearn: 0.0654437\ttotal: 11.2s\tremaining: 737ms\n",
      "938:\tlearn: 0.0654319\ttotal: 11.2s\tremaining: 725ms\n",
      "939:\tlearn: 0.0654223\ttotal: 11.2s\tremaining: 714ms\n",
      "940:\tlearn: 0.0654030\ttotal: 11.2s\tremaining: 701ms\n",
      "941:\tlearn: 0.0653865\ttotal: 11.2s\tremaining: 689ms\n",
      "942:\tlearn: 0.0653675\ttotal: 11.2s\tremaining: 678ms\n",
      "943:\tlearn: 0.0653445\ttotal: 11.2s\tremaining: 666ms\n",
      "944:\tlearn: 0.0653365\ttotal: 11.2s\tremaining: 654ms\n",
      "945:\tlearn: 0.0653213\ttotal: 11.2s\tremaining: 642ms\n",
      "946:\tlearn: 0.0652997\ttotal: 11.3s\tremaining: 630ms\n",
      "947:\tlearn: 0.0652755\ttotal: 11.3s\tremaining: 618ms\n",
      "948:\tlearn: 0.0652509\ttotal: 11.3s\tremaining: 606ms\n",
      "949:\tlearn: 0.0652426\ttotal: 11.3s\tremaining: 594ms\n",
      "950:\tlearn: 0.0652290\ttotal: 11.3s\tremaining: 582ms\n",
      "951:\tlearn: 0.0652233\ttotal: 11.3s\tremaining: 570ms\n",
      "952:\tlearn: 0.0652055\ttotal: 11.3s\tremaining: 558ms\n",
      "953:\tlearn: 0.0651864\ttotal: 11.3s\tremaining: 546ms\n",
      "954:\tlearn: 0.0651798\ttotal: 11.3s\tremaining: 535ms\n",
      "955:\tlearn: 0.0651613\ttotal: 11.4s\tremaining: 523ms\n",
      "956:\tlearn: 0.0651443\ttotal: 11.4s\tremaining: 511ms\n",
      "957:\tlearn: 0.0651005\ttotal: 11.4s\tremaining: 499ms\n",
      "958:\tlearn: 0.0650786\ttotal: 11.4s\tremaining: 487ms\n",
      "959:\tlearn: 0.0650643\ttotal: 11.4s\tremaining: 475ms\n",
      "960:\tlearn: 0.0650533\ttotal: 11.4s\tremaining: 463ms\n",
      "961:\tlearn: 0.0650422\ttotal: 11.4s\tremaining: 451ms\n",
      "962:\tlearn: 0.0650304\ttotal: 11.4s\tremaining: 439ms\n",
      "963:\tlearn: 0.0650062\ttotal: 11.4s\tremaining: 427ms\n",
      "964:\tlearn: 0.0649936\ttotal: 11.5s\tremaining: 416ms\n",
      "965:\tlearn: 0.0649828\ttotal: 11.5s\tremaining: 404ms\n",
      "966:\tlearn: 0.0649595\ttotal: 11.5s\tremaining: 392ms\n",
      "967:\tlearn: 0.0649399\ttotal: 11.5s\tremaining: 380ms\n",
      "968:\tlearn: 0.0649349\ttotal: 11.5s\tremaining: 368ms\n",
      "969:\tlearn: 0.0649176\ttotal: 11.5s\tremaining: 356ms\n",
      "970:\tlearn: 0.0649032\ttotal: 11.5s\tremaining: 344ms\n",
      "971:\tlearn: 0.0648936\ttotal: 11.5s\tremaining: 332ms\n",
      "972:\tlearn: 0.0648751\ttotal: 11.5s\tremaining: 320ms\n",
      "973:\tlearn: 0.0648627\ttotal: 11.6s\tremaining: 308ms\n",
      "974:\tlearn: 0.0648390\ttotal: 11.6s\tremaining: 297ms\n",
      "975:\tlearn: 0.0648157\ttotal: 11.6s\tremaining: 285ms\n",
      "976:\tlearn: 0.0648025\ttotal: 11.6s\tremaining: 273ms\n",
      "977:\tlearn: 0.0647933\ttotal: 11.6s\tremaining: 261ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978:\tlearn: 0.0647794\ttotal: 11.6s\tremaining: 249ms\n",
      "979:\tlearn: 0.0647594\ttotal: 11.6s\tremaining: 237ms\n",
      "980:\tlearn: 0.0647519\ttotal: 11.6s\tremaining: 225ms\n",
      "981:\tlearn: 0.0647313\ttotal: 11.6s\tremaining: 213ms\n",
      "982:\tlearn: 0.0647151\ttotal: 11.6s\tremaining: 201ms\n",
      "983:\tlearn: 0.0647071\ttotal: 11.7s\tremaining: 190ms\n",
      "984:\tlearn: 0.0646850\ttotal: 11.7s\tremaining: 178ms\n",
      "985:\tlearn: 0.0646745\ttotal: 11.7s\tremaining: 166ms\n",
      "986:\tlearn: 0.0646534\ttotal: 11.7s\tremaining: 154ms\n",
      "987:\tlearn: 0.0646417\ttotal: 11.7s\tremaining: 142ms\n",
      "988:\tlearn: 0.0646251\ttotal: 11.7s\tremaining: 130ms\n",
      "989:\tlearn: 0.0646076\ttotal: 11.7s\tremaining: 118ms\n",
      "990:\tlearn: 0.0645961\ttotal: 11.7s\tremaining: 107ms\n",
      "991:\tlearn: 0.0645860\ttotal: 11.7s\tremaining: 94.7ms\n",
      "992:\tlearn: 0.0645636\ttotal: 11.8s\tremaining: 82.9ms\n",
      "993:\tlearn: 0.0645534\ttotal: 11.8s\tremaining: 71.1ms\n",
      "994:\tlearn: 0.0645443\ttotal: 11.8s\tremaining: 59.2ms\n",
      "995:\tlearn: 0.0645354\ttotal: 11.8s\tremaining: 47.4ms\n",
      "996:\tlearn: 0.0645296\ttotal: 11.8s\tremaining: 35.5ms\n",
      "997:\tlearn: 0.0645219\ttotal: 11.8s\tremaining: 23.7ms\n",
      "998:\tlearn: 0.0645039\ttotal: 11.8s\tremaining: 11.8ms\n",
      "999:\tlearn: 0.0644847\ttotal: 11.8s\tremaining: 0us\n",
      "logloss: 22.456608816630993\n",
      "logloss: 23.01920830552185\n",
      "logloss: 23.623667784123736\n",
      "logloss: 23.256733403958254\n",
      "logloss: 23.56163578692205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:27:14,364] Finished trial#0 resulted in value: 0.959446554680546. Current best value is 0.959446554680546 with parameters: {'booster': 'dart', 'alpha': 5.467006519249799e-06, 'max_depth': 7, 'eta': 2.764853887455123e-06, 'gamma': 0.041725415619138694, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00041319581513775, 'skip_drop': 4.242086203304623e-08}.\n",
      "[I 2019-11-20 22:27:30,046] Finished trial#1 resulted in value: 0.9592576372790826. Current best value is 0.959446554680546 with parameters: {'booster': 'dart', 'alpha': 5.467006519249799e-06, 'max_depth': 7, 'eta': 2.764853887455123e-06, 'gamma': 0.041725415619138694, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00041319581513775, 'skip_drop': 4.242086203304623e-08}.\n",
      "[I 2019-11-20 22:27:35,145] Finished trial#2 resulted in value: 0.9594034175261603. Current best value is 0.959446554680546 with parameters: {'booster': 'dart', 'alpha': 5.467006519249799e-06, 'max_depth': 7, 'eta': 2.764853887455123e-06, 'gamma': 0.041725415619138694, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00041319581513775, 'skip_drop': 4.242086203304623e-08}.\n",
      "[I 2019-11-20 22:27:45,028] Finished trial#3 resulted in value: 0.9580355553969977. Current best value is 0.959446554680546 with parameters: {'booster': 'dart', 'alpha': 5.467006519249799e-06, 'max_depth': 7, 'eta': 2.764853887455123e-06, 'gamma': 0.041725415619138694, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00041319581513775, 'skip_drop': 4.242086203304623e-08}.\n",
      "[I 2019-11-20 22:27:49,553] Finished trial#4 resulted in value: 0.9595176907713983. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:10,973] Finished trial#5 resulted in value: 0.5706752249488938. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:21,121] Finished trial#6 resulted in value: 0.9580842040184973. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:22,960] Finished trial#7 resulted in value: 0.9570510830855085. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:31,333] Finished trial#8 resulted in value: 0.9585916358377038. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:42,851] Finished trial#9 resulted in value: 0.9572205550392109. Current best value is 0.9595176907713983 with parameters: {'booster': 'gbtree', 'alpha': 0.00019912328204352663, 'max_depth': 5, 'eta': 1.9174563291711226e-08, 'gamma': 8.468917487399461e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:45,328] Finished trial#10 resulted in value: 0.9595426265141566. Current best value is 0.9595426265141566 with parameters: {'booster': 'gbtree', 'alpha': 0.0027418249584827553, 'max_depth': 2, 'eta': 3.932851867667593e-08, 'gamma': 0.00032403708985203694, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:47,161] Finished trial#11 resulted in value: 0.9571818565885384. Current best value is 0.9595426265141566 with parameters: {'booster': 'gbtree', 'alpha': 0.0027418249584827553, 'max_depth': 2, 'eta': 3.932851867667593e-08, 'gamma': 0.00032403708985203694, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:50,462] Finished trial#12 resulted in value: 0.9598057165099085. Current best value is 0.9598057165099085 with parameters: {'booster': 'gbtree', 'alpha': 0.006137695629294103, 'max_depth': 3, 'eta': 2.2729383145249416e-08, 'gamma': 0.001114837976040319, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:53,515] Finished trial#13 resulted in value: 0.9598177561967232. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:28:57,252] Finished trial#14 resulted in value: 0.9582576207829737. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:29:03,576] Finished trial#15 resulted in value: 0.9592146880968402. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:29:07,352] Finished trial#16 resulted in value: 0.9565316863572443. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:29:36,684] Finished trial#17 resulted in value: 0.9586038589002719. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:29:39,203] Finished trial#18 resulted in value: 0.9595596453845221. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:29:42,299] Finished trial#19 resulted in value: 0.9594578031654759. Current best value is 0.9598177561967232 with parameters: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.011981318202155073, 'max_depth': 3, 'eta': 2.798499277899691e-07, 'gamma': 0.0019368421361087614, 'grow_policy': 'lossguide'}\n",
      "\n",
      "[22:29:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:30:10,930] Finished trial#0 resulted in value: 0.9590365219205721. Current best value is 0.9590365219205721 with parameters: {'booster': 'dart', 'alpha': 0.001430622078659958, 'max_depth': 13, 'eta': 0.009894285469876826, 'gamma': 0.009612860606188885, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0026827799077475374, 'skip_drop': 0.16262302295105988}.\n",
      "[I 2019-11-20 22:30:19,147] Finished trial#1 resulted in value: 0.9586482221954402. Current best value is 0.9590365219205721 with parameters: {'booster': 'dart', 'alpha': 0.001430622078659958, 'max_depth': 13, 'eta': 0.009894285469876826, 'gamma': 0.009612860606188885, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0026827799077475374, 'skip_drop': 0.16262302295105988}.\n",
      "[I 2019-11-20 22:30:24,407] Finished trial#2 resulted in value: 0.9598497534308184. Current best value is 0.9598497534308184 with parameters: {'booster': 'gbtree', 'alpha': 6.66802849893159e-06, 'max_depth': 7, 'eta': 1.347378120466421e-06, 'gamma': 0.00021566256679422056, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:30:29,245] Finished trial#3 resulted in value: 0.9599622724491235. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:31:15,077] Finished trial#4 resulted in value: 0.9581405350716896. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:31:58,548] Finished trial#5 resulted in value: 0.9580086481600951. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:32:42,623] Finished trial#6 resulted in value: 0.9581381478300223. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:32:49,964] Finished trial#7 resulted in value: 0.9590636969466082. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:32:55,323] Finished trial#8 resulted in value: 0.9597639772749942. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:02,541] Finished trial#9 resulted in value: 0.9598397494800771. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:04,408] Finished trial#10 resulted in value: 0.9575788881266825. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:09,234] Finished trial#11 resulted in value: 0.9583693046642366. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:13,979] Finished trial#12 resulted in value: 0.9598961789915588. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:17,758] Finished trial#13 resulted in value: 0.9598813597602673. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:23,993] Finished trial#14 resulted in value: 0.9594363934662828. Current best value is 0.9599622724491235 with parameters: {'booster': 'gbtree', 'alpha': 3.855167157725083e-07, 'max_depth': 6, 'eta': 0.42139020807231714, 'gamma': 1.1820334177941011e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:27,816] Finished trial#15 resulted in value: 0.960090094699454. Current best value is 0.960090094699454 with parameters: {'booster': 'gbtree', 'alpha': 2.461489955538252e-06, 'max_depth': 4, 'eta': 2.594998584141292e-07, 'gamma': 8.95488268891678e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:31,076] Finished trial#16 resulted in value: 0.960262624666335. Current best value is 0.960262624666335 with parameters: {'booster': 'gbtree', 'alpha': 6.382305696486548e-06, 'max_depth': 3, 'eta': 7.984831578671018e-08, 'gamma': 5.864887871925556e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:33,523] Finished trial#17 resulted in value: 0.9596791090867998. Current best value is 0.960262624666335 with parameters: {'booster': 'gbtree', 'alpha': 6.382305696486548e-06, 'max_depth': 3, 'eta': 7.984831578671018e-08, 'gamma': 5.864887871925556e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:37,263] Finished trial#18 resulted in value: 0.9601437441161323. Current best value is 0.960262624666335 with parameters: {'booster': 'gbtree', 'alpha': 6.382305696486548e-06, 'max_depth': 3, 'eta': 7.984831578671018e-08, 'gamma': 5.864887871925556e-07, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:33:41,125] Finished trial#19 resulted in value: 0.9602423044680848. Current best value is 0.960262624666335 with parameters: {'booster': 'gbtree', 'alpha': 6.382305696486548e-06, 'max_depth': 3, 'eta': 7.984831578671018e-08, 'gamma': 5.864887871925556e-07, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 6.382305696486548e-06, 'max_depth': 3, 'eta': 7.984831578671018e-08, 'gamma': 5.864887871925556e-07, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[22:33:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:33:47,399] Finished trial#0 resulted in value: 0.960267257926034. Current best value is 0.960267257926034 with parameters: {'booster': 'gbtree', 'alpha': 0.021074193974668386, 'max_depth': 7, 'eta': 4.3417370643744257e-07, 'gamma': 2.3711580076315076e-06, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:34:03,854] Finished trial#1 resulted in value: 0.9602830236898179. Current best value is 0.9602830236898179 with parameters: {'booster': 'dart', 'alpha': 1.9516666353218994e-07, 'max_depth': 7, 'eta': 8.567099662061724e-08, 'gamma': 1.24745821431392e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.001968374477182454, 'skip_drop': 1.3039029776726324e-05}.\n",
      "[I 2019-11-20 22:34:14,890] Finished trial#2 resulted in value: 0.9606384049387268. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:34:31,261] Finished trial#3 resulted in value: 0.571194449877304. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:34:38,957] Finished trial#4 resulted in value: 0.9598546171604943. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:34:45,383] Finished trial#5 resulted in value: 0.9606136638416863. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:34:59,755] Finished trial#6 resulted in value: 0.9598493115195513. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:35:11,931] Finished trial#7 resulted in value: -1.1632452106761755. Current best value is 0.9606384049387268 with parameters: {'booster': 'dart', 'alpha': 3.1618269794700236e-08, 'max_depth': 4, 'eta': 4.212979746066548e-06, 'gamma': 3.4476828863126853e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.590850376977981e-08, 'skip_drop': 0.00012210096662538278}.\n",
      "[I 2019-11-20 22:35:24,325] Finished trial#8 resulted in value: 0.9606581460353072. Current best value is 0.9606581460353072 with parameters: {'booster': 'dart', 'alpha': 0.3563643344849392, 'max_depth': 7, 'eta': 0.033193653639313446, 'gamma': 0.046901235510157124, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0854658201701595e-06, 'skip_drop': 4.1131678566283345e-08}.\n",
      "[I 2019-11-20 22:35:34,363] Finished trial#9 resulted in value: 0.9597532988122912. Current best value is 0.9606581460353072 with parameters: {'booster': 'dart', 'alpha': 0.3563643344849392, 'max_depth': 7, 'eta': 0.033193653639313446, 'gamma': 0.046901235510157124, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0854658201701595e-06, 'skip_drop': 4.1131678566283345e-08}.\n",
      "[I 2019-11-20 22:35:45,579] Finished trial#10 resulted in value: -0.5378109269263315. Current best value is 0.9606581460353072 with parameters: {'booster': 'dart', 'alpha': 0.3563643344849392, 'max_depth': 7, 'eta': 0.033193653639313446, 'gamma': 0.046901235510157124, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0854658201701595e-06, 'skip_drop': 4.1131678566283345e-08}.\n",
      "[I 2019-11-20 22:35:52,976] Finished trial#11 resulted in value: 0.9584809744218301. Current best value is 0.9606581460353072 with parameters: {'booster': 'dart', 'alpha': 0.3563643344849392, 'max_depth': 7, 'eta': 0.033193653639313446, 'gamma': 0.046901235510157124, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0854658201701595e-06, 'skip_drop': 4.1131678566283345e-08}.\n",
      "[I 2019-11-20 22:36:00,423] Finished trial#12 resulted in value: 0.9582562469585743. Current best value is 0.9606581460353072 with parameters: {'booster': 'dart', 'alpha': 0.3563643344849392, 'max_depth': 7, 'eta': 0.033193653639313446, 'gamma': 0.046901235510157124, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 2.0854658201701595e-06, 'skip_drop': 4.1131678566283345e-08}.\n",
      "[I 2019-11-20 22:36:10,193] Finished trial#13 resulted in value: 0.9609039382719515. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:36:32,493] Finished trial#14 resulted in value: 0.959908220291763. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:36:42,289] Finished trial#15 resulted in value: 0.960826498331081. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:36:54,826] Finished trial#16 resulted in value: 0.9608448350718671. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:36:59,906] Finished trial#17 resulted in value: 0.958116030337545. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:37:51,778] Finished trial#18 resulted in value: 0.9589111430499034. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n",
      "[I 2019-11-20 22:38:06,190] Finished trial#19 resulted in value: 0.9608767855722726. Current best value is 0.9609039382719515 with parameters: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 1.2580786434928108e-08, 'max_depth': 3, 'eta': 0.7478711536810951, 'gamma': 0.00012523166354540382, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.215671318760046e-08, 'skip_drop': 1.956571170605706e-06}\n",
      "\n",
      "[22:38:06] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 22:38:19,708] Finished trial#0 resulted in value: 0.9594067788618149. Current best value is 0.9594067788618149 with parameters: {'booster': 'gbtree', 'alpha': 0.00027497183080624443, 'max_depth': 19, 'eta': 2.311844373497529e-07, 'gamma': 0.0026083639022135043, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:39:01,208] Finished trial#1 resulted in value: 0.9593873731670142. Current best value is 0.9594067788618149 with parameters: {'booster': 'gbtree', 'alpha': 0.00027497183080624443, 'max_depth': 19, 'eta': 2.311844373497529e-07, 'gamma': 0.0026083639022135043, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:39:16,321] Finished trial#2 resulted in value: 0.9590190433904571. Current best value is 0.9594067788618149 with parameters: {'booster': 'gbtree', 'alpha': 0.00027497183080624443, 'max_depth': 19, 'eta': 2.311844373497529e-07, 'gamma': 0.0026083639022135043, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:39:33,305] Finished trial#3 resulted in value: 0.9411706505054174. Current best value is 0.9594067788618149 with parameters: {'booster': 'gbtree', 'alpha': 0.00027497183080624443, 'max_depth': 19, 'eta': 2.311844373497529e-07, 'gamma': 0.0026083639022135043, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:40:06,563] Finished trial#4 resulted in value: 0.959564037069414. Current best value is 0.959564037069414 with parameters: {'booster': 'dart', 'alpha': 0.008511316489945655, 'max_depth': 12, 'eta': 0.15180912892818335, 'gamma': 0.000990594148554914, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.00020625123100038268, 'skip_drop': 9.540230683402949e-05}.\n",
      "[I 2019-11-20 22:40:12,129] Finished trial#5 resulted in value: 0.9601279350912038. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:40:26,217] Finished trial#6 resulted in value: 0.9592297728307477. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:20,314] Finished trial#7 resulted in value: 0.9593207932492704. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:33,251] Finished trial#8 resulted in value: 0.9596943058770314. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:35,623] Finished trial#9 resulted in value: 0.9583235235603663. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:40,197] Finished trial#10 resulted in value: 0.9594916795264762. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:45,845] Finished trial#11 resulted in value: 0.9584251118079525. Current best value is 0.9601279350912038 with parameters: {'booster': 'gbtree', 'alpha': 6.06129733330194e-08, 'max_depth': 8, 'eta': 0.12902596784355796, 'gamma': 0.12882641681103935, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:41:57,323] Finished trial#12 resulted in value: 0.9605319659782907. Current best value is 0.9605319659782907 with parameters: {'booster': 'gbtree', 'alpha': 1.0428674989249524e-05, 'max_depth': 11, 'eta': 0.9431086967460823, 'gamma': 0.03196900250528285, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:42:04,946] Finished trial#13 resulted in value: 0.9604210374807934. Current best value is 0.9605319659782907 with parameters: {'booster': 'gbtree', 'alpha': 1.0428674989249524e-05, 'max_depth': 11, 'eta': 0.9431086967460823, 'gamma': 0.03196900250528285, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:42:08,053] Finished trial#14 resulted in value: 0.9607613862466546. Current best value is 0.9607613862466546 with parameters: {'booster': 'gbtree', 'alpha': 6.863871338966504e-06, 'max_depth': 3, 'eta': 2.275284590774367e-05, 'gamma': 0.022849468030699316, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:42:10,131] Finished trial#15 resulted in value: 0.9582145297380353. Current best value is 0.9607613862466546 with parameters: {'booster': 'gbtree', 'alpha': 6.863871338966504e-06, 'max_depth': 3, 'eta': 2.275284590774367e-05, 'gamma': 0.022849468030699316, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 22:42:16,645] Finished trial#16 resulted in value: 0.9608455501740417. Current best value is 0.9608455501740417 with parameters: {'booster': 'gbtree', 'alpha': 5.927788950664177e-07, 'max_depth': 5, 'eta': 3.1739453381828127e-06, 'gamma': 0.008863502584917433, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:42:23,996] Finished trial#17 resulted in value: 0.9607196073512535. Current best value is 0.9608455501740417 with parameters: {'booster': 'gbtree', 'alpha': 5.927788950664177e-07, 'max_depth': 5, 'eta': 3.1739453381828127e-06, 'gamma': 0.008863502584917433, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:42:27,171] Finished trial#18 resulted in value: 0.9609161778486145. Current best value is 0.9609161778486145 with parameters: {'booster': 'gbtree', 'alpha': 5.164271448534921e-07, 'max_depth': 3, 'eta': 1.3074177048527252e-08, 'gamma': 2.2160021166279705e-05, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 22:42:31,060] Finished trial#19 resulted in value: 0.960698698511082. Current best value is 0.9609161778486145 with parameters: {'booster': 'gbtree', 'alpha': 5.164271448534921e-07, 'max_depth': 3, 'eta': 1.3074177048527252e-08, 'gamma': 2.2160021166279705e-05, 'grow_policy': 'lossguide'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 5.164271448534921e-07, 'max_depth': 3, 'eta': 1.3074177048527252e-08, 'gamma': 2.2160021166279705e-05, 'grow_policy': 'lossguide'}\n",
      "\n",
      "[22:42:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "logloss: 22.302119024890192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoost\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "# tensorflowの警告抑制\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# スタッキング\n",
    "# ----------------------------------\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# models.pyにModel1Xgb, Model1NN, Model2Linearを定義しているものとする\n",
    "# 各クラスは、fitで学習し、predictで予測値の確率を出力する\n",
    "\n",
    "\n",
    "# 学習データに対する「目的変数を知らない」予測値と、テストデータに対する予測値を返す関数\n",
    "def predict_cv(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    # クロスバリデーションで学習・予測を行い、予測値とインデックスを保存する\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # テストデータに対する予測値の平均をとる\n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test\n",
    "\n",
    "\n",
    "# 1層目のモデル\n",
    "\n",
    "model_1a =xgb.XGBRegressor(**{'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06})\n",
    "pred_train_1a, pred_test_1a = predict_cv(model_1a, X, y, test)\n",
    "\n",
    "model_1b = lgb.LGBMRegressor(**{ 'depth': 22, 'learning_rate': 0.15022735769525583, 'random_strength': 40, 'bagging_temperature': 0.03207087240546606, 'od_type': 'Iter', 'od_wait': 41})\n",
    "pred_train_1b, pred_test_1b = predict_cv(model_1b, X, y, test)\n",
    "\n",
    "model_1c = LGBRegressorCV(n_trials=10)\n",
    "pred_train_1c, pred_test_1c = predict_cv(model_1c, X, y, test)\n",
    "\n",
    "model_1d = RFRegressorCV(n_trials=10)\n",
    "pred_train_1d, pred_test_1d = predict_cv(model_1d, X, y, test)\n",
    "\n",
    "model_1e= CatBoost({'learning_rate': 0.17321274085322982, 'random_strength': 94, 'bagging_temperature': 0.2804904005490844, 'od_type': 'IncToDec', 'od_wait': 50})\n",
    "pred_train_1e, pred_test_1e = predict_cv(model_1e, X, y, test)\n",
    "\n",
    "\n",
    "# 1層目のモデルの評価\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1a))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1b))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1c))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1d))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1e))}')\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成\n",
    "train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b,'pred_1c': pred_train_1c,'pred_1d': pred_train_1d,'pred_1e': pred_train_1e})\n",
    "test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b, 'pred_1c': pred_test_1c, 'pred_1d': pred_test_1d, 'pred_1e': pred_test_1e})\n",
    "\n",
    "\n",
    "# 2層目のモデル\n",
    "# pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n",
    "# pred_test_2は、2層目のモデルのテストデータの予測値\n",
    "model_2 = XGBRegressorCV(n_trials=20)\n",
    "pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, y, test_x_2)\n",
    "\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([519.57227, 267.2549 , 205.62401, ..., 521.78644, 147.4098 ,\n",
       "       635.16693], dtype=float32)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=np.exp(pred_test_2 )\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:23:37,643] Finished trial#0 resulted in value: 0.9305498191433168. Current best value is 0.9305498191433168 with parameters: {'booster': 'gbtree', 'alpha': 0.05398288589157677, 'max_depth': 3, 'eta': 0.7140800447741135, 'gamma': 0.00024190393122103457, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-19 14:24:54,582] Finished trial#1 resulted in value: 0.9603226552134976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:17,469] Finished trial#2 resulted in value: 0.9596122329185025. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:38,640] Finished trial#3 resulted in value: 0.9598963266814737. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:01,822] Finished trial#4 resulted in value: 0.9445592847731454. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:25,405] Finished trial#5 resulted in value: 0.9587629061169235. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:41,400] Finished trial#6 resulted in value: -387.9041239755456. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:54,677] Finished trial#7 resulted in value: 0.9592436087634768. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:28:56,906] Finished trial#8 resulted in value: 0.9566239935926616. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:07,426] Finished trial#9 resulted in value: 0.9567988998760226. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:42,840] Finished trial#10 resulted in value: 0.9476482257292294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:32:27,596] Finished trial#11 resulted in value: 0.956698401395849. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:33:16,474] Finished trial#12 resulted in value: -4.055874265306564. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:14,085] Finished trial#13 resulted in value: 0.9601138026638152. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:59,910] Finished trial#14 resulted in value: 0.9565527450078622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:33,531] Finished trial#15 resulted in value: 0.9446207766477638. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:48,702] Finished trial#16 resulted in value: -1399.7011359154567. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:00,125] Finished trial#17 resulted in value: 0.958760245390202. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:30,110] Finished trial#18 resulted in value: 0.9574032702359846. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:37:53,151] Finished trial#19 resulted in value: 0.9581539820151755. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:00,500] Finished trial#20 resulted in value: 0.956370606897214. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:18,114] Finished trial#21 resulted in value: 0.9586995742275745. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:41,289] Finished trial#22 resulted in value: 0.9594710457867158. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:06,406] Finished trial#23 resulted in value: 0.9545935103825889. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:24,873] Finished trial#24 resulted in value: 0.9594131128640582. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:52,900] Finished trial#25 resulted in value: 0.9568289978095595. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:40:18,111] Finished trial#26 resulted in value: 0.9568401840594071. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:44:38,410] Finished trial#27 resulted in value: 0.37976798637976894. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:09,769] Finished trial#28 resulted in value: 0.9569202272257933. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:31,764] Finished trial#29 resulted in value: 0.9530933288937604. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:46:20,190] Finished trial#30 resulted in value: 0.9596207162459487. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:47:21,110] Finished trial#31 resulted in value: 0.9587299746964092. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:22,964] Finished trial#32 resulted in value: 0.37901072492524795. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:59,562] Finished trial#33 resulted in value: 0.9557504185473518. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:50:19,887] Finished trial#34 resulted in value: 0.6920560246130634. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:07,764] Finished trial#35 resulted in value: 0.9563269444075339. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:15,479] Finished trial#36 resulted in value: 0.8104693935211186. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:44,993] Finished trial#37 resulted in value: 0.9596979309846085. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:52:08,960] Finished trial#38 resulted in value: 0.9594897252942587. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:34,318] Finished trial#39 resulted in value: 0.9560476206065488. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:59,365] Finished trial#40 resulted in value: 0.9589511941685105. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:35,311] Finished trial#41 resulted in value: 0.9557815871347504. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:57,821] Finished trial#42 resulted in value: 0.9596402683393249. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:22,898] Finished trial#43 resulted in value: 0.9596506393831744. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:49,563] Finished trial#44 resulted in value: 0.9583432840218722. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:12,913] Finished trial#45 resulted in value: 0.9503219866901705. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:44,323] Finished trial#46 resulted in value: 0.9439230284396045. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:07,679] Finished trial#47 resulted in value: 0.9577484164792291. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:33,616] Finished trial#48 resulted in value: 0.9588307979464294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:54,266] Finished trial#49 resulted in value: 0.9595208600484575. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:13,349] Finished trial#50 resulted in value: 0.958859235560175. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:34,458] Finished trial#51 resulted in value: 0.959801537313227. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:01,690] Finished trial#52 resulted in value: 0.9586469259847702. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:37,602] Finished trial#53 resulted in value: 0.9567041406226083. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:03,668] Finished trial#54 resulted in value: 0.9503185290450906. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:33,333] Finished trial#55 resulted in value: 0.9596033634027707. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:55,627] Finished trial#56 resulted in value: 0.9579607729400174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:30,560] Finished trial#57 resulted in value: 0.9559231059905114. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:56,760] Finished trial#58 resulted in value: 0.9597679573180903. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:23,527] Finished trial#59 resulted in value: 0.9580893053324463. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:52,486] Finished trial#60 resulted in value: 0.9557485563030305. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:11,578] Finished trial#61 resulted in value: 0.9594123430012976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:34,128] Finished trial#62 resulted in value: 0.9598668026754119. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:58,324] Finished trial#63 resulted in value: 0.9596332658609208. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:19,827] Finished trial#64 resulted in value: 0.9590550944936261. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:51,268] Finished trial#65 resulted in value: 0.9564344930902546. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:17,973] Finished trial#66 resulted in value: 0.9529373023400145. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:36,048] Finished trial#67 resulted in value: 0.9574774739825571. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:12,644] Finished trial#68 resulted in value: 0.9426763980484459. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:31,281] Finished trial#69 resulted in value: 0.9593833804168816. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:00,170] Finished trial#70 resulted in value: 0.9591893243636337. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:22,291] Finished trial#71 resulted in value: 0.9595051111829601. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:44,957] Finished trial#72 resulted in value: 0.9593363815773163. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:11,092] Finished trial#73 resulted in value: 0.958386606182769. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:38,646] Finished trial#74 resulted in value: 0.9576461057484386. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:08:02,243] Finished trial#75 resulted in value: 0.9592386374748388. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 15:09:04,771] Finished trial#76 resulted in value: 0.9597318738254854. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:06,931] Finished trial#77 resulted in value: 0.95899052150256. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:50,430] Finished trial#78 resulted in value: 0.9497551679431391. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:11:37,312] Finished trial#79 resulted in value: 0.9535350937350803. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:13:11,867] Finished trial#80 resulted in value: 0.37795736793164736. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:14:17,509] Finished trial#81 resulted in value: 0.9600409927854597. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:15:17,231] Finished trial#82 resulted in value: 0.9601820948004374. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:16:51,760] Finished trial#83 resulted in value: 0.9599088646926296. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:17:55,563] Finished trial#84 resulted in value: 0.9582942766014618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:19:16,561] Finished trial#85 resulted in value: 0.9556363922125138. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:20:50,623] Finished trial#86 resulted in value: 0.9598415829020268. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:21:56,978] Finished trial#87 resulted in value: 0.9591565948983902. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:23:02,511] Finished trial#88 resulted in value: 0.9592573940724174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:24:21,418] Finished trial#89 resulted in value: 0.9583596724830622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:25:05,185] Finished trial#90 resulted in value: 0.9579814074019618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:26:03,650] Finished trial#91 resulted in value: 0.9596818974789345. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:27:14,538] Finished trial#92 resulted in value: 0.9597103277990394. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:28:23,695] Finished trial#93 resulted in value: 0.9499916301977823. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:29:22,181] Finished trial#94 resulted in value: 0.9596422136661624. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:15,336] Finished trial#95 resulted in value: 0.9590829201283609. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:33,575] Finished trial#96 resulted in value: -498.1587888134253. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:19,094] Finished trial#97 resulted in value: 0.9524442506788089. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:49,887] Finished trial#98 resulted in value: 0.9479648390482927. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:33:28,437] Finished trial#99 resulted in value: 0.9563083963830048. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}\n",
      "\n",
      "[15:33:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# xgbr = XGBRegressorCV(n_trials=40)\n",
    "\n",
    "xgbr.fit(X, y)\n",
    "\n",
    "pred=np.exp(xgbr.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub = pd.DataFrame(pd.read_csv(\"test_data.csv\")['id'])\n",
    "sub[\"y\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)\n",
    "pred=np.exp( model.predict(test))\n",
    "sub = pd.DataFrame(pd.read_csv(\"test_data.csv\")['id'])\n",
    "sub[\"y\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
