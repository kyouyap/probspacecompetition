{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML \n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "warnings.filterwarnings('ignore')\n",
    "train=pd.read_csv(\"train_data.csv\")\n",
    "test=pd.read_csv(\"test_data.csv\")\n",
    "import pandas_profiling as pdp\n",
    "df=pd.concat([train,test],sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pdp\n",
    "def preprocess():\n",
    "    train=pd.read_csv(\"train_data.csv\")\n",
    "    test=pd.read_csv(\"test_data.csv\")\n",
    "    df=pd.concat([train,test],sort=False)\n",
    "    df.age=df.age.map(lambda x:57 if x>58 else x)\n",
    "    df.num_child=df.num_child.map(lambda x:7 if x>7 else x)\n",
    "    df.study_time=df.study_time.map(lambda x:17 if x>17 else x)\n",
    "    df[\"familiy_num\"]=1+df.partner+df.num_child\n",
    "    arealist=list(train.groupby(\"area\").mean().salary.sort_values().index)\n",
    "    areadic={}\n",
    "    for i,area in enumerate(arealist):\n",
    "        areadic[area]=i+1\n",
    "    df.area=df.area.map(areadic)\n",
    "    df.position=df.position+1\n",
    "    df.sex=df.sex-1\n",
    "    df.salary=np.log(df.salary)\n",
    "    df[\"agexposition\"]=df.age*df.position.map(lambda x:1.5 if x==1 else x)\n",
    "    df.education=df.education+1\n",
    "#     df=pd.get_dummies(df,drop_first=True, columns=['position','education'])\n",
    "#     df.drop([\"sex\"],axis=1)\n",
    "    train=df.dropna().drop([\"id\"],axis=1)\n",
    "    test=df[df.salary.isnull()].drop([\"id\"],axis=1)\n",
    "    test=test.drop([\"salary\",],axis=1)\n",
    "    X = train.drop([\"salary\"],axis=1)\n",
    "    y = train.salary\n",
    "    return X,y,test,df\n",
    "X,y,test,df=preprocess()\n",
    "#original 23.625521293118492\n",
    "train=pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.235450246603516, 21.939352391567876, 22.5758178205997, 22.740848425970245, 21.628926730218478]\n",
      "22.224079122991963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor(**{'booster': 'gblinear',\n",
    "                           'iterations': 309, 'depth': 16, 'learning_rate': 0.1,\n",
    "                           'random_strength': 48, 'bagging_temperature': 19.715729096205934, \n",
    "                           'od_type': 'Iter', 'od_wait': 26, 'lambda_l1': 0.726486176355415, \n",
    "                           'lambda_l2': 0.00044177449020498015, 'num_leaves': 188,\n",
    "                           'feature_fraction': 0.9443254919883529, 'bagging_fraction': 0.9271673814820428, \n",
    "                           'bagging_freq': 2, 'min_child_samples': 17})\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.applymap(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from deap import algorithms, base, creator, tools, gp\n",
    "X_train, X_test,y_train, y_test = train_test_split(np.array(X),np.array(y),test_size=0.8,random_state=123)\n",
    "X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.168366\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l2: 0.143661\n",
      "[3]\tvalid_0's l2: 0.123262\n",
      "[4]\tvalid_0's l2: 0.105956\n",
      "[5]\tvalid_0's l2: 0.0918256\n",
      "[6]\tvalid_0's l2: 0.0794009\n",
      "[7]\tvalid_0's l2: 0.0695068\n",
      "[8]\tvalid_0's l2: 0.0613051\n",
      "[9]\tvalid_0's l2: 0.0550128\n",
      "[10]\tvalid_0's l2: 0.0489397\n",
      "[11]\tvalid_0's l2: 0.0443457\n",
      "[12]\tvalid_0's l2: 0.0402479\n",
      "[13]\tvalid_0's l2: 0.0369653\n",
      "[14]\tvalid_0's l2: 0.0341288\n",
      "[15]\tvalid_0's l2: 0.0317089\n",
      "[16]\tvalid_0's l2: 0.0297169\n",
      "[17]\tvalid_0's l2: 0.0278021\n",
      "[18]\tvalid_0's l2: 0.0261425\n",
      "[19]\tvalid_0's l2: 0.0249164\n",
      "[20]\tvalid_0's l2: 0.0236687\n",
      "[21]\tvalid_0's l2: 0.0226069\n",
      "[22]\tvalid_0's l2: 0.0218491\n",
      "[23]\tvalid_0's l2: 0.0210339\n",
      "[24]\tvalid_0's l2: 0.0203182\n",
      "[25]\tvalid_0's l2: 0.0197036\n",
      "[26]\tvalid_0's l2: 0.0192431\n",
      "[27]\tvalid_0's l2: 0.0186945\n",
      "[28]\tvalid_0's l2: 0.0183283\n",
      "[29]\tvalid_0's l2: 0.0179544\n",
      "[30]\tvalid_0's l2: 0.0175463\n",
      "[31]\tvalid_0's l2: 0.0172912\n",
      "[32]\tvalid_0's l2: 0.017136\n",
      "[33]\tvalid_0's l2: 0.0169628\n",
      "[34]\tvalid_0's l2: 0.0167378\n",
      "[35]\tvalid_0's l2: 0.0165391\n",
      "[36]\tvalid_0's l2: 0.0163019\n",
      "[37]\tvalid_0's l2: 0.0160647\n",
      "[38]\tvalid_0's l2: 0.0159235\n",
      "[39]\tvalid_0's l2: 0.0157339\n",
      "[40]\tvalid_0's l2: 0.0155698\n",
      "[41]\tvalid_0's l2: 0.015474\n",
      "[42]\tvalid_0's l2: 0.015294\n",
      "[43]\tvalid_0's l2: 0.0151858\n",
      "[44]\tvalid_0's l2: 0.0151012\n",
      "[45]\tvalid_0's l2: 0.0149873\n",
      "[46]\tvalid_0's l2: 0.0147592\n",
      "[47]\tvalid_0's l2: 0.0146381\n",
      "[48]\tvalid_0's l2: 0.0145657\n",
      "[49]\tvalid_0's l2: 0.0144669\n",
      "[50]\tvalid_0's l2: 0.0142749\n",
      "[51]\tvalid_0's l2: 0.014162\n",
      "[52]\tvalid_0's l2: 0.0141026\n",
      "[53]\tvalid_0's l2: 0.014024\n",
      "[54]\tvalid_0's l2: 0.0139428\n",
      "[55]\tvalid_0's l2: 0.0138855\n",
      "[56]\tvalid_0's l2: 0.0138441\n",
      "[57]\tvalid_0's l2: 0.0137953\n",
      "[58]\tvalid_0's l2: 0.0137439\n",
      "[59]\tvalid_0's l2: 0.013662\n",
      "[60]\tvalid_0's l2: 0.013557\n",
      "[61]\tvalid_0's l2: 0.0135314\n",
      "[62]\tvalid_0's l2: 0.013409\n",
      "[63]\tvalid_0's l2: 0.0132805\n",
      "[64]\tvalid_0's l2: 0.0132537\n",
      "[65]\tvalid_0's l2: 0.0131991\n",
      "[66]\tvalid_0's l2: 0.0131246\n",
      "[67]\tvalid_0's l2: 0.0131006\n",
      "[68]\tvalid_0's l2: 0.0130803\n",
      "[69]\tvalid_0's l2: 0.0129889\n",
      "[70]\tvalid_0's l2: 0.0129469\n",
      "[71]\tvalid_0's l2: 0.0129328\n",
      "[72]\tvalid_0's l2: 0.0128399\n",
      "[73]\tvalid_0's l2: 0.0127712\n",
      "[74]\tvalid_0's l2: 0.0127333\n",
      "[75]\tvalid_0's l2: 0.0126464\n",
      "[76]\tvalid_0's l2: 0.0125939\n",
      "[77]\tvalid_0's l2: 0.0125545\n",
      "[78]\tvalid_0's l2: 0.0125468\n",
      "[79]\tvalid_0's l2: 0.0125008\n",
      "[80]\tvalid_0's l2: 0.0124359\n",
      "[81]\tvalid_0's l2: 0.0124147\n",
      "[82]\tvalid_0's l2: 0.0124091\n",
      "[83]\tvalid_0's l2: 0.0124045\n",
      "[84]\tvalid_0's l2: 0.0123119\n",
      "[85]\tvalid_0's l2: 0.0122829\n",
      "[86]\tvalid_0's l2: 0.0122405\n",
      "[87]\tvalid_0's l2: 0.0121888\n",
      "[88]\tvalid_0's l2: 0.0121765\n",
      "[89]\tvalid_0's l2: 0.012134\n",
      "[90]\tvalid_0's l2: 0.0121206\n",
      "[91]\tvalid_0's l2: 0.012087\n",
      "[92]\tvalid_0's l2: 0.0120629\n",
      "[93]\tvalid_0's l2: 0.012059\n",
      "[94]\tvalid_0's l2: 0.0119951\n",
      "[95]\tvalid_0's l2: 0.0119753\n",
      "[96]\tvalid_0's l2: 0.0119217\n",
      "[97]\tvalid_0's l2: 0.0118872\n",
      "[98]\tvalid_0's l2: 0.0118627\n",
      "[99]\tvalid_0's l2: 0.0118188\n",
      "[100]\tvalid_0's l2: 0.011814\n",
      "[101]\tvalid_0's l2: 0.0117931\n",
      "[102]\tvalid_0's l2: 0.0117396\n",
      "[103]\tvalid_0's l2: 0.0117245\n",
      "[104]\tvalid_0's l2: 0.011716\n",
      "[105]\tvalid_0's l2: 0.0116911\n",
      "[106]\tvalid_0's l2: 0.0116514\n",
      "[107]\tvalid_0's l2: 0.0116317\n",
      "[108]\tvalid_0's l2: 0.0116084\n",
      "[109]\tvalid_0's l2: 0.011584\n",
      "[110]\tvalid_0's l2: 0.0115777\n",
      "[111]\tvalid_0's l2: 0.0115634\n",
      "[112]\tvalid_0's l2: 0.0115338\n",
      "[113]\tvalid_0's l2: 0.0115145\n",
      "[114]\tvalid_0's l2: 0.0114626\n",
      "[115]\tvalid_0's l2: 0.0114366\n",
      "[116]\tvalid_0's l2: 0.0114278\n",
      "[117]\tvalid_0's l2: 0.0113916\n",
      "[118]\tvalid_0's l2: 0.011354\n",
      "[119]\tvalid_0's l2: 0.0113594\n",
      "[120]\tvalid_0's l2: 0.011343\n",
      "[121]\tvalid_0's l2: 0.0113337\n",
      "[122]\tvalid_0's l2: 0.0113159\n",
      "[123]\tvalid_0's l2: 0.0112865\n",
      "[124]\tvalid_0's l2: 0.0112572\n",
      "[125]\tvalid_0's l2: 0.0112598\n",
      "[126]\tvalid_0's l2: 0.0112514\n",
      "[127]\tvalid_0's l2: 0.0112456\n",
      "[128]\tvalid_0's l2: 0.0112339\n",
      "[129]\tvalid_0's l2: 0.0112387\n",
      "[130]\tvalid_0's l2: 0.0112067\n",
      "[131]\tvalid_0's l2: 0.0111914\n",
      "[132]\tvalid_0's l2: 0.0111716\n",
      "[133]\tvalid_0's l2: 0.0111289\n",
      "[134]\tvalid_0's l2: 0.0111046\n",
      "[135]\tvalid_0's l2: 0.0110932\n",
      "[136]\tvalid_0's l2: 0.0110733\n",
      "[137]\tvalid_0's l2: 0.0110623\n",
      "[138]\tvalid_0's l2: 0.0110487\n",
      "[139]\tvalid_0's l2: 0.0110431\n",
      "[140]\tvalid_0's l2: 0.0110247\n",
      "[141]\tvalid_0's l2: 0.0110066\n",
      "[142]\tvalid_0's l2: 0.0110018\n",
      "[143]\tvalid_0's l2: 0.010972\n",
      "[144]\tvalid_0's l2: 0.0109621\n",
      "[145]\tvalid_0's l2: 0.0109586\n",
      "[146]\tvalid_0's l2: 0.0109454\n",
      "[147]\tvalid_0's l2: 0.0109358\n",
      "[148]\tvalid_0's l2: 0.0109123\n",
      "[149]\tvalid_0's l2: 0.0109071\n",
      "[150]\tvalid_0's l2: 0.0109015\n",
      "[151]\tvalid_0's l2: 0.0109004\n",
      "[152]\tvalid_0's l2: 0.0108919\n",
      "[153]\tvalid_0's l2: 0.010873\n",
      "[154]\tvalid_0's l2: 0.0108627\n",
      "[155]\tvalid_0's l2: 0.0108567\n",
      "[156]\tvalid_0's l2: 0.0108276\n",
      "[157]\tvalid_0's l2: 0.010822\n",
      "[158]\tvalid_0's l2: 0.0108166\n",
      "[159]\tvalid_0's l2: 0.0108063\n",
      "[160]\tvalid_0's l2: 0.0107995\n",
      "[161]\tvalid_0's l2: 0.0107975\n",
      "[162]\tvalid_0's l2: 0.0107913\n",
      "[163]\tvalid_0's l2: 0.0107641\n",
      "[164]\tvalid_0's l2: 0.0107633\n",
      "[165]\tvalid_0's l2: 0.0107409\n",
      "[166]\tvalid_0's l2: 0.0107274\n",
      "[167]\tvalid_0's l2: 0.0107119\n",
      "[168]\tvalid_0's l2: 0.0107066\n",
      "[169]\tvalid_0's l2: 0.010705\n",
      "[170]\tvalid_0's l2: 0.0107058\n",
      "[171]\tvalid_0's l2: 0.0107027\n",
      "[172]\tvalid_0's l2: 0.010706\n",
      "[173]\tvalid_0's l2: 0.0107009\n",
      "[174]\tvalid_0's l2: 0.0107026\n",
      "[175]\tvalid_0's l2: 0.010689\n",
      "[176]\tvalid_0's l2: 0.0106868\n",
      "[177]\tvalid_0's l2: 0.0106826\n",
      "[178]\tvalid_0's l2: 0.0106829\n",
      "[179]\tvalid_0's l2: 0.0106786\n",
      "[180]\tvalid_0's l2: 0.0106754\n",
      "[181]\tvalid_0's l2: 0.0106308\n",
      "[182]\tvalid_0's l2: 0.0106263\n",
      "[183]\tvalid_0's l2: 0.0106218\n",
      "[184]\tvalid_0's l2: 0.0106107\n",
      "[185]\tvalid_0's l2: 0.0106117\n",
      "[186]\tvalid_0's l2: 0.0106078\n",
      "[187]\tvalid_0's l2: 0.0106091\n",
      "[188]\tvalid_0's l2: 0.0106059\n",
      "[189]\tvalid_0's l2: 0.0106053\n",
      "[190]\tvalid_0's l2: 0.0106027\n",
      "[191]\tvalid_0's l2: 0.0105973\n",
      "[192]\tvalid_0's l2: 0.0105966\n",
      "[193]\tvalid_0's l2: 0.0105956\n",
      "[194]\tvalid_0's l2: 0.0105911\n",
      "[195]\tvalid_0's l2: 0.0105966\n",
      "[196]\tvalid_0's l2: 0.0105882\n",
      "[197]\tvalid_0's l2: 0.0105817\n",
      "[198]\tvalid_0's l2: 0.0105833\n",
      "[199]\tvalid_0's l2: 0.0105831\n",
      "[200]\tvalid_0's l2: 0.0105785\n",
      "[201]\tvalid_0's l2: 0.0105772\n",
      "[202]\tvalid_0's l2: 0.0105771\n",
      "[203]\tvalid_0's l2: 0.0105735\n",
      "[204]\tvalid_0's l2: 0.0105742\n",
      "[205]\tvalid_0's l2: 0.010578\n",
      "[206]\tvalid_0's l2: 0.0105731\n",
      "[207]\tvalid_0's l2: 0.0105625\n",
      "[208]\tvalid_0's l2: 0.0105619\n",
      "[209]\tvalid_0's l2: 0.0105644\n",
      "[210]\tvalid_0's l2: 0.0105563\n",
      "[211]\tvalid_0's l2: 0.0105535\n",
      "[212]\tvalid_0's l2: 0.0105491\n",
      "[213]\tvalid_0's l2: 0.010545\n",
      "[214]\tvalid_0's l2: 0.0105449\n",
      "[215]\tvalid_0's l2: 0.0105402\n",
      "[216]\tvalid_0's l2: 0.0105303\n",
      "[217]\tvalid_0's l2: 0.0105132\n",
      "[218]\tvalid_0's l2: 0.0105077\n",
      "[219]\tvalid_0's l2: 0.010489\n",
      "[220]\tvalid_0's l2: 0.0104743\n",
      "[221]\tvalid_0's l2: 0.0104632\n",
      "[222]\tvalid_0's l2: 0.01046\n",
      "[223]\tvalid_0's l2: 0.0104327\n",
      "[224]\tvalid_0's l2: 0.0104116\n",
      "[225]\tvalid_0's l2: 0.0104041\n",
      "[226]\tvalid_0's l2: 0.0103918\n",
      "[227]\tvalid_0's l2: 0.0103737\n",
      "[228]\tvalid_0's l2: 0.010382\n",
      "[229]\tvalid_0's l2: 0.0103484\n",
      "[230]\tvalid_0's l2: 0.0103431\n",
      "[231]\tvalid_0's l2: 0.010312\n",
      "[232]\tvalid_0's l2: 0.0102954\n",
      "[233]\tvalid_0's l2: 0.0102805\n",
      "[234]\tvalid_0's l2: 0.0102654\n",
      "[235]\tvalid_0's l2: 0.0102599\n",
      "[236]\tvalid_0's l2: 0.0102616\n",
      "[237]\tvalid_0's l2: 0.0102661\n",
      "[238]\tvalid_0's l2: 0.0102684\n",
      "[239]\tvalid_0's l2: 0.0102643\n",
      "[240]\tvalid_0's l2: 0.0102661\n",
      "[241]\tvalid_0's l2: 0.0102613\n",
      "[242]\tvalid_0's l2: 0.0102546\n",
      "[243]\tvalid_0's l2: 0.0102485\n",
      "[244]\tvalid_0's l2: 0.0102498\n",
      "[245]\tvalid_0's l2: 0.0102419\n",
      "[246]\tvalid_0's l2: 0.0102364\n",
      "[247]\tvalid_0's l2: 0.010234\n",
      "[248]\tvalid_0's l2: 0.0102387\n",
      "[249]\tvalid_0's l2: 0.0102352\n",
      "[250]\tvalid_0's l2: 0.0102332\n",
      "[251]\tvalid_0's l2: 0.0102327\n",
      "[252]\tvalid_0's l2: 0.0102213\n",
      "[253]\tvalid_0's l2: 0.0102194\n",
      "[254]\tvalid_0's l2: 0.010211\n",
      "[255]\tvalid_0's l2: 0.0102027\n",
      "[256]\tvalid_0's l2: 0.0102052\n",
      "[257]\tvalid_0's l2: 0.0102045\n",
      "[258]\tvalid_0's l2: 0.0102091\n",
      "[259]\tvalid_0's l2: 0.0102111\n",
      "[260]\tvalid_0's l2: 0.010214\n",
      "[261]\tvalid_0's l2: 0.0102156\n",
      "[262]\tvalid_0's l2: 0.0102236\n",
      "[263]\tvalid_0's l2: 0.0102223\n",
      "[264]\tvalid_0's l2: 0.0102206\n",
      "[265]\tvalid_0's l2: 0.0102175\n",
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's l2: 0.0102027\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データセットを生成する\n",
    "lgb_train = lgb.Dataset(X_train2, y_train2)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "lgbm_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 8,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'subsample' : 0.8,\n",
    "    'reg_alpha' : 1,\n",
    "    'reg_lambda' : 1,\n",
    "    'objective': 'regression',\n",
    "\n",
    "    }\n",
    "\n",
    "# 上記のパラメータでモデルを学習する\n",
    "model = lgb.train(lgbm_params, lgb_train,\n",
    "                  # モデルの評価用データを渡す\n",
    "                  valid_sets=lgb_eval,\n",
    "                  # 最大で 1000 ラウンドまで学習する\n",
    "                  num_boost_round=1000,\n",
    "                  # 10 ラウンド経過しても性能が向上しないときは学習を打ち切る\n",
    "                  early_stopping_rounds=10)\n",
    "\n",
    "# 特徴量の重要度をプロットする\n",
    "lgb.plot_importance(model, figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.733957083678845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# テストデータを予測する\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# auc を計算する\n",
    "mae=mean_absolute_error(np.exp(y_pred),np.exp( y_test))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 除算関数の定義\n",
    "# 左項 / 右項で右項が0の場合1を代入する\n",
    "def protectedDiv(left, right):\n",
    "    eps = 1.0e-7\n",
    "    tmp = np.zeros(len(left))\n",
    "    tmp[np.abs(right) >= eps] = left[np.abs(right) >= eps] / right[np.abs(right) >= eps]\n",
    "    tmp[np.abs(right) < eps] = 1.0\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シード\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# 適合度を最大化するような木構造を個体として定義\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期値の計算\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "def cross_val(model,X,y):\n",
    "    scores = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "#     print(scores)\n",
    "    return np.array(scores).mean()\n",
    "\n",
    "\n",
    "# 学習データの5-fold CVのAUCスコアを評価指標の初期値とする\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "n_features = X_train.shape[1]\n",
    "# 上記のパラメータでモデルを学習する\n",
    "model =RFR()\n",
    "prev_auc = cross_val(model,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# メインループ\n",
    "# resultsに特徴量数、学習データのAUCスコア（5-fold CV）、テストデータのAUCスコアを保持する\n",
    "# exprsに生成された特徴量の表記を保持する\n",
    "results = []\n",
    "exprs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin   \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8252\t0  \t31.0641\t28.366\t300   \t0.349547\t4.28667\t0  \t14 \t2  \t300   \t2.49088\n",
      "1  \t154   \t30.1545\t1  \t31.0641\t29.2421\t154   \t0.370838\t4.31667\t1  \t15 \t1  \t154   \t2.56964\n",
      "2  \t145   \t30.345 \t2  \t31.5826\t28.449 \t145   \t0.493968\t4.55333\t2  \t18 \t1  \t145   \t3.15285\n",
      "3  \t160   \t30.45  \t3  \t31.5826\t28.925 \t160   \t0.612198\t7.01667\t3  \t18 \t1  \t160   \t4.38745\n",
      "4  \t166   \t30.6235\t4  \t32.2731\t29.1703\t166   \t0.718633\t11.25  \t4  \t18 \t2  \t166   \t3.24769\n",
      "5  \t169   \t30.8913\t5  \t34.5372\t28.7783\t169   \t0.96747 \t11.96  \t5  \t20 \t1  \t169   \t2.19509\n",
      "6  \t166   \t31.2113\t6  \t34.5372\t28.7871\t166   \t1.20422 \t11.73  \t6  \t18 \t3  \t166   \t2.40633\n",
      "7  \t152   \t31.9626\t7  \t34.5372\t28.5794\t152   \t1.51475 \t13.3167\t7  \t20 \t3  \t152   \t3.19527\n",
      "8  \t142   \t32.7195\t8  \t34.5372\t28.0946\t142   \t1.5617  \t15.0867\t8  \t21 \t1  \t142   \t2.36061\n",
      "9  \t163   \t33.0971\t9  \t34.6911\t28.5121\t163   \t1.69844 \t15.53  \t9  \t22 \t9  \t163   \t1.9687 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [27:40<45:39:47, 1660.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t153   \t33.1823\t10 \t34.6911\t28.5289\t153   \t1.72638 \t15.3167\t10 \t22 \t1  \t153   \t2.49461\n",
      "1\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin   \tnevals\tstd     \tavg \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8129\t0  \t31.0965\t27.609\t300   \t0.362879\t3.82\t0  \t14 \t2  \t300   \t2.33686\n",
      "1  \t149   \t30.1457\t1  \t31.8663\t29.2014\t149   \t0.377917\t4.10333\t1  \t14 \t1  \t149   \t2.5125 \n",
      "2  \t179   \t30.3308\t2  \t32.362 \t28.7986\t179   \t0.571123\t4.72333\t2  \t12 \t1  \t179   \t2.40003\n",
      "3  \t182   \t30.7906\t3  \t33.9955\t27.9672\t182   \t0.842117\t5.91667\t3  \t14 \t1  \t182   \t2.15168\n",
      "4  \t165   \t31.4844\t4  \t34.2308\t29.0898\t165   \t1.13617 \t6.95333\t4  \t18 \t1  \t165   \t2.6928 \n",
      "5  \t174   \t32.0746\t5  \t34.5987\t29.1984\t174   \t1.41503 \t9.06333\t5  \t18 \t3  \t174   \t4.11655\n",
      "6  \t149   \t32.8995\t6  \t34.6089\t28.8229\t149   \t1.60963 \t12.8633\t6  \t25 \t1  \t149   \t3.70378\n",
      "7  \t162   \t33.1176\t7  \t34.6089\t29.2779\t162   \t1.68317 \t13.9967\t7  \t24 \t2  \t162   \t2.57358\n",
      "8  \t173   \t33.1297\t8  \t34.9122\t29.2005\t173   \t1.76949 \t13.7833\t8  \t23 \t2  \t173   \t2.92741\n",
      "9  \t184   \t33.1344\t9  \t34.9122\t29.0711\t184   \t1.80512 \t13.58  \t9  \t25 \t1  \t184   \t2.9917 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [1:00:47<47:51:54, 1758.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t158   \t33.3517\t10 \t34.9122\t29.1877\t158   \t1.77671 \t13.6433\t10 \t25 \t1  \t158   \t2.83951\n",
      "2\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8152\t0  \t31.0661\t28.0506\t300   \t0.359914\t4.07667\t0  \t14 \t2  \t300   \t2.51611\n",
      "1  \t163   \t30.1291\t1  \t32.1593\t27.9265\t163   \t0.449253\t4.04   \t1  \t16 \t1  \t163   \t2.67178\n",
      "2  \t174   \t30.3093\t2  \t32.4045\t29.0973\t174   \t0.617073\t5.61667\t2  \t16 \t1  \t174   \t3.46743\n",
      "3  \t145   \t30.7482\t3  \t32.7219\t28.415 \t145   \t0.830558\t8.55   \t3  \t18 \t2  \t145   \t4.30048\n",
      "4  \t147   \t31.5166\t4  \t33.2389\t28.8211\t147   \t1.0378  \t7.87333\t4  \t18 \t1  \t147   \t2.90585\n",
      "5  \t177   \t31.8307\t5  \t33.7472\t29.2251\t177   \t1.14956 \t9.32   \t5  \t19 \t2  \t177   \t2.35321\n",
      "6  \t172   \t32.1456\t6  \t35.3141\t29.1413\t172   \t1.21473 \t9.96   \t6  \t17 \t1  \t172   \t2.05387\n",
      "7  \t174   \t32.4134\t7  \t35.3141\t29.136 \t174   \t1.40223 \t10.3867\t7  \t18 \t1  \t174   \t2.3446 \n",
      "8  \t158   \t32.8242\t8  \t35.82  \t29.0861\t158   \t1.58284 \t11.3233\t8  \t20 \t1  \t158   \t2.77827\n",
      "9  \t174   \t33.5298\t9  \t35.9917\t29.0207\t174   \t1.85668 \t13.0067\t9  \t22 \t1  \t174   \t2.94957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [1:34:29<49:30:46, 1837.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t174   \t33.9462\t10 \t36.1155\t29.4039\t174   \t2.01106 \t13.41  \t10 \t22 \t1  \t174   \t2.83817\n",
      "3\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8158\t0  \t31.3122\t28.3207\t300   \t0.355655\t3.84\t0  \t14 \t2  \t300   \t2.24672\n",
      "1  \t180   \t30.0815\t1  \t31.8829\t28.9745\t180   \t0.422505\t4.64\t1  \t14 \t1  \t180   \t3.00839\n",
      "2  \t170   \t30.2741\t2  \t31.8829\t28.9608\t170   \t0.523777\t6.57333\t2  \t18 \t1  \t170   \t4.5707 \n",
      "3  \t174   \t30.4391\t3  \t31.8829\t28.672 \t174   \t0.621642\t7.71333\t3  \t19 \t1  \t174   \t4.69303\n",
      "4  \t145   \t30.6615\t4  \t31.8829\t28.5361\t145   \t0.886157\t5.70667\t4  \t15 \t1  \t145   \t3.91586\n",
      "5  \t161   \t30.8384\t5  \t31.8829\t28.1206\t161   \t1.06555 \t3.95   \t5  \t11 \t1  \t161   \t0.956121\n",
      "6  \t167   \t30.7979\t6  \t31.9305\t27.7063\t167   \t1.15758 \t4.01   \t6  \t10 \t1  \t167   \t0.854342\n",
      "7  \t169   \t30.7771\t7  \t31.9305\t28.3365\t169   \t1.14502 \t4.11333\t7  \t10 \t1  \t169   \t0.887218\n",
      "8  \t157   \t30.875 \t8  \t31.9305\t28.3644\t157   \t1.11227 \t4.07333\t8  \t9  \t1  \t157   \t0.83743 \n",
      "9  \t157   \t30.9358\t9  \t31.9305\t28.1407\t157   \t1.09898 \t3.99667\t9  \t8  \t1  \t157   \t0.670812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [2:09:46<51:14:09, 1921.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t152   \t31.0207\t10 \t31.9305\t28.1284\t152   \t1.05083 \t4.07   \t10 \t10 \t1  \t152   \t0.79902 \n",
      "4\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg   \tgen\tmax   \tmin    \tnevals\tstd     \tavg \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.851\t0  \t31.209\t28.1094\t300   \t0.378314\t3.97\t0  \t14 \t2  \t300   \t2.41849\n",
      "1  \t178   \t30.0872\t1  \t31.5131\t28.9646\t178   \t0.427349\t4.54333\t1  \t18 \t1  \t178   \t2.7943 \n",
      "2  \t157   \t30.2764\t2  \t31.5131\t28.4276\t157   \t0.539713\t4.9    \t2  \t15 \t1  \t157   \t2.80891\n",
      "3  \t172   \t30.4931\t3  \t31.5131\t27.6617\t172   \t0.663204\t5      \t3  \t16 \t1  \t172   \t2.30796\n",
      "4  \t182   \t30.5491\t4  \t31.7374\t28.9856\t182   \t0.780185\t5.03333\t4  \t16 \t1  \t182   \t2.19216\n",
      "5  \t170   \t30.7118\t5  \t31.9958\t27.9781\t170   \t0.873569\t4.33333\t5  \t16 \t1  \t170   \t1.67597\n",
      "6  \t171   \t30.7994\t6  \t31.9958\t28.7278\t171   \t0.848533\t4.07   \t6  \t8  \t2  \t171   \t0.620564\n",
      "7  \t169   \t30.8862\t7  \t31.9958\t28.0702\t169   \t0.952381\t4.07333\t7  \t9  \t2  \t169   \t0.758038\n",
      "8  \t157   \t30.9486\t8  \t31.9958\t27.5672\t157   \t1.04427 \t4.05667\t8  \t9  \t1  \t157   \t0.83274 \n",
      "9  \t149   \t30.9877\t9  \t31.9958\t28.7341\t149   \t1.09509 \t4.12667\t9  \t9  \t1  \t149   \t0.862529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [2:51:29<55:18:13, 2095.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t161   \t30.9566\t10 \t31.9958\t29.0632\t161   \t1.07075 \t4.06   \t10 \t9  \t1  \t161   \t0.763588\n",
      "5\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8477\t0  \t32.0837\t28.6293\t300   \t0.372849\t4.32667\t0  \t15 \t2  \t300   \t2.68948\n",
      "1  \t163   \t30.1411\t1  \t32.7644\t28.2497\t163   \t0.548469\t4.64333\t1  \t11 \t2  \t163   \t2.21422\n",
      "2  \t158   \t30.5254\t2  \t33.4836\t29.0451\t158   \t0.853931\t4.86   \t2  \t15 \t1  \t158   \t1.96987\n",
      "3  \t157   \t31.252 \t3  \t33.7566\t27.5835\t157   \t1.31494 \t5.21667\t3  \t13 \t1  \t157   \t1.8211 \n",
      "4  \t161   \t32.055 \t4  \t33.9976\t29.3622\t161   \t1.35538 \t5.65   \t4  \t14 \t1  \t161   \t1.62301\n",
      "5  \t159   \t32.2485\t5  \t34.4744\t29.1273\t159   \t1.59505 \t5.42667\t5  \t14 \t1  \t159   \t1.73914\n",
      "6  \t164   \t32.3432\t6  \t34.4744\t28.7224\t164   \t1.68317 \t6.23   \t6  \t13 \t1  \t164   \t2.06166\n",
      "7  \t164   \t32.7446\t7  \t34.6154\t28.7242\t164   \t1.68087 \t7.50667\t7  \t13 \t1  \t164   \t2.00747\n",
      "8  \t159   \t32.6255\t8  \t34.6154\t28.0821\t159   \t1.92135 \t7.43   \t8  \t15 \t1  \t159   \t2.00127\n",
      "9  \t173   \t32.6576\t9  \t34.6154\t28.1226\t173   \t2.00995 \t7.08667\t9  \t12 \t1  \t173   \t1.73565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [3:31:34<57:08:51, 2188.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t166   \t32.7926\t10 \t34.6154\t28.6726\t166   \t1.97881 \t6.91667\t10 \t13 \t1  \t166   \t1.6542 \n",
      "6\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8278\t0  \t31.5756\t27.8913\t300   \t0.355288\t4.26667\t0  \t14 \t2  \t300   \t2.69732\n",
      "1  \t153   \t30.1075\t1  \t31.5756\t28.9431\t153   \t0.46044 \t4.90333\t1  \t17 \t1  \t153   \t3.19802\n",
      "2  \t169   \t30.2924\t2  \t31.6545\t27.9182\t169   \t0.593011\t5.17333\t2  \t18 \t1  \t169   \t3.20156\n",
      "3  \t165   \t30.592 \t3  \t31.8108\t28.9111\t165   \t0.756089\t4.07   \t3  \t14 \t1  \t165   \t2.28439\n",
      "4  \t175   \t30.7659\t4  \t31.8212\t28.7714\t175   \t0.885713\t3.19333\t4  \t12 \t1  \t175   \t1.08748\n",
      "5  \t172   \t30.9175\t5  \t31.8212\t29.0071\t172   \t0.880849\t3.08333\t5  \t7  \t1  \t172   \t0.574214\n",
      "6  \t162   \t30.9514\t6  \t31.892 \t28.9396\t162   \t0.941068\t3.07667\t6  \t9  \t1  \t162   \t0.526741\n",
      "7  \t192   \t30.8061\t7  \t32.0828\t28.7595\t192   \t0.977946\t3.19   \t7  \t9  \t1  \t192   \t0.966385\n",
      "8  \t164   \t30.8704\t8  \t32.0828\t28.8503\t164   \t1.01573 \t3.13   \t8  \t8  \t1  \t164   \t0.720948\n",
      "9  \t164   \t31.0497\t9  \t32.0828\t29.0366\t164   \t0.983645\t3.02333\t9  \t7  \t1  \t164   \t0.419669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [4:27:23<65:32:09, 2536.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t174   \t30.9927\t10 \t32.0828\t28.9933\t174   \t1.04104 \t3.18333\t10 \t9  \t1  \t174   \t0.932589\n",
      "7\n",
      "   \t      \t                    fitness                    \t                      size                     \n",
      "   \t      \t-----------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd    \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8386\t0  \t31.5038\t28.0738\t300   \t0.35881\t4.08667\t0  \t15 \t2  \t300   \t2.44796\n",
      "1  \t173   \t30.1285\t1  \t31.7329\t28.0673\t173   \t0.440859\t4.79   \t1  \t14 \t1  \t173   \t2.404  \n",
      "2  \t169   \t30.3023\t2  \t31.7329\t29.0563\t169   \t0.598221\t4.34667\t2  \t11 \t1  \t169   \t1.61033\n",
      "3  \t166   \t30.6457\t3  \t32.4044\t29.099 \t166   \t0.796634\t4.36667\t3  \t11 \t1  \t166   \t1.43256\n",
      "4  \t158   \t31.0042\t4  \t32.4044\t28.9562\t158   \t0.904466\t4.44667\t4  \t8  \t1  \t158   \t0.990196\n",
      "5  \t154   \t31.1499\t5  \t33.0436\t28.909 \t154   \t1.06035 \t4.88   \t5  \t11 \t1  \t154   \t1.27499 \n",
      "6  \t160   \t31.4075\t6  \t33.0436\t28.6588\t160   \t1.16629 \t5.30333\t6  \t11 \t1  \t160   \t1.34337 \n",
      "7  \t169   \t31.5059\t7  \t33.4656\t29.1796\t169   \t1.2665  \t5.15667\t7  \t11 \t1  \t169   \t1.29825 \n",
      "8  \t183   \t31.707 \t8  \t33.4656\t28.9761\t183   \t1.37099 \t5.12667\t8  \t11 \t1  \t183   \t1.22091 \n",
      "9  \t165   \t31.9855\t9  \t33.4656\t29.2209\t165   \t1.38817 \t5.15667\t9  \t10 \t1  \t165   \t1.26443 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [5:00:54<60:47:40, 2378.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t184   \t31.9934\t10 \t33.4656\t28.9825\t184   \t1.41663 \t5.10667\t10 \t10 \t1  \t184   \t1.28917 \n",
      "8\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8429\t0  \t31.0564\t27.9774\t300   \t0.382578\t4.20333\t0  \t15 \t2  \t300   \t2.48234\n",
      "1  \t148   \t30.1688\t1  \t31.0564\t27.9234\t148   \t0.393931\t4.64333\t1  \t15 \t1  \t148   \t2.6146 \n",
      "2  \t169   \t30.2686\t2  \t31.5313\t28.3423\t169   \t0.551622\t4.64   \t2  \t19 \t1  \t169   \t2.44889\n",
      "3  \t162   \t30.472 \t3  \t31.5313\t28.9031\t162   \t0.616192\t4.62   \t3  \t15 \t1  \t162   \t2.01385\n",
      "4  \t156   \t30.6601\t4  \t34.0132\t28.9131\t156   \t0.725889\t4.82667\t4  \t12 \t1  \t156   \t1.733  \n",
      "5  \t170   \t30.9196\t5  \t34.1129\t28.9047\t170   \t1.0586  \t6.10667\t5  \t13 \t1  \t170   \t1.34237\n",
      "6  \t174   \t31.6379\t6  \t34.6181\t28.9982\t174   \t1.61505 \t7.04667\t6  \t17 \t2  \t174   \t1.70621\n",
      "7  \t159   \t32.5697\t7  \t34.6181\t29.1883\t159   \t1.82656 \t8.23333\t7  \t16 \t2  \t159   \t1.8918 \n",
      "8  \t176   \t32.2073\t8  \t34.6181\t29.0475\t176   \t2.07622 \t8.45667\t8  \t14 \t1  \t176   \t1.85152\n",
      "9  \t159   \t32.6349\t9  \t34.6181\t29.142 \t159   \t2.04428 \t8.64667\t9  \t16 \t2  \t159   \t2.15758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [5:42:53<61:11:57, 2421.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t147   \t32.8919\t10 \t34.6181\t29.0821\t147   \t2.08009 \t8.47333\t10 \t16 \t1  \t147   \t2.07909\n",
      "9\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8421\t0  \t31.3176\t28.0122\t300   \t0.352749\t4.50333\t0  \t15 \t2  \t300   \t2.91261\n",
      "1  \t178   \t30.1058\t1  \t31.3176\t29.0176\t178   \t0.393364\t4.96667\t1  \t15 \t1  \t178   \t3.15683\n",
      "2  \t184   \t30.2298\t2  \t31.7726\t27.9303\t184   \t0.585315\t4.86   \t2  \t16 \t1  \t184   \t2.85197\n",
      "3  \t186   \t30.6356\t3  \t31.9307\t28.8027\t186   \t0.778697\t4.11333\t3  \t15 \t1  \t186   \t2.29648\n",
      "4  \t154   \t30.9269\t4  \t32.1507\t28.9009\t154   \t0.91821 \t3.95667\t4  \t13 \t2  \t154   \t1.75351\n",
      "5  \t165   \t31.0139\t5  \t32.1507\t28.8721\t165   \t0.959977\t3.98   \t5  \t9  \t2  \t165   \t1.70086\n",
      "6  \t159   \t31.0488\t6  \t32.1507\t28.7663\t159   \t1.03602 \t4.17   \t6  \t13 \t1  \t159   \t1.87112\n",
      "7  \t190   \t30.9235\t7  \t32.1507\t28.7877\t190   \t1.05617 \t6.02667\t7  \t12 \t1  \t190   \t2.01807\n",
      "8  \t155   \t31.1391\t8  \t32.1655\t29.0704\t155   \t1.0998  \t6.95667\t8  \t14 \t1  \t155   \t1.43345\n",
      "9  \t178   \t31.0579\t9  \t32.2405\t27.9645\t178   \t1.09586 \t7.19667\t9  \t13 \t3  \t178   \t1.37283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [6:25:29<61:32:25, 2461.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t179   \t31.1172\t10 \t32.2405\t28.9601\t179   \t1.07701 \t7.56667\t10 \t13 \t1  \t179   \t1.7375 \n",
      "10\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax    \tmin    \tnevals\tstd     \tavg \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8135\t0  \t30.8193\t28.8952\t300   \t0.334345\t3.96\t0  \t14 \t2  \t300   \t2.46274\n",
      "1  \t165   \t30.1106\t1  \t32.6009\t28.8002\t165   \t0.384439\t4.96\t1  \t17 \t1  \t165   \t3.31337\n",
      "2  \t155   \t30.2815\t2  \t32.8547\t28.9988\t155   \t0.540287\t5.86333\t2  \t16 \t1  \t155   \t3.68755\n",
      "3  \t164   \t30.5042\t3  \t33.001 \t28.9366\t164   \t0.77467 \t8.65   \t3  \t17 \t1  \t164   \t3.34278\n",
      "4  \t171   \t31.2384\t4  \t33.8967\t28.4293\t171   \t1.18248 \t8.68667\t4  \t16 \t1  \t171   \t2.49035\n",
      "5  \t164   \t31.7258\t5  \t33.8967\t28.2636\t164   \t1.33767 \t9.21333\t5  \t18 \t1  \t164   \t2.73639\n",
      "6  \t180   \t31.8171\t6  \t33.8967\t29.2965\t180   \t1.47484 \t9.72333\t6  \t20 \t1  \t180   \t2.8308 \n",
      "7  \t176   \t32.1414\t7  \t33.8967\t29.0436\t176   \t1.57022 \t10.58  \t7  \t20 \t1  \t176   \t2.69015\n",
      "8  \t158   \t32.5371\t8  \t34.0652\t29.2926\t158   \t1.65508 \t11.3867\t8  \t21 \t1  \t158   \t2.5253 \n",
      "9  \t161   \t32.5168\t9  \t34.1313\t29.142 \t161   \t1.69484 \t11.7267\t9  \t21 \t2  \t161   \t2.34065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [7:12:38<63:34:52, 2571.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t163   \t32.5909\t10 \t34.1313\t28.8893\t163   \t1.64828 \t11.6033\t10 \t17 \t3  \t163   \t2.09904\n",
      "11\n",
      "   \t      \t                    fitness                     \t                      size                     \n",
      "   \t      \t------------------------------------------------\t-----------------------------------------------\n",
      "gen\tnevals\tavg    \tgen\tmax   \tmin    \tnevals\tstd     \tavg    \tgen\tmax\tmin\tnevals\tstd    \n",
      "0  \t300   \t29.8225\t0  \t31.207\t27.6816\t300   \t0.393946\t4.16333\t0  \t14 \t2  \t300   \t2.41592\n",
      "1  \t158   \t30.1511\t1  \t31.8623\t28.0402\t158   \t0.436335\t5.04333\t1  \t16 \t1  \t158   \t2.56414\n",
      "2  \t176   \t30.3045\t2  \t32.0112\t28.7097\t176   \t0.604046\t4.71   \t2  \t12 \t1  \t176   \t2.35214\n",
      "3  \t161   \t30.6326\t3  \t32.0112\t28.9987\t161   \t0.814162\t3.56667\t3  \t10 \t1  \t161   \t1.62447\n",
      "4  \t177   \t30.8643\t4  \t32.0112\t28.8923\t177   \t0.943793\t3.04333\t4  \t7  \t1  \t177   \t0.566971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-70f79d949aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#     print( toolbox, mstats,hof)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meaSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalloffame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-70f79d949aaa>\u001b[0m in \u001b[0;36meval_genfeat\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnew_feat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX_train_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_feat_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 評価、選択、交叉、突然変異の設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-71234c4f054e>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import operator,math, random, time\n",
    "j=0\n",
    "for i in tqdm(range(100)):\n",
    "    print(j)\n",
    "    j+=1\n",
    "    # 構文木として利用可能な演算の定義\n",
    "    pset = gp.PrimitiveSet(\"MAIN\", n_features)\n",
    "    pset.addPrimitive(operator.add, 2)\n",
    "    pset.addPrimitive(operator.sub, 2)\n",
    "    pset.addPrimitive(operator.mul, 2)\n",
    "    pset.addPrimitive(protectedDiv, 2)\n",
    "    pset.addPrimitive(operator.neg, 1)\n",
    "    pset.addPrimitive(np.cos, 1)\n",
    "    pset.addPrimitive(np.sin, 1)\n",
    "    pset.addPrimitive(np.tan, 1)\n",
    "\n",
    "    # 関数のデフォルト値の設定\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3)\n",
    "    toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"compile\", gp.compile, pset=pset)\n",
    "\n",
    "    # 評価関数の設定\n",
    "    # 新しく生成した変数を元の変数に加えて5-fold CVを求める\n",
    "    def eval_genfeat(individual):\n",
    "        func = toolbox.compile(expr=individual)\n",
    "        features_train = [X_train[:,i] for i in range(n_features)]\n",
    "        new_feat_train = func(*features_train)\n",
    "        X_train_tmp = np.c_[X_train, new_feat_train]\n",
    "        return np.mean(cross_val(model, X_train_tmp, y_train)),\n",
    "\n",
    "    # 評価、選択、交叉、突然変異の設定\n",
    "    # 選択はサイズ10のトーナメント方式、交叉は1点交叉、突然変異は深さ2のランダム構文木生成と定義\n",
    "    toolbox.register(\"evaluate\", eval_genfeat)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=10)\n",
    "    toolbox.register(\"mate\", gp.cxOnePoint)\n",
    "    toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n",
    "    toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n",
    "\n",
    "    # 構文木の制約の設定\n",
    "    # 交叉や突然変異で深さ5以上の木ができないようにする\n",
    "    toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=5))\n",
    "    toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=5)) \n",
    "\n",
    "    # 世代ごとの個体とベスト解を保持するクラスの生成\n",
    "    pop = toolbox.population(n=300)\n",
    "    hof = tools.HallOfFame(1)\n",
    "\n",
    "    # 統計量の表示設定\n",
    "    stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats_size = tools.Statistics(len)\n",
    "    mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n",
    "    mstats.register(\"avg\", np.mean)\n",
    "    mstats.register(\"std\", np.std)\n",
    "    mstats.register(\"min\", np.min)\n",
    "    mstats.register(\"max\", np.max)\n",
    "\n",
    "    # 進化の実行\n",
    "    # 交叉確率50%、突然変異確率10%、10世代まで進化\n",
    "    start_time = time.time()\n",
    "#     print( toolbox, mstats,hof)\n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, 0.5, 0.1, 10, stats=mstats, halloffame=hof, verbose=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # ベスト解とAUCの保持\n",
    "    best_expr = hof[0]\n",
    "    best_auc = mstats.compile(pop)[\"fitness\"][\"max\"]\n",
    "\n",
    "    # 5-fold CVのAUCスコアが前ステップのAUCを超えていた場合\n",
    "    # 生成変数を学習、テストデータに追加し、ベストAUCを更新する\n",
    "    if prev_auc > best_auc:\n",
    "        # 生成変数の追加\n",
    "        func = toolbox.compile(expr=best_expr)\n",
    "        features_train = [X_train[:,i] for i in range(n_features)]\n",
    "        features_test = [X_test[:,i] for i in range(n_features)]\n",
    "        new_feat_train = func(*features_train)\n",
    "        new_feat_test = func(*features_test)\n",
    "        X_train = np.c_[X_train, new_feat_train]\n",
    "        X_test = np.c_[X_test, new_feat_test]\n",
    "\n",
    "        ### テストAUCの計算（プロット用）\n",
    "        model.fit(X_train, y_train)\n",
    "        train_auc = cross_val(y_train, model.predict_proba(X_train)[:,1])\n",
    "        test_auc = cross_val(y_test, model.predict_proba(X_test)[:,1])\n",
    "\n",
    "        # ベストAUCの更新と特徴量数の加算\n",
    "        prev_auc = best_auc\n",
    "        n_features += 1\n",
    "\n",
    "        # 表示と出力用データの保持\n",
    "        print(n_features, best_auc, train_auc, test_auc, end_time - start_time)\n",
    "        results.append([n_features, best_auc, train_auc, test_auc])\n",
    "        exprs.append(best_expr)\n",
    "\n",
    "        # 変数追加後の特徴量数が30を超えた場合break\n",
    "        if n_features >= 30:\n",
    "            break\n",
    "        print(prev_auc,best_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成した構文木の出力\n",
    "print()\n",
    "print(\"### Generated feature expression\")\n",
    "for expr in exprs:\n",
    "    print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-76eae1d95b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2525\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rg = RFR(n_jobs=-1, random_state=2525)\n",
    " \n",
    "rg.fit(train_X,train_y)\n",
    "print(rg.score(valid_X,valid_y))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(rg.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "fti = rg.feature_importances_\n",
    "\n",
    "print('Feature Importances:')\n",
    "for i,feat in enumerate(valid_X.columns):\n",
    "    print('\\t{0:10s} : {1:>.6f}'.format(feat, fti[i]))\n",
    "col_names = test.columns.values\n",
    "col_names_ = col_names[np.argsort(rg.feature_importances_)[::-1]]\n",
    "col_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.764094288724273, 22.792014845341466, 23.487210364564227, 23.352820410841307, 22.616116417087895]\n",
      "23.00245126531183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "model=lgb.LGBMRegressor()\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "#noadditional 23.625521293118492\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 特徴量を選択して、複数のモデルで精度を調査する\n",
    "from scipy.stats import mstats\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from sklearn import metrics,  feature_selection, ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\n",
    "# from imblearn import under_sampling, over_sampling\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "base_color = 'darkturquoise'\n",
    "base_color2 = 'gray'\n",
    "\n",
    "def generate_cmap(colors):\n",
    "    \n",
    "    values = range(len(colors))\n",
    "    vmax = np.ceil(np.max(values))\n",
    "    color_list = []\n",
    "    for v, c in zip(values, colors):\n",
    "        color_list.append((v/vmax, c))\n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list('custom_cmap', color_list)\n",
    "cm = generate_cmap([base_color2, 'white', base_color])\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    \"\"\"RMSE (Root Mean Square Error: 平均二乗誤差平方根) を計算する関数\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "params={'booster': 'dart', \n",
    "        'alpha': 0.009385870161072372, \n",
    "        'max_depth': 9, 'eta': 2.3698818355249718e-07,\n",
    "        'gamma': 3.167530221746867e-05, \n",
    "        'grow_policy': 'lossguide',\n",
    "        'sample_type': 'weighted',\n",
    "        'normalize_type': 'forest',\n",
    "        'rate_drop': 3.1207262366715483e-08,\n",
    "        'skip_drop': 1.2650261386504368e-05}\n",
    "\n",
    "\n",
    "models = [\n",
    " \n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    " \n",
    "    #Gaussian Processes\n",
    "#     gaussian_process.GaussianProcessRegressor(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.Ridge(),\n",
    "\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeRegressor(),\n",
    "    tree.ExtraTreeRegressor(),\n",
    " \n",
    "    #xgboost\n",
    "    xgb. XGBRegressor(),\n",
    "    lgb.LGBMRegressor(),\n",
    "#     cat.CatBoostRegressor(),\n",
    "    \n",
    "]\n",
    " \n",
    "df_compare = pd.DataFrame(columns=['name', 'train_rmse', 'valid_rmse', 'time'])\n",
    "score_funcs = {\n",
    "    'rmse': make_scorer(rmse_score),\n",
    "}\n",
    "\n",
    "for model in tqdm(models):\n",
    "    \n",
    "    name = model.__class__.__name__\n",
    "    \n",
    "    cv_rlts = model_selection.cross_validate(model,X,y, scoring=score_funcs, cv=10, return_train_score=True)\n",
    " \n",
    "    for i in range(10):\n",
    "        s = pd.Series([name, cv_rlts['train_rmse'][i], cv_rlts['test_rmse'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name=name+str(i))\n",
    "        df_compare = df_compare.append(s)\n",
    "        \n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=df_compare, y='name', x='valid_rmse', orient='h', color=base_color, linewidth=0.5, width=0.5)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = X.columns.tolist()\n",
    " \n",
    "# positive_cnt = int(df['salary'].sum())\n",
    "\n",
    "feature_importance_models = [\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.ExtraTreesRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(),\n",
    "    tree.DecisionTreeRegressor(),\n",
    "    xgb.XGBRegressor(),\n",
    "    lgb.LGBMRegressor()\n",
    "]\n",
    " \n",
    "scoring = ['rsme']\n",
    "df_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\n",
    "df_rfe_cols_cnt['cnt'] = 0\n",
    " \n",
    "for i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n",
    "    \n",
    "    rfe = feature_selection.RFECV(model, step=3)\n",
    "    rfe.fit(X, y)\n",
    "#     print(rfe.get_support())\n",
    "    rfe_cols = X[cols].columns.values[rfe.get_support()]\n",
    "    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n",
    "    \n",
    "df_rfe_cols_cnt.plot(kind='bar', color=base_color, figsize=(15, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = df_rfe_cols_cnt[df_rfe_cols_cnt['cnt'] < 4].index\n",
    "X=X.drop(x_cols,axis=1)\n",
    "test=test.drop(x_cols,axis=1)\n",
    "train_X, valid_X,train_y, valid_y = train_test_split(X,y,test_size=0.2,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_list = ['area', 'sex', 'partner','education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class RidgeCV():\n",
    "    model_cls = Ridge\n",
    "\n",
    "    def __init__(self, n_trials=100):\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.reset_index(drop=True)\n",
    "            y = y.reset_index(drop=True)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self, n_trials=self.n_trials)\n",
    "        self.best_trial = study.best_trial\n",
    "\n",
    "        print()\n",
    "        print(\"Best score:\", round(self.best_trial.value, 2))\n",
    "        print(\"Best params:\", self.best_trial.params)\n",
    "        print()\n",
    "\n",
    "        self.best_model = self.model_cls(**self.best_trial.params)\n",
    "        self.best_model.fit(self.X, self.y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = pd.DataFrame(X.values.reshape(1, -1))\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        return self.best_model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X)\n",
    "            y = pd.DataFrame(y)\n",
    "\n",
    "        return self.best_model.score(X, y)\n",
    "\n",
    "    def kfold_cv(self, model, splits=5):\n",
    "        scores = []\n",
    "\n",
    "        kf = KFold(n_splits=splits, shuffle=True)\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            scores.append(r2_score(model.predict(X_test), y_test))\n",
    "\n",
    "        score = np.array(scores).mean()\n",
    "        return score\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class XGBRegressorCV(RidgeCV):\n",
    "    model_cls = xgb.XGBRegressor\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        booster = trial.suggest_categorical('booster', ['gbtree', 'dart'])\n",
    "        alpha = trial.suggest_loguniform('alpha', 1e-8, 1.0)\n",
    "\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "        eta = trial.suggest_loguniform('eta', 1e-8, 1.0)\n",
    "        gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "        grow_policy = trial.suggest_categorical(\n",
    "            'grow_policy', ['depthwise', 'lossguide'])\n",
    "\n",
    "        if booster == 'gbtree':\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy)\n",
    "        elif booster == 'dart':\n",
    "            sample_type = trial.suggest_categorical('sample_type',\n",
    "                                                    ['uniform', 'weighted'])\n",
    "            normalize_type = trial.suggest_categorical('normalize_type',\n",
    "                                                       ['tree', 'forest'])\n",
    "            rate_drop = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
    "            skip_drop = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
    "            model = self.model_cls(silent=1, booster=booster,\n",
    "                                   alpha=alpha, max_depth=max_depth, eta=eta,\n",
    "                                   gamma=gamma, grow_policy=grow_policy,\n",
    "                                   sample_type=sample_type,\n",
    "                                   normalize_type=normalize_type,\n",
    "                                   rate_drop=rate_drop, skip_drop=skip_drop)\n",
    "\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "xgbr = XGBRegressorCV(n_trials=100)\n",
    "# xgbr.fit(train_X, train_y)\n",
    "\n",
    "# pred_y=xgbr.predict(valid_X)\n",
    "# score=mean_absolute_error(np.exp(valid_y),np.exp(xgbr.predict(valid_X)))\n",
    "# print(f'MAE:{score:4f}')\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "model = XGBRegressorCV(n_trials=10)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "#MAE:22.821581(drop)dummy\n",
    "#MAE:22.753628(nodrop)dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM+optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "def min_max_normalization(x):\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    x_norm = (x - x_min) / ( x_max - x_min)\n",
    "    return x_norm\n",
    "def min_max_renormalization(x1,x2):\n",
    "    x_min = x2.min()\n",
    "    x_max = x2.max()\n",
    "#     x_norm = (x2 - x_min) / ( x_max - x_min)\n",
    "    x=x1*  ( x_max - x_min)+x_min\n",
    "    return x\n",
    "\n",
    "import lightgbm as lgb\n",
    "# from sklearn.metrics import log_loss\n",
    "cat_list = ['area', 'sex', 'partner','education']\n",
    "class LGBRegressorCV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'eval_metric':'mae',\n",
    "            'booster':trial.suggest_categorical('booster',['gbtree','gblinear']),\n",
    "            'loss_function': 'fair',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "#             'depth' : trial.suggest_int('depth', 4, 25),  \n",
    "            'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50),\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressorCV(n_trials=20)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "# model = XGBRegressorCV(n_trials=20)\n",
    "scores = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(mean_absolute_error(np.exp(model.predict(X_test)),np.exp( y_test)))\n",
    "print(scores)\n",
    "print(np.array(scores).mean())\n",
    "# checktesty=valid_y.sort_values()\n",
    "# checktestX=valid_X.ix[list(checktesty.index)]\n",
    "# checktesty=checktesty.reset_index(drop=True)#これと\n",
    "\n",
    "# checkpred=pd.DataFrame(model.predict(checktestX))#これ\n",
    "# check=pd.concat([checktesty,checkpred], axis=1)\n",
    "# check.columns=[\"actual\",\"predict\"]\n",
    "# check.plot(alpha=0.5)\n",
    "#MAE:23.144563 (nodrop)dummy\n",
    "#MAE:23.742147(drop)dummy\n",
    "#MAE:23.902507nodummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "class LGBRegressor1CV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'xentropy loss',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 8, 20), \n",
    "#             'depth':16,\n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressor1CV(n_trials=40)\n",
    "train_ylgm=min_max_normalization(train_y)\n",
    "model.fit(train_X,train_ylgm)\n",
    "\n",
    "pred_y=min_max_renormalization(model.predict(valid_X),train_y)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LGBRegressor2CV(RidgeCV):\n",
    "    model_cls = lgb.LGBMRegressor\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            \n",
    "            'loss_function': 'regression_l1',\n",
    "            'iterations' : trial.suggest_int('iterations', 50, 400),                      \n",
    "            'depth' : trial.suggest_int('depth', 4, 25),                                      \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(**params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "\n",
    "model=LGBRegressor2CV(n_trials=40)\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost+optuna(non recomended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoost\n",
    "# from sklearn.metrics import log_loss\n",
    "\n",
    "class CatRegressorCV(RidgeCV):\n",
    "    model_cls = CatBoost\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'loss_function': 'MAE',\n",
    "#             'iterations' : trial.suggest_int('iterations', 50, 300),                      \n",
    "            'depth' : trial.suggest_int('depth', 6, 10),                                      \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),               \n",
    "            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n",
    "            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "            'od_wait' :trial.suggest_int('od_wait', 10, 50)\n",
    "    }\n",
    "        model=self.model_cls(params)\n",
    "        score = self.kfold_cv(model)\n",
    "        return score\n",
    "# model=CatRegressorCV(n_trials=10)\n",
    "model=CatBoost({'depth': 7, 'learning_rate': 0.074638569770399, 'random_strength': 59, 'bagging_temperature': 0.011038111194790014, 'od_type': 'IncToDec', 'od_wait': 12})\n",
    "# model=CatBoost({'depth':16,'learning_rate': 0.05235680460545009, 'random_strength': 68, 'bagging_temperature': 0.5832808626920046, 'od_type': 'IncToDec', 'od_wait': 10, })\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "pred_y=model.predict(valid_X)\n",
    "score=mean_absolute_error(np.exp(valid_y),np.exp(model.predict(valid_X)))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engeneering for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "categorical_variable_names = [\"position\",\"sex\",\"education\"]\n",
    "# カテゴリ変数に一括変換\n",
    "# x_dummy = pd.get_dummies(df[categorical_variable_names], drop_first=True,columns=categorical_variable_names)\n",
    "# X_nn=df.drop(categorical_variable_names, axis=1)\n",
    "X_nn=df\n",
    "sscaler =StandardScaler()\n",
    "sscaler.fit(X_nn)  \n",
    "\n",
    "x_datas_std =sscaler.transform(X_nn)\n",
    "x_datas_std = pd.DataFrame(x_datas_std, columns=X_nn.columns)\n",
    "X_nn= pd.concat([x_datas_std,], axis=1)\n",
    "\n",
    "\n",
    "train_X_nn=X_nn.drop(\"id\",axis=1)\n",
    "test_nn=X_nn[X_nn.salary.isnull()].drop(\"id\",axis=1)\n",
    "test_nn= test_nn.drop([\"salary\",],axis=1)\n",
    "train_X_nn= train_X_nn.dropna().drop([\"salary\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>1.027084</td>\n",
       "      <td>1.544468</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>1.097992</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>0.818742</td>\n",
       "      <td>-0.243349</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>-0.192567</td>\n",
       "      <td>0.292875</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>0.069682</td>\n",
       "      <td>1.584501</td>\n",
       "      <td>-0.539406</td>\n",
       "      <td>0.283116</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.070094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.643375</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>0.145629</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>1.023458</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.691715</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-0.992122</td>\n",
       "      <td>-0.753363</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.755483</td>\n",
       "      <td>-0.737849</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.678180</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>-0.950787</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-0.567845</td>\n",
       "      <td>-0.516979</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-0.491214</td>\n",
       "      <td>-0.537098</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.760128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.036941</td>\n",
       "      <td>1.470845</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>-0.982563</td>\n",
       "      <td>-0.771663</td>\n",
       "      <td>-0.234012</td>\n",
       "      <td>-1.293932</td>\n",
       "      <td>0.447636</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.878731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>1.463169</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>1.249976</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>-0.388501</td>\n",
       "      <td>0.332472</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>0.876595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>-0.996214</td>\n",
       "      <td>-1.130761</td>\n",
       "      <td>0.808237</td>\n",
       "      <td>1.009041</td>\n",
       "      <td>0.998734</td>\n",
       "      <td>0.708483</td>\n",
       "      <td>-0.085928</td>\n",
       "      <td>-1.145594</td>\n",
       "      <td>-0.840184</td>\n",
       "      <td>-0.086690</td>\n",
       "      <td>0.036335</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>-0.902451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>-0.176420</td>\n",
       "      <td>-0.286387</td>\n",
       "      <td>-1.621326</td>\n",
       "      <td>-0.991040</td>\n",
       "      <td>-1.001267</td>\n",
       "      <td>-0.708058</td>\n",
       "      <td>1.707344</td>\n",
       "      <td>-0.584697</td>\n",
       "      <td>-1.143269</td>\n",
       "      <td>-0.841217</td>\n",
       "      <td>1.500567</td>\n",
       "      <td>-0.833279</td>\n",
       "      <td>-0.451760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       position       age      area       sex   partner  num_child  education  \\\n",
       "0     -0.176420  1.027084  1.544468  1.009041  0.998734   0.708483  -0.085928   \n",
       "1      0.643375 -0.192567  0.292875 -0.991040 -1.001267  -0.708058  -0.982563   \n",
       "2      0.643375  0.276529  0.145629 -0.991040 -1.001267  -0.708058   0.810708   \n",
       "3     -0.996214 -1.036941  1.691715  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "4     -0.996214 -0.755483 -0.737849  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "...         ...       ...       ...       ...       ...        ...        ...   \n",
       "20995 -0.996214 -0.567845 -0.516979  1.009041 -1.001267  -0.708058  -0.085928   \n",
       "20996 -0.996214 -1.036941  1.470845  1.009041 -1.001267  -0.708058  -0.982563   \n",
       "20997  1.463169  0.276529  1.249976  1.009041 -1.001267  -0.708058   0.810708   \n",
       "20998 -0.996214 -1.130761  0.808237  1.009041  0.998734   0.708483  -0.085928   \n",
       "20999 -0.176420 -0.286387 -1.621326 -0.991040 -1.001267  -0.708058   1.707344   \n",
       "\n",
       "       service_length  study_time   commute  overtime  familiy_num  \\\n",
       "0            1.097992   -0.537098  0.818742 -0.243349     0.832908   \n",
       "1            0.069682    1.584501 -0.539406  0.283116    -0.833279   \n",
       "2            0.163165    0.069073 -0.992122  1.023458    -0.833279   \n",
       "3           -0.771663   -0.234012 -0.992122 -0.753363    -0.833279   \n",
       "4           -0.678180   -0.234012 -1.293932 -0.950787    -0.833279   \n",
       "...               ...         ...       ...       ...          ...   \n",
       "20995       -0.491214   -0.537098 -1.293932  0.793129    -0.833279   \n",
       "20996       -0.771663   -0.234012 -1.293932  0.447636    -0.833279   \n",
       "20997        0.163165    0.069073 -0.388501  0.332472    -0.833279   \n",
       "20998       -1.145594   -0.840184 -0.086690  0.036335     0.832908   \n",
       "20999       -0.584697   -1.143269 -0.841217  1.500567    -0.833279   \n",
       "\n",
       "       agexposition  \n",
       "0         -0.008975  \n",
       "1          0.070094  \n",
       "2          0.307300  \n",
       "3         -0.878731  \n",
       "4         -0.807569  \n",
       "...             ...  \n",
       "20995     -0.760128  \n",
       "20996     -0.878731  \n",
       "20997      0.876595  \n",
       "20998     -0.902451  \n",
       "20999     -0.451760  \n",
       "\n",
       "[21000 rows x 13 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16800 samples, validate on 4200 samples\n",
      "Epoch 1/100\n",
      "16800/16800 [==============================] - ETA: 31s - loss: 32.925 - ETA: 13s - loss: 24.315 - ETA: 9s - loss: 16.529 - ETA: 6s - loss: 12.08 - ETA: 5s - loss: 9.5510 - ETA: 4s - loss: 7.818 - ETA: 4s - loss: 6.887 - ETA: 3s - loss: 6.059 - ETA: 3s - loss: 5.394 - ETA: 3s - loss: 4.912 - ETA: 3s - loss: 4.621 - ETA: 3s - loss: 4.368 - ETA: 2s - loss: 4.060 - ETA: 2s - loss: 3.879 - ETA: 2s - loss: 3.705 - ETA: 2s - loss: 3.481 - ETA: 2s - loss: 3.279 - ETA: 2s - loss: 3.098 - ETA: 2s - loss: 2.938 - ETA: 2s - loss: 2.839 - ETA: 2s - loss: 2.749 - ETA: 2s - loss: 2.664 - ETA: 2s - loss: 2.587 - ETA: 2s - loss: 2.509 - ETA: 2s - loss: 2.438 - ETA: 1s - loss: 2.339 - ETA: 1s - loss: 2.276 - ETA: 1s - loss: 2.218 - ETA: 1s - loss: 2.162 - ETA: 1s - loss: 2.109 - ETA: 1s - loss: 2.035 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.924 - ETA: 1s - loss: 1.882 - ETA: 1s - loss: 1.823 - ETA: 1s - loss: 1.785 - ETA: 1s - loss: 1.749 - ETA: 1s - loss: 1.698 - ETA: 1s - loss: 1.665 - ETA: 0s - loss: 1.634 - ETA: 0s - loss: 1.588 - ETA: 0s - loss: 1.559 - ETA: 0s - loss: 1.531 - ETA: 0s - loss: 1.491 - ETA: 0s - loss: 1.453 - ETA: 0s - loss: 1.429 - ETA: 0s - loss: 1.393 - ETA: 0s - loss: 1.371 - ETA: 0s - loss: 1.338 - ETA: 0s - loss: 1.307 - ETA: 0s - loss: 1.287 - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 1.248 - ETA: 0s - loss: 1.230 - 4s 221us/step - loss: 1.2191 - val_loss: 0.0552\n",
      "Epoch 2/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.061 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.055 - ETA: 3s - loss: 0.052 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.049 - ETA: 3s - loss: 0.047 - ETA: 3s - loss: 0.046 - ETA: 2s - loss: 0.045 - ETA: 2s - loss: 0.044 - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.039 - ETA: 2s - loss: 0.038 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 1s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.032 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 1s - loss: 0.031 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - 4s 247us/step - loss: 0.0292 - val_loss: 0.0201\n",
      "Epoch 3/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 219us/step - loss: 0.0206 - val_loss: 0.0210\n",
      "Epoch 4/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.020 - 4s 223us/step - loss: 0.0201 - val_loss: 0.0187\n",
      "Epoch 5/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 202us/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 6/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 201us/step - loss: 0.0185 - val_loss: 0.0249\n",
      "Epoch 7/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 4s 217us/step - loss: 0.0189 - val_loss: 0.0243\n",
      "Epoch 8/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 242us/step - loss: 0.0175 - val_loss: 0.0164\n",
      "Epoch 9/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.017 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 296us/step - loss: 0.0173 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 5s 314us/step - loss: 0.0173 - val_loss: 0.0235\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.020 - ETA: 7s - loss: 0.015 - ETA: 8s - loss: 0.019 - ETA: 5s - loss: 0.019 - ETA: 4s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 5s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 4s 256us/step - loss: 0.0162 - val_loss: 0.0224\n",
      "Epoch 12/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 281us/step - loss: 0.0165 - val_loss: 0.0172\n",
      "Epoch 13/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 5s 316us/step - loss: 0.0168 - val_loss: 0.0299\n",
      "Epoch 14/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.031 - ETA: 2s - loss: 0.024 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.026 - ETA: 3s - loss: 0.025 - ETA: 3s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 4s 231us/step - loss: 0.0178 - val_loss: 0.0198\n",
      "Epoch 15/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 214us/step - loss: 0.0149 - val_loss: 0.0159\n",
      "Epoch 16/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 234us/step - loss: 0.0145 - val_loss: 0.0136\n",
      "Epoch 17/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.015 - 4s 247us/step - loss: 0.0159 - val_loss: 0.0148\n",
      "Epoch 18/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 10s - loss: 0.01 - ETA: 8s - loss: 0.0118 - ETA: 8s - loss: 0.012 - ETA: 8s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 320us/step - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 5s 321us/step - loss: 0.0140 - val_loss: 0.0179\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.016 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.015 - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 6s 329us/step - loss: 0.0147 - val_loss: 0.0158\n",
      "Epoch 21/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 4s - loss: 0.015 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 235us/step - loss: 0.0137 - val_loss: 0.0145\n",
      "Epoch 22/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 205us/step - loss: 0.0124 - val_loss: 0.0140\n",
      "Epoch 23/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 3s 182us/step - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 24/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 4s 209us/step - loss: 0.0145 - val_loss: 0.0188\n",
      "Epoch 25/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.023 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.021 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.017 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 5s 304us/step - loss: 0.0131 - val_loss: 0.0220\n",
      "Epoch 26/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - 5s 278us/step - loss: 0.0120 - val_loss: 0.0129\n",
      "Epoch 27/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 257us/step - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 28/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 255us/step - loss: 0.0134 - val_loss: 0.0152\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 240us/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 30/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 232us/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 31/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 299us/step - loss: 0.0111 - val_loss: 0.0151\n",
      "Epoch 32/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 317us/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 33/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 5s 276us/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 34/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 4s 238us/step - loss: 0.0131 - val_loss: 0.0284\n",
      "Epoch 35/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.024 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 224us/step - loss: 0.0126 - val_loss: 0.0142\n",
      "Epoch 36/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 6s 334us/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 6s - loss: 0.011 - ETA: 5s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 384us/step - loss: 0.0107 - val_loss: 0.0207\n",
      "Epoch 38/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 4s - loss: 0.016 - ETA: 4s - loss: 0.016 - ETA: 5s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 7s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 6s - loss: 0.015 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 5s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 7s 429us/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 39/100\n",
      "16800/16800 [==============================] - ETA: 7s - loss: 0.009 - ETA: 9s - loss: 0.009 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.009 - ETA: 7s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 346us/step - loss: 0.0108 - val_loss: 0.0198\n",
      "Epoch 40/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.022 - ETA: 4s - loss: 0.015 - ETA: 4s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.016 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 4s 219us/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 41/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 292us/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 42/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 218us/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 43/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 238us/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 44/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 239us/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 45/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 223us/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 244us/step - loss: 0.0114 - val_loss: 0.0130\n",
      "Epoch 47/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 195us/step - loss: 0.0107 - val_loss: 0.0170\n",
      "Epoch 48/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 4s 250us/step - loss: 0.0115 - val_loss: 0.0137\n",
      "Epoch 49/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 7s - loss: 0.009 - ETA: 7s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 6s - loss: 0.010 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 212us/step - loss: 0.0098 - val_loss: 0.0130\n",
      "Epoch 50/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 5s 300us/step - loss: 0.0115 - val_loss: 0.0210\n",
      "Epoch 51/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 3s - loss: 0.016 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.013 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 256us/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 52/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.006 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 256us/step - loss: 0.0094 - val_loss: 0.0201\n",
      "Epoch 53/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 4s 247us/step - loss: 0.0104 - val_loss: 0.0181\n",
      "Epoch 54/100\n",
      "16800/16800 [==============================] - ETA: 14s - loss: 0.01 - ETA: 8s - loss: 0.0142 - ETA: 7s - loss: 0.012 - ETA: 7s - loss: 0.012 - ETA: 6s - loss: 0.012 - ETA: 5s - loss: 0.011 - ETA: 5s - loss: 0.010 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 278us/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 277us/step - loss: 0.0094 - val_loss: 0.0138\n",
      "Epoch 56/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.016 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 322us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 57/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 280us/step - loss: 0.0100 - val_loss: 0.0185\n",
      "Epoch 58/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.018 - ETA: 6s - loss: 0.013 - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 339us/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 59/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.005 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 7s 422us/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 60/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 320us/step - loss: 0.0095 - val_loss: 0.0103\n",
      "Epoch 61/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 280us/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 62/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 312us/step - loss: 0.0107 - val_loss: 0.0154\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 6s - loss: 0.013 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 311us/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 64/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 297us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 65/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0109\n",
      "Epoch 66/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 291us/step - loss: 0.0093 - val_loss: 0.0100\n",
      "Epoch 67/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.005 - ETA: 6s - loss: 0.006 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 287us/step - loss: 0.0091 - val_loss: 0.0110\n",
      "Epoch 68/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 289us/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 69/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 271us/step - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 70/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 171us/step - loss: 0.0090 - val_loss: 0.0108\n",
      "Epoch 71/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 170us/step - loss: 0.0092 - val_loss: 0.0109\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 4s 224us/step - loss: 0.0092 - val_loss: 0.0121\n",
      "Epoch 73/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.015 - ETA: 3s - loss: 0.014 - ETA: 4s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.014 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 6s 336us/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 74/100\n",
      "16800/16800 [==============================] - ETA: 6s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 6s 338us/step - loss: 0.0091 - val_loss: 0.0104\n",
      "Epoch 75/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 318us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 76/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 5s 293us/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 77/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 285us/step - loss: 0.0087 - val_loss: 0.0104\n",
      "Epoch 78/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 292us/step - loss: 0.0082 - val_loss: 0.0094\n",
      "Epoch 79/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 8s - loss: 0.007 - ETA: 7s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 274us/step - loss: 0.0087 - val_loss: 0.0161\n",
      "Epoch 81/100\n",
      "16800/16800 [==============================] - ETA: 2s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 3s 183us/step - loss: 0.0085 - val_loss: 0.0107\n",
      "Epoch 82/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 3s 173us/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 83/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.013 - ETA: 4s - loss: 0.014 - ETA: 4s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - 5s 318us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 84/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 312us/step - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 85/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 86/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0082 - val_loss: 0.0103\n",
      "Epoch 87/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 281us/step - loss: 0.0083 - val_loss: 0.0112\n",
      "Epoch 88/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 280us/step - loss: 0.0079 - val_loss: 0.0103\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.006 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 296us/step - loss: 0.0082 - val_loss: 0.0127\n",
      "Epoch 90/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0111\n",
      "Epoch 91/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.009 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 286us/step - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 92/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.010 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 298us/step - loss: 0.0084 - val_loss: 0.0103\n",
      "Epoch 93/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 294us/step - loss: 0.0086 - val_loss: 0.0109\n",
      "Epoch 94/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 4s - loss: 0.006 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 280us/step - loss: 0.0085 - val_loss: 0.0095\n",
      "Epoch 95/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.007 - ETA: 6s - loss: 0.006 - ETA: 6s - loss: 0.007 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.008 - ETA: 6s - loss: 0.007 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 287us/step - loss: 0.0084 - val_loss: 0.0114\n",
      "Epoch 96/100\n",
      "16800/16800 [==============================] - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.008 - ETA: 5s - loss: 0.007 - ETA: 5s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 301us/step - loss: 0.0081 - val_loss: 0.0104\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 4s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 5s 296us/step - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 98/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.006 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 1s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 289us/step - loss: 0.0080 - val_loss: 0.0106\n",
      "Epoch 99/100\n",
      "16800/16800 [==============================] - ETA: 5s - loss: 0.006 - ETA: 6s - loss: 0.008 - ETA: 5s - loss: 0.009 - ETA: 5s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 4s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.007 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 293us/step - loss: 0.0086 - val_loss: 0.0169\n",
      "Epoch 100/100\n",
      "16800/16800 [==============================] - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.010 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.009 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 3s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 1s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 5s 279us/step - loss: 0.0081 - val_loss: 0.0091\n"
     ]
    }
   ],
   "source": [
    "#ニューラルネットワークモデルの生成\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(800, activation = 'relu'))\n",
    "model.add(Dense(100, activation =  'relu'))\n",
    "model.add(Dense(1))\n",
    "# モデルをコンパイル \n",
    "from keras.optimizers import Adam\n",
    "# ylgm=min_max_normalization(y)\n",
    "train_X_nn, valid_X_nn,train_y_nn, valid_y_nn = train_test_split(train_X_nn,y,test_size=0.2,random_state=43)\n",
    "\n",
    "model.compile(Adam(lr=1e-3), loss=\"mean_squared_error\")\n",
    "#トレーニングデータで学習し，テストデータで評価（平均2乗誤差を用いる）\n",
    "history = model.fit(np.array(train_X_nn), np.array(train_y_nn), batch_size=128, epochs=100, verbose=1, \n",
    "          validation_data=(np.array(valid_X_nn), np.array(valid_y_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200/4200 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 138us/step\n",
      "0.009095980622583912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x134851e10>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#結果の表示\n",
    "import matplotlib.pyplot as plt #プロット用のライブラリを利用\n",
    "\n",
    "print(model.evaluate(valid_X_nn, valid_y_nn))\n",
    "\n",
    "train_acc = history.history['loss']\n",
    "test_acc = history.history['val_loss']\n",
    "x = np.arange(len(train_acc))\n",
    "plt.plot(x, train_acc, label = 'train mse')\n",
    "plt.plot(x, test_acc, label = 'test mse')\n",
    "plt.legend() #グラフの線の説明を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:25.749117\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(valid_X_nn)\n",
    "score=mean_absolute_error(np.exp(valid_y_nn),np.exp(pred_y))\n",
    "print(f'MAE:{score:4f}')\n",
    "pred=model.predict(test_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>sex</th>\n",
       "      <th>partner</th>\n",
       "      <th>num_child</th>\n",
       "      <th>education</th>\n",
       "      <th>service_length</th>\n",
       "      <th>study_time</th>\n",
       "      <th>commute</th>\n",
       "      <th>overtime</th>\n",
       "      <th>familiy_num</th>\n",
       "      <th>agexposition</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7</td>\n",
       "      <td>156.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>230.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>147.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      position  age  area  sex  partner  num_child  education  service_length  \\\n",
       "0            4   39    14    1        1          5          2              19   \n",
       "1            2   31    27    0        0          0          5               0   \n",
       "2            1   20    45    1        1          2          1               2   \n",
       "3            1   28    37    1        0          0          1              10   \n",
       "4            2   41    16    1        0          0          1              23   \n",
       "...        ...  ...   ...  ...      ...        ...        ...             ...   \n",
       "8995         2   43    14    1        0          0          1              25   \n",
       "8996         3   40     5    1        0          0          1              22   \n",
       "8997         5   46    24    0        0          0          1              28   \n",
       "8998         1   22    13    0        0          0          1               4   \n",
       "8999         3   49    14    0        1          0          3              27   \n",
       "\n",
       "      study_time  commute  overtime  familiy_num  agexposition  cluster  \n",
       "0            1.0      1.8      14.2            7         156.0       19  \n",
       "1            0.0      0.5      18.6            1          62.0        0  \n",
       "2            2.0      1.2       2.3            4          30.0       14  \n",
       "3            3.0      0.3       0.0            1          42.0        9  \n",
       "4            3.0      0.5      10.1            1          82.0        4  \n",
       "...          ...      ...       ...          ...           ...      ...  \n",
       "8995         3.0      0.7       0.0            1          86.0        4  \n",
       "8996         8.0      0.7       5.7            1         120.0        7  \n",
       "8997         2.0      0.8       0.0            1         230.0       17  \n",
       "8998         0.0      0.1       0.7            1          33.0       12  \n",
       "8999         0.0      1.7      11.0            2         147.0       19  \n",
       "\n",
       "[9000 rows x 14 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:17:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:17:58] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[14:18:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:18:30,045] Finished trial#0 resulted in value: 0.6043565819234415. Current best value is 0.6043565819234415 with parameters: {'booster': 'gblinear', 'iterations': 122, 'depth': 20, 'learning_rate': 0.011708411691531534, 'random_strength': 43, 'bagging_temperature': 24.461831978594116, 'od_type': 'Iter', 'od_wait': 47, 'lambda_l1': 0.0003256224275922188, 'lambda_l2': 4.238936922941192e-05, 'num_leaves': 43, 'feature_fraction': 0.7707248327545355, 'bagging_fraction': 0.8906911477099924, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:18:32,208] Finished trial#1 resulted in value: 0.9415110780513425. Current best value is 0.9415110780513425 with parameters: {'booster': 'gbtree', 'iterations': 63, 'depth': 9, 'learning_rate': 0.032220248836418316, 'random_strength': 70, 'bagging_temperature': 21.15100928817564, 'od_type': 'Iter', 'od_wait': 17, 'lambda_l1': 0.2958777513373701, 'lambda_l2': 1.0311880248242327e-07, 'num_leaves': 63, 'feature_fraction': 0.7224174599715385, 'bagging_fraction': 0.6391599746303983, 'bagging_freq': 2, 'min_child_samples': 25}.\n",
      "[I 2019-11-20 14:18:36,339] Finished trial#2 resulted in value: 0.9530811099517871. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:38,686] Finished trial#3 resulted in value: 0.9305080833646834. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:40,879] Finished trial#4 resulted in value: 0.9383860228549192. Current best value is 0.9530811099517871 with parameters: {'booster': 'gblinear', 'iterations': 198, 'depth': 23, 'learning_rate': 0.12709626830734774, 'random_strength': 24, 'bagging_temperature': 0.025615950458741196, 'od_type': 'Iter', 'od_wait': 44, 'lambda_l1': 5.309886042563428e-06, 'lambda_l2': 1.3123541757631394e-08, 'num_leaves': 167, 'feature_fraction': 0.4916212379430805, 'bagging_fraction': 0.9394891760090883, 'bagging_freq': 1, 'min_child_samples': 41}.\n",
      "[I 2019-11-20 14:18:44,539] Finished trial#5 resulted in value: 0.955420168697476. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:48,111] Finished trial#6 resulted in value: 0.5256091000865739. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:50,255] Finished trial#7 resulted in value: 0.7874055549283296. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:54,479] Finished trial#8 resulted in value: 0.8835014084365532. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:18:56,836] Finished trial#9 resulted in value: 0.8831033143208874. Current best value is 0.955420168697476 with parameters: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 102, 'depth': 24, 'learning_rate': 0.07564483141599862, 'random_strength': 34, 'bagging_temperature': 0.010013033889029407, 'od_type': 'IncToDec', 'od_wait': 40, 'lambda_l1': 2.436670462541337e-08, 'lambda_l2': 3.3298068705485434e-07, 'num_leaves': 185, 'feature_fraction': 0.6542427094364054, 'bagging_fraction': 0.7734891236667175, 'bagging_freq': 4, 'min_child_samples': 33}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:18:59,299] Finished trial#0 resulted in value: 0.956099778308922. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:01,342] Finished trial#1 resulted in value: 0.938048402195524. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:03,021] Finished trial#2 resulted in value: 0.9412773184926577. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:05,075] Finished trial#3 resulted in value: 0.9383871941380209. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:07,496] Finished trial#4 resulted in value: 0.9530004129290692. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:09,693] Finished trial#5 resulted in value: 0.9517698894095877. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:12,551] Finished trial#6 resulted in value: 0.9526862633492964. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:14,684] Finished trial#7 resulted in value: 0.9256051497057376. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:17,752] Finished trial#8 resulted in value: 0.953977499885235. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n",
      "[I 2019-11-20 14:19:20,307] Finished trial#9 resulted in value: 0.9521265541453993. Current best value is 0.956099778308922 with parameters: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gblinear', 'iterations': 157, 'depth': 13, 'learning_rate': 0.1842567962272112, 'random_strength': 4, 'bagging_temperature': 10.723387215185724, 'od_type': 'Iter', 'od_wait': 25, 'lambda_l1': 2.905608158726086e-06, 'lambda_l2': 4.313764658300423e-06, 'num_leaves': 32, 'feature_fraction': 0.8474055963151101, 'bagging_fraction': 0.7091896738287273, 'bagging_freq': 5, 'min_child_samples': 17}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:19:23,115] Finished trial#0 resulted in value: 0.9425957163349349. Current best value is 0.9425957163349349 with parameters: {'booster': 'gbtree', 'iterations': 174, 'depth': 14, 'learning_rate': 0.5421405403866358, 'random_strength': 80, 'bagging_temperature': 0.04017628837898865, 'od_type': 'IncToDec', 'od_wait': 22, 'lambda_l1': 3.452892041612629e-08, 'lambda_l2': 0.00014667709942794993, 'num_leaves': 103, 'feature_fraction': 0.9507822358048381, 'bagging_fraction': 0.6487603668695829, 'bagging_freq': 6, 'min_child_samples': 90}.\n",
      "[I 2019-11-20 14:19:24,625] Finished trial#1 resulted in value: 0.9495955420462524. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:26,237] Finished trial#2 resulted in value: 0.8348422613017638. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:27,708] Finished trial#3 resulted in value: 0.9357670660837147. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:31,823] Finished trial#4 resulted in value: 0.9451798740530399. Current best value is 0.9495955420462524 with parameters: {'booster': 'gbtree', 'iterations': 246, 'depth': 19, 'learning_rate': 0.3259652196348137, 'random_strength': 13, 'bagging_temperature': 0.34173796237011045, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 3.4039322856281136e-07, 'lambda_l2': 0.026138284414901847, 'num_leaves': 25, 'feature_fraction': 0.48925008381647617, 'bagging_fraction': 0.49868016049223113, 'bagging_freq': 6, 'min_child_samples': 56}.\n",
      "[I 2019-11-20 14:19:33,505] Finished trial#5 resulted in value: 0.9512997991105854. Current best value is 0.9512997991105854 with parameters: {'booster': 'gbtree', 'iterations': 337, 'depth': 20, 'learning_rate': 0.07729673940129299, 'random_strength': 37, 'bagging_temperature': 0.3589275470407121, 'od_type': 'IncToDec', 'od_wait': 18, 'lambda_l1': 0.011211890401376249, 'lambda_l2': 1.082503848783411e-06, 'num_leaves': 39, 'feature_fraction': 0.5931287505247991, 'bagging_fraction': 0.44324275382737355, 'bagging_freq': 7, 'min_child_samples': 50}.\n",
      "[I 2019-11-20 14:19:37,170] Finished trial#6 resulted in value: 0.9535112447534292. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:39,650] Finished trial#7 resulted in value: 0.5789989267837456. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:41,276] Finished trial#8 resulted in value: 0.8707083123082388. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n",
      "[I 2019-11-20 14:19:44,821] Finished trial#9 resulted in value: 0.9134533367641637. Current best value is 0.9535112447534292 with parameters: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.95\n",
      "Best params: {'booster': 'gbtree', 'iterations': 306, 'depth': 19, 'learning_rate': 0.25694387506947475, 'random_strength': 99, 'bagging_temperature': 0.039897530190058386, 'od_type': 'Iter', 'od_wait': 32, 'lambda_l1': 0.004267031874016318, 'lambda_l2': 2.6469306427132437, 'num_leaves': 147, 'feature_fraction': 0.8673456624254294, 'bagging_fraction': 0.9387013838286312, 'bagging_freq': 2, 'min_child_samples': 88}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:19:49,342] Finished trial#0 resulted in value: 0.9436375317909936. Current best value is 0.9436375317909936 with parameters: {'booster': 'gbtree', 'iterations': 140, 'depth': 16, 'learning_rate': 0.032043648081768836, 'random_strength': 22, 'bagging_temperature': 38.75767354433763, 'od_type': 'IncToDec', 'od_wait': 11, 'lambda_l1': 0.06919274572425878, 'lambda_l2': 0.043941525494697556, 'num_leaves': 70, 'feature_fraction': 0.8153141563251047, 'bagging_fraction': 0.44211734721374485, 'bagging_freq': 1, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:19:52,782] Finished trial#1 resulted in value: 0.4833872013348449. Current best value is 0.9436375317909936 with parameters: {'booster': 'gbtree', 'iterations': 140, 'depth': 16, 'learning_rate': 0.032043648081768836, 'random_strength': 22, 'bagging_temperature': 38.75767354433763, 'od_type': 'IncToDec', 'od_wait': 11, 'lambda_l1': 0.06919274572425878, 'lambda_l2': 0.043941525494697556, 'num_leaves': 70, 'feature_fraction': 0.8153141563251047, 'bagging_fraction': 0.44211734721374485, 'bagging_freq': 1, 'min_child_samples': 33}.\n",
      "[I 2019-11-20 14:19:55,263] Finished trial#2 resulted in value: 0.953354739204389. Current best value is 0.953354739204389 with parameters: {'booster': 'gbtree', 'iterations': 377, 'depth': 4, 'learning_rate': 0.28817006864353856, 'random_strength': 38, 'bagging_temperature': 0.1978806336288733, 'od_type': 'IncToDec', 'od_wait': 31, 'lambda_l1': 1.0539365288391849e-05, 'lambda_l2': 0.37718058055527987, 'num_leaves': 50, 'feature_fraction': 0.49656668255830977, 'bagging_fraction': 0.8895635902633579, 'bagging_freq': 3, 'min_child_samples': 75}.\n",
      "[I 2019-11-20 14:19:58,094] Finished trial#3 resulted in value: 0.9337457353389416. Current best value is 0.953354739204389 with parameters: {'booster': 'gbtree', 'iterations': 377, 'depth': 4, 'learning_rate': 0.28817006864353856, 'random_strength': 38, 'bagging_temperature': 0.1978806336288733, 'od_type': 'IncToDec', 'od_wait': 31, 'lambda_l1': 1.0539365288391849e-05, 'lambda_l2': 0.37718058055527987, 'num_leaves': 50, 'feature_fraction': 0.49656668255830977, 'bagging_fraction': 0.8895635902633579, 'bagging_freq': 3, 'min_child_samples': 75}.\n",
      "[I 2019-11-20 14:20:04,365] Finished trial#4 resulted in value: 0.9556549036561319. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:06,370] Finished trial#5 resulted in value: 0.9339572594792331. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:08,185] Finished trial#6 resulted in value: 0.9491532523010999. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:11,093] Finished trial#7 resulted in value: 0.9139825604491474. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:12,790] Finished trial#8 resulted in value: 0.7389463650744487. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n",
      "[I 2019-11-20 14:20:14,756] Finished trial#9 resulted in value: 0.6717983969536553. Current best value is 0.9556549036561319 with parameters: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'iterations': 251, 'depth': 16, 'learning_rate': 0.114893963089329, 'random_strength': 25, 'bagging_temperature': 9.68557383353437, 'od_type': 'IncToDec', 'od_wait': 35, 'lambda_l1': 7.184486071604375e-07, 'lambda_l2': 0.14886036221139537, 'num_leaves': 240, 'feature_fraction': 0.9318277077150058, 'bagging_fraction': 0.8028440804026264, 'bagging_freq': 6, 'min_child_samples': 24}\n",
      "\n",
      "0:\tlearn: 0.4379078\ttotal: 9.48ms\tremaining: 9.47s\n",
      "1:\tlearn: 0.4275838\ttotal: 17.8ms\tremaining: 8.88s\n",
      "2:\tlearn: 0.4172781\ttotal: 23.2ms\tremaining: 7.72s\n",
      "3:\tlearn: 0.4074042\ttotal: 30.5ms\tremaining: 7.6s\n",
      "4:\tlearn: 0.3977807\ttotal: 35.5ms\tremaining: 7.06s\n",
      "5:\tlearn: 0.3885448\ttotal: 44.1ms\tremaining: 7.31s\n",
      "6:\tlearn: 0.3792993\ttotal: 51.9ms\tremaining: 7.36s\n",
      "7:\tlearn: 0.3708988\ttotal: 61.5ms\tremaining: 7.62s\n",
      "8:\tlearn: 0.3627070\ttotal: 73.1ms\tremaining: 8.05s\n",
      "9:\tlearn: 0.3547298\ttotal: 84.8ms\tremaining: 8.39s\n",
      "10:\tlearn: 0.3470742\ttotal: 97.9ms\tremaining: 8.8s\n",
      "11:\tlearn: 0.3391469\ttotal: 109ms\tremaining: 8.97s\n",
      "12:\tlearn: 0.3316150\ttotal: 119ms\tremaining: 9.02s\n",
      "13:\tlearn: 0.3246369\ttotal: 127ms\tremaining: 8.92s\n",
      "14:\tlearn: 0.3178956\ttotal: 143ms\tremaining: 9.38s\n",
      "15:\tlearn: 0.3110082\ttotal: 150ms\tremaining: 9.22s\n",
      "16:\tlearn: 0.3045172\ttotal: 155ms\tremaining: 8.96s\n",
      "17:\tlearn: 0.2981711\ttotal: 161ms\tremaining: 8.79s\n",
      "18:\tlearn: 0.2922796\ttotal: 166ms\tremaining: 8.58s\n",
      "19:\tlearn: 0.2863589\ttotal: 171ms\tremaining: 8.39s\n",
      "20:\tlearn: 0.2806532\ttotal: 183ms\tremaining: 8.52s\n",
      "21:\tlearn: 0.2752674\ttotal: 189ms\tremaining: 8.4s\n",
      "22:\tlearn: 0.2696920\ttotal: 196ms\tremaining: 8.34s\n",
      "23:\tlearn: 0.2646486\ttotal: 202ms\tremaining: 8.22s\n",
      "24:\tlearn: 0.2596521\ttotal: 208ms\tremaining: 8.1s\n",
      "25:\tlearn: 0.2551176\ttotal: 213ms\tremaining: 7.97s\n",
      "26:\tlearn: 0.2505730\ttotal: 220ms\tremaining: 7.94s\n",
      "27:\tlearn: 0.2463309\ttotal: 225ms\tremaining: 7.82s\n",
      "28:\tlearn: 0.2422309\ttotal: 231ms\tremaining: 7.73s\n",
      "29:\tlearn: 0.2378867\ttotal: 235ms\tremaining: 7.61s\n",
      "30:\tlearn: 0.2337069\ttotal: 241ms\tremaining: 7.52s\n",
      "31:\tlearn: 0.2296324\ttotal: 246ms\tremaining: 7.43s\n",
      "32:\tlearn: 0.2256271\ttotal: 251ms\tremaining: 7.35s\n",
      "33:\tlearn: 0.2220109\ttotal: 256ms\tremaining: 7.27s\n",
      "34:\tlearn: 0.2186837\ttotal: 261ms\tremaining: 7.19s\n",
      "35:\tlearn: 0.2155680\ttotal: 266ms\tremaining: 7.13s\n",
      "36:\tlearn: 0.2122006\ttotal: 272ms\tremaining: 7.07s\n",
      "37:\tlearn: 0.2088621\ttotal: 276ms\tremaining: 7s\n",
      "38:\tlearn: 0.2058567\ttotal: 282ms\tremaining: 6.94s\n",
      "39:\tlearn: 0.2030110\ttotal: 288ms\tremaining: 6.9s\n",
      "40:\tlearn: 0.2003221\ttotal: 293ms\tremaining: 6.85s\n",
      "41:\tlearn: 0.1976403\ttotal: 298ms\tremaining: 6.79s\n",
      "42:\tlearn: 0.1952482\ttotal: 303ms\tremaining: 6.74s\n",
      "43:\tlearn: 0.1925376\ttotal: 308ms\tremaining: 6.69s\n",
      "44:\tlearn: 0.1902030\ttotal: 313ms\tremaining: 6.64s\n",
      "45:\tlearn: 0.1880066\ttotal: 318ms\tremaining: 6.6s\n",
      "46:\tlearn: 0.1858651\ttotal: 324ms\tremaining: 6.56s\n",
      "47:\tlearn: 0.1837542\ttotal: 329ms\tremaining: 6.52s\n",
      "48:\tlearn: 0.1817112\ttotal: 334ms\tremaining: 6.48s\n",
      "49:\tlearn: 0.1794884\ttotal: 339ms\tremaining: 6.44s\n",
      "50:\tlearn: 0.1775916\ttotal: 344ms\tremaining: 6.4s\n",
      "51:\tlearn: 0.1758250\ttotal: 353ms\tremaining: 6.43s\n",
      "52:\tlearn: 0.1739601\ttotal: 360ms\tremaining: 6.43s\n",
      "53:\tlearn: 0.1720406\ttotal: 367ms\tremaining: 6.43s\n",
      "54:\tlearn: 0.1705263\ttotal: 372ms\tremaining: 6.4s\n",
      "55:\tlearn: 0.1688571\ttotal: 381ms\tremaining: 6.42s\n",
      "56:\tlearn: 0.1671213\ttotal: 389ms\tremaining: 6.43s\n",
      "57:\tlearn: 0.1654609\ttotal: 394ms\tremaining: 6.4s\n",
      "58:\tlearn: 0.1640678\ttotal: 399ms\tremaining: 6.37s\n",
      "59:\tlearn: 0.1625170\ttotal: 404ms\tremaining: 6.33s\n",
      "60:\tlearn: 0.1611926\ttotal: 409ms\tremaining: 6.3s\n",
      "61:\tlearn: 0.1597531\ttotal: 415ms\tremaining: 6.27s\n",
      "62:\tlearn: 0.1586154\ttotal: 420ms\tremaining: 6.24s\n",
      "63:\tlearn: 0.1574752\ttotal: 425ms\tremaining: 6.21s\n",
      "64:\tlearn: 0.1561201\ttotal: 430ms\tremaining: 6.19s\n",
      "65:\tlearn: 0.1550788\ttotal: 436ms\tremaining: 6.17s\n",
      "66:\tlearn: 0.1537834\ttotal: 443ms\tremaining: 6.17s\n",
      "67:\tlearn: 0.1526765\ttotal: 449ms\tremaining: 6.15s\n",
      "68:\tlearn: 0.1516859\ttotal: 454ms\tremaining: 6.13s\n",
      "69:\tlearn: 0.1508132\ttotal: 460ms\tremaining: 6.11s\n",
      "70:\tlearn: 0.1498110\ttotal: 465ms\tremaining: 6.08s\n",
      "71:\tlearn: 0.1489518\ttotal: 472ms\tremaining: 6.08s\n",
      "72:\tlearn: 0.1481080\ttotal: 479ms\tremaining: 6.08s\n",
      "73:\tlearn: 0.1470856\ttotal: 486ms\tremaining: 6.08s\n",
      "74:\tlearn: 0.1463174\ttotal: 493ms\tremaining: 6.07s\n",
      "75:\tlearn: 0.1454997\ttotal: 499ms\tremaining: 6.07s\n",
      "76:\tlearn: 0.1448378\ttotal: 504ms\tremaining: 6.04s\n",
      "77:\tlearn: 0.1441396\ttotal: 509ms\tremaining: 6.02s\n",
      "78:\tlearn: 0.1433696\ttotal: 514ms\tremaining: 6s\n",
      "79:\tlearn: 0.1427612\ttotal: 519ms\tremaining: 5.97s\n",
      "80:\tlearn: 0.1421226\ttotal: 525ms\tremaining: 5.95s\n",
      "81:\tlearn: 0.1415375\ttotal: 530ms\tremaining: 5.93s\n",
      "82:\tlearn: 0.1409578\ttotal: 537ms\tremaining: 5.94s\n",
      "83:\tlearn: 0.1402608\ttotal: 542ms\tremaining: 5.91s\n",
      "84:\tlearn: 0.1397326\ttotal: 549ms\tremaining: 5.91s\n",
      "85:\tlearn: 0.1391547\ttotal: 555ms\tremaining: 5.9s\n",
      "86:\tlearn: 0.1385481\ttotal: 560ms\tremaining: 5.88s\n",
      "87:\tlearn: 0.1380731\ttotal: 566ms\tremaining: 5.86s\n",
      "88:\tlearn: 0.1373662\ttotal: 571ms\tremaining: 5.85s\n",
      "89:\tlearn: 0.1367306\ttotal: 577ms\tremaining: 5.83s\n",
      "90:\tlearn: 0.1360538\ttotal: 582ms\tremaining: 5.81s\n",
      "91:\tlearn: 0.1353600\ttotal: 587ms\tremaining: 5.79s\n",
      "92:\tlearn: 0.1348423\ttotal: 592ms\tremaining: 5.78s\n",
      "93:\tlearn: 0.1344051\ttotal: 597ms\tremaining: 5.76s\n",
      "94:\tlearn: 0.1339423\ttotal: 602ms\tremaining: 5.74s\n",
      "95:\tlearn: 0.1335740\ttotal: 608ms\tremaining: 5.72s\n",
      "96:\tlearn: 0.1331749\ttotal: 614ms\tremaining: 5.71s\n",
      "97:\tlearn: 0.1328333\ttotal: 619ms\tremaining: 5.7s\n",
      "98:\tlearn: 0.1321947\ttotal: 624ms\tremaining: 5.68s\n",
      "99:\tlearn: 0.1317014\ttotal: 629ms\tremaining: 5.66s\n",
      "100:\tlearn: 0.1313400\ttotal: 635ms\tremaining: 5.65s\n",
      "101:\tlearn: 0.1309153\ttotal: 640ms\tremaining: 5.63s\n",
      "102:\tlearn: 0.1304477\ttotal: 646ms\tremaining: 5.63s\n",
      "103:\tlearn: 0.1301293\ttotal: 651ms\tremaining: 5.61s\n",
      "104:\tlearn: 0.1297833\ttotal: 656ms\tremaining: 5.59s\n",
      "105:\tlearn: 0.1294095\ttotal: 661ms\tremaining: 5.58s\n",
      "106:\tlearn: 0.1289525\ttotal: 666ms\tremaining: 5.56s\n",
      "107:\tlearn: 0.1285886\ttotal: 671ms\tremaining: 5.54s\n",
      "108:\tlearn: 0.1282437\ttotal: 676ms\tremaining: 5.52s\n",
      "109:\tlearn: 0.1278965\ttotal: 681ms\tremaining: 5.51s\n",
      "110:\tlearn: 0.1276418\ttotal: 686ms\tremaining: 5.5s\n",
      "111:\tlearn: 0.1272142\ttotal: 691ms\tremaining: 5.48s\n",
      "112:\tlearn: 0.1269173\ttotal: 696ms\tremaining: 5.46s\n",
      "113:\tlearn: 0.1266602\ttotal: 701ms\tremaining: 5.45s\n",
      "114:\tlearn: 0.1263385\ttotal: 706ms\tremaining: 5.43s\n",
      "115:\tlearn: 0.1259109\ttotal: 711ms\tremaining: 5.42s\n",
      "116:\tlearn: 0.1256775\ttotal: 716ms\tremaining: 5.41s\n",
      "117:\tlearn: 0.1254524\ttotal: 722ms\tremaining: 5.39s\n",
      "118:\tlearn: 0.1250409\ttotal: 727ms\tremaining: 5.38s\n",
      "119:\tlearn: 0.1248424\ttotal: 732ms\tremaining: 5.37s\n",
      "120:\tlearn: 0.1244909\ttotal: 737ms\tremaining: 5.36s\n",
      "121:\tlearn: 0.1243022\ttotal: 743ms\tremaining: 5.35s\n",
      "122:\tlearn: 0.1240458\ttotal: 748ms\tremaining: 5.33s\n",
      "123:\tlearn: 0.1235485\ttotal: 753ms\tremaining: 5.32s\n",
      "124:\tlearn: 0.1232401\ttotal: 759ms\tremaining: 5.31s\n",
      "125:\tlearn: 0.1230764\ttotal: 764ms\tremaining: 5.3s\n",
      "126:\tlearn: 0.1227724\ttotal: 771ms\tremaining: 5.3s\n",
      "127:\tlearn: 0.1226080\ttotal: 778ms\tremaining: 5.3s\n",
      "128:\tlearn: 0.1223530\ttotal: 784ms\tremaining: 5.29s\n",
      "129:\tlearn: 0.1220298\ttotal: 788ms\tremaining: 5.28s\n",
      "130:\tlearn: 0.1218838\ttotal: 793ms\tremaining: 5.26s\n",
      "131:\tlearn: 0.1217269\ttotal: 798ms\tremaining: 5.25s\n",
      "132:\tlearn: 0.1213280\ttotal: 804ms\tremaining: 5.24s\n",
      "133:\tlearn: 0.1209985\ttotal: 809ms\tremaining: 5.23s\n",
      "134:\tlearn: 0.1207907\ttotal: 815ms\tremaining: 5.22s\n",
      "135:\tlearn: 0.1206111\ttotal: 821ms\tremaining: 5.21s\n",
      "136:\tlearn: 0.1204028\ttotal: 826ms\tremaining: 5.2s\n",
      "137:\tlearn: 0.1202731\ttotal: 833ms\tremaining: 5.2s\n",
      "138:\tlearn: 0.1201308\ttotal: 838ms\tremaining: 5.19s\n",
      "139:\tlearn: 0.1200278\ttotal: 843ms\tremaining: 5.17s\n",
      "140:\tlearn: 0.1197411\ttotal: 848ms\tremaining: 5.16s\n",
      "141:\tlearn: 0.1193948\ttotal: 853ms\tremaining: 5.15s\n",
      "142:\tlearn: 0.1191470\ttotal: 858ms\tremaining: 5.14s\n",
      "143:\tlearn: 0.1187580\ttotal: 863ms\tremaining: 5.13s\n",
      "144:\tlearn: 0.1184650\ttotal: 869ms\tremaining: 5.12s\n",
      "145:\tlearn: 0.1182621\ttotal: 873ms\tremaining: 5.11s\n",
      "146:\tlearn: 0.1179877\ttotal: 878ms\tremaining: 5.1s\n",
      "147:\tlearn: 0.1177549\ttotal: 883ms\tremaining: 5.08s\n",
      "148:\tlearn: 0.1175510\ttotal: 889ms\tremaining: 5.08s\n",
      "149:\tlearn: 0.1172671\ttotal: 894ms\tremaining: 5.06s\n",
      "150:\tlearn: 0.1170068\ttotal: 899ms\tremaining: 5.05s\n",
      "151:\tlearn: 0.1167415\ttotal: 905ms\tremaining: 5.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152:\tlearn: 0.1165467\ttotal: 910ms\tremaining: 5.04s\n",
      "153:\tlearn: 0.1163725\ttotal: 915ms\tremaining: 5.03s\n",
      "154:\tlearn: 0.1162362\ttotal: 920ms\tremaining: 5.01s\n",
      "155:\tlearn: 0.1160617\ttotal: 925ms\tremaining: 5s\n",
      "156:\tlearn: 0.1159258\ttotal: 929ms\tremaining: 4.99s\n",
      "157:\tlearn: 0.1158152\ttotal: 934ms\tremaining: 4.98s\n",
      "158:\tlearn: 0.1157085\ttotal: 939ms\tremaining: 4.97s\n",
      "159:\tlearn: 0.1155783\ttotal: 944ms\tremaining: 4.96s\n",
      "160:\tlearn: 0.1154813\ttotal: 950ms\tremaining: 4.95s\n",
      "161:\tlearn: 0.1152914\ttotal: 955ms\tremaining: 4.94s\n",
      "162:\tlearn: 0.1151079\ttotal: 961ms\tremaining: 4.93s\n",
      "163:\tlearn: 0.1148463\ttotal: 966ms\tremaining: 4.92s\n",
      "164:\tlearn: 0.1147656\ttotal: 971ms\tremaining: 4.92s\n",
      "165:\tlearn: 0.1145215\ttotal: 978ms\tremaining: 4.91s\n",
      "166:\tlearn: 0.1143694\ttotal: 984ms\tremaining: 4.91s\n",
      "167:\tlearn: 0.1141285\ttotal: 988ms\tremaining: 4.89s\n",
      "168:\tlearn: 0.1138725\ttotal: 994ms\tremaining: 4.89s\n",
      "169:\tlearn: 0.1137617\ttotal: 999ms\tremaining: 4.88s\n",
      "170:\tlearn: 0.1135394\ttotal: 1s\tremaining: 4.87s\n",
      "171:\tlearn: 0.1133940\ttotal: 1.01s\tremaining: 4.86s\n",
      "172:\tlearn: 0.1133171\ttotal: 1.02s\tremaining: 4.88s\n",
      "173:\tlearn: 0.1132710\ttotal: 1.03s\tremaining: 4.89s\n",
      "174:\tlearn: 0.1131041\ttotal: 1.04s\tremaining: 4.89s\n",
      "175:\tlearn: 0.1128832\ttotal: 1.05s\tremaining: 4.91s\n",
      "176:\tlearn: 0.1127658\ttotal: 1.06s\tremaining: 4.92s\n",
      "177:\tlearn: 0.1126166\ttotal: 1.07s\tremaining: 4.94s\n",
      "178:\tlearn: 0.1124073\ttotal: 1.08s\tremaining: 4.97s\n",
      "179:\tlearn: 0.1122908\ttotal: 1.09s\tremaining: 4.98s\n",
      "180:\tlearn: 0.1121027\ttotal: 1.1s\tremaining: 4.98s\n",
      "181:\tlearn: 0.1119248\ttotal: 1.11s\tremaining: 4.97s\n",
      "182:\tlearn: 0.1118283\ttotal: 1.11s\tremaining: 4.96s\n",
      "183:\tlearn: 0.1116846\ttotal: 1.12s\tremaining: 4.95s\n",
      "184:\tlearn: 0.1116067\ttotal: 1.12s\tremaining: 4.94s\n",
      "185:\tlearn: 0.1114694\ttotal: 1.13s\tremaining: 4.93s\n",
      "186:\tlearn: 0.1113518\ttotal: 1.13s\tremaining: 4.92s\n",
      "187:\tlearn: 0.1112752\ttotal: 1.14s\tremaining: 4.92s\n",
      "188:\tlearn: 0.1111222\ttotal: 1.14s\tremaining: 4.91s\n",
      "189:\tlearn: 0.1110053\ttotal: 1.15s\tremaining: 4.89s\n",
      "190:\tlearn: 0.1108860\ttotal: 1.15s\tremaining: 4.89s\n",
      "191:\tlearn: 0.1106839\ttotal: 1.16s\tremaining: 4.88s\n",
      "192:\tlearn: 0.1106293\ttotal: 1.17s\tremaining: 4.88s\n",
      "193:\tlearn: 0.1104835\ttotal: 1.17s\tremaining: 4.87s\n",
      "194:\tlearn: 0.1104137\ttotal: 1.18s\tremaining: 4.87s\n",
      "195:\tlearn: 0.1102824\ttotal: 1.18s\tremaining: 4.86s\n",
      "196:\tlearn: 0.1102174\ttotal: 1.19s\tremaining: 4.85s\n",
      "197:\tlearn: 0.1101139\ttotal: 1.19s\tremaining: 4.84s\n",
      "198:\tlearn: 0.1099180\ttotal: 1.2s\tremaining: 4.83s\n",
      "199:\tlearn: 0.1098204\ttotal: 1.21s\tremaining: 4.82s\n",
      "200:\tlearn: 0.1097206\ttotal: 1.21s\tremaining: 4.81s\n",
      "201:\tlearn: 0.1096678\ttotal: 1.22s\tremaining: 4.83s\n",
      "202:\tlearn: 0.1095754\ttotal: 1.23s\tremaining: 4.83s\n",
      "203:\tlearn: 0.1095171\ttotal: 1.24s\tremaining: 4.83s\n",
      "204:\tlearn: 0.1093734\ttotal: 1.25s\tremaining: 4.84s\n",
      "205:\tlearn: 0.1093031\ttotal: 1.26s\tremaining: 4.86s\n",
      "206:\tlearn: 0.1092272\ttotal: 1.27s\tremaining: 4.85s\n",
      "207:\tlearn: 0.1091550\ttotal: 1.27s\tremaining: 4.84s\n",
      "208:\tlearn: 0.1090476\ttotal: 1.28s\tremaining: 4.83s\n",
      "209:\tlearn: 0.1089124\ttotal: 1.28s\tremaining: 4.83s\n",
      "210:\tlearn: 0.1087761\ttotal: 1.29s\tremaining: 4.82s\n",
      "211:\tlearn: 0.1087183\ttotal: 1.29s\tremaining: 4.81s\n",
      "212:\tlearn: 0.1085947\ttotal: 1.3s\tremaining: 4.81s\n",
      "213:\tlearn: 0.1085531\ttotal: 1.31s\tremaining: 4.8s\n",
      "214:\tlearn: 0.1084887\ttotal: 1.31s\tremaining: 4.79s\n",
      "215:\tlearn: 0.1084259\ttotal: 1.32s\tremaining: 4.79s\n",
      "216:\tlearn: 0.1083573\ttotal: 1.32s\tremaining: 4.78s\n",
      "217:\tlearn: 0.1082960\ttotal: 1.33s\tremaining: 4.77s\n",
      "218:\tlearn: 0.1082389\ttotal: 1.33s\tremaining: 4.76s\n",
      "219:\tlearn: 0.1081318\ttotal: 1.34s\tremaining: 4.75s\n",
      "220:\tlearn: 0.1079800\ttotal: 1.35s\tremaining: 4.75s\n",
      "221:\tlearn: 0.1078802\ttotal: 1.35s\tremaining: 4.74s\n",
      "222:\tlearn: 0.1077598\ttotal: 1.36s\tremaining: 4.73s\n",
      "223:\tlearn: 0.1076390\ttotal: 1.36s\tremaining: 4.72s\n",
      "224:\tlearn: 0.1075600\ttotal: 1.37s\tremaining: 4.71s\n",
      "225:\tlearn: 0.1075062\ttotal: 1.37s\tremaining: 4.7s\n",
      "226:\tlearn: 0.1073832\ttotal: 1.38s\tremaining: 4.69s\n",
      "227:\tlearn: 0.1072460\ttotal: 1.38s\tremaining: 4.69s\n",
      "228:\tlearn: 0.1071706\ttotal: 1.39s\tremaining: 4.68s\n",
      "229:\tlearn: 0.1071110\ttotal: 1.39s\tremaining: 4.67s\n",
      "230:\tlearn: 0.1070673\ttotal: 1.4s\tremaining: 4.66s\n",
      "231:\tlearn: 0.1070022\ttotal: 1.41s\tremaining: 4.65s\n",
      "232:\tlearn: 0.1069216\ttotal: 1.41s\tremaining: 4.65s\n",
      "233:\tlearn: 0.1068052\ttotal: 1.42s\tremaining: 4.64s\n",
      "234:\tlearn: 0.1067170\ttotal: 1.42s\tremaining: 4.63s\n",
      "235:\tlearn: 0.1066061\ttotal: 1.43s\tremaining: 4.62s\n",
      "236:\tlearn: 0.1064745\ttotal: 1.43s\tremaining: 4.61s\n",
      "237:\tlearn: 0.1063795\ttotal: 1.44s\tremaining: 4.61s\n",
      "238:\tlearn: 0.1063168\ttotal: 1.45s\tremaining: 4.6s\n",
      "239:\tlearn: 0.1062153\ttotal: 1.45s\tremaining: 4.59s\n",
      "240:\tlearn: 0.1061137\ttotal: 1.46s\tremaining: 4.59s\n",
      "241:\tlearn: 0.1060883\ttotal: 1.46s\tremaining: 4.58s\n",
      "242:\tlearn: 0.1060551\ttotal: 1.47s\tremaining: 4.58s\n",
      "243:\tlearn: 0.1059931\ttotal: 1.47s\tremaining: 4.57s\n",
      "244:\tlearn: 0.1059321\ttotal: 1.48s\tremaining: 4.55s\n",
      "245:\tlearn: 0.1058868\ttotal: 1.48s\tremaining: 4.54s\n",
      "246:\tlearn: 0.1057802\ttotal: 1.49s\tremaining: 4.54s\n",
      "247:\tlearn: 0.1057110\ttotal: 1.49s\tremaining: 4.53s\n",
      "248:\tlearn: 0.1056161\ttotal: 1.5s\tremaining: 4.51s\n",
      "249:\tlearn: 0.1055168\ttotal: 1.5s\tremaining: 4.5s\n",
      "250:\tlearn: 0.1054498\ttotal: 1.51s\tremaining: 4.5s\n",
      "251:\tlearn: 0.1053853\ttotal: 1.51s\tremaining: 4.49s\n",
      "252:\tlearn: 0.1052637\ttotal: 1.52s\tremaining: 4.49s\n",
      "253:\tlearn: 0.1051578\ttotal: 1.52s\tremaining: 4.48s\n",
      "254:\tlearn: 0.1051060\ttotal: 1.53s\tremaining: 4.48s\n",
      "255:\tlearn: 0.1050697\ttotal: 1.54s\tremaining: 4.47s\n",
      "256:\tlearn: 0.1050390\ttotal: 1.54s\tremaining: 4.47s\n",
      "257:\tlearn: 0.1050003\ttotal: 1.55s\tremaining: 4.46s\n",
      "258:\tlearn: 0.1049055\ttotal: 1.56s\tremaining: 4.45s\n",
      "259:\tlearn: 0.1048494\ttotal: 1.56s\tremaining: 4.44s\n",
      "260:\tlearn: 0.1047498\ttotal: 1.57s\tremaining: 4.43s\n",
      "261:\tlearn: 0.1046900\ttotal: 1.57s\tremaining: 4.43s\n",
      "262:\tlearn: 0.1046625\ttotal: 1.58s\tremaining: 4.42s\n",
      "263:\tlearn: 0.1046055\ttotal: 1.58s\tremaining: 4.41s\n",
      "264:\tlearn: 0.1044852\ttotal: 1.59s\tremaining: 4.4s\n",
      "265:\tlearn: 0.1044331\ttotal: 1.59s\tremaining: 4.39s\n",
      "266:\tlearn: 0.1043092\ttotal: 1.6s\tremaining: 4.38s\n",
      "267:\tlearn: 0.1042316\ttotal: 1.6s\tremaining: 4.38s\n",
      "268:\tlearn: 0.1041530\ttotal: 1.61s\tremaining: 4.37s\n",
      "269:\tlearn: 0.1041170\ttotal: 1.61s\tremaining: 4.36s\n",
      "270:\tlearn: 0.1040686\ttotal: 1.62s\tremaining: 4.35s\n",
      "271:\tlearn: 0.1039467\ttotal: 1.62s\tremaining: 4.34s\n",
      "272:\tlearn: 0.1039230\ttotal: 1.63s\tremaining: 4.33s\n",
      "273:\tlearn: 0.1038683\ttotal: 1.63s\tremaining: 4.33s\n",
      "274:\tlearn: 0.1037840\ttotal: 1.64s\tremaining: 4.32s\n",
      "275:\tlearn: 0.1037568\ttotal: 1.64s\tremaining: 4.31s\n",
      "276:\tlearn: 0.1036709\ttotal: 1.65s\tremaining: 4.31s\n",
      "277:\tlearn: 0.1036199\ttotal: 1.65s\tremaining: 4.3s\n",
      "278:\tlearn: 0.1035570\ttotal: 1.66s\tremaining: 4.29s\n",
      "279:\tlearn: 0.1034806\ttotal: 1.66s\tremaining: 4.28s\n",
      "280:\tlearn: 0.1034093\ttotal: 1.67s\tremaining: 4.27s\n",
      "281:\tlearn: 0.1033820\ttotal: 1.68s\tremaining: 4.27s\n",
      "282:\tlearn: 0.1033350\ttotal: 1.68s\tremaining: 4.26s\n",
      "283:\tlearn: 0.1032612\ttotal: 1.69s\tremaining: 4.25s\n",
      "284:\tlearn: 0.1032125\ttotal: 1.69s\tremaining: 4.24s\n",
      "285:\tlearn: 0.1031400\ttotal: 1.7s\tremaining: 4.24s\n",
      "286:\tlearn: 0.1030921\ttotal: 1.7s\tremaining: 4.23s\n",
      "287:\tlearn: 0.1030364\ttotal: 1.71s\tremaining: 4.22s\n",
      "288:\tlearn: 0.1030044\ttotal: 1.71s\tremaining: 4.22s\n",
      "289:\tlearn: 0.1028850\ttotal: 1.72s\tremaining: 4.21s\n",
      "290:\tlearn: 0.1028379\ttotal: 1.72s\tremaining: 4.2s\n",
      "291:\tlearn: 0.1027740\ttotal: 1.73s\tremaining: 4.19s\n",
      "292:\tlearn: 0.1027106\ttotal: 1.73s\tremaining: 4.19s\n",
      "293:\tlearn: 0.1026736\ttotal: 1.74s\tremaining: 4.18s\n",
      "294:\tlearn: 0.1026069\ttotal: 1.75s\tremaining: 4.18s\n",
      "295:\tlearn: 0.1024561\ttotal: 1.75s\tremaining: 4.17s\n",
      "296:\tlearn: 0.1023794\ttotal: 1.76s\tremaining: 4.17s\n",
      "297:\tlearn: 0.1022726\ttotal: 1.76s\tremaining: 4.16s\n",
      "298:\tlearn: 0.1022417\ttotal: 1.77s\tremaining: 4.15s\n",
      "299:\tlearn: 0.1021255\ttotal: 1.78s\tremaining: 4.14s\n",
      "300:\tlearn: 0.1020903\ttotal: 1.78s\tremaining: 4.14s\n",
      "301:\tlearn: 0.1020381\ttotal: 1.79s\tremaining: 4.13s\n",
      "302:\tlearn: 0.1019913\ttotal: 1.79s\tremaining: 4.12s\n",
      "303:\tlearn: 0.1019216\ttotal: 1.8s\tremaining: 4.11s\n",
      "304:\tlearn: 0.1018910\ttotal: 1.8s\tremaining: 4.11s\n",
      "305:\tlearn: 0.1018240\ttotal: 1.81s\tremaining: 4.1s\n",
      "306:\tlearn: 0.1017286\ttotal: 1.81s\tremaining: 4.09s\n",
      "307:\tlearn: 0.1016354\ttotal: 1.82s\tremaining: 4.08s\n",
      "308:\tlearn: 0.1015120\ttotal: 1.82s\tremaining: 4.08s\n",
      "309:\tlearn: 0.1014410\ttotal: 1.83s\tremaining: 4.07s\n",
      "310:\tlearn: 0.1013964\ttotal: 1.83s\tremaining: 4.06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311:\tlearn: 0.1013417\ttotal: 1.84s\tremaining: 4.05s\n",
      "312:\tlearn: 0.1012289\ttotal: 1.84s\tremaining: 4.05s\n",
      "313:\tlearn: 0.1011535\ttotal: 1.85s\tremaining: 4.04s\n",
      "314:\tlearn: 0.1011357\ttotal: 1.85s\tremaining: 4.03s\n",
      "315:\tlearn: 0.1010483\ttotal: 1.86s\tremaining: 4.03s\n",
      "316:\tlearn: 0.1009671\ttotal: 1.86s\tremaining: 4.02s\n",
      "317:\tlearn: 0.1008972\ttotal: 1.87s\tremaining: 4.01s\n",
      "318:\tlearn: 0.1008379\ttotal: 1.88s\tremaining: 4s\n",
      "319:\tlearn: 0.1007997\ttotal: 1.88s\tremaining: 4s\n",
      "320:\tlearn: 0.1007392\ttotal: 1.89s\tremaining: 4s\n",
      "321:\tlearn: 0.1007043\ttotal: 1.89s\tremaining: 3.99s\n",
      "322:\tlearn: 0.1006687\ttotal: 1.9s\tremaining: 3.98s\n",
      "323:\tlearn: 0.1006323\ttotal: 1.91s\tremaining: 3.98s\n",
      "324:\tlearn: 0.1006008\ttotal: 1.91s\tremaining: 3.97s\n",
      "325:\tlearn: 0.1005380\ttotal: 1.92s\tremaining: 3.96s\n",
      "326:\tlearn: 0.1004865\ttotal: 1.92s\tremaining: 3.95s\n",
      "327:\tlearn: 0.1004422\ttotal: 1.93s\tremaining: 3.95s\n",
      "328:\tlearn: 0.1003798\ttotal: 1.93s\tremaining: 3.94s\n",
      "329:\tlearn: 0.1002908\ttotal: 1.94s\tremaining: 3.93s\n",
      "330:\tlearn: 0.1002673\ttotal: 1.94s\tremaining: 3.92s\n",
      "331:\tlearn: 0.1001878\ttotal: 1.95s\tremaining: 3.92s\n",
      "332:\tlearn: 0.1001370\ttotal: 1.95s\tremaining: 3.91s\n",
      "333:\tlearn: 0.1001003\ttotal: 1.96s\tremaining: 3.9s\n",
      "334:\tlearn: 0.1000575\ttotal: 1.97s\tremaining: 3.91s\n",
      "335:\tlearn: 0.1000214\ttotal: 1.98s\tremaining: 3.91s\n",
      "336:\tlearn: 0.0999484\ttotal: 1.99s\tremaining: 3.91s\n",
      "337:\tlearn: 0.0998697\ttotal: 2s\tremaining: 3.91s\n",
      "338:\tlearn: 0.0997939\ttotal: 2.02s\tremaining: 3.93s\n",
      "339:\tlearn: 0.0997201\ttotal: 2.04s\tremaining: 3.96s\n",
      "340:\tlearn: 0.0996772\ttotal: 2.05s\tremaining: 3.95s\n",
      "341:\tlearn: 0.0996202\ttotal: 2.05s\tremaining: 3.95s\n",
      "342:\tlearn: 0.0996008\ttotal: 2.06s\tremaining: 3.94s\n",
      "343:\tlearn: 0.0995334\ttotal: 2.06s\tremaining: 3.93s\n",
      "344:\tlearn: 0.0994661\ttotal: 2.07s\tremaining: 3.92s\n",
      "345:\tlearn: 0.0993954\ttotal: 2.07s\tremaining: 3.92s\n",
      "346:\tlearn: 0.0993662\ttotal: 2.08s\tremaining: 3.91s\n",
      "347:\tlearn: 0.0993094\ttotal: 2.08s\tremaining: 3.9s\n",
      "348:\tlearn: 0.0992805\ttotal: 2.09s\tremaining: 3.89s\n",
      "349:\tlearn: 0.0992451\ttotal: 2.09s\tremaining: 3.89s\n",
      "350:\tlearn: 0.0991972\ttotal: 2.1s\tremaining: 3.88s\n",
      "351:\tlearn: 0.0991746\ttotal: 2.1s\tremaining: 3.87s\n",
      "352:\tlearn: 0.0991296\ttotal: 2.11s\tremaining: 3.87s\n",
      "353:\tlearn: 0.0990904\ttotal: 2.12s\tremaining: 3.86s\n",
      "354:\tlearn: 0.0990680\ttotal: 2.12s\tremaining: 3.85s\n",
      "355:\tlearn: 0.0990062\ttotal: 2.13s\tremaining: 3.85s\n",
      "356:\tlearn: 0.0989452\ttotal: 2.13s\tremaining: 3.85s\n",
      "357:\tlearn: 0.0989155\ttotal: 2.14s\tremaining: 3.84s\n",
      "358:\tlearn: 0.0988815\ttotal: 2.15s\tremaining: 3.83s\n",
      "359:\tlearn: 0.0988393\ttotal: 2.15s\tremaining: 3.83s\n",
      "360:\tlearn: 0.0987442\ttotal: 2.16s\tremaining: 3.82s\n",
      "361:\tlearn: 0.0987040\ttotal: 2.17s\tremaining: 3.82s\n",
      "362:\tlearn: 0.0986424\ttotal: 2.17s\tremaining: 3.81s\n",
      "363:\tlearn: 0.0985916\ttotal: 2.18s\tremaining: 3.81s\n",
      "364:\tlearn: 0.0985549\ttotal: 2.18s\tremaining: 3.8s\n",
      "365:\tlearn: 0.0985285\ttotal: 2.19s\tremaining: 3.79s\n",
      "366:\tlearn: 0.0985038\ttotal: 2.19s\tremaining: 3.78s\n",
      "367:\tlearn: 0.0984746\ttotal: 2.2s\tremaining: 3.77s\n",
      "368:\tlearn: 0.0984215\ttotal: 2.21s\tremaining: 3.77s\n",
      "369:\tlearn: 0.0983827\ttotal: 2.21s\tremaining: 3.77s\n",
      "370:\tlearn: 0.0983592\ttotal: 2.22s\tremaining: 3.76s\n",
      "371:\tlearn: 0.0983258\ttotal: 2.23s\tremaining: 3.76s\n",
      "372:\tlearn: 0.0982904\ttotal: 2.27s\tremaining: 3.81s\n",
      "373:\tlearn: 0.0982600\ttotal: 2.28s\tremaining: 3.81s\n",
      "374:\tlearn: 0.0982179\ttotal: 2.29s\tremaining: 3.81s\n",
      "375:\tlearn: 0.0981679\ttotal: 2.3s\tremaining: 3.82s\n",
      "376:\tlearn: 0.0981436\ttotal: 2.31s\tremaining: 3.82s\n",
      "377:\tlearn: 0.0981110\ttotal: 2.32s\tremaining: 3.81s\n",
      "378:\tlearn: 0.0980718\ttotal: 2.33s\tremaining: 3.81s\n",
      "379:\tlearn: 0.0980397\ttotal: 2.33s\tremaining: 3.81s\n",
      "380:\tlearn: 0.0980075\ttotal: 2.34s\tremaining: 3.81s\n",
      "381:\tlearn: 0.0979813\ttotal: 2.36s\tremaining: 3.82s\n",
      "382:\tlearn: 0.0979272\ttotal: 2.37s\tremaining: 3.82s\n",
      "383:\tlearn: 0.0978898\ttotal: 2.38s\tremaining: 3.82s\n",
      "384:\tlearn: 0.0978235\ttotal: 2.39s\tremaining: 3.82s\n",
      "385:\tlearn: 0.0977887\ttotal: 2.4s\tremaining: 3.82s\n",
      "386:\tlearn: 0.0977431\ttotal: 2.41s\tremaining: 3.82s\n",
      "387:\tlearn: 0.0977108\ttotal: 2.44s\tremaining: 3.84s\n",
      "388:\tlearn: 0.0976326\ttotal: 2.45s\tremaining: 3.84s\n",
      "389:\tlearn: 0.0976019\ttotal: 2.46s\tremaining: 3.84s\n",
      "390:\tlearn: 0.0975718\ttotal: 2.46s\tremaining: 3.84s\n",
      "391:\tlearn: 0.0975436\ttotal: 2.48s\tremaining: 3.85s\n",
      "392:\tlearn: 0.0974982\ttotal: 2.49s\tremaining: 3.84s\n",
      "393:\tlearn: 0.0974484\ttotal: 2.5s\tremaining: 3.84s\n",
      "394:\tlearn: 0.0973813\ttotal: 2.5s\tremaining: 3.84s\n",
      "395:\tlearn: 0.0973563\ttotal: 2.51s\tremaining: 3.83s\n",
      "396:\tlearn: 0.0973081\ttotal: 2.52s\tremaining: 3.83s\n",
      "397:\tlearn: 0.0972857\ttotal: 2.54s\tremaining: 3.83s\n",
      "398:\tlearn: 0.0972555\ttotal: 2.54s\tremaining: 3.83s\n",
      "399:\tlearn: 0.0972252\ttotal: 2.56s\tremaining: 3.83s\n",
      "400:\tlearn: 0.0971994\ttotal: 2.56s\tremaining: 3.83s\n",
      "401:\tlearn: 0.0971714\ttotal: 2.57s\tremaining: 3.82s\n",
      "402:\tlearn: 0.0971397\ttotal: 2.58s\tremaining: 3.82s\n",
      "403:\tlearn: 0.0971151\ttotal: 2.58s\tremaining: 3.81s\n",
      "404:\tlearn: 0.0970862\ttotal: 2.59s\tremaining: 3.8s\n",
      "405:\tlearn: 0.0970646\ttotal: 2.59s\tremaining: 3.79s\n",
      "406:\tlearn: 0.0969873\ttotal: 2.6s\tremaining: 3.79s\n",
      "407:\tlearn: 0.0969347\ttotal: 2.6s\tremaining: 3.78s\n",
      "408:\tlearn: 0.0969027\ttotal: 2.61s\tremaining: 3.77s\n",
      "409:\tlearn: 0.0968507\ttotal: 2.61s\tremaining: 3.76s\n",
      "410:\tlearn: 0.0968156\ttotal: 2.62s\tremaining: 3.75s\n",
      "411:\tlearn: 0.0967665\ttotal: 2.63s\tremaining: 3.75s\n",
      "412:\tlearn: 0.0967182\ttotal: 2.63s\tremaining: 3.74s\n",
      "413:\tlearn: 0.0966725\ttotal: 2.64s\tremaining: 3.74s\n",
      "414:\tlearn: 0.0966267\ttotal: 2.65s\tremaining: 3.73s\n",
      "415:\tlearn: 0.0965915\ttotal: 2.65s\tremaining: 3.73s\n",
      "416:\tlearn: 0.0965465\ttotal: 2.66s\tremaining: 3.72s\n",
      "417:\tlearn: 0.0964731\ttotal: 2.67s\tremaining: 3.71s\n",
      "418:\tlearn: 0.0964388\ttotal: 2.67s\tremaining: 3.7s\n",
      "419:\tlearn: 0.0964077\ttotal: 2.68s\tremaining: 3.7s\n",
      "420:\tlearn: 0.0963769\ttotal: 2.68s\tremaining: 3.69s\n",
      "421:\tlearn: 0.0963229\ttotal: 2.69s\tremaining: 3.68s\n",
      "422:\tlearn: 0.0962993\ttotal: 2.69s\tremaining: 3.67s\n",
      "423:\tlearn: 0.0962803\ttotal: 2.7s\tremaining: 3.67s\n",
      "424:\tlearn: 0.0962529\ttotal: 2.7s\tremaining: 3.66s\n",
      "425:\tlearn: 0.0962031\ttotal: 2.71s\tremaining: 3.65s\n",
      "426:\tlearn: 0.0961805\ttotal: 2.71s\tremaining: 3.64s\n",
      "427:\tlearn: 0.0961471\ttotal: 2.72s\tremaining: 3.63s\n",
      "428:\tlearn: 0.0961254\ttotal: 2.72s\tremaining: 3.63s\n",
      "429:\tlearn: 0.0960782\ttotal: 2.73s\tremaining: 3.62s\n",
      "430:\tlearn: 0.0960585\ttotal: 2.73s\tremaining: 3.61s\n",
      "431:\tlearn: 0.0960178\ttotal: 2.74s\tremaining: 3.6s\n",
      "432:\tlearn: 0.0959653\ttotal: 2.75s\tremaining: 3.6s\n",
      "433:\tlearn: 0.0959394\ttotal: 2.75s\tremaining: 3.59s\n",
      "434:\tlearn: 0.0958894\ttotal: 2.76s\tremaining: 3.58s\n",
      "435:\tlearn: 0.0958367\ttotal: 2.77s\tremaining: 3.58s\n",
      "436:\tlearn: 0.0957886\ttotal: 2.77s\tremaining: 3.57s\n",
      "437:\tlearn: 0.0957639\ttotal: 2.78s\tremaining: 3.56s\n",
      "438:\tlearn: 0.0957455\ttotal: 2.78s\tremaining: 3.56s\n",
      "439:\tlearn: 0.0957219\ttotal: 2.79s\tremaining: 3.55s\n",
      "440:\tlearn: 0.0956734\ttotal: 2.8s\tremaining: 3.54s\n",
      "441:\tlearn: 0.0956627\ttotal: 2.8s\tremaining: 3.54s\n",
      "442:\tlearn: 0.0956433\ttotal: 2.81s\tremaining: 3.53s\n",
      "443:\tlearn: 0.0956059\ttotal: 2.81s\tremaining: 3.52s\n",
      "444:\tlearn: 0.0955576\ttotal: 2.82s\tremaining: 3.52s\n",
      "445:\tlearn: 0.0955139\ttotal: 2.83s\tremaining: 3.51s\n",
      "446:\tlearn: 0.0954903\ttotal: 2.83s\tremaining: 3.5s\n",
      "447:\tlearn: 0.0954128\ttotal: 2.84s\tremaining: 3.5s\n",
      "448:\tlearn: 0.0953894\ttotal: 2.84s\tremaining: 3.49s\n",
      "449:\tlearn: 0.0953718\ttotal: 2.85s\tremaining: 3.48s\n",
      "450:\tlearn: 0.0953433\ttotal: 2.85s\tremaining: 3.48s\n",
      "451:\tlearn: 0.0952842\ttotal: 2.86s\tremaining: 3.47s\n",
      "452:\tlearn: 0.0952652\ttotal: 2.87s\tremaining: 3.46s\n",
      "453:\tlearn: 0.0952180\ttotal: 2.87s\tremaining: 3.45s\n",
      "454:\tlearn: 0.0951512\ttotal: 2.88s\tremaining: 3.45s\n",
      "455:\tlearn: 0.0950725\ttotal: 2.88s\tremaining: 3.44s\n",
      "456:\tlearn: 0.0950311\ttotal: 2.89s\tremaining: 3.43s\n",
      "457:\tlearn: 0.0949898\ttotal: 2.89s\tremaining: 3.42s\n",
      "458:\tlearn: 0.0949460\ttotal: 2.9s\tremaining: 3.42s\n",
      "459:\tlearn: 0.0949038\ttotal: 2.9s\tremaining: 3.41s\n",
      "460:\tlearn: 0.0948443\ttotal: 2.91s\tremaining: 3.4s\n",
      "461:\tlearn: 0.0948260\ttotal: 2.92s\tremaining: 3.4s\n",
      "462:\tlearn: 0.0947961\ttotal: 2.93s\tremaining: 3.4s\n",
      "463:\tlearn: 0.0947702\ttotal: 2.94s\tremaining: 3.4s\n",
      "464:\tlearn: 0.0947524\ttotal: 2.95s\tremaining: 3.4s\n",
      "465:\tlearn: 0.0947111\ttotal: 2.96s\tremaining: 3.4s\n",
      "466:\tlearn: 0.0946888\ttotal: 2.97s\tremaining: 3.39s\n",
      "467:\tlearn: 0.0946452\ttotal: 2.98s\tremaining: 3.39s\n",
      "468:\tlearn: 0.0946046\ttotal: 3s\tremaining: 3.39s\n",
      "469:\tlearn: 0.0945715\ttotal: 3s\tremaining: 3.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470:\tlearn: 0.0945620\ttotal: 3.01s\tremaining: 3.38s\n",
      "471:\tlearn: 0.0945444\ttotal: 3.01s\tremaining: 3.37s\n",
      "472:\tlearn: 0.0945121\ttotal: 3.02s\tremaining: 3.36s\n",
      "473:\tlearn: 0.0944936\ttotal: 3.02s\tremaining: 3.35s\n",
      "474:\tlearn: 0.0944686\ttotal: 3.03s\tremaining: 3.35s\n",
      "475:\tlearn: 0.0944266\ttotal: 3.03s\tremaining: 3.34s\n",
      "476:\tlearn: 0.0944065\ttotal: 3.04s\tremaining: 3.33s\n",
      "477:\tlearn: 0.0943807\ttotal: 3.04s\tremaining: 3.33s\n",
      "478:\tlearn: 0.0943570\ttotal: 3.05s\tremaining: 3.32s\n",
      "479:\tlearn: 0.0943197\ttotal: 3.06s\tremaining: 3.31s\n",
      "480:\tlearn: 0.0942893\ttotal: 3.06s\tremaining: 3.3s\n",
      "481:\tlearn: 0.0942501\ttotal: 3.07s\tremaining: 3.29s\n",
      "482:\tlearn: 0.0942131\ttotal: 3.07s\tremaining: 3.29s\n",
      "483:\tlearn: 0.0941968\ttotal: 3.08s\tremaining: 3.28s\n",
      "484:\tlearn: 0.0941738\ttotal: 3.08s\tremaining: 3.27s\n",
      "485:\tlearn: 0.0941185\ttotal: 3.09s\tremaining: 3.27s\n",
      "486:\tlearn: 0.0940913\ttotal: 3.09s\tremaining: 3.26s\n",
      "487:\tlearn: 0.0940785\ttotal: 3.1s\tremaining: 3.25s\n",
      "488:\tlearn: 0.0940637\ttotal: 3.11s\tremaining: 3.25s\n",
      "489:\tlearn: 0.0940315\ttotal: 3.12s\tremaining: 3.25s\n",
      "490:\tlearn: 0.0939809\ttotal: 3.13s\tremaining: 3.24s\n",
      "491:\tlearn: 0.0939608\ttotal: 3.13s\tremaining: 3.23s\n",
      "492:\tlearn: 0.0939441\ttotal: 3.14s\tremaining: 3.23s\n",
      "493:\tlearn: 0.0938831\ttotal: 3.14s\tremaining: 3.22s\n",
      "494:\tlearn: 0.0938692\ttotal: 3.15s\tremaining: 3.21s\n",
      "495:\tlearn: 0.0938323\ttotal: 3.15s\tremaining: 3.21s\n",
      "496:\tlearn: 0.0937981\ttotal: 3.16s\tremaining: 3.2s\n",
      "497:\tlearn: 0.0937822\ttotal: 3.16s\tremaining: 3.19s\n",
      "498:\tlearn: 0.0937555\ttotal: 3.17s\tremaining: 3.18s\n",
      "499:\tlearn: 0.0937252\ttotal: 3.17s\tremaining: 3.17s\n",
      "500:\tlearn: 0.0937093\ttotal: 3.18s\tremaining: 3.17s\n",
      "501:\tlearn: 0.0936740\ttotal: 3.19s\tremaining: 3.16s\n",
      "502:\tlearn: 0.0936492\ttotal: 3.19s\tremaining: 3.15s\n",
      "503:\tlearn: 0.0936211\ttotal: 3.2s\tremaining: 3.15s\n",
      "504:\tlearn: 0.0935945\ttotal: 3.21s\tremaining: 3.14s\n",
      "505:\tlearn: 0.0935614\ttotal: 3.21s\tremaining: 3.13s\n",
      "506:\tlearn: 0.0935103\ttotal: 3.21s\tremaining: 3.13s\n",
      "507:\tlearn: 0.0934829\ttotal: 3.22s\tremaining: 3.12s\n",
      "508:\tlearn: 0.0934557\ttotal: 3.23s\tremaining: 3.11s\n",
      "509:\tlearn: 0.0934225\ttotal: 3.23s\tremaining: 3.1s\n",
      "510:\tlearn: 0.0934085\ttotal: 3.24s\tremaining: 3.1s\n",
      "511:\tlearn: 0.0933916\ttotal: 3.24s\tremaining: 3.09s\n",
      "512:\tlearn: 0.0933671\ttotal: 3.25s\tremaining: 3.08s\n",
      "513:\tlearn: 0.0933439\ttotal: 3.25s\tremaining: 3.08s\n",
      "514:\tlearn: 0.0933284\ttotal: 3.26s\tremaining: 3.07s\n",
      "515:\tlearn: 0.0933059\ttotal: 3.27s\tremaining: 3.06s\n",
      "516:\tlearn: 0.0932914\ttotal: 3.27s\tremaining: 3.06s\n",
      "517:\tlearn: 0.0932800\ttotal: 3.28s\tremaining: 3.05s\n",
      "518:\tlearn: 0.0932611\ttotal: 3.28s\tremaining: 3.04s\n",
      "519:\tlearn: 0.0932449\ttotal: 3.29s\tremaining: 3.03s\n",
      "520:\tlearn: 0.0932169\ttotal: 3.29s\tremaining: 3.03s\n",
      "521:\tlearn: 0.0932016\ttotal: 3.3s\tremaining: 3.02s\n",
      "522:\tlearn: 0.0931765\ttotal: 3.31s\tremaining: 3.01s\n",
      "523:\tlearn: 0.0931493\ttotal: 3.31s\tremaining: 3.01s\n",
      "524:\tlearn: 0.0931338\ttotal: 3.31s\tremaining: 3s\n",
      "525:\tlearn: 0.0931007\ttotal: 3.32s\tremaining: 2.99s\n",
      "526:\tlearn: 0.0930745\ttotal: 3.33s\tremaining: 2.98s\n",
      "527:\tlearn: 0.0930515\ttotal: 3.33s\tremaining: 2.98s\n",
      "528:\tlearn: 0.0930191\ttotal: 3.34s\tremaining: 2.97s\n",
      "529:\tlearn: 0.0929956\ttotal: 3.34s\tremaining: 2.96s\n",
      "530:\tlearn: 0.0929251\ttotal: 3.35s\tremaining: 2.96s\n",
      "531:\tlearn: 0.0928484\ttotal: 3.35s\tremaining: 2.95s\n",
      "532:\tlearn: 0.0928195\ttotal: 3.36s\tremaining: 2.94s\n",
      "533:\tlearn: 0.0927941\ttotal: 3.37s\tremaining: 2.94s\n",
      "534:\tlearn: 0.0927724\ttotal: 3.37s\tremaining: 2.93s\n",
      "535:\tlearn: 0.0927366\ttotal: 3.38s\tremaining: 2.92s\n",
      "536:\tlearn: 0.0927261\ttotal: 3.38s\tremaining: 2.92s\n",
      "537:\tlearn: 0.0926916\ttotal: 3.39s\tremaining: 2.91s\n",
      "538:\tlearn: 0.0926856\ttotal: 3.39s\tremaining: 2.9s\n",
      "539:\tlearn: 0.0926358\ttotal: 3.4s\tremaining: 2.89s\n",
      "540:\tlearn: 0.0926217\ttotal: 3.4s\tremaining: 2.89s\n",
      "541:\tlearn: 0.0925991\ttotal: 3.41s\tremaining: 2.88s\n",
      "542:\tlearn: 0.0925820\ttotal: 3.41s\tremaining: 2.87s\n",
      "543:\tlearn: 0.0925450\ttotal: 3.42s\tremaining: 2.87s\n",
      "544:\tlearn: 0.0925202\ttotal: 3.42s\tremaining: 2.86s\n",
      "545:\tlearn: 0.0925018\ttotal: 3.43s\tremaining: 2.85s\n",
      "546:\tlearn: 0.0924830\ttotal: 3.44s\tremaining: 2.85s\n",
      "547:\tlearn: 0.0924619\ttotal: 3.44s\tremaining: 2.84s\n",
      "548:\tlearn: 0.0924521\ttotal: 3.45s\tremaining: 2.83s\n",
      "549:\tlearn: 0.0924362\ttotal: 3.45s\tremaining: 2.83s\n",
      "550:\tlearn: 0.0924139\ttotal: 3.46s\tremaining: 2.82s\n",
      "551:\tlearn: 0.0923773\ttotal: 3.46s\tremaining: 2.81s\n",
      "552:\tlearn: 0.0923466\ttotal: 3.47s\tremaining: 2.8s\n",
      "553:\tlearn: 0.0923195\ttotal: 3.47s\tremaining: 2.8s\n",
      "554:\tlearn: 0.0922918\ttotal: 3.48s\tremaining: 2.79s\n",
      "555:\tlearn: 0.0922596\ttotal: 3.48s\tremaining: 2.78s\n",
      "556:\tlearn: 0.0922440\ttotal: 3.49s\tremaining: 2.78s\n",
      "557:\tlearn: 0.0922226\ttotal: 3.5s\tremaining: 2.77s\n",
      "558:\tlearn: 0.0921998\ttotal: 3.51s\tremaining: 2.77s\n",
      "559:\tlearn: 0.0921857\ttotal: 3.52s\tremaining: 2.76s\n",
      "560:\tlearn: 0.0921531\ttotal: 3.52s\tremaining: 2.76s\n",
      "561:\tlearn: 0.0921223\ttotal: 3.53s\tremaining: 2.75s\n",
      "562:\tlearn: 0.0921068\ttotal: 3.53s\tremaining: 2.74s\n",
      "563:\tlearn: 0.0920927\ttotal: 3.54s\tremaining: 2.74s\n",
      "564:\tlearn: 0.0920786\ttotal: 3.54s\tremaining: 2.73s\n",
      "565:\tlearn: 0.0920638\ttotal: 3.55s\tremaining: 2.72s\n",
      "566:\tlearn: 0.0920437\ttotal: 3.56s\tremaining: 2.72s\n",
      "567:\tlearn: 0.0920119\ttotal: 3.57s\tremaining: 2.71s\n",
      "568:\tlearn: 0.0919885\ttotal: 3.58s\tremaining: 2.71s\n",
      "569:\tlearn: 0.0919613\ttotal: 3.58s\tremaining: 2.7s\n",
      "570:\tlearn: 0.0919367\ttotal: 3.59s\tremaining: 2.69s\n",
      "571:\tlearn: 0.0919243\ttotal: 3.59s\tremaining: 2.69s\n",
      "572:\tlearn: 0.0918820\ttotal: 3.6s\tremaining: 2.68s\n",
      "573:\tlearn: 0.0918631\ttotal: 3.61s\tremaining: 2.68s\n",
      "574:\tlearn: 0.0918224\ttotal: 3.61s\tremaining: 2.67s\n",
      "575:\tlearn: 0.0918022\ttotal: 3.62s\tremaining: 2.66s\n",
      "576:\tlearn: 0.0917822\ttotal: 3.63s\tremaining: 2.66s\n",
      "577:\tlearn: 0.0917594\ttotal: 3.63s\tremaining: 2.65s\n",
      "578:\tlearn: 0.0917337\ttotal: 3.64s\tremaining: 2.65s\n",
      "579:\tlearn: 0.0917174\ttotal: 3.65s\tremaining: 2.64s\n",
      "580:\tlearn: 0.0916755\ttotal: 3.65s\tremaining: 2.63s\n",
      "581:\tlearn: 0.0916587\ttotal: 3.66s\tremaining: 2.63s\n",
      "582:\tlearn: 0.0916411\ttotal: 3.66s\tremaining: 2.62s\n",
      "583:\tlearn: 0.0916244\ttotal: 3.67s\tremaining: 2.61s\n",
      "584:\tlearn: 0.0915933\ttotal: 3.67s\tremaining: 2.61s\n",
      "585:\tlearn: 0.0915699\ttotal: 3.68s\tremaining: 2.6s\n",
      "586:\tlearn: 0.0915530\ttotal: 3.69s\tremaining: 2.59s\n",
      "587:\tlearn: 0.0915257\ttotal: 3.69s\tremaining: 2.59s\n",
      "588:\tlearn: 0.0915117\ttotal: 3.7s\tremaining: 2.58s\n",
      "589:\tlearn: 0.0914957\ttotal: 3.71s\tremaining: 2.58s\n",
      "590:\tlearn: 0.0914796\ttotal: 3.71s\tremaining: 2.57s\n",
      "591:\tlearn: 0.0914367\ttotal: 3.72s\tremaining: 2.56s\n",
      "592:\tlearn: 0.0914152\ttotal: 3.72s\tremaining: 2.55s\n",
      "593:\tlearn: 0.0913654\ttotal: 3.73s\tremaining: 2.55s\n",
      "594:\tlearn: 0.0913540\ttotal: 3.73s\tremaining: 2.54s\n",
      "595:\tlearn: 0.0913402\ttotal: 3.74s\tremaining: 2.53s\n",
      "596:\tlearn: 0.0913205\ttotal: 3.74s\tremaining: 2.53s\n",
      "597:\tlearn: 0.0913084\ttotal: 3.75s\tremaining: 2.52s\n",
      "598:\tlearn: 0.0912663\ttotal: 3.76s\tremaining: 2.51s\n",
      "599:\tlearn: 0.0912574\ttotal: 3.76s\tremaining: 2.51s\n",
      "600:\tlearn: 0.0912477\ttotal: 3.77s\tremaining: 2.5s\n",
      "601:\tlearn: 0.0912136\ttotal: 3.77s\tremaining: 2.49s\n",
      "602:\tlearn: 0.0911674\ttotal: 3.78s\tremaining: 2.49s\n",
      "603:\tlearn: 0.0911312\ttotal: 3.79s\tremaining: 2.48s\n",
      "604:\tlearn: 0.0911047\ttotal: 3.79s\tremaining: 2.47s\n",
      "605:\tlearn: 0.0910953\ttotal: 3.79s\tremaining: 2.47s\n",
      "606:\tlearn: 0.0910807\ttotal: 3.8s\tremaining: 2.46s\n",
      "607:\tlearn: 0.0910471\ttotal: 3.81s\tremaining: 2.45s\n",
      "608:\tlearn: 0.0910166\ttotal: 3.81s\tremaining: 2.45s\n",
      "609:\tlearn: 0.0910012\ttotal: 3.82s\tremaining: 2.44s\n",
      "610:\tlearn: 0.0909774\ttotal: 3.82s\tremaining: 2.43s\n",
      "611:\tlearn: 0.0909624\ttotal: 3.83s\tremaining: 2.43s\n",
      "612:\tlearn: 0.0909351\ttotal: 3.83s\tremaining: 2.42s\n",
      "613:\tlearn: 0.0908958\ttotal: 3.84s\tremaining: 2.41s\n",
      "614:\tlearn: 0.0908699\ttotal: 3.85s\tremaining: 2.41s\n",
      "615:\tlearn: 0.0908436\ttotal: 3.85s\tremaining: 2.4s\n",
      "616:\tlearn: 0.0908292\ttotal: 3.86s\tremaining: 2.4s\n",
      "617:\tlearn: 0.0908153\ttotal: 3.87s\tremaining: 2.4s\n",
      "618:\tlearn: 0.0907990\ttotal: 3.89s\tremaining: 2.4s\n",
      "619:\tlearn: 0.0907867\ttotal: 3.91s\tremaining: 2.39s\n",
      "620:\tlearn: 0.0907785\ttotal: 3.92s\tremaining: 2.39s\n",
      "621:\tlearn: 0.0907587\ttotal: 3.93s\tremaining: 2.39s\n",
      "622:\tlearn: 0.0907425\ttotal: 3.94s\tremaining: 2.38s\n",
      "623:\tlearn: 0.0907186\ttotal: 3.95s\tremaining: 2.38s\n",
      "624:\tlearn: 0.0907066\ttotal: 3.96s\tremaining: 2.37s\n",
      "625:\tlearn: 0.0906858\ttotal: 3.96s\tremaining: 2.37s\n",
      "626:\tlearn: 0.0906699\ttotal: 3.96s\tremaining: 2.36s\n",
      "627:\tlearn: 0.0906577\ttotal: 3.97s\tremaining: 2.35s\n",
      "628:\tlearn: 0.0906482\ttotal: 3.98s\tremaining: 2.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629:\tlearn: 0.0906094\ttotal: 3.99s\tremaining: 2.34s\n",
      "630:\tlearn: 0.0905952\ttotal: 3.99s\tremaining: 2.33s\n",
      "631:\tlearn: 0.0905749\ttotal: 4s\tremaining: 2.33s\n",
      "632:\tlearn: 0.0905552\ttotal: 4s\tremaining: 2.32s\n",
      "633:\tlearn: 0.0905365\ttotal: 4.01s\tremaining: 2.31s\n",
      "634:\tlearn: 0.0905192\ttotal: 4.01s\tremaining: 2.31s\n",
      "635:\tlearn: 0.0905057\ttotal: 4.02s\tremaining: 2.3s\n",
      "636:\tlearn: 0.0904956\ttotal: 4.02s\tremaining: 2.29s\n",
      "637:\tlearn: 0.0904720\ttotal: 4.03s\tremaining: 2.29s\n",
      "638:\tlearn: 0.0904428\ttotal: 4.04s\tremaining: 2.28s\n",
      "639:\tlearn: 0.0904354\ttotal: 4.04s\tremaining: 2.27s\n",
      "640:\tlearn: 0.0903969\ttotal: 4.05s\tremaining: 2.27s\n",
      "641:\tlearn: 0.0903672\ttotal: 4.05s\tremaining: 2.26s\n",
      "642:\tlearn: 0.0903582\ttotal: 4.06s\tremaining: 2.25s\n",
      "643:\tlearn: 0.0903410\ttotal: 4.06s\tremaining: 2.25s\n",
      "644:\tlearn: 0.0903029\ttotal: 4.07s\tremaining: 2.24s\n",
      "645:\tlearn: 0.0902697\ttotal: 4.07s\tremaining: 2.23s\n",
      "646:\tlearn: 0.0902406\ttotal: 4.08s\tremaining: 2.23s\n",
      "647:\tlearn: 0.0902333\ttotal: 4.08s\tremaining: 2.22s\n",
      "648:\tlearn: 0.0902018\ttotal: 4.09s\tremaining: 2.21s\n",
      "649:\tlearn: 0.0901765\ttotal: 4.1s\tremaining: 2.21s\n",
      "650:\tlearn: 0.0901608\ttotal: 4.1s\tremaining: 2.2s\n",
      "651:\tlearn: 0.0901513\ttotal: 4.11s\tremaining: 2.19s\n",
      "652:\tlearn: 0.0901219\ttotal: 4.11s\tremaining: 2.19s\n",
      "653:\tlearn: 0.0900913\ttotal: 4.12s\tremaining: 2.18s\n",
      "654:\tlearn: 0.0900732\ttotal: 4.13s\tremaining: 2.17s\n",
      "655:\tlearn: 0.0900620\ttotal: 4.13s\tremaining: 2.17s\n",
      "656:\tlearn: 0.0900298\ttotal: 4.13s\tremaining: 2.16s\n",
      "657:\tlearn: 0.0900141\ttotal: 4.14s\tremaining: 2.15s\n",
      "658:\tlearn: 0.0900019\ttotal: 4.15s\tremaining: 2.15s\n",
      "659:\tlearn: 0.0899822\ttotal: 4.15s\tremaining: 2.14s\n",
      "660:\tlearn: 0.0899421\ttotal: 4.16s\tremaining: 2.13s\n",
      "661:\tlearn: 0.0899338\ttotal: 4.17s\tremaining: 2.13s\n",
      "662:\tlearn: 0.0899143\ttotal: 4.17s\tremaining: 2.12s\n",
      "663:\tlearn: 0.0899053\ttotal: 4.18s\tremaining: 2.12s\n",
      "664:\tlearn: 0.0898769\ttotal: 4.19s\tremaining: 2.11s\n",
      "665:\tlearn: 0.0898457\ttotal: 4.19s\tremaining: 2.1s\n",
      "666:\tlearn: 0.0898204\ttotal: 4.2s\tremaining: 2.1s\n",
      "667:\tlearn: 0.0897950\ttotal: 4.2s\tremaining: 2.09s\n",
      "668:\tlearn: 0.0897727\ttotal: 4.21s\tremaining: 2.08s\n",
      "669:\tlearn: 0.0897535\ttotal: 4.21s\tremaining: 2.08s\n",
      "670:\tlearn: 0.0897262\ttotal: 4.22s\tremaining: 2.07s\n",
      "671:\tlearn: 0.0897101\ttotal: 4.22s\tremaining: 2.06s\n",
      "672:\tlearn: 0.0897024\ttotal: 4.23s\tremaining: 2.06s\n",
      "673:\tlearn: 0.0896956\ttotal: 4.24s\tremaining: 2.05s\n",
      "674:\tlearn: 0.0896830\ttotal: 4.24s\tremaining: 2.04s\n",
      "675:\tlearn: 0.0896685\ttotal: 4.25s\tremaining: 2.04s\n",
      "676:\tlearn: 0.0896602\ttotal: 4.25s\tremaining: 2.03s\n",
      "677:\tlearn: 0.0896472\ttotal: 4.26s\tremaining: 2.02s\n",
      "678:\tlearn: 0.0896140\ttotal: 4.26s\tremaining: 2.02s\n",
      "679:\tlearn: 0.0895963\ttotal: 4.27s\tremaining: 2.01s\n",
      "680:\tlearn: 0.0895725\ttotal: 4.28s\tremaining: 2s\n",
      "681:\tlearn: 0.0895546\ttotal: 4.28s\tremaining: 2s\n",
      "682:\tlearn: 0.0895376\ttotal: 4.29s\tremaining: 1.99s\n",
      "683:\tlearn: 0.0895324\ttotal: 4.29s\tremaining: 1.98s\n",
      "684:\tlearn: 0.0895205\ttotal: 4.3s\tremaining: 1.98s\n",
      "685:\tlearn: 0.0894992\ttotal: 4.3s\tremaining: 1.97s\n",
      "686:\tlearn: 0.0894849\ttotal: 4.31s\tremaining: 1.96s\n",
      "687:\tlearn: 0.0894690\ttotal: 4.32s\tremaining: 1.96s\n",
      "688:\tlearn: 0.0894615\ttotal: 4.32s\tremaining: 1.95s\n",
      "689:\tlearn: 0.0894545\ttotal: 4.33s\tremaining: 1.94s\n",
      "690:\tlearn: 0.0894442\ttotal: 4.33s\tremaining: 1.94s\n",
      "691:\tlearn: 0.0894246\ttotal: 4.34s\tremaining: 1.93s\n",
      "692:\tlearn: 0.0894094\ttotal: 4.34s\tremaining: 1.92s\n",
      "693:\tlearn: 0.0893965\ttotal: 4.35s\tremaining: 1.92s\n",
      "694:\tlearn: 0.0893842\ttotal: 4.36s\tremaining: 1.91s\n",
      "695:\tlearn: 0.0893524\ttotal: 4.36s\tremaining: 1.91s\n",
      "696:\tlearn: 0.0893397\ttotal: 4.37s\tremaining: 1.9s\n",
      "697:\tlearn: 0.0893277\ttotal: 4.37s\tremaining: 1.89s\n",
      "698:\tlearn: 0.0893152\ttotal: 4.38s\tremaining: 1.89s\n",
      "699:\tlearn: 0.0893033\ttotal: 4.38s\tremaining: 1.88s\n",
      "700:\tlearn: 0.0892938\ttotal: 4.39s\tremaining: 1.87s\n",
      "701:\tlearn: 0.0892765\ttotal: 4.39s\tremaining: 1.86s\n",
      "702:\tlearn: 0.0892480\ttotal: 4.4s\tremaining: 1.86s\n",
      "703:\tlearn: 0.0892421\ttotal: 4.41s\tremaining: 1.85s\n",
      "704:\tlearn: 0.0892210\ttotal: 4.41s\tremaining: 1.84s\n",
      "705:\tlearn: 0.0891924\ttotal: 4.42s\tremaining: 1.84s\n",
      "706:\tlearn: 0.0891666\ttotal: 4.42s\tremaining: 1.83s\n",
      "707:\tlearn: 0.0891520\ttotal: 4.43s\tremaining: 1.83s\n",
      "708:\tlearn: 0.0891388\ttotal: 4.43s\tremaining: 1.82s\n",
      "709:\tlearn: 0.0891251\ttotal: 4.44s\tremaining: 1.81s\n",
      "710:\tlearn: 0.0891111\ttotal: 4.45s\tremaining: 1.81s\n",
      "711:\tlearn: 0.0891007\ttotal: 4.45s\tremaining: 1.8s\n",
      "712:\tlearn: 0.0890914\ttotal: 4.46s\tremaining: 1.79s\n",
      "713:\tlearn: 0.0890643\ttotal: 4.47s\tremaining: 1.79s\n",
      "714:\tlearn: 0.0890559\ttotal: 4.47s\tremaining: 1.78s\n",
      "715:\tlearn: 0.0890428\ttotal: 4.48s\tremaining: 1.78s\n",
      "716:\tlearn: 0.0890298\ttotal: 4.48s\tremaining: 1.77s\n",
      "717:\tlearn: 0.0890108\ttotal: 4.49s\tremaining: 1.76s\n",
      "718:\tlearn: 0.0889853\ttotal: 4.49s\tremaining: 1.76s\n",
      "719:\tlearn: 0.0889666\ttotal: 4.5s\tremaining: 1.75s\n",
      "720:\tlearn: 0.0889604\ttotal: 4.51s\tremaining: 1.74s\n",
      "721:\tlearn: 0.0889459\ttotal: 4.51s\tremaining: 1.74s\n",
      "722:\tlearn: 0.0889222\ttotal: 4.52s\tremaining: 1.73s\n",
      "723:\tlearn: 0.0889065\ttotal: 4.52s\tremaining: 1.72s\n",
      "724:\tlearn: 0.0888912\ttotal: 4.53s\tremaining: 1.72s\n",
      "725:\tlearn: 0.0888800\ttotal: 4.53s\tremaining: 1.71s\n",
      "726:\tlearn: 0.0888673\ttotal: 4.54s\tremaining: 1.7s\n",
      "727:\tlearn: 0.0888541\ttotal: 4.54s\tremaining: 1.7s\n",
      "728:\tlearn: 0.0888428\ttotal: 4.55s\tremaining: 1.69s\n",
      "729:\tlearn: 0.0888314\ttotal: 4.55s\tremaining: 1.68s\n",
      "730:\tlearn: 0.0888192\ttotal: 4.56s\tremaining: 1.68s\n",
      "731:\tlearn: 0.0888040\ttotal: 4.57s\tremaining: 1.67s\n",
      "732:\tlearn: 0.0887910\ttotal: 4.57s\tremaining: 1.67s\n",
      "733:\tlearn: 0.0887725\ttotal: 4.58s\tremaining: 1.66s\n",
      "734:\tlearn: 0.0887498\ttotal: 4.58s\tremaining: 1.65s\n",
      "735:\tlearn: 0.0887371\ttotal: 4.59s\tremaining: 1.65s\n",
      "736:\tlearn: 0.0887273\ttotal: 4.59s\tremaining: 1.64s\n",
      "737:\tlearn: 0.0887161\ttotal: 4.61s\tremaining: 1.63s\n",
      "738:\tlearn: 0.0886944\ttotal: 4.61s\tremaining: 1.63s\n",
      "739:\tlearn: 0.0886857\ttotal: 4.62s\tremaining: 1.62s\n",
      "740:\tlearn: 0.0886768\ttotal: 4.63s\tremaining: 1.62s\n",
      "741:\tlearn: 0.0886530\ttotal: 4.63s\tremaining: 1.61s\n",
      "742:\tlearn: 0.0886423\ttotal: 4.64s\tremaining: 1.6s\n",
      "743:\tlearn: 0.0886353\ttotal: 4.64s\tremaining: 1.6s\n",
      "744:\tlearn: 0.0886167\ttotal: 4.65s\tremaining: 1.59s\n",
      "745:\tlearn: 0.0885958\ttotal: 4.65s\tremaining: 1.58s\n",
      "746:\tlearn: 0.0885825\ttotal: 4.66s\tremaining: 1.58s\n",
      "747:\tlearn: 0.0885711\ttotal: 4.67s\tremaining: 1.57s\n",
      "748:\tlearn: 0.0885588\ttotal: 4.67s\tremaining: 1.56s\n",
      "749:\tlearn: 0.0885499\ttotal: 4.68s\tremaining: 1.56s\n",
      "750:\tlearn: 0.0885379\ttotal: 4.68s\tremaining: 1.55s\n",
      "751:\tlearn: 0.0885273\ttotal: 4.69s\tremaining: 1.54s\n",
      "752:\tlearn: 0.0885130\ttotal: 4.69s\tremaining: 1.54s\n",
      "753:\tlearn: 0.0885058\ttotal: 4.7s\tremaining: 1.53s\n",
      "754:\tlearn: 0.0884936\ttotal: 4.7s\tremaining: 1.53s\n",
      "755:\tlearn: 0.0884813\ttotal: 4.71s\tremaining: 1.52s\n",
      "756:\tlearn: 0.0884679\ttotal: 4.72s\tremaining: 1.51s\n",
      "757:\tlearn: 0.0884418\ttotal: 4.72s\tremaining: 1.51s\n",
      "758:\tlearn: 0.0884290\ttotal: 4.73s\tremaining: 1.5s\n",
      "759:\tlearn: 0.0884187\ttotal: 4.74s\tremaining: 1.5s\n",
      "760:\tlearn: 0.0884062\ttotal: 4.74s\tremaining: 1.49s\n",
      "761:\tlearn: 0.0883966\ttotal: 4.75s\tremaining: 1.48s\n",
      "762:\tlearn: 0.0883844\ttotal: 4.75s\tremaining: 1.48s\n",
      "763:\tlearn: 0.0883586\ttotal: 4.76s\tremaining: 1.47s\n",
      "764:\tlearn: 0.0883445\ttotal: 4.77s\tremaining: 1.46s\n",
      "765:\tlearn: 0.0883244\ttotal: 4.77s\tremaining: 1.46s\n",
      "766:\tlearn: 0.0883069\ttotal: 4.78s\tremaining: 1.45s\n",
      "767:\tlearn: 0.0883013\ttotal: 4.78s\tremaining: 1.44s\n",
      "768:\tlearn: 0.0882724\ttotal: 4.79s\tremaining: 1.44s\n",
      "769:\tlearn: 0.0882602\ttotal: 4.79s\tremaining: 1.43s\n",
      "770:\tlearn: 0.0882514\ttotal: 4.8s\tremaining: 1.43s\n",
      "771:\tlearn: 0.0882313\ttotal: 4.81s\tremaining: 1.42s\n",
      "772:\tlearn: 0.0882086\ttotal: 4.82s\tremaining: 1.42s\n",
      "773:\tlearn: 0.0881787\ttotal: 4.83s\tremaining: 1.41s\n",
      "774:\tlearn: 0.0881574\ttotal: 4.85s\tremaining: 1.41s\n",
      "775:\tlearn: 0.0881455\ttotal: 4.86s\tremaining: 1.4s\n",
      "776:\tlearn: 0.0881123\ttotal: 4.88s\tremaining: 1.4s\n",
      "777:\tlearn: 0.0881033\ttotal: 4.89s\tremaining: 1.39s\n",
      "778:\tlearn: 0.0880909\ttotal: 4.9s\tremaining: 1.39s\n",
      "779:\tlearn: 0.0880788\ttotal: 4.9s\tremaining: 1.38s\n",
      "780:\tlearn: 0.0880662\ttotal: 4.91s\tremaining: 1.38s\n",
      "781:\tlearn: 0.0880567\ttotal: 4.91s\tremaining: 1.37s\n",
      "782:\tlearn: 0.0880512\ttotal: 4.92s\tremaining: 1.36s\n",
      "783:\tlearn: 0.0880110\ttotal: 4.92s\tremaining: 1.36s\n",
      "784:\tlearn: 0.0880013\ttotal: 4.93s\tremaining: 1.35s\n",
      "785:\tlearn: 0.0879876\ttotal: 4.94s\tremaining: 1.34s\n",
      "786:\tlearn: 0.0879772\ttotal: 4.94s\tremaining: 1.34s\n",
      "787:\tlearn: 0.0879667\ttotal: 4.95s\tremaining: 1.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788:\tlearn: 0.0879566\ttotal: 4.96s\tremaining: 1.33s\n",
      "789:\tlearn: 0.0879391\ttotal: 4.96s\tremaining: 1.32s\n",
      "790:\tlearn: 0.0879272\ttotal: 4.97s\tremaining: 1.31s\n",
      "791:\tlearn: 0.0879057\ttotal: 4.98s\tremaining: 1.31s\n",
      "792:\tlearn: 0.0878943\ttotal: 4.98s\tremaining: 1.3s\n",
      "793:\tlearn: 0.0878823\ttotal: 4.99s\tremaining: 1.29s\n",
      "794:\tlearn: 0.0878723\ttotal: 4.99s\tremaining: 1.29s\n",
      "795:\tlearn: 0.0878622\ttotal: 5s\tremaining: 1.28s\n",
      "796:\tlearn: 0.0878542\ttotal: 5.01s\tremaining: 1.27s\n",
      "797:\tlearn: 0.0878458\ttotal: 5.01s\tremaining: 1.27s\n",
      "798:\tlearn: 0.0878365\ttotal: 5.02s\tremaining: 1.26s\n",
      "799:\tlearn: 0.0878292\ttotal: 5.02s\tremaining: 1.26s\n",
      "800:\tlearn: 0.0878215\ttotal: 5.03s\tremaining: 1.25s\n",
      "801:\tlearn: 0.0878149\ttotal: 5.04s\tremaining: 1.24s\n",
      "802:\tlearn: 0.0878066\ttotal: 5.04s\tremaining: 1.24s\n",
      "803:\tlearn: 0.0877834\ttotal: 5.05s\tremaining: 1.23s\n",
      "804:\tlearn: 0.0877697\ttotal: 5.05s\tremaining: 1.22s\n",
      "805:\tlearn: 0.0877631\ttotal: 5.06s\tremaining: 1.22s\n",
      "806:\tlearn: 0.0877344\ttotal: 5.07s\tremaining: 1.21s\n",
      "807:\tlearn: 0.0876938\ttotal: 5.07s\tremaining: 1.21s\n",
      "808:\tlearn: 0.0876811\ttotal: 5.08s\tremaining: 1.2s\n",
      "809:\tlearn: 0.0876590\ttotal: 5.08s\tremaining: 1.19s\n",
      "810:\tlearn: 0.0876411\ttotal: 5.09s\tremaining: 1.19s\n",
      "811:\tlearn: 0.0876317\ttotal: 5.1s\tremaining: 1.18s\n",
      "812:\tlearn: 0.0876239\ttotal: 5.1s\tremaining: 1.17s\n",
      "813:\tlearn: 0.0876152\ttotal: 5.11s\tremaining: 1.17s\n",
      "814:\tlearn: 0.0876060\ttotal: 5.11s\tremaining: 1.16s\n",
      "815:\tlearn: 0.0875943\ttotal: 5.12s\tremaining: 1.15s\n",
      "816:\tlearn: 0.0875722\ttotal: 5.12s\tremaining: 1.15s\n",
      "817:\tlearn: 0.0875635\ttotal: 5.13s\tremaining: 1.14s\n",
      "818:\tlearn: 0.0875540\ttotal: 5.14s\tremaining: 1.14s\n",
      "819:\tlearn: 0.0875403\ttotal: 5.14s\tremaining: 1.13s\n",
      "820:\tlearn: 0.0875322\ttotal: 5.15s\tremaining: 1.12s\n",
      "821:\tlearn: 0.0875072\ttotal: 5.16s\tremaining: 1.12s\n",
      "822:\tlearn: 0.0874702\ttotal: 5.16s\tremaining: 1.11s\n",
      "823:\tlearn: 0.0874599\ttotal: 5.17s\tremaining: 1.1s\n",
      "824:\tlearn: 0.0874488\ttotal: 5.17s\tremaining: 1.1s\n",
      "825:\tlearn: 0.0874359\ttotal: 5.18s\tremaining: 1.09s\n",
      "826:\tlearn: 0.0874263\ttotal: 5.19s\tremaining: 1.08s\n",
      "827:\tlearn: 0.0874221\ttotal: 5.19s\tremaining: 1.08s\n",
      "828:\tlearn: 0.0874121\ttotal: 5.2s\tremaining: 1.07s\n",
      "829:\tlearn: 0.0874044\ttotal: 5.2s\tremaining: 1.06s\n",
      "830:\tlearn: 0.0873902\ttotal: 5.21s\tremaining: 1.06s\n",
      "831:\tlearn: 0.0873783\ttotal: 5.21s\tremaining: 1.05s\n",
      "832:\tlearn: 0.0873586\ttotal: 5.22s\tremaining: 1.05s\n",
      "833:\tlearn: 0.0873488\ttotal: 5.23s\tremaining: 1.04s\n",
      "834:\tlearn: 0.0873308\ttotal: 5.23s\tremaining: 1.03s\n",
      "835:\tlearn: 0.0873197\ttotal: 5.24s\tremaining: 1.03s\n",
      "836:\tlearn: 0.0872945\ttotal: 5.25s\tremaining: 1.02s\n",
      "837:\tlearn: 0.0872862\ttotal: 5.25s\tremaining: 1.01s\n",
      "838:\tlearn: 0.0872719\ttotal: 5.26s\tremaining: 1.01s\n",
      "839:\tlearn: 0.0872646\ttotal: 5.26s\tremaining: 1s\n",
      "840:\tlearn: 0.0872547\ttotal: 5.27s\tremaining: 996ms\n",
      "841:\tlearn: 0.0872464\ttotal: 5.28s\tremaining: 990ms\n",
      "842:\tlearn: 0.0872367\ttotal: 5.28s\tremaining: 984ms\n",
      "843:\tlearn: 0.0872287\ttotal: 5.29s\tremaining: 977ms\n",
      "844:\tlearn: 0.0872205\ttotal: 5.29s\tremaining: 971ms\n",
      "845:\tlearn: 0.0872118\ttotal: 5.3s\tremaining: 964ms\n",
      "846:\tlearn: 0.0872073\ttotal: 5.3s\tremaining: 958ms\n",
      "847:\tlearn: 0.0872014\ttotal: 5.31s\tremaining: 952ms\n",
      "848:\tlearn: 0.0871958\ttotal: 5.32s\tremaining: 945ms\n",
      "849:\tlearn: 0.0871847\ttotal: 5.32s\tremaining: 939ms\n",
      "850:\tlearn: 0.0871695\ttotal: 5.33s\tremaining: 933ms\n",
      "851:\tlearn: 0.0871547\ttotal: 5.33s\tremaining: 927ms\n",
      "852:\tlearn: 0.0871458\ttotal: 5.34s\tremaining: 920ms\n",
      "853:\tlearn: 0.0871200\ttotal: 5.35s\tremaining: 914ms\n",
      "854:\tlearn: 0.0870951\ttotal: 5.35s\tremaining: 908ms\n",
      "855:\tlearn: 0.0870688\ttotal: 5.36s\tremaining: 902ms\n",
      "856:\tlearn: 0.0870369\ttotal: 5.37s\tremaining: 895ms\n",
      "857:\tlearn: 0.0870264\ttotal: 5.37s\tremaining: 889ms\n",
      "858:\tlearn: 0.0869992\ttotal: 5.38s\tremaining: 883ms\n",
      "859:\tlearn: 0.0869950\ttotal: 5.38s\tremaining: 877ms\n",
      "860:\tlearn: 0.0869886\ttotal: 5.39s\tremaining: 870ms\n",
      "861:\tlearn: 0.0869800\ttotal: 5.4s\tremaining: 864ms\n",
      "862:\tlearn: 0.0869748\ttotal: 5.4s\tremaining: 858ms\n",
      "863:\tlearn: 0.0869659\ttotal: 5.41s\tremaining: 851ms\n",
      "864:\tlearn: 0.0869587\ttotal: 5.41s\tremaining: 845ms\n",
      "865:\tlearn: 0.0869505\ttotal: 5.42s\tremaining: 838ms\n",
      "866:\tlearn: 0.0869419\ttotal: 5.42s\tremaining: 832ms\n",
      "867:\tlearn: 0.0869339\ttotal: 5.43s\tremaining: 826ms\n",
      "868:\tlearn: 0.0869213\ttotal: 5.43s\tremaining: 819ms\n",
      "869:\tlearn: 0.0869127\ttotal: 5.44s\tremaining: 813ms\n",
      "870:\tlearn: 0.0869020\ttotal: 5.45s\tremaining: 807ms\n",
      "871:\tlearn: 0.0868924\ttotal: 5.45s\tremaining: 800ms\n",
      "872:\tlearn: 0.0868867\ttotal: 5.46s\tremaining: 794ms\n",
      "873:\tlearn: 0.0868654\ttotal: 5.46s\tremaining: 787ms\n",
      "874:\tlearn: 0.0868579\ttotal: 5.47s\tremaining: 781ms\n",
      "875:\tlearn: 0.0868521\ttotal: 5.47s\tremaining: 775ms\n",
      "876:\tlearn: 0.0868447\ttotal: 5.48s\tremaining: 768ms\n",
      "877:\tlearn: 0.0868348\ttotal: 5.48s\tremaining: 762ms\n",
      "878:\tlearn: 0.0868206\ttotal: 5.49s\tremaining: 756ms\n",
      "879:\tlearn: 0.0868088\ttotal: 5.5s\tremaining: 750ms\n",
      "880:\tlearn: 0.0867977\ttotal: 5.5s\tremaining: 743ms\n",
      "881:\tlearn: 0.0867833\ttotal: 5.51s\tremaining: 737ms\n",
      "882:\tlearn: 0.0867698\ttotal: 5.52s\tremaining: 731ms\n",
      "883:\tlearn: 0.0867603\ttotal: 5.52s\tremaining: 725ms\n",
      "884:\tlearn: 0.0867452\ttotal: 5.53s\tremaining: 718ms\n",
      "885:\tlearn: 0.0867155\ttotal: 5.53s\tremaining: 712ms\n",
      "886:\tlearn: 0.0866988\ttotal: 5.54s\tremaining: 706ms\n",
      "887:\tlearn: 0.0866931\ttotal: 5.54s\tremaining: 699ms\n",
      "888:\tlearn: 0.0866724\ttotal: 5.55s\tremaining: 693ms\n",
      "889:\tlearn: 0.0866645\ttotal: 5.56s\tremaining: 687ms\n",
      "890:\tlearn: 0.0866485\ttotal: 5.56s\tremaining: 681ms\n",
      "891:\tlearn: 0.0866281\ttotal: 5.57s\tremaining: 674ms\n",
      "892:\tlearn: 0.0866072\ttotal: 5.57s\tremaining: 668ms\n",
      "893:\tlearn: 0.0865966\ttotal: 5.58s\tremaining: 662ms\n",
      "894:\tlearn: 0.0865861\ttotal: 5.59s\tremaining: 655ms\n",
      "895:\tlearn: 0.0865809\ttotal: 5.59s\tremaining: 649ms\n",
      "896:\tlearn: 0.0865553\ttotal: 5.6s\tremaining: 643ms\n",
      "897:\tlearn: 0.0865438\ttotal: 5.6s\tremaining: 636ms\n",
      "898:\tlearn: 0.0865266\ttotal: 5.61s\tremaining: 630ms\n",
      "899:\tlearn: 0.0865203\ttotal: 5.62s\tremaining: 624ms\n",
      "900:\tlearn: 0.0865018\ttotal: 5.62s\tremaining: 618ms\n",
      "901:\tlearn: 0.0864947\ttotal: 5.63s\tremaining: 612ms\n",
      "902:\tlearn: 0.0864798\ttotal: 5.63s\tremaining: 605ms\n",
      "903:\tlearn: 0.0864517\ttotal: 5.64s\tremaining: 599ms\n",
      "904:\tlearn: 0.0864395\ttotal: 5.65s\tremaining: 593ms\n",
      "905:\tlearn: 0.0864302\ttotal: 5.65s\tremaining: 587ms\n",
      "906:\tlearn: 0.0864237\ttotal: 5.66s\tremaining: 580ms\n",
      "907:\tlearn: 0.0864098\ttotal: 5.67s\tremaining: 574ms\n",
      "908:\tlearn: 0.0864046\ttotal: 5.67s\tremaining: 568ms\n",
      "909:\tlearn: 0.0863921\ttotal: 5.68s\tremaining: 561ms\n",
      "910:\tlearn: 0.0863832\ttotal: 5.68s\tremaining: 555ms\n",
      "911:\tlearn: 0.0863581\ttotal: 5.7s\tremaining: 550ms\n",
      "912:\tlearn: 0.0863272\ttotal: 5.7s\tremaining: 544ms\n",
      "913:\tlearn: 0.0863200\ttotal: 5.71s\tremaining: 538ms\n",
      "914:\tlearn: 0.0863155\ttotal: 5.72s\tremaining: 531ms\n",
      "915:\tlearn: 0.0863057\ttotal: 5.72s\tremaining: 525ms\n",
      "916:\tlearn: 0.0862996\ttotal: 5.73s\tremaining: 519ms\n",
      "917:\tlearn: 0.0862927\ttotal: 5.74s\tremaining: 512ms\n",
      "918:\tlearn: 0.0862768\ttotal: 5.74s\tremaining: 506ms\n",
      "919:\tlearn: 0.0862707\ttotal: 5.76s\tremaining: 501ms\n",
      "920:\tlearn: 0.0862608\ttotal: 5.77s\tremaining: 495ms\n",
      "921:\tlearn: 0.0862524\ttotal: 5.78s\tremaining: 489ms\n",
      "922:\tlearn: 0.0862437\ttotal: 5.79s\tremaining: 483ms\n",
      "923:\tlearn: 0.0862386\ttotal: 5.81s\tremaining: 478ms\n",
      "924:\tlearn: 0.0862340\ttotal: 5.82s\tremaining: 472ms\n",
      "925:\tlearn: 0.0862201\ttotal: 5.84s\tremaining: 466ms\n",
      "926:\tlearn: 0.0862079\ttotal: 5.84s\tremaining: 460ms\n",
      "927:\tlearn: 0.0861868\ttotal: 5.85s\tremaining: 454ms\n",
      "928:\tlearn: 0.0861768\ttotal: 5.87s\tremaining: 448ms\n",
      "929:\tlearn: 0.0861696\ttotal: 5.87s\tremaining: 442ms\n",
      "930:\tlearn: 0.0861603\ttotal: 5.88s\tremaining: 436ms\n",
      "931:\tlearn: 0.0861496\ttotal: 5.88s\tremaining: 429ms\n",
      "932:\tlearn: 0.0861401\ttotal: 5.89s\tremaining: 423ms\n",
      "933:\tlearn: 0.0861298\ttotal: 5.9s\tremaining: 417ms\n",
      "934:\tlearn: 0.0861155\ttotal: 5.91s\tremaining: 411ms\n",
      "935:\tlearn: 0.0861067\ttotal: 5.91s\tremaining: 404ms\n",
      "936:\tlearn: 0.0861029\ttotal: 5.92s\tremaining: 398ms\n",
      "937:\tlearn: 0.0860826\ttotal: 5.92s\tremaining: 391ms\n",
      "938:\tlearn: 0.0860733\ttotal: 5.93s\tremaining: 385ms\n",
      "939:\tlearn: 0.0860640\ttotal: 5.93s\tremaining: 379ms\n",
      "940:\tlearn: 0.0860412\ttotal: 5.94s\tremaining: 372ms\n",
      "941:\tlearn: 0.0860290\ttotal: 5.94s\tremaining: 366ms\n",
      "942:\tlearn: 0.0860202\ttotal: 5.95s\tremaining: 360ms\n",
      "943:\tlearn: 0.0860089\ttotal: 5.96s\tremaining: 353ms\n",
      "944:\tlearn: 0.0860047\ttotal: 5.96s\tremaining: 347ms\n",
      "945:\tlearn: 0.0859846\ttotal: 5.97s\tremaining: 341ms\n",
      "946:\tlearn: 0.0859795\ttotal: 5.97s\tremaining: 334ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947:\tlearn: 0.0859685\ttotal: 5.98s\tremaining: 328ms\n",
      "948:\tlearn: 0.0859523\ttotal: 5.99s\tremaining: 322ms\n",
      "949:\tlearn: 0.0859447\ttotal: 5.99s\tremaining: 315ms\n",
      "950:\tlearn: 0.0859307\ttotal: 6s\tremaining: 309ms\n",
      "951:\tlearn: 0.0859121\ttotal: 6s\tremaining: 303ms\n",
      "952:\tlearn: 0.0858940\ttotal: 6.01s\tremaining: 296ms\n",
      "953:\tlearn: 0.0858879\ttotal: 6.01s\tremaining: 290ms\n",
      "954:\tlearn: 0.0858786\ttotal: 6.02s\tremaining: 284ms\n",
      "955:\tlearn: 0.0858653\ttotal: 6.03s\tremaining: 277ms\n",
      "956:\tlearn: 0.0858591\ttotal: 6.03s\tremaining: 271ms\n",
      "957:\tlearn: 0.0858502\ttotal: 6.04s\tremaining: 265ms\n",
      "958:\tlearn: 0.0858441\ttotal: 6.04s\tremaining: 258ms\n",
      "959:\tlearn: 0.0858362\ttotal: 6.05s\tremaining: 252ms\n",
      "960:\tlearn: 0.0858128\ttotal: 6.05s\tremaining: 246ms\n",
      "961:\tlearn: 0.0858062\ttotal: 6.06s\tremaining: 239ms\n",
      "962:\tlearn: 0.0858007\ttotal: 6.07s\tremaining: 233ms\n",
      "963:\tlearn: 0.0857941\ttotal: 6.07s\tremaining: 227ms\n",
      "964:\tlearn: 0.0857874\ttotal: 6.08s\tremaining: 220ms\n",
      "965:\tlearn: 0.0857810\ttotal: 6.08s\tremaining: 214ms\n",
      "966:\tlearn: 0.0857730\ttotal: 6.09s\tremaining: 208ms\n",
      "967:\tlearn: 0.0857584\ttotal: 6.09s\tremaining: 201ms\n",
      "968:\tlearn: 0.0857406\ttotal: 6.1s\tremaining: 195ms\n",
      "969:\tlearn: 0.0857350\ttotal: 6.1s\tremaining: 189ms\n",
      "970:\tlearn: 0.0857277\ttotal: 6.11s\tremaining: 182ms\n",
      "971:\tlearn: 0.0857175\ttotal: 6.11s\tremaining: 176ms\n",
      "972:\tlearn: 0.0857120\ttotal: 6.12s\tremaining: 170ms\n",
      "973:\tlearn: 0.0857045\ttotal: 6.13s\tremaining: 164ms\n",
      "974:\tlearn: 0.0856998\ttotal: 6.13s\tremaining: 157ms\n",
      "975:\tlearn: 0.0856954\ttotal: 6.13s\tremaining: 151ms\n",
      "976:\tlearn: 0.0856873\ttotal: 6.14s\tremaining: 145ms\n",
      "977:\tlearn: 0.0856819\ttotal: 6.15s\tremaining: 138ms\n",
      "978:\tlearn: 0.0856714\ttotal: 6.15s\tremaining: 132ms\n",
      "979:\tlearn: 0.0856582\ttotal: 6.16s\tremaining: 126ms\n",
      "980:\tlearn: 0.0856453\ttotal: 6.16s\tremaining: 119ms\n",
      "981:\tlearn: 0.0856393\ttotal: 6.17s\tremaining: 113ms\n",
      "982:\tlearn: 0.0856253\ttotal: 6.17s\tremaining: 107ms\n",
      "983:\tlearn: 0.0856023\ttotal: 6.18s\tremaining: 101ms\n",
      "984:\tlearn: 0.0855903\ttotal: 6.19s\tremaining: 94.3ms\n",
      "985:\tlearn: 0.0855755\ttotal: 6.2s\tremaining: 88ms\n",
      "986:\tlearn: 0.0855608\ttotal: 6.21s\tremaining: 81.7ms\n",
      "987:\tlearn: 0.0855525\ttotal: 6.21s\tremaining: 75.4ms\n",
      "988:\tlearn: 0.0855354\ttotal: 6.21s\tremaining: 69.1ms\n",
      "989:\tlearn: 0.0855192\ttotal: 6.22s\tremaining: 62.8ms\n",
      "990:\tlearn: 0.0854967\ttotal: 6.23s\tremaining: 56.6ms\n",
      "991:\tlearn: 0.0854885\ttotal: 6.23s\tremaining: 50.3ms\n",
      "992:\tlearn: 0.0854770\ttotal: 6.24s\tremaining: 44ms\n",
      "993:\tlearn: 0.0854678\ttotal: 6.24s\tremaining: 37.7ms\n",
      "994:\tlearn: 0.0854573\ttotal: 6.25s\tremaining: 31.4ms\n",
      "995:\tlearn: 0.0854470\ttotal: 6.25s\tremaining: 25.1ms\n",
      "996:\tlearn: 0.0854411\ttotal: 6.26s\tremaining: 18.8ms\n",
      "997:\tlearn: 0.0854374\ttotal: 6.26s\tremaining: 12.6ms\n",
      "998:\tlearn: 0.0854239\ttotal: 6.27s\tremaining: 6.28ms\n",
      "999:\tlearn: 0.0854197\ttotal: 6.28s\tremaining: 0us\n",
      "0:\tlearn: 0.4375681\ttotal: 4.6ms\tremaining: 4.6s\n",
      "1:\tlearn: 0.4273997\ttotal: 12.9ms\tremaining: 6.43s\n",
      "2:\tlearn: 0.4170340\ttotal: 22.4ms\tremaining: 7.45s\n",
      "3:\tlearn: 0.4071677\ttotal: 28.1ms\tremaining: 6.99s\n",
      "4:\tlearn: 0.3975075\ttotal: 35.8ms\tremaining: 7.13s\n",
      "5:\tlearn: 0.3882622\ttotal: 43ms\tremaining: 7.12s\n",
      "6:\tlearn: 0.3794578\ttotal: 49.7ms\tremaining: 7.05s\n",
      "7:\tlearn: 0.3711158\ttotal: 56.4ms\tremaining: 7s\n",
      "8:\tlearn: 0.3629338\ttotal: 64.4ms\tremaining: 7.09s\n",
      "9:\tlearn: 0.3548936\ttotal: 70.4ms\tremaining: 6.97s\n",
      "10:\tlearn: 0.3472370\ttotal: 76.3ms\tremaining: 6.86s\n",
      "11:\tlearn: 0.3392926\ttotal: 81.7ms\tremaining: 6.72s\n",
      "12:\tlearn: 0.3318008\ttotal: 87.4ms\tremaining: 6.63s\n",
      "13:\tlearn: 0.3246898\ttotal: 93.7ms\tremaining: 6.6s\n",
      "14:\tlearn: 0.3179857\ttotal: 100ms\tremaining: 6.57s\n",
      "15:\tlearn: 0.3110779\ttotal: 108ms\tremaining: 6.66s\n",
      "16:\tlearn: 0.3045444\ttotal: 115ms\tremaining: 6.62s\n",
      "17:\tlearn: 0.2983775\ttotal: 120ms\tremaining: 6.57s\n",
      "18:\tlearn: 0.2921985\ttotal: 126ms\tremaining: 6.5s\n",
      "19:\tlearn: 0.2863031\ttotal: 132ms\tremaining: 6.46s\n",
      "20:\tlearn: 0.2807274\ttotal: 138ms\tremaining: 6.41s\n",
      "21:\tlearn: 0.2753290\ttotal: 143ms\tremaining: 6.35s\n",
      "22:\tlearn: 0.2696846\ttotal: 150ms\tremaining: 6.38s\n",
      "23:\tlearn: 0.2647418\ttotal: 158ms\tremaining: 6.41s\n",
      "24:\tlearn: 0.2595709\ttotal: 163ms\tremaining: 6.37s\n",
      "25:\tlearn: 0.2547906\ttotal: 169ms\tremaining: 6.35s\n",
      "26:\tlearn: 0.2499724\ttotal: 175ms\tremaining: 6.31s\n",
      "27:\tlearn: 0.2457177\ttotal: 180ms\tremaining: 6.26s\n",
      "28:\tlearn: 0.2415947\ttotal: 186ms\tremaining: 6.22s\n",
      "29:\tlearn: 0.2372442\ttotal: 195ms\tremaining: 6.29s\n",
      "30:\tlearn: 0.2331626\ttotal: 200ms\tremaining: 6.26s\n",
      "31:\tlearn: 0.2291876\ttotal: 206ms\tremaining: 6.23s\n",
      "32:\tlearn: 0.2252001\ttotal: 214ms\tremaining: 6.28s\n",
      "33:\tlearn: 0.2215205\ttotal: 223ms\tremaining: 6.32s\n",
      "34:\tlearn: 0.2181971\ttotal: 232ms\tremaining: 6.4s\n",
      "35:\tlearn: 0.2147137\ttotal: 238ms\tremaining: 6.37s\n",
      "36:\tlearn: 0.2113232\ttotal: 244ms\tremaining: 6.34s\n",
      "37:\tlearn: 0.2080402\ttotal: 249ms\tremaining: 6.3s\n",
      "38:\tlearn: 0.2050479\ttotal: 255ms\tremaining: 6.28s\n",
      "39:\tlearn: 0.2022844\ttotal: 262ms\tremaining: 6.29s\n",
      "40:\tlearn: 0.1995068\ttotal: 269ms\tremaining: 6.28s\n",
      "41:\tlearn: 0.1968845\ttotal: 275ms\tremaining: 6.26s\n",
      "42:\tlearn: 0.1944824\ttotal: 282ms\tremaining: 6.27s\n",
      "43:\tlearn: 0.1917944\ttotal: 288ms\tremaining: 6.25s\n",
      "44:\tlearn: 0.1893666\ttotal: 293ms\tremaining: 6.22s\n",
      "45:\tlearn: 0.1871902\ttotal: 298ms\tremaining: 6.19s\n",
      "46:\tlearn: 0.1850897\ttotal: 304ms\tremaining: 6.17s\n",
      "47:\tlearn: 0.1830720\ttotal: 310ms\tremaining: 6.16s\n",
      "48:\tlearn: 0.1807917\ttotal: 316ms\tremaining: 6.13s\n",
      "49:\tlearn: 0.1785608\ttotal: 322ms\tremaining: 6.11s\n",
      "50:\tlearn: 0.1766737\ttotal: 328ms\tremaining: 6.1s\n",
      "51:\tlearn: 0.1747988\ttotal: 334ms\tremaining: 6.08s\n",
      "52:\tlearn: 0.1727687\ttotal: 340ms\tremaining: 6.07s\n",
      "53:\tlearn: 0.1710936\ttotal: 346ms\tremaining: 6.05s\n",
      "54:\tlearn: 0.1695567\ttotal: 358ms\tremaining: 6.15s\n",
      "55:\tlearn: 0.1680063\ttotal: 369ms\tremaining: 6.22s\n",
      "56:\tlearn: 0.1665242\ttotal: 385ms\tremaining: 6.37s\n",
      "57:\tlearn: 0.1649814\ttotal: 401ms\tremaining: 6.51s\n",
      "58:\tlearn: 0.1636418\ttotal: 415ms\tremaining: 6.61s\n",
      "59:\tlearn: 0.1622814\ttotal: 425ms\tremaining: 6.66s\n",
      "60:\tlearn: 0.1609781\ttotal: 441ms\tremaining: 6.79s\n",
      "61:\tlearn: 0.1595449\ttotal: 451ms\tremaining: 6.83s\n",
      "62:\tlearn: 0.1581926\ttotal: 458ms\tremaining: 6.8s\n",
      "63:\tlearn: 0.1570802\ttotal: 463ms\tremaining: 6.77s\n",
      "64:\tlearn: 0.1556814\ttotal: 469ms\tremaining: 6.74s\n",
      "65:\tlearn: 0.1546019\ttotal: 475ms\tremaining: 6.72s\n",
      "66:\tlearn: 0.1533243\ttotal: 482ms\tremaining: 6.71s\n",
      "67:\tlearn: 0.1522056\ttotal: 488ms\tremaining: 6.68s\n",
      "68:\tlearn: 0.1512724\ttotal: 496ms\tremaining: 6.69s\n",
      "69:\tlearn: 0.1503990\ttotal: 502ms\tremaining: 6.66s\n",
      "70:\tlearn: 0.1492425\ttotal: 509ms\tremaining: 6.66s\n",
      "71:\tlearn: 0.1483554\ttotal: 518ms\tremaining: 6.67s\n",
      "72:\tlearn: 0.1473263\ttotal: 524ms\tremaining: 6.65s\n",
      "73:\tlearn: 0.1463176\ttotal: 530ms\tremaining: 6.63s\n",
      "74:\tlearn: 0.1454580\ttotal: 536ms\tremaining: 6.61s\n",
      "75:\tlearn: 0.1444018\ttotal: 541ms\tremaining: 6.58s\n",
      "76:\tlearn: 0.1436510\ttotal: 548ms\tremaining: 6.57s\n",
      "77:\tlearn: 0.1429662\ttotal: 554ms\tremaining: 6.54s\n",
      "78:\tlearn: 0.1422522\ttotal: 559ms\tremaining: 6.52s\n",
      "79:\tlearn: 0.1415456\ttotal: 565ms\tremaining: 6.5s\n",
      "80:\tlearn: 0.1408893\ttotal: 573ms\tremaining: 6.5s\n",
      "81:\tlearn: 0.1403002\ttotal: 578ms\tremaining: 6.47s\n",
      "82:\tlearn: 0.1396759\ttotal: 583ms\tremaining: 6.45s\n",
      "83:\tlearn: 0.1389708\ttotal: 589ms\tremaining: 6.43s\n",
      "84:\tlearn: 0.1384311\ttotal: 595ms\tremaining: 6.4s\n",
      "85:\tlearn: 0.1379127\ttotal: 601ms\tremaining: 6.38s\n",
      "86:\tlearn: 0.1373994\ttotal: 606ms\tremaining: 6.36s\n",
      "87:\tlearn: 0.1369772\ttotal: 612ms\tremaining: 6.34s\n",
      "88:\tlearn: 0.1363158\ttotal: 618ms\tremaining: 6.32s\n",
      "89:\tlearn: 0.1358192\ttotal: 624ms\tremaining: 6.31s\n",
      "90:\tlearn: 0.1351551\ttotal: 632ms\tremaining: 6.31s\n",
      "91:\tlearn: 0.1345266\ttotal: 677ms\tremaining: 6.68s\n",
      "92:\tlearn: 0.1339838\ttotal: 688ms\tremaining: 6.71s\n",
      "93:\tlearn: 0.1335647\ttotal: 695ms\tremaining: 6.7s\n",
      "94:\tlearn: 0.1330586\ttotal: 702ms\tremaining: 6.69s\n",
      "95:\tlearn: 0.1326303\ttotal: 709ms\tremaining: 6.68s\n",
      "96:\tlearn: 0.1322671\ttotal: 716ms\tremaining: 6.66s\n",
      "97:\tlearn: 0.1319060\ttotal: 723ms\tremaining: 6.65s\n",
      "98:\tlearn: 0.1313530\ttotal: 729ms\tremaining: 6.63s\n",
      "99:\tlearn: 0.1308373\ttotal: 735ms\tremaining: 6.62s\n",
      "100:\tlearn: 0.1304477\ttotal: 740ms\tremaining: 6.59s\n",
      "101:\tlearn: 0.1300292\ttotal: 746ms\tremaining: 6.57s\n",
      "102:\tlearn: 0.1295804\ttotal: 752ms\tremaining: 6.55s\n",
      "103:\tlearn: 0.1292596\ttotal: 758ms\tremaining: 6.53s\n",
      "104:\tlearn: 0.1287509\ttotal: 764ms\tremaining: 6.51s\n",
      "105:\tlearn: 0.1282333\ttotal: 770ms\tremaining: 6.49s\n",
      "106:\tlearn: 0.1278001\ttotal: 777ms\tremaining: 6.48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107:\tlearn: 0.1275223\ttotal: 785ms\tremaining: 6.48s\n",
      "108:\tlearn: 0.1271520\ttotal: 791ms\tremaining: 6.47s\n",
      "109:\tlearn: 0.1268135\ttotal: 797ms\tremaining: 6.45s\n",
      "110:\tlearn: 0.1265366\ttotal: 806ms\tremaining: 6.45s\n",
      "111:\tlearn: 0.1261140\ttotal: 815ms\tremaining: 6.46s\n",
      "112:\tlearn: 0.1258029\ttotal: 823ms\tremaining: 6.46s\n",
      "113:\tlearn: 0.1255937\ttotal: 831ms\tremaining: 6.46s\n",
      "114:\tlearn: 0.1251622\ttotal: 840ms\tremaining: 6.46s\n",
      "115:\tlearn: 0.1248245\ttotal: 847ms\tremaining: 6.45s\n",
      "116:\tlearn: 0.1245952\ttotal: 852ms\tremaining: 6.43s\n",
      "117:\tlearn: 0.1243935\ttotal: 857ms\tremaining: 6.41s\n",
      "118:\tlearn: 0.1240225\ttotal: 863ms\tremaining: 6.39s\n",
      "119:\tlearn: 0.1238233\ttotal: 868ms\tremaining: 6.37s\n",
      "120:\tlearn: 0.1235284\ttotal: 874ms\tremaining: 6.35s\n",
      "121:\tlearn: 0.1233332\ttotal: 879ms\tremaining: 6.33s\n",
      "122:\tlearn: 0.1229611\ttotal: 888ms\tremaining: 6.33s\n",
      "123:\tlearn: 0.1226377\ttotal: 894ms\tremaining: 6.31s\n",
      "124:\tlearn: 0.1224269\ttotal: 899ms\tremaining: 6.29s\n",
      "125:\tlearn: 0.1222544\ttotal: 904ms\tremaining: 6.27s\n",
      "126:\tlearn: 0.1220891\ttotal: 911ms\tremaining: 6.26s\n",
      "127:\tlearn: 0.1216933\ttotal: 917ms\tremaining: 6.25s\n",
      "128:\tlearn: 0.1214093\ttotal: 923ms\tremaining: 6.23s\n",
      "129:\tlearn: 0.1210492\ttotal: 932ms\tremaining: 6.23s\n",
      "130:\tlearn: 0.1208952\ttotal: 938ms\tremaining: 6.22s\n",
      "131:\tlearn: 0.1207451\ttotal: 945ms\tremaining: 6.22s\n",
      "132:\tlearn: 0.1203686\ttotal: 951ms\tremaining: 6.2s\n",
      "133:\tlearn: 0.1200389\ttotal: 957ms\tremaining: 6.18s\n",
      "134:\tlearn: 0.1197920\ttotal: 962ms\tremaining: 6.16s\n",
      "135:\tlearn: 0.1196143\ttotal: 969ms\tremaining: 6.16s\n",
      "136:\tlearn: 0.1194700\ttotal: 975ms\tremaining: 6.14s\n",
      "137:\tlearn: 0.1193037\ttotal: 981ms\tremaining: 6.12s\n",
      "138:\tlearn: 0.1191546\ttotal: 986ms\tremaining: 6.11s\n",
      "139:\tlearn: 0.1190366\ttotal: 992ms\tremaining: 6.09s\n",
      "140:\tlearn: 0.1189086\ttotal: 997ms\tremaining: 6.07s\n",
      "141:\tlearn: 0.1187910\ttotal: 1s\tremaining: 6.06s\n",
      "142:\tlearn: 0.1185760\ttotal: 1.01s\tremaining: 6.05s\n",
      "143:\tlearn: 0.1184407\ttotal: 1.02s\tremaining: 6.04s\n",
      "144:\tlearn: 0.1181057\ttotal: 1.02s\tremaining: 6.03s\n",
      "145:\tlearn: 0.1179964\ttotal: 1.03s\tremaining: 6.01s\n",
      "146:\tlearn: 0.1178210\ttotal: 1.03s\tremaining: 6s\n",
      "147:\tlearn: 0.1175987\ttotal: 1.04s\tremaining: 5.98s\n",
      "148:\tlearn: 0.1174018\ttotal: 1.04s\tremaining: 5.97s\n",
      "149:\tlearn: 0.1170976\ttotal: 1.05s\tremaining: 5.97s\n",
      "150:\tlearn: 0.1168666\ttotal: 1.06s\tremaining: 5.98s\n",
      "151:\tlearn: 0.1166825\ttotal: 1.07s\tremaining: 5.96s\n",
      "152:\tlearn: 0.1164652\ttotal: 1.07s\tremaining: 5.95s\n",
      "153:\tlearn: 0.1163771\ttotal: 1.08s\tremaining: 5.93s\n",
      "154:\tlearn: 0.1161460\ttotal: 1.09s\tremaining: 5.92s\n",
      "155:\tlearn: 0.1159207\ttotal: 1.09s\tremaining: 5.91s\n",
      "156:\tlearn: 0.1158328\ttotal: 1.1s\tremaining: 5.91s\n",
      "157:\tlearn: 0.1157247\ttotal: 1.1s\tremaining: 5.89s\n",
      "158:\tlearn: 0.1155140\ttotal: 1.11s\tremaining: 5.88s\n",
      "159:\tlearn: 0.1153761\ttotal: 1.12s\tremaining: 5.86s\n",
      "160:\tlearn: 0.1152807\ttotal: 1.12s\tremaining: 5.85s\n",
      "161:\tlearn: 0.1151633\ttotal: 1.13s\tremaining: 5.84s\n",
      "162:\tlearn: 0.1150801\ttotal: 1.13s\tremaining: 5.82s\n",
      "163:\tlearn: 0.1149586\ttotal: 1.14s\tremaining: 5.81s\n",
      "164:\tlearn: 0.1147289\ttotal: 1.15s\tremaining: 5.79s\n",
      "165:\tlearn: 0.1144961\ttotal: 1.15s\tremaining: 5.78s\n",
      "166:\tlearn: 0.1142778\ttotal: 1.16s\tremaining: 5.76s\n",
      "167:\tlearn: 0.1140542\ttotal: 1.16s\tremaining: 5.75s\n",
      "168:\tlearn: 0.1138044\ttotal: 1.17s\tremaining: 5.74s\n",
      "169:\tlearn: 0.1135991\ttotal: 1.17s\tremaining: 5.72s\n",
      "170:\tlearn: 0.1134924\ttotal: 1.19s\tremaining: 5.75s\n",
      "171:\tlearn: 0.1133356\ttotal: 1.2s\tremaining: 5.76s\n",
      "172:\tlearn: 0.1132823\ttotal: 1.2s\tremaining: 5.76s\n",
      "173:\tlearn: 0.1132346\ttotal: 1.21s\tremaining: 5.76s\n",
      "174:\tlearn: 0.1131060\ttotal: 1.23s\tremaining: 5.79s\n",
      "175:\tlearn: 0.1130064\ttotal: 1.23s\tremaining: 5.78s\n",
      "176:\tlearn: 0.1128591\ttotal: 1.24s\tremaining: 5.77s\n",
      "177:\tlearn: 0.1127441\ttotal: 1.25s\tremaining: 5.75s\n",
      "178:\tlearn: 0.1125604\ttotal: 1.25s\tremaining: 5.74s\n",
      "179:\tlearn: 0.1124575\ttotal: 1.26s\tremaining: 5.73s\n",
      "180:\tlearn: 0.1122623\ttotal: 1.26s\tremaining: 5.71s\n",
      "181:\tlearn: 0.1120439\ttotal: 1.27s\tremaining: 5.71s\n",
      "182:\tlearn: 0.1119025\ttotal: 1.27s\tremaining: 5.7s\n",
      "183:\tlearn: 0.1118142\ttotal: 1.28s\tremaining: 5.68s\n",
      "184:\tlearn: 0.1116835\ttotal: 1.29s\tremaining: 5.68s\n",
      "185:\tlearn: 0.1115444\ttotal: 1.29s\tremaining: 5.67s\n",
      "186:\tlearn: 0.1114329\ttotal: 1.3s\tremaining: 5.66s\n",
      "187:\tlearn: 0.1112517\ttotal: 1.31s\tremaining: 5.66s\n",
      "188:\tlearn: 0.1111280\ttotal: 1.32s\tremaining: 5.68s\n",
      "189:\tlearn: 0.1109807\ttotal: 1.33s\tremaining: 5.69s\n",
      "190:\tlearn: 0.1108905\ttotal: 1.35s\tremaining: 5.73s\n",
      "191:\tlearn: 0.1106745\ttotal: 1.36s\tremaining: 5.74s\n",
      "192:\tlearn: 0.1106041\ttotal: 1.37s\tremaining: 5.75s\n",
      "193:\tlearn: 0.1104537\ttotal: 1.38s\tremaining: 5.75s\n",
      "194:\tlearn: 0.1102964\ttotal: 1.4s\tremaining: 5.76s\n",
      "195:\tlearn: 0.1101689\ttotal: 1.41s\tremaining: 5.77s\n",
      "196:\tlearn: 0.1100937\ttotal: 1.41s\tremaining: 5.76s\n",
      "197:\tlearn: 0.1099884\ttotal: 1.42s\tremaining: 5.75s\n",
      "198:\tlearn: 0.1098342\ttotal: 1.42s\tremaining: 5.73s\n",
      "199:\tlearn: 0.1097266\ttotal: 1.43s\tremaining: 5.72s\n",
      "200:\tlearn: 0.1096391\ttotal: 1.44s\tremaining: 5.7s\n",
      "201:\tlearn: 0.1095847\ttotal: 1.44s\tremaining: 5.69s\n",
      "202:\tlearn: 0.1094893\ttotal: 1.45s\tremaining: 5.68s\n",
      "203:\tlearn: 0.1094394\ttotal: 1.45s\tremaining: 5.67s\n",
      "204:\tlearn: 0.1092827\ttotal: 1.46s\tremaining: 5.65s\n",
      "205:\tlearn: 0.1092103\ttotal: 1.46s\tremaining: 5.64s\n",
      "206:\tlearn: 0.1090858\ttotal: 1.47s\tremaining: 5.63s\n",
      "207:\tlearn: 0.1090191\ttotal: 1.48s\tremaining: 5.62s\n",
      "208:\tlearn: 0.1089116\ttotal: 1.48s\tremaining: 5.62s\n",
      "209:\tlearn: 0.1087635\ttotal: 1.49s\tremaining: 5.61s\n",
      "210:\tlearn: 0.1086019\ttotal: 1.5s\tremaining: 5.59s\n",
      "211:\tlearn: 0.1085416\ttotal: 1.5s\tremaining: 5.58s\n",
      "212:\tlearn: 0.1084352\ttotal: 1.51s\tremaining: 5.57s\n",
      "213:\tlearn: 0.1083747\ttotal: 1.51s\tremaining: 5.55s\n",
      "214:\tlearn: 0.1083203\ttotal: 1.52s\tremaining: 5.54s\n",
      "215:\tlearn: 0.1082716\ttotal: 1.52s\tremaining: 5.53s\n",
      "216:\tlearn: 0.1081718\ttotal: 1.53s\tremaining: 5.51s\n",
      "217:\tlearn: 0.1081361\ttotal: 1.53s\tremaining: 5.5s\n",
      "218:\tlearn: 0.1080761\ttotal: 1.54s\tremaining: 5.49s\n",
      "219:\tlearn: 0.1080344\ttotal: 1.54s\tremaining: 5.47s\n",
      "220:\tlearn: 0.1079266\ttotal: 1.55s\tremaining: 5.46s\n",
      "221:\tlearn: 0.1078488\ttotal: 1.55s\tremaining: 5.45s\n",
      "222:\tlearn: 0.1077812\ttotal: 1.56s\tremaining: 5.44s\n",
      "223:\tlearn: 0.1076827\ttotal: 1.57s\tremaining: 5.43s\n",
      "224:\tlearn: 0.1076194\ttotal: 1.57s\tremaining: 5.42s\n",
      "225:\tlearn: 0.1075633\ttotal: 1.58s\tremaining: 5.42s\n",
      "226:\tlearn: 0.1074832\ttotal: 1.59s\tremaining: 5.4s\n",
      "227:\tlearn: 0.1073599\ttotal: 1.59s\tremaining: 5.39s\n",
      "228:\tlearn: 0.1072647\ttotal: 1.6s\tremaining: 5.38s\n",
      "229:\tlearn: 0.1071185\ttotal: 1.6s\tremaining: 5.37s\n",
      "230:\tlearn: 0.1069577\ttotal: 1.61s\tremaining: 5.36s\n",
      "231:\tlearn: 0.1068889\ttotal: 1.61s\tremaining: 5.34s\n",
      "232:\tlearn: 0.1068122\ttotal: 1.62s\tremaining: 5.33s\n",
      "233:\tlearn: 0.1066961\ttotal: 1.63s\tremaining: 5.32s\n",
      "234:\tlearn: 0.1065609\ttotal: 1.63s\tremaining: 5.31s\n",
      "235:\tlearn: 0.1064424\ttotal: 1.64s\tremaining: 5.3s\n",
      "236:\tlearn: 0.1063894\ttotal: 1.64s\tremaining: 5.29s\n",
      "237:\tlearn: 0.1062669\ttotal: 1.65s\tremaining: 5.28s\n",
      "238:\tlearn: 0.1061364\ttotal: 1.65s\tremaining: 5.26s\n",
      "239:\tlearn: 0.1060134\ttotal: 1.66s\tremaining: 5.25s\n",
      "240:\tlearn: 0.1058757\ttotal: 1.67s\tremaining: 5.24s\n",
      "241:\tlearn: 0.1058105\ttotal: 1.67s\tremaining: 5.23s\n",
      "242:\tlearn: 0.1057605\ttotal: 1.68s\tremaining: 5.22s\n",
      "243:\tlearn: 0.1057106\ttotal: 1.68s\tremaining: 5.21s\n",
      "244:\tlearn: 0.1056067\ttotal: 1.69s\tremaining: 5.2s\n",
      "245:\tlearn: 0.1055297\ttotal: 1.69s\tremaining: 5.19s\n",
      "246:\tlearn: 0.1054194\ttotal: 1.7s\tremaining: 5.18s\n",
      "247:\tlearn: 0.1053455\ttotal: 1.71s\tremaining: 5.17s\n",
      "248:\tlearn: 0.1052567\ttotal: 1.71s\tremaining: 5.16s\n",
      "249:\tlearn: 0.1051666\ttotal: 1.72s\tremaining: 5.15s\n",
      "250:\tlearn: 0.1051036\ttotal: 1.72s\tremaining: 5.14s\n",
      "251:\tlearn: 0.1050085\ttotal: 1.73s\tremaining: 5.13s\n",
      "252:\tlearn: 0.1049366\ttotal: 1.74s\tremaining: 5.12s\n",
      "253:\tlearn: 0.1048751\ttotal: 1.74s\tremaining: 5.11s\n",
      "254:\tlearn: 0.1048389\ttotal: 1.75s\tremaining: 5.1s\n",
      "255:\tlearn: 0.1047982\ttotal: 1.75s\tremaining: 5.09s\n",
      "256:\tlearn: 0.1047661\ttotal: 1.76s\tremaining: 5.08s\n",
      "257:\tlearn: 0.1047063\ttotal: 1.76s\tremaining: 5.08s\n",
      "258:\tlearn: 0.1046400\ttotal: 1.77s\tremaining: 5.07s\n",
      "259:\tlearn: 0.1045855\ttotal: 1.78s\tremaining: 5.06s\n",
      "260:\tlearn: 0.1045441\ttotal: 1.78s\tremaining: 5.05s\n",
      "261:\tlearn: 0.1044245\ttotal: 1.79s\tremaining: 5.04s\n",
      "262:\tlearn: 0.1043704\ttotal: 1.79s\tremaining: 5.03s\n",
      "263:\tlearn: 0.1043103\ttotal: 1.8s\tremaining: 5.02s\n",
      "264:\tlearn: 0.1042210\ttotal: 1.8s\tremaining: 5.01s\n",
      "265:\tlearn: 0.1041575\ttotal: 1.81s\tremaining: 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266:\tlearn: 0.1041339\ttotal: 1.82s\tremaining: 4.99s\n",
      "267:\tlearn: 0.1039895\ttotal: 1.82s\tremaining: 4.98s\n",
      "268:\tlearn: 0.1039277\ttotal: 1.83s\tremaining: 4.97s\n",
      "269:\tlearn: 0.1038997\ttotal: 1.84s\tremaining: 4.97s\n",
      "270:\tlearn: 0.1038228\ttotal: 1.84s\tremaining: 4.96s\n",
      "271:\tlearn: 0.1037696\ttotal: 1.85s\tremaining: 4.95s\n",
      "272:\tlearn: 0.1037496\ttotal: 1.85s\tremaining: 4.94s\n",
      "273:\tlearn: 0.1036703\ttotal: 1.86s\tremaining: 4.93s\n",
      "274:\tlearn: 0.1035776\ttotal: 1.86s\tremaining: 4.92s\n",
      "275:\tlearn: 0.1035198\ttotal: 1.87s\tremaining: 4.91s\n",
      "276:\tlearn: 0.1034572\ttotal: 1.88s\tremaining: 4.9s\n",
      "277:\tlearn: 0.1033905\ttotal: 1.88s\tremaining: 4.89s\n",
      "278:\tlearn: 0.1033210\ttotal: 1.89s\tremaining: 4.88s\n",
      "279:\tlearn: 0.1032425\ttotal: 1.89s\tremaining: 4.87s\n",
      "280:\tlearn: 0.1031813\ttotal: 1.9s\tremaining: 4.86s\n",
      "281:\tlearn: 0.1031369\ttotal: 1.91s\tremaining: 4.86s\n",
      "282:\tlearn: 0.1030395\ttotal: 1.91s\tremaining: 4.85s\n",
      "283:\tlearn: 0.1029730\ttotal: 1.92s\tremaining: 4.84s\n",
      "284:\tlearn: 0.1029374\ttotal: 1.93s\tremaining: 4.83s\n",
      "285:\tlearn: 0.1028960\ttotal: 1.93s\tremaining: 4.83s\n",
      "286:\tlearn: 0.1028202\ttotal: 1.94s\tremaining: 4.82s\n",
      "287:\tlearn: 0.1027360\ttotal: 1.95s\tremaining: 4.81s\n",
      "288:\tlearn: 0.1027120\ttotal: 1.96s\tremaining: 4.81s\n",
      "289:\tlearn: 0.1026479\ttotal: 1.97s\tremaining: 4.82s\n",
      "290:\tlearn: 0.1025784\ttotal: 1.98s\tremaining: 4.83s\n",
      "291:\tlearn: 0.1024857\ttotal: 2s\tremaining: 4.84s\n",
      "292:\tlearn: 0.1024194\ttotal: 2s\tremaining: 4.83s\n",
      "293:\tlearn: 0.1023682\ttotal: 2.01s\tremaining: 4.83s\n",
      "294:\tlearn: 0.1023297\ttotal: 2.02s\tremaining: 4.82s\n",
      "295:\tlearn: 0.1022792\ttotal: 2.02s\tremaining: 4.81s\n",
      "296:\tlearn: 0.1022107\ttotal: 2.03s\tremaining: 4.8s\n",
      "297:\tlearn: 0.1021185\ttotal: 2.04s\tremaining: 4.81s\n",
      "298:\tlearn: 0.1020403\ttotal: 2.05s\tremaining: 4.82s\n",
      "299:\tlearn: 0.1019519\ttotal: 2.07s\tremaining: 4.83s\n",
      "300:\tlearn: 0.1019119\ttotal: 2.08s\tremaining: 4.84s\n",
      "301:\tlearn: 0.1018556\ttotal: 2.09s\tremaining: 4.83s\n",
      "302:\tlearn: 0.1018035\ttotal: 2.1s\tremaining: 4.82s\n",
      "303:\tlearn: 0.1017590\ttotal: 2.1s\tremaining: 4.81s\n",
      "304:\tlearn: 0.1017298\ttotal: 2.1s\tremaining: 4.8s\n",
      "305:\tlearn: 0.1016859\ttotal: 2.11s\tremaining: 4.79s\n",
      "306:\tlearn: 0.1016208\ttotal: 2.12s\tremaining: 4.79s\n",
      "307:\tlearn: 0.1015152\ttotal: 2.13s\tremaining: 4.79s\n",
      "308:\tlearn: 0.1014831\ttotal: 2.13s\tremaining: 4.77s\n",
      "309:\tlearn: 0.1014399\ttotal: 2.14s\tremaining: 4.76s\n",
      "310:\tlearn: 0.1014001\ttotal: 2.15s\tremaining: 4.76s\n",
      "311:\tlearn: 0.1013106\ttotal: 2.16s\tremaining: 4.75s\n",
      "312:\tlearn: 0.1012647\ttotal: 2.16s\tremaining: 4.75s\n",
      "313:\tlearn: 0.1011787\ttotal: 2.17s\tremaining: 4.74s\n",
      "314:\tlearn: 0.1011186\ttotal: 2.18s\tremaining: 4.73s\n",
      "315:\tlearn: 0.1010592\ttotal: 2.18s\tremaining: 4.72s\n",
      "316:\tlearn: 0.1009948\ttotal: 2.19s\tremaining: 4.71s\n",
      "317:\tlearn: 0.1009666\ttotal: 2.19s\tremaining: 4.71s\n",
      "318:\tlearn: 0.1008868\ttotal: 2.2s\tremaining: 4.7s\n",
      "319:\tlearn: 0.1008052\ttotal: 2.21s\tremaining: 4.69s\n",
      "320:\tlearn: 0.1007502\ttotal: 2.22s\tremaining: 4.69s\n",
      "321:\tlearn: 0.1006896\ttotal: 2.22s\tremaining: 4.68s\n",
      "322:\tlearn: 0.1006077\ttotal: 2.23s\tremaining: 4.67s\n",
      "323:\tlearn: 0.1005445\ttotal: 2.24s\tremaining: 4.67s\n",
      "324:\tlearn: 0.1005253\ttotal: 2.24s\tremaining: 4.66s\n",
      "325:\tlearn: 0.1004550\ttotal: 2.25s\tremaining: 4.65s\n",
      "326:\tlearn: 0.1004008\ttotal: 2.25s\tremaining: 4.64s\n",
      "327:\tlearn: 0.1003553\ttotal: 2.27s\tremaining: 4.64s\n",
      "328:\tlearn: 0.1002994\ttotal: 2.28s\tremaining: 4.64s\n",
      "329:\tlearn: 0.1002193\ttotal: 2.29s\tremaining: 4.64s\n",
      "330:\tlearn: 0.1001861\ttotal: 2.3s\tremaining: 4.66s\n",
      "331:\tlearn: 0.1001304\ttotal: 2.31s\tremaining: 4.66s\n",
      "332:\tlearn: 0.1000466\ttotal: 2.33s\tremaining: 4.66s\n",
      "333:\tlearn: 0.0999849\ttotal: 2.35s\tremaining: 4.68s\n",
      "334:\tlearn: 0.0999550\ttotal: 2.35s\tremaining: 4.67s\n",
      "335:\tlearn: 0.0998656\ttotal: 2.36s\tremaining: 4.66s\n",
      "336:\tlearn: 0.0997965\ttotal: 2.36s\tremaining: 4.65s\n",
      "337:\tlearn: 0.0997456\ttotal: 2.37s\tremaining: 4.64s\n",
      "338:\tlearn: 0.0996987\ttotal: 2.37s\tremaining: 4.63s\n",
      "339:\tlearn: 0.0996378\ttotal: 2.38s\tremaining: 4.62s\n",
      "340:\tlearn: 0.0995886\ttotal: 2.39s\tremaining: 4.61s\n",
      "341:\tlearn: 0.0995468\ttotal: 2.39s\tremaining: 4.6s\n",
      "342:\tlearn: 0.0995086\ttotal: 2.4s\tremaining: 4.59s\n",
      "343:\tlearn: 0.0994679\ttotal: 2.4s\tremaining: 4.59s\n",
      "344:\tlearn: 0.0993876\ttotal: 2.41s\tremaining: 4.58s\n",
      "345:\tlearn: 0.0993149\ttotal: 2.42s\tremaining: 4.57s\n",
      "346:\tlearn: 0.0992802\ttotal: 2.43s\tremaining: 4.57s\n",
      "347:\tlearn: 0.0992519\ttotal: 2.43s\tremaining: 4.56s\n",
      "348:\tlearn: 0.0992167\ttotal: 2.44s\tremaining: 4.55s\n",
      "349:\tlearn: 0.0991740\ttotal: 2.44s\tremaining: 4.54s\n",
      "350:\tlearn: 0.0991179\ttotal: 2.45s\tremaining: 4.53s\n",
      "351:\tlearn: 0.0990931\ttotal: 2.46s\tremaining: 4.52s\n",
      "352:\tlearn: 0.0989952\ttotal: 2.46s\tremaining: 4.51s\n",
      "353:\tlearn: 0.0989558\ttotal: 2.47s\tremaining: 4.5s\n",
      "354:\tlearn: 0.0989301\ttotal: 2.47s\tremaining: 4.5s\n",
      "355:\tlearn: 0.0988692\ttotal: 2.48s\tremaining: 4.49s\n",
      "356:\tlearn: 0.0988482\ttotal: 2.49s\tremaining: 4.48s\n",
      "357:\tlearn: 0.0988131\ttotal: 2.49s\tremaining: 4.47s\n",
      "358:\tlearn: 0.0987863\ttotal: 2.5s\tremaining: 4.46s\n",
      "359:\tlearn: 0.0987529\ttotal: 2.5s\tremaining: 4.45s\n",
      "360:\tlearn: 0.0986612\ttotal: 2.51s\tremaining: 4.44s\n",
      "361:\tlearn: 0.0986121\ttotal: 2.52s\tremaining: 4.43s\n",
      "362:\tlearn: 0.0985812\ttotal: 2.52s\tremaining: 4.42s\n",
      "363:\tlearn: 0.0985421\ttotal: 2.53s\tremaining: 4.41s\n",
      "364:\tlearn: 0.0985097\ttotal: 2.53s\tremaining: 4.41s\n",
      "365:\tlearn: 0.0984627\ttotal: 2.54s\tremaining: 4.4s\n",
      "366:\tlearn: 0.0984252\ttotal: 2.55s\tremaining: 4.4s\n",
      "367:\tlearn: 0.0983852\ttotal: 2.56s\tremaining: 4.39s\n",
      "368:\tlearn: 0.0983319\ttotal: 2.56s\tremaining: 4.38s\n",
      "369:\tlearn: 0.0982912\ttotal: 2.57s\tremaining: 4.37s\n",
      "370:\tlearn: 0.0982521\ttotal: 2.57s\tremaining: 4.36s\n",
      "371:\tlearn: 0.0982341\ttotal: 2.58s\tremaining: 4.35s\n",
      "372:\tlearn: 0.0982017\ttotal: 2.58s\tremaining: 4.34s\n",
      "373:\tlearn: 0.0981595\ttotal: 2.59s\tremaining: 4.33s\n",
      "374:\tlearn: 0.0981174\ttotal: 2.6s\tremaining: 4.33s\n",
      "375:\tlearn: 0.0980862\ttotal: 2.6s\tremaining: 4.32s\n",
      "376:\tlearn: 0.0980636\ttotal: 2.61s\tremaining: 4.31s\n",
      "377:\tlearn: 0.0980340\ttotal: 2.61s\tremaining: 4.3s\n",
      "378:\tlearn: 0.0979971\ttotal: 2.62s\tremaining: 4.29s\n",
      "379:\tlearn: 0.0979639\ttotal: 2.63s\tremaining: 4.28s\n",
      "380:\tlearn: 0.0979283\ttotal: 2.63s\tremaining: 4.27s\n",
      "381:\tlearn: 0.0978870\ttotal: 2.64s\tremaining: 4.26s\n",
      "382:\tlearn: 0.0978518\ttotal: 2.64s\tremaining: 4.26s\n",
      "383:\tlearn: 0.0978218\ttotal: 2.65s\tremaining: 4.25s\n",
      "384:\tlearn: 0.0977751\ttotal: 2.65s\tremaining: 4.24s\n",
      "385:\tlearn: 0.0977147\ttotal: 2.66s\tremaining: 4.23s\n",
      "386:\tlearn: 0.0976894\ttotal: 2.67s\tremaining: 4.23s\n",
      "387:\tlearn: 0.0976527\ttotal: 2.67s\tremaining: 4.22s\n",
      "388:\tlearn: 0.0976333\ttotal: 2.68s\tremaining: 4.21s\n",
      "389:\tlearn: 0.0975510\ttotal: 2.68s\tremaining: 4.2s\n",
      "390:\tlearn: 0.0975036\ttotal: 2.69s\tremaining: 4.19s\n",
      "391:\tlearn: 0.0974767\ttotal: 2.69s\tremaining: 4.18s\n",
      "392:\tlearn: 0.0974564\ttotal: 2.7s\tremaining: 4.17s\n",
      "393:\tlearn: 0.0973774\ttotal: 2.71s\tremaining: 4.16s\n",
      "394:\tlearn: 0.0973000\ttotal: 2.71s\tremaining: 4.15s\n",
      "395:\tlearn: 0.0972532\ttotal: 2.72s\tremaining: 4.15s\n",
      "396:\tlearn: 0.0972206\ttotal: 2.72s\tremaining: 4.14s\n",
      "397:\tlearn: 0.0971908\ttotal: 2.73s\tremaining: 4.13s\n",
      "398:\tlearn: 0.0971516\ttotal: 2.74s\tremaining: 4.13s\n",
      "399:\tlearn: 0.0971325\ttotal: 2.75s\tremaining: 4.12s\n",
      "400:\tlearn: 0.0970882\ttotal: 2.75s\tremaining: 4.11s\n",
      "401:\tlearn: 0.0970369\ttotal: 2.76s\tremaining: 4.1s\n",
      "402:\tlearn: 0.0969485\ttotal: 2.77s\tremaining: 4.1s\n",
      "403:\tlearn: 0.0968959\ttotal: 2.78s\tremaining: 4.09s\n",
      "404:\tlearn: 0.0968738\ttotal: 2.78s\tremaining: 4.09s\n",
      "405:\tlearn: 0.0967901\ttotal: 2.79s\tremaining: 4.08s\n",
      "406:\tlearn: 0.0967584\ttotal: 2.8s\tremaining: 4.07s\n",
      "407:\tlearn: 0.0966897\ttotal: 2.8s\tremaining: 4.07s\n",
      "408:\tlearn: 0.0966642\ttotal: 2.81s\tremaining: 4.06s\n",
      "409:\tlearn: 0.0966448\ttotal: 2.81s\tremaining: 4.05s\n",
      "410:\tlearn: 0.0966305\ttotal: 2.82s\tremaining: 4.04s\n",
      "411:\tlearn: 0.0965781\ttotal: 2.83s\tremaining: 4.03s\n",
      "412:\tlearn: 0.0965481\ttotal: 2.83s\tremaining: 4.02s\n",
      "413:\tlearn: 0.0965156\ttotal: 2.84s\tremaining: 4.01s\n",
      "414:\tlearn: 0.0964913\ttotal: 2.84s\tremaining: 4.01s\n",
      "415:\tlearn: 0.0964684\ttotal: 2.85s\tremaining: 4s\n",
      "416:\tlearn: 0.0964442\ttotal: 2.86s\tremaining: 4s\n",
      "417:\tlearn: 0.0964014\ttotal: 2.86s\tremaining: 3.99s\n",
      "418:\tlearn: 0.0963716\ttotal: 2.87s\tremaining: 3.98s\n",
      "419:\tlearn: 0.0963442\ttotal: 2.88s\tremaining: 3.97s\n",
      "420:\tlearn: 0.0962850\ttotal: 2.88s\tremaining: 3.96s\n",
      "421:\tlearn: 0.0962467\ttotal: 2.89s\tremaining: 3.96s\n",
      "422:\tlearn: 0.0962306\ttotal: 2.89s\tremaining: 3.95s\n",
      "423:\tlearn: 0.0962136\ttotal: 2.9s\tremaining: 3.94s\n",
      "424:\tlearn: 0.0961893\ttotal: 2.9s\tremaining: 3.93s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425:\tlearn: 0.0961688\ttotal: 2.91s\tremaining: 3.93s\n",
      "426:\tlearn: 0.0961488\ttotal: 2.92s\tremaining: 3.92s\n",
      "427:\tlearn: 0.0961301\ttotal: 2.93s\tremaining: 3.91s\n",
      "428:\tlearn: 0.0960791\ttotal: 2.93s\tremaining: 3.91s\n",
      "429:\tlearn: 0.0960539\ttotal: 2.94s\tremaining: 3.9s\n",
      "430:\tlearn: 0.0960330\ttotal: 2.94s\tremaining: 3.89s\n",
      "431:\tlearn: 0.0959952\ttotal: 2.95s\tremaining: 3.88s\n",
      "432:\tlearn: 0.0959352\ttotal: 2.96s\tremaining: 3.87s\n",
      "433:\tlearn: 0.0958881\ttotal: 2.96s\tremaining: 3.86s\n",
      "434:\tlearn: 0.0958699\ttotal: 2.97s\tremaining: 3.85s\n",
      "435:\tlearn: 0.0958195\ttotal: 2.97s\tremaining: 3.85s\n",
      "436:\tlearn: 0.0957985\ttotal: 2.98s\tremaining: 3.84s\n",
      "437:\tlearn: 0.0957551\ttotal: 2.98s\tremaining: 3.83s\n",
      "438:\tlearn: 0.0957058\ttotal: 2.99s\tremaining: 3.82s\n",
      "439:\tlearn: 0.0956630\ttotal: 2.99s\tremaining: 3.81s\n",
      "440:\tlearn: 0.0956253\ttotal: 3s\tremaining: 3.8s\n",
      "441:\tlearn: 0.0955973\ttotal: 3.01s\tremaining: 3.8s\n",
      "442:\tlearn: 0.0955745\ttotal: 3.01s\tremaining: 3.79s\n",
      "443:\tlearn: 0.0955423\ttotal: 3.02s\tremaining: 3.78s\n",
      "444:\tlearn: 0.0954749\ttotal: 3.03s\tremaining: 3.77s\n",
      "445:\tlearn: 0.0954566\ttotal: 3.03s\tremaining: 3.77s\n",
      "446:\tlearn: 0.0954211\ttotal: 3.04s\tremaining: 3.76s\n",
      "447:\tlearn: 0.0953773\ttotal: 3.04s\tremaining: 3.75s\n",
      "448:\tlearn: 0.0953360\ttotal: 3.05s\tremaining: 3.75s\n",
      "449:\tlearn: 0.0953204\ttotal: 3.06s\tremaining: 3.74s\n",
      "450:\tlearn: 0.0953073\ttotal: 3.06s\tremaining: 3.73s\n",
      "451:\tlearn: 0.0952725\ttotal: 3.07s\tremaining: 3.72s\n",
      "452:\tlearn: 0.0952187\ttotal: 3.08s\tremaining: 3.71s\n",
      "453:\tlearn: 0.0952045\ttotal: 3.08s\tremaining: 3.71s\n",
      "454:\tlearn: 0.0951893\ttotal: 3.09s\tremaining: 3.7s\n",
      "455:\tlearn: 0.0951552\ttotal: 3.09s\tremaining: 3.69s\n",
      "456:\tlearn: 0.0951365\ttotal: 3.1s\tremaining: 3.68s\n",
      "457:\tlearn: 0.0951215\ttotal: 3.1s\tremaining: 3.67s\n",
      "458:\tlearn: 0.0951022\ttotal: 3.11s\tremaining: 3.66s\n",
      "459:\tlearn: 0.0950770\ttotal: 3.11s\tremaining: 3.66s\n",
      "460:\tlearn: 0.0950592\ttotal: 3.12s\tremaining: 3.65s\n",
      "461:\tlearn: 0.0950422\ttotal: 3.13s\tremaining: 3.64s\n",
      "462:\tlearn: 0.0950286\ttotal: 3.13s\tremaining: 3.63s\n",
      "463:\tlearn: 0.0950147\ttotal: 3.14s\tremaining: 3.63s\n",
      "464:\tlearn: 0.0950024\ttotal: 3.14s\tremaining: 3.62s\n",
      "465:\tlearn: 0.0949639\ttotal: 3.15s\tremaining: 3.61s\n",
      "466:\tlearn: 0.0949479\ttotal: 3.15s\tremaining: 3.6s\n",
      "467:\tlearn: 0.0949359\ttotal: 3.16s\tremaining: 3.59s\n",
      "468:\tlearn: 0.0949144\ttotal: 3.17s\tremaining: 3.58s\n",
      "469:\tlearn: 0.0948716\ttotal: 3.17s\tremaining: 3.58s\n",
      "470:\tlearn: 0.0948556\ttotal: 3.18s\tremaining: 3.57s\n",
      "471:\tlearn: 0.0947996\ttotal: 3.18s\tremaining: 3.56s\n",
      "472:\tlearn: 0.0947264\ttotal: 3.19s\tremaining: 3.55s\n",
      "473:\tlearn: 0.0947059\ttotal: 3.2s\tremaining: 3.55s\n",
      "474:\tlearn: 0.0946920\ttotal: 3.21s\tremaining: 3.55s\n",
      "475:\tlearn: 0.0946436\ttotal: 3.22s\tremaining: 3.55s\n",
      "476:\tlearn: 0.0946225\ttotal: 3.24s\tremaining: 3.55s\n",
      "477:\tlearn: 0.0945807\ttotal: 3.25s\tremaining: 3.55s\n",
      "478:\tlearn: 0.0945630\ttotal: 3.26s\tremaining: 3.55s\n",
      "479:\tlearn: 0.0945496\ttotal: 3.27s\tremaining: 3.55s\n",
      "480:\tlearn: 0.0945384\ttotal: 3.29s\tremaining: 3.55s\n",
      "481:\tlearn: 0.0945125\ttotal: 3.3s\tremaining: 3.54s\n",
      "482:\tlearn: 0.0944727\ttotal: 3.3s\tremaining: 3.53s\n",
      "483:\tlearn: 0.0944270\ttotal: 3.31s\tremaining: 3.53s\n",
      "484:\tlearn: 0.0943922\ttotal: 3.31s\tremaining: 3.52s\n",
      "485:\tlearn: 0.0943482\ttotal: 3.32s\tremaining: 3.51s\n",
      "486:\tlearn: 0.0942843\ttotal: 3.33s\tremaining: 3.5s\n",
      "487:\tlearn: 0.0942483\ttotal: 3.33s\tremaining: 3.49s\n",
      "488:\tlearn: 0.0942198\ttotal: 3.34s\tremaining: 3.49s\n",
      "489:\tlearn: 0.0941696\ttotal: 3.34s\tremaining: 3.48s\n",
      "490:\tlearn: 0.0941521\ttotal: 3.35s\tremaining: 3.47s\n",
      "491:\tlearn: 0.0940946\ttotal: 3.35s\tremaining: 3.46s\n",
      "492:\tlearn: 0.0940765\ttotal: 3.36s\tremaining: 3.46s\n",
      "493:\tlearn: 0.0940491\ttotal: 3.37s\tremaining: 3.45s\n",
      "494:\tlearn: 0.0940164\ttotal: 3.37s\tremaining: 3.44s\n",
      "495:\tlearn: 0.0939962\ttotal: 3.38s\tremaining: 3.43s\n",
      "496:\tlearn: 0.0939835\ttotal: 3.39s\tremaining: 3.43s\n",
      "497:\tlearn: 0.0939679\ttotal: 3.4s\tremaining: 3.42s\n",
      "498:\tlearn: 0.0939394\ttotal: 3.4s\tremaining: 3.42s\n",
      "499:\tlearn: 0.0938865\ttotal: 3.41s\tremaining: 3.41s\n",
      "500:\tlearn: 0.0938747\ttotal: 3.42s\tremaining: 3.4s\n",
      "501:\tlearn: 0.0938532\ttotal: 3.42s\tremaining: 3.39s\n",
      "502:\tlearn: 0.0938164\ttotal: 3.43s\tremaining: 3.39s\n",
      "503:\tlearn: 0.0938050\ttotal: 3.43s\tremaining: 3.38s\n",
      "504:\tlearn: 0.0937838\ttotal: 3.44s\tremaining: 3.37s\n",
      "505:\tlearn: 0.0937717\ttotal: 3.44s\tremaining: 3.36s\n",
      "506:\tlearn: 0.0937354\ttotal: 3.45s\tremaining: 3.35s\n",
      "507:\tlearn: 0.0937267\ttotal: 3.46s\tremaining: 3.35s\n",
      "508:\tlearn: 0.0936966\ttotal: 3.46s\tremaining: 3.34s\n",
      "509:\tlearn: 0.0936754\ttotal: 3.47s\tremaining: 3.33s\n",
      "510:\tlearn: 0.0936677\ttotal: 3.47s\tremaining: 3.32s\n",
      "511:\tlearn: 0.0936596\ttotal: 3.48s\tremaining: 3.31s\n",
      "512:\tlearn: 0.0936452\ttotal: 3.49s\tremaining: 3.31s\n",
      "513:\tlearn: 0.0936162\ttotal: 3.5s\tremaining: 3.31s\n",
      "514:\tlearn: 0.0935976\ttotal: 3.5s\tremaining: 3.3s\n",
      "515:\tlearn: 0.0935781\ttotal: 3.51s\tremaining: 3.29s\n",
      "516:\tlearn: 0.0935515\ttotal: 3.52s\tremaining: 3.28s\n",
      "517:\tlearn: 0.0935394\ttotal: 3.52s\tremaining: 3.28s\n",
      "518:\tlearn: 0.0935253\ttotal: 3.53s\tremaining: 3.27s\n",
      "519:\tlearn: 0.0934921\ttotal: 3.53s\tremaining: 3.26s\n",
      "520:\tlearn: 0.0934613\ttotal: 3.54s\tremaining: 3.25s\n",
      "521:\tlearn: 0.0934464\ttotal: 3.54s\tremaining: 3.24s\n",
      "522:\tlearn: 0.0934340\ttotal: 3.55s\tremaining: 3.24s\n",
      "523:\tlearn: 0.0933692\ttotal: 3.55s\tremaining: 3.23s\n",
      "524:\tlearn: 0.0933243\ttotal: 3.56s\tremaining: 3.22s\n",
      "525:\tlearn: 0.0933149\ttotal: 3.56s\tremaining: 3.21s\n",
      "526:\tlearn: 0.0932943\ttotal: 3.57s\tremaining: 3.2s\n",
      "527:\tlearn: 0.0932708\ttotal: 3.58s\tremaining: 3.2s\n",
      "528:\tlearn: 0.0932461\ttotal: 3.58s\tremaining: 3.19s\n",
      "529:\tlearn: 0.0932102\ttotal: 3.59s\tremaining: 3.18s\n",
      "530:\tlearn: 0.0931849\ttotal: 3.59s\tremaining: 3.17s\n",
      "531:\tlearn: 0.0931541\ttotal: 3.6s\tremaining: 3.17s\n",
      "532:\tlearn: 0.0931419\ttotal: 3.6s\tremaining: 3.16s\n",
      "533:\tlearn: 0.0931315\ttotal: 3.61s\tremaining: 3.15s\n",
      "534:\tlearn: 0.0930931\ttotal: 3.62s\tremaining: 3.14s\n",
      "535:\tlearn: 0.0930785\ttotal: 3.62s\tremaining: 3.14s\n",
      "536:\tlearn: 0.0930611\ttotal: 3.63s\tremaining: 3.13s\n",
      "537:\tlearn: 0.0930228\ttotal: 3.64s\tremaining: 3.12s\n",
      "538:\tlearn: 0.0929983\ttotal: 3.64s\tremaining: 3.12s\n",
      "539:\tlearn: 0.0929678\ttotal: 3.65s\tremaining: 3.11s\n",
      "540:\tlearn: 0.0929282\ttotal: 3.65s\tremaining: 3.1s\n",
      "541:\tlearn: 0.0929172\ttotal: 3.66s\tremaining: 3.09s\n",
      "542:\tlearn: 0.0928906\ttotal: 3.66s\tremaining: 3.08s\n",
      "543:\tlearn: 0.0928377\ttotal: 3.67s\tremaining: 3.08s\n",
      "544:\tlearn: 0.0928247\ttotal: 3.68s\tremaining: 3.07s\n",
      "545:\tlearn: 0.0927965\ttotal: 3.68s\tremaining: 3.06s\n",
      "546:\tlearn: 0.0927812\ttotal: 3.69s\tremaining: 3.05s\n",
      "547:\tlearn: 0.0927358\ttotal: 3.69s\tremaining: 3.04s\n",
      "548:\tlearn: 0.0927207\ttotal: 3.7s\tremaining: 3.04s\n",
      "549:\tlearn: 0.0927059\ttotal: 3.7s\tremaining: 3.03s\n",
      "550:\tlearn: 0.0926891\ttotal: 3.71s\tremaining: 3.02s\n",
      "551:\tlearn: 0.0926313\ttotal: 3.71s\tremaining: 3.01s\n",
      "552:\tlearn: 0.0926162\ttotal: 3.72s\tremaining: 3.01s\n",
      "553:\tlearn: 0.0925834\ttotal: 3.73s\tremaining: 3s\n",
      "554:\tlearn: 0.0925714\ttotal: 3.73s\tremaining: 2.99s\n",
      "555:\tlearn: 0.0925197\ttotal: 3.74s\tremaining: 2.98s\n",
      "556:\tlearn: 0.0925035\ttotal: 3.74s\tremaining: 2.98s\n",
      "557:\tlearn: 0.0924805\ttotal: 3.75s\tremaining: 2.97s\n",
      "558:\tlearn: 0.0924490\ttotal: 3.75s\tremaining: 2.96s\n",
      "559:\tlearn: 0.0924087\ttotal: 3.76s\tremaining: 2.95s\n",
      "560:\tlearn: 0.0923945\ttotal: 3.77s\tremaining: 2.95s\n",
      "561:\tlearn: 0.0923587\ttotal: 3.77s\tremaining: 2.94s\n",
      "562:\tlearn: 0.0923302\ttotal: 3.78s\tremaining: 2.93s\n",
      "563:\tlearn: 0.0923089\ttotal: 3.78s\tremaining: 2.92s\n",
      "564:\tlearn: 0.0922824\ttotal: 3.79s\tremaining: 2.92s\n",
      "565:\tlearn: 0.0922645\ttotal: 3.79s\tremaining: 2.91s\n",
      "566:\tlearn: 0.0922259\ttotal: 3.8s\tremaining: 2.9s\n",
      "567:\tlearn: 0.0921714\ttotal: 3.8s\tremaining: 2.89s\n",
      "568:\tlearn: 0.0921613\ttotal: 3.81s\tremaining: 2.88s\n",
      "569:\tlearn: 0.0921453\ttotal: 3.81s\tremaining: 2.88s\n",
      "570:\tlearn: 0.0921340\ttotal: 3.82s\tremaining: 2.87s\n",
      "571:\tlearn: 0.0921050\ttotal: 3.83s\tremaining: 2.86s\n",
      "572:\tlearn: 0.0920835\ttotal: 3.83s\tremaining: 2.85s\n",
      "573:\tlearn: 0.0920729\ttotal: 3.84s\tremaining: 2.85s\n",
      "574:\tlearn: 0.0920504\ttotal: 3.84s\tremaining: 2.84s\n",
      "575:\tlearn: 0.0920263\ttotal: 3.85s\tremaining: 2.83s\n",
      "576:\tlearn: 0.0920084\ttotal: 3.85s\tremaining: 2.83s\n",
      "577:\tlearn: 0.0919783\ttotal: 3.86s\tremaining: 2.82s\n",
      "578:\tlearn: 0.0919456\ttotal: 3.86s\tremaining: 2.81s\n",
      "579:\tlearn: 0.0919199\ttotal: 3.87s\tremaining: 2.8s\n",
      "580:\tlearn: 0.0919102\ttotal: 3.88s\tremaining: 2.8s\n",
      "581:\tlearn: 0.0918888\ttotal: 3.88s\tremaining: 2.79s\n",
      "582:\tlearn: 0.0918715\ttotal: 3.89s\tremaining: 2.78s\n",
      "583:\tlearn: 0.0918623\ttotal: 3.9s\tremaining: 2.78s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584:\tlearn: 0.0918403\ttotal: 3.9s\tremaining: 2.77s\n",
      "585:\tlearn: 0.0918318\ttotal: 3.91s\tremaining: 2.76s\n",
      "586:\tlearn: 0.0918223\ttotal: 3.91s\tremaining: 2.75s\n",
      "587:\tlearn: 0.0917999\ttotal: 3.92s\tremaining: 2.75s\n",
      "588:\tlearn: 0.0917896\ttotal: 3.92s\tremaining: 2.74s\n",
      "589:\tlearn: 0.0917686\ttotal: 3.93s\tremaining: 2.73s\n",
      "590:\tlearn: 0.0917447\ttotal: 3.94s\tremaining: 2.72s\n",
      "591:\tlearn: 0.0917294\ttotal: 3.94s\tremaining: 2.72s\n",
      "592:\tlearn: 0.0917209\ttotal: 3.95s\tremaining: 2.71s\n",
      "593:\tlearn: 0.0917106\ttotal: 3.95s\tremaining: 2.7s\n",
      "594:\tlearn: 0.0916853\ttotal: 3.96s\tremaining: 2.69s\n",
      "595:\tlearn: 0.0916733\ttotal: 3.96s\tremaining: 2.69s\n",
      "596:\tlearn: 0.0916634\ttotal: 3.97s\tremaining: 2.68s\n",
      "597:\tlearn: 0.0916415\ttotal: 3.98s\tremaining: 2.67s\n",
      "598:\tlearn: 0.0916117\ttotal: 3.98s\tremaining: 2.67s\n",
      "599:\tlearn: 0.0915959\ttotal: 3.99s\tremaining: 2.66s\n",
      "600:\tlearn: 0.0915856\ttotal: 3.99s\tremaining: 2.65s\n",
      "601:\tlearn: 0.0915357\ttotal: 4s\tremaining: 2.64s\n",
      "602:\tlearn: 0.0915184\ttotal: 4s\tremaining: 2.63s\n",
      "603:\tlearn: 0.0914932\ttotal: 4.01s\tremaining: 2.63s\n",
      "604:\tlearn: 0.0914608\ttotal: 4.01s\tremaining: 2.62s\n",
      "605:\tlearn: 0.0914526\ttotal: 4.02s\tremaining: 2.62s\n",
      "606:\tlearn: 0.0914427\ttotal: 4.03s\tremaining: 2.61s\n",
      "607:\tlearn: 0.0914010\ttotal: 4.03s\tremaining: 2.6s\n",
      "608:\tlearn: 0.0913877\ttotal: 4.04s\tremaining: 2.59s\n",
      "609:\tlearn: 0.0913614\ttotal: 4.04s\tremaining: 2.58s\n",
      "610:\tlearn: 0.0913330\ttotal: 4.05s\tremaining: 2.58s\n",
      "611:\tlearn: 0.0913108\ttotal: 4.05s\tremaining: 2.57s\n",
      "612:\tlearn: 0.0912892\ttotal: 4.06s\tremaining: 2.56s\n",
      "613:\tlearn: 0.0912695\ttotal: 4.07s\tremaining: 2.56s\n",
      "614:\tlearn: 0.0912627\ttotal: 4.07s\tremaining: 2.55s\n",
      "615:\tlearn: 0.0912433\ttotal: 4.08s\tremaining: 2.54s\n",
      "616:\tlearn: 0.0912342\ttotal: 4.09s\tremaining: 2.54s\n",
      "617:\tlearn: 0.0912227\ttotal: 4.09s\tremaining: 2.53s\n",
      "618:\tlearn: 0.0912146\ttotal: 4.1s\tremaining: 2.52s\n",
      "619:\tlearn: 0.0911896\ttotal: 4.1s\tremaining: 2.51s\n",
      "620:\tlearn: 0.0911715\ttotal: 4.11s\tremaining: 2.51s\n",
      "621:\tlearn: 0.0911195\ttotal: 4.11s\tremaining: 2.5s\n",
      "622:\tlearn: 0.0910870\ttotal: 4.12s\tremaining: 2.49s\n",
      "623:\tlearn: 0.0910494\ttotal: 4.13s\tremaining: 2.49s\n",
      "624:\tlearn: 0.0910319\ttotal: 4.13s\tremaining: 2.48s\n",
      "625:\tlearn: 0.0910231\ttotal: 4.14s\tremaining: 2.47s\n",
      "626:\tlearn: 0.0909934\ttotal: 4.14s\tremaining: 2.46s\n",
      "627:\tlearn: 0.0909599\ttotal: 4.16s\tremaining: 2.46s\n",
      "628:\tlearn: 0.0909353\ttotal: 4.17s\tremaining: 2.46s\n",
      "629:\tlearn: 0.0909192\ttotal: 4.18s\tremaining: 2.46s\n",
      "630:\tlearn: 0.0909073\ttotal: 4.19s\tremaining: 2.45s\n",
      "631:\tlearn: 0.0908917\ttotal: 4.2s\tremaining: 2.45s\n",
      "632:\tlearn: 0.0908572\ttotal: 4.21s\tremaining: 2.44s\n",
      "633:\tlearn: 0.0908380\ttotal: 4.22s\tremaining: 2.44s\n",
      "634:\tlearn: 0.0908162\ttotal: 4.24s\tremaining: 2.43s\n",
      "635:\tlearn: 0.0908017\ttotal: 4.24s\tremaining: 2.43s\n",
      "636:\tlearn: 0.0907965\ttotal: 4.25s\tremaining: 2.42s\n",
      "637:\tlearn: 0.0907901\ttotal: 4.25s\tremaining: 2.41s\n",
      "638:\tlearn: 0.0907812\ttotal: 4.26s\tremaining: 2.4s\n",
      "639:\tlearn: 0.0907372\ttotal: 4.26s\tremaining: 2.4s\n",
      "640:\tlearn: 0.0907221\ttotal: 4.27s\tremaining: 2.39s\n",
      "641:\tlearn: 0.0906982\ttotal: 4.28s\tremaining: 2.39s\n",
      "642:\tlearn: 0.0906795\ttotal: 4.29s\tremaining: 2.38s\n",
      "643:\tlearn: 0.0906551\ttotal: 4.29s\tremaining: 2.37s\n",
      "644:\tlearn: 0.0906461\ttotal: 4.3s\tremaining: 2.36s\n",
      "645:\tlearn: 0.0906374\ttotal: 4.3s\tremaining: 2.36s\n",
      "646:\tlearn: 0.0905891\ttotal: 4.31s\tremaining: 2.35s\n",
      "647:\tlearn: 0.0905643\ttotal: 4.31s\tremaining: 2.34s\n",
      "648:\tlearn: 0.0905478\ttotal: 4.32s\tremaining: 2.33s\n",
      "649:\tlearn: 0.0905389\ttotal: 4.32s\tremaining: 2.33s\n",
      "650:\tlearn: 0.0905155\ttotal: 4.33s\tremaining: 2.32s\n",
      "651:\tlearn: 0.0904902\ttotal: 4.33s\tremaining: 2.31s\n",
      "652:\tlearn: 0.0904774\ttotal: 4.34s\tremaining: 2.31s\n",
      "653:\tlearn: 0.0904726\ttotal: 4.35s\tremaining: 2.3s\n",
      "654:\tlearn: 0.0904483\ttotal: 4.35s\tremaining: 2.29s\n",
      "655:\tlearn: 0.0904359\ttotal: 4.36s\tremaining: 2.29s\n",
      "656:\tlearn: 0.0904255\ttotal: 4.36s\tremaining: 2.28s\n",
      "657:\tlearn: 0.0903937\ttotal: 4.37s\tremaining: 2.27s\n",
      "658:\tlearn: 0.0903781\ttotal: 4.37s\tremaining: 2.26s\n",
      "659:\tlearn: 0.0903722\ttotal: 4.38s\tremaining: 2.26s\n",
      "660:\tlearn: 0.0903256\ttotal: 4.38s\tremaining: 2.25s\n",
      "661:\tlearn: 0.0903196\ttotal: 4.39s\tremaining: 2.24s\n",
      "662:\tlearn: 0.0902882\ttotal: 4.4s\tremaining: 2.23s\n",
      "663:\tlearn: 0.0902721\ttotal: 4.4s\tremaining: 2.23s\n",
      "664:\tlearn: 0.0902543\ttotal: 4.41s\tremaining: 2.22s\n",
      "665:\tlearn: 0.0902246\ttotal: 4.41s\tremaining: 2.21s\n",
      "666:\tlearn: 0.0902163\ttotal: 4.42s\tremaining: 2.21s\n",
      "667:\tlearn: 0.0901963\ttotal: 4.42s\tremaining: 2.2s\n",
      "668:\tlearn: 0.0901749\ttotal: 4.43s\tremaining: 2.19s\n",
      "669:\tlearn: 0.0901329\ttotal: 4.43s\tremaining: 2.18s\n",
      "670:\tlearn: 0.0901211\ttotal: 4.44s\tremaining: 2.18s\n",
      "671:\tlearn: 0.0900801\ttotal: 4.45s\tremaining: 2.17s\n",
      "672:\tlearn: 0.0900575\ttotal: 4.45s\tremaining: 2.16s\n",
      "673:\tlearn: 0.0900489\ttotal: 4.46s\tremaining: 2.15s\n",
      "674:\tlearn: 0.0900305\ttotal: 4.46s\tremaining: 2.15s\n",
      "675:\tlearn: 0.0900139\ttotal: 4.47s\tremaining: 2.14s\n",
      "676:\tlearn: 0.0900002\ttotal: 4.47s\tremaining: 2.13s\n",
      "677:\tlearn: 0.0899731\ttotal: 4.48s\tremaining: 2.13s\n",
      "678:\tlearn: 0.0899430\ttotal: 4.49s\tremaining: 2.12s\n",
      "679:\tlearn: 0.0899096\ttotal: 4.5s\tremaining: 2.12s\n",
      "680:\tlearn: 0.0899014\ttotal: 4.5s\tremaining: 2.11s\n",
      "681:\tlearn: 0.0898867\ttotal: 4.51s\tremaining: 2.1s\n",
      "682:\tlearn: 0.0898708\ttotal: 4.52s\tremaining: 2.1s\n",
      "683:\tlearn: 0.0898599\ttotal: 4.52s\tremaining: 2.09s\n",
      "684:\tlearn: 0.0898485\ttotal: 4.53s\tremaining: 2.08s\n",
      "685:\tlearn: 0.0898270\ttotal: 4.53s\tremaining: 2.08s\n",
      "686:\tlearn: 0.0898066\ttotal: 4.54s\tremaining: 2.07s\n",
      "687:\tlearn: 0.0897835\ttotal: 4.54s\tremaining: 2.06s\n",
      "688:\tlearn: 0.0897593\ttotal: 4.55s\tremaining: 2.05s\n",
      "689:\tlearn: 0.0897331\ttotal: 4.55s\tremaining: 2.05s\n",
      "690:\tlearn: 0.0897045\ttotal: 4.56s\tremaining: 2.04s\n",
      "691:\tlearn: 0.0896965\ttotal: 4.57s\tremaining: 2.03s\n",
      "692:\tlearn: 0.0896844\ttotal: 4.57s\tremaining: 2.02s\n",
      "693:\tlearn: 0.0896729\ttotal: 4.58s\tremaining: 2.02s\n",
      "694:\tlearn: 0.0896665\ttotal: 4.58s\tremaining: 2.01s\n",
      "695:\tlearn: 0.0896441\ttotal: 4.59s\tremaining: 2s\n",
      "696:\tlearn: 0.0896164\ttotal: 4.59s\tremaining: 2s\n",
      "697:\tlearn: 0.0895745\ttotal: 4.6s\tremaining: 1.99s\n",
      "698:\tlearn: 0.0895510\ttotal: 4.6s\tremaining: 1.98s\n",
      "699:\tlearn: 0.0895170\ttotal: 4.61s\tremaining: 1.98s\n",
      "700:\tlearn: 0.0894917\ttotal: 4.61s\tremaining: 1.97s\n",
      "701:\tlearn: 0.0894726\ttotal: 4.62s\tremaining: 1.96s\n",
      "702:\tlearn: 0.0894417\ttotal: 4.63s\tremaining: 1.95s\n",
      "703:\tlearn: 0.0894271\ttotal: 4.63s\tremaining: 1.95s\n",
      "704:\tlearn: 0.0893902\ttotal: 4.64s\tremaining: 1.94s\n",
      "705:\tlearn: 0.0893801\ttotal: 4.64s\tremaining: 1.93s\n",
      "706:\tlearn: 0.0893702\ttotal: 4.65s\tremaining: 1.93s\n",
      "707:\tlearn: 0.0893582\ttotal: 4.66s\tremaining: 1.92s\n",
      "708:\tlearn: 0.0893470\ttotal: 4.67s\tremaining: 1.92s\n",
      "709:\tlearn: 0.0893391\ttotal: 4.68s\tremaining: 1.91s\n",
      "710:\tlearn: 0.0893278\ttotal: 4.69s\tremaining: 1.91s\n",
      "711:\tlearn: 0.0892809\ttotal: 4.7s\tremaining: 1.9s\n",
      "712:\tlearn: 0.0892643\ttotal: 4.7s\tremaining: 1.89s\n",
      "713:\tlearn: 0.0892559\ttotal: 4.71s\tremaining: 1.89s\n",
      "714:\tlearn: 0.0892278\ttotal: 4.71s\tremaining: 1.88s\n",
      "715:\tlearn: 0.0892142\ttotal: 4.72s\tremaining: 1.87s\n",
      "716:\tlearn: 0.0891965\ttotal: 4.73s\tremaining: 1.86s\n",
      "717:\tlearn: 0.0891835\ttotal: 4.73s\tremaining: 1.86s\n",
      "718:\tlearn: 0.0891570\ttotal: 4.74s\tremaining: 1.85s\n",
      "719:\tlearn: 0.0891436\ttotal: 4.74s\tremaining: 1.84s\n",
      "720:\tlearn: 0.0891384\ttotal: 4.75s\tremaining: 1.84s\n",
      "721:\tlearn: 0.0891122\ttotal: 4.75s\tremaining: 1.83s\n",
      "722:\tlearn: 0.0890921\ttotal: 4.76s\tremaining: 1.82s\n",
      "723:\tlearn: 0.0890835\ttotal: 4.77s\tremaining: 1.82s\n",
      "724:\tlearn: 0.0890634\ttotal: 4.77s\tremaining: 1.81s\n",
      "725:\tlearn: 0.0890551\ttotal: 4.78s\tremaining: 1.8s\n",
      "726:\tlearn: 0.0890447\ttotal: 4.79s\tremaining: 1.8s\n",
      "727:\tlearn: 0.0890155\ttotal: 4.79s\tremaining: 1.79s\n",
      "728:\tlearn: 0.0889759\ttotal: 4.79s\tremaining: 1.78s\n",
      "729:\tlearn: 0.0889585\ttotal: 4.8s\tremaining: 1.77s\n",
      "730:\tlearn: 0.0889473\ttotal: 4.81s\tremaining: 1.77s\n",
      "731:\tlearn: 0.0889328\ttotal: 4.81s\tremaining: 1.76s\n",
      "732:\tlearn: 0.0889252\ttotal: 4.82s\tremaining: 1.75s\n",
      "733:\tlearn: 0.0889062\ttotal: 4.82s\tremaining: 1.75s\n",
      "734:\tlearn: 0.0888847\ttotal: 4.83s\tremaining: 1.74s\n",
      "735:\tlearn: 0.0888775\ttotal: 4.83s\tremaining: 1.73s\n",
      "736:\tlearn: 0.0888577\ttotal: 4.84s\tremaining: 1.73s\n",
      "737:\tlearn: 0.0888483\ttotal: 4.85s\tremaining: 1.72s\n",
      "738:\tlearn: 0.0888395\ttotal: 4.85s\tremaining: 1.71s\n",
      "739:\tlearn: 0.0888317\ttotal: 4.86s\tremaining: 1.71s\n",
      "740:\tlearn: 0.0888041\ttotal: 4.87s\tremaining: 1.7s\n",
      "741:\tlearn: 0.0887957\ttotal: 4.87s\tremaining: 1.69s\n",
      "742:\tlearn: 0.0887882\ttotal: 4.88s\tremaining: 1.69s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743:\tlearn: 0.0887617\ttotal: 4.88s\tremaining: 1.68s\n",
      "744:\tlearn: 0.0887530\ttotal: 4.89s\tremaining: 1.67s\n",
      "745:\tlearn: 0.0887391\ttotal: 4.9s\tremaining: 1.67s\n",
      "746:\tlearn: 0.0887237\ttotal: 4.91s\tremaining: 1.66s\n",
      "747:\tlearn: 0.0887124\ttotal: 4.92s\tremaining: 1.66s\n",
      "748:\tlearn: 0.0886779\ttotal: 4.93s\tremaining: 1.65s\n",
      "749:\tlearn: 0.0886626\ttotal: 4.94s\tremaining: 1.65s\n",
      "750:\tlearn: 0.0886426\ttotal: 4.95s\tremaining: 1.64s\n",
      "751:\tlearn: 0.0886291\ttotal: 4.96s\tremaining: 1.64s\n",
      "752:\tlearn: 0.0886034\ttotal: 4.97s\tremaining: 1.63s\n",
      "753:\tlearn: 0.0885885\ttotal: 4.99s\tremaining: 1.63s\n",
      "754:\tlearn: 0.0885735\ttotal: 5s\tremaining: 1.62s\n",
      "755:\tlearn: 0.0885656\ttotal: 5.01s\tremaining: 1.62s\n",
      "756:\tlearn: 0.0885286\ttotal: 5.01s\tremaining: 1.61s\n",
      "757:\tlearn: 0.0885150\ttotal: 5.02s\tremaining: 1.6s\n",
      "758:\tlearn: 0.0885060\ttotal: 5.03s\tremaining: 1.6s\n",
      "759:\tlearn: 0.0884954\ttotal: 5.04s\tremaining: 1.59s\n",
      "760:\tlearn: 0.0884807\ttotal: 5.04s\tremaining: 1.58s\n",
      "761:\tlearn: 0.0884695\ttotal: 5.05s\tremaining: 1.58s\n",
      "762:\tlearn: 0.0884568\ttotal: 5.06s\tremaining: 1.57s\n",
      "763:\tlearn: 0.0884461\ttotal: 5.07s\tremaining: 1.57s\n",
      "764:\tlearn: 0.0884359\ttotal: 5.09s\tremaining: 1.56s\n",
      "765:\tlearn: 0.0884243\ttotal: 5.11s\tremaining: 1.56s\n",
      "766:\tlearn: 0.0884064\ttotal: 5.13s\tremaining: 1.56s\n",
      "767:\tlearn: 0.0883942\ttotal: 5.14s\tremaining: 1.55s\n",
      "768:\tlearn: 0.0883722\ttotal: 5.15s\tremaining: 1.55s\n",
      "769:\tlearn: 0.0883522\ttotal: 5.16s\tremaining: 1.54s\n",
      "770:\tlearn: 0.0883391\ttotal: 5.17s\tremaining: 1.53s\n",
      "771:\tlearn: 0.0883210\ttotal: 5.18s\tremaining: 1.53s\n",
      "772:\tlearn: 0.0883130\ttotal: 5.19s\tremaining: 1.52s\n",
      "773:\tlearn: 0.0883047\ttotal: 5.2s\tremaining: 1.52s\n",
      "774:\tlearn: 0.0882847\ttotal: 5.2s\tremaining: 1.51s\n",
      "775:\tlearn: 0.0882769\ttotal: 5.21s\tremaining: 1.5s\n",
      "776:\tlearn: 0.0882688\ttotal: 5.21s\tremaining: 1.5s\n",
      "777:\tlearn: 0.0882516\ttotal: 5.22s\tremaining: 1.49s\n",
      "778:\tlearn: 0.0882353\ttotal: 5.22s\tremaining: 1.48s\n",
      "779:\tlearn: 0.0882246\ttotal: 5.23s\tremaining: 1.48s\n",
      "780:\tlearn: 0.0882067\ttotal: 5.24s\tremaining: 1.47s\n",
      "781:\tlearn: 0.0881907\ttotal: 5.24s\tremaining: 1.46s\n",
      "782:\tlearn: 0.0881815\ttotal: 5.25s\tremaining: 1.45s\n",
      "783:\tlearn: 0.0881678\ttotal: 5.25s\tremaining: 1.45s\n",
      "784:\tlearn: 0.0881589\ttotal: 5.26s\tremaining: 1.44s\n",
      "785:\tlearn: 0.0881451\ttotal: 5.27s\tremaining: 1.43s\n",
      "786:\tlearn: 0.0881347\ttotal: 5.27s\tremaining: 1.43s\n",
      "787:\tlearn: 0.0880961\ttotal: 5.28s\tremaining: 1.42s\n",
      "788:\tlearn: 0.0880813\ttotal: 5.28s\tremaining: 1.41s\n",
      "789:\tlearn: 0.0880657\ttotal: 5.29s\tremaining: 1.41s\n",
      "790:\tlearn: 0.0880440\ttotal: 5.29s\tremaining: 1.4s\n",
      "791:\tlearn: 0.0880335\ttotal: 5.3s\tremaining: 1.39s\n",
      "792:\tlearn: 0.0880108\ttotal: 5.3s\tremaining: 1.38s\n",
      "793:\tlearn: 0.0879904\ttotal: 5.31s\tremaining: 1.38s\n",
      "794:\tlearn: 0.0879840\ttotal: 5.32s\tremaining: 1.37s\n",
      "795:\tlearn: 0.0879549\ttotal: 5.32s\tremaining: 1.36s\n",
      "796:\tlearn: 0.0879494\ttotal: 5.33s\tremaining: 1.36s\n",
      "797:\tlearn: 0.0879242\ttotal: 5.34s\tremaining: 1.35s\n",
      "798:\tlearn: 0.0879149\ttotal: 5.34s\tremaining: 1.34s\n",
      "799:\tlearn: 0.0878881\ttotal: 5.35s\tremaining: 1.34s\n",
      "800:\tlearn: 0.0878829\ttotal: 5.36s\tremaining: 1.33s\n",
      "801:\tlearn: 0.0878672\ttotal: 5.36s\tremaining: 1.32s\n",
      "802:\tlearn: 0.0878559\ttotal: 5.37s\tremaining: 1.32s\n",
      "803:\tlearn: 0.0878466\ttotal: 5.37s\tremaining: 1.31s\n",
      "804:\tlearn: 0.0878356\ttotal: 5.38s\tremaining: 1.3s\n",
      "805:\tlearn: 0.0878265\ttotal: 5.38s\tremaining: 1.29s\n",
      "806:\tlearn: 0.0878121\ttotal: 5.39s\tremaining: 1.29s\n",
      "807:\tlearn: 0.0877830\ttotal: 5.39s\tremaining: 1.28s\n",
      "808:\tlearn: 0.0877691\ttotal: 5.4s\tremaining: 1.27s\n",
      "809:\tlearn: 0.0877612\ttotal: 5.41s\tremaining: 1.27s\n",
      "810:\tlearn: 0.0877409\ttotal: 5.41s\tremaining: 1.26s\n",
      "811:\tlearn: 0.0877215\ttotal: 5.42s\tremaining: 1.25s\n",
      "812:\tlearn: 0.0877082\ttotal: 5.42s\tremaining: 1.25s\n",
      "813:\tlearn: 0.0876932\ttotal: 5.43s\tremaining: 1.24s\n",
      "814:\tlearn: 0.0876767\ttotal: 5.44s\tremaining: 1.23s\n",
      "815:\tlearn: 0.0876692\ttotal: 5.44s\tremaining: 1.23s\n",
      "816:\tlearn: 0.0876583\ttotal: 5.45s\tremaining: 1.22s\n",
      "817:\tlearn: 0.0876464\ttotal: 5.45s\tremaining: 1.21s\n",
      "818:\tlearn: 0.0876328\ttotal: 5.46s\tremaining: 1.21s\n",
      "819:\tlearn: 0.0876116\ttotal: 5.46s\tremaining: 1.2s\n",
      "820:\tlearn: 0.0875971\ttotal: 5.47s\tremaining: 1.19s\n",
      "821:\tlearn: 0.0875833\ttotal: 5.47s\tremaining: 1.19s\n",
      "822:\tlearn: 0.0875795\ttotal: 5.48s\tremaining: 1.18s\n",
      "823:\tlearn: 0.0875646\ttotal: 5.49s\tremaining: 1.17s\n",
      "824:\tlearn: 0.0875517\ttotal: 5.49s\tremaining: 1.16s\n",
      "825:\tlearn: 0.0875451\ttotal: 5.5s\tremaining: 1.16s\n",
      "826:\tlearn: 0.0875246\ttotal: 5.5s\tremaining: 1.15s\n",
      "827:\tlearn: 0.0875042\ttotal: 5.51s\tremaining: 1.14s\n",
      "828:\tlearn: 0.0874859\ttotal: 5.51s\tremaining: 1.14s\n",
      "829:\tlearn: 0.0874786\ttotal: 5.52s\tremaining: 1.13s\n",
      "830:\tlearn: 0.0874733\ttotal: 5.52s\tremaining: 1.12s\n",
      "831:\tlearn: 0.0874638\ttotal: 5.53s\tremaining: 1.12s\n",
      "832:\tlearn: 0.0874463\ttotal: 5.53s\tremaining: 1.11s\n",
      "833:\tlearn: 0.0874219\ttotal: 5.54s\tremaining: 1.1s\n",
      "834:\tlearn: 0.0874111\ttotal: 5.54s\tremaining: 1.09s\n",
      "835:\tlearn: 0.0874018\ttotal: 5.55s\tremaining: 1.09s\n",
      "836:\tlearn: 0.0873898\ttotal: 5.56s\tremaining: 1.08s\n",
      "837:\tlearn: 0.0873802\ttotal: 5.56s\tremaining: 1.07s\n",
      "838:\tlearn: 0.0873720\ttotal: 5.57s\tremaining: 1.07s\n",
      "839:\tlearn: 0.0873623\ttotal: 5.57s\tremaining: 1.06s\n",
      "840:\tlearn: 0.0873568\ttotal: 5.58s\tremaining: 1.05s\n",
      "841:\tlearn: 0.0873490\ttotal: 5.58s\tremaining: 1.05s\n",
      "842:\tlearn: 0.0873433\ttotal: 5.59s\tremaining: 1.04s\n",
      "843:\tlearn: 0.0873321\ttotal: 5.6s\tremaining: 1.03s\n",
      "844:\tlearn: 0.0873255\ttotal: 5.6s\tremaining: 1.03s\n",
      "845:\tlearn: 0.0873158\ttotal: 5.61s\tremaining: 1.02s\n",
      "846:\tlearn: 0.0873014\ttotal: 5.61s\tremaining: 1.01s\n",
      "847:\tlearn: 0.0872876\ttotal: 5.62s\tremaining: 1.01s\n",
      "848:\tlearn: 0.0872696\ttotal: 5.63s\tremaining: 1s\n",
      "849:\tlearn: 0.0872580\ttotal: 5.63s\tremaining: 995ms\n",
      "850:\tlearn: 0.0872484\ttotal: 5.64s\tremaining: 988ms\n",
      "851:\tlearn: 0.0872367\ttotal: 5.65s\tremaining: 981ms\n",
      "852:\tlearn: 0.0872246\ttotal: 5.65s\tremaining: 974ms\n",
      "853:\tlearn: 0.0872006\ttotal: 5.66s\tremaining: 967ms\n",
      "854:\tlearn: 0.0871815\ttotal: 5.66s\tremaining: 960ms\n",
      "855:\tlearn: 0.0871674\ttotal: 5.67s\tremaining: 954ms\n",
      "856:\tlearn: 0.0871564\ttotal: 5.67s\tremaining: 947ms\n",
      "857:\tlearn: 0.0871444\ttotal: 5.68s\tremaining: 940ms\n",
      "858:\tlearn: 0.0871313\ttotal: 5.69s\tremaining: 934ms\n",
      "859:\tlearn: 0.0871215\ttotal: 5.69s\tremaining: 927ms\n",
      "860:\tlearn: 0.0871112\ttotal: 5.7s\tremaining: 920ms\n",
      "861:\tlearn: 0.0871040\ttotal: 5.7s\tremaining: 913ms\n",
      "862:\tlearn: 0.0870963\ttotal: 5.71s\tremaining: 906ms\n",
      "863:\tlearn: 0.0870865\ttotal: 5.71s\tremaining: 900ms\n",
      "864:\tlearn: 0.0870762\ttotal: 5.72s\tremaining: 893ms\n",
      "865:\tlearn: 0.0870680\ttotal: 5.73s\tremaining: 886ms\n",
      "866:\tlearn: 0.0870603\ttotal: 5.73s\tremaining: 879ms\n",
      "867:\tlearn: 0.0870398\ttotal: 5.74s\tremaining: 873ms\n",
      "868:\tlearn: 0.0870297\ttotal: 5.74s\tremaining: 866ms\n",
      "869:\tlearn: 0.0870186\ttotal: 5.75s\tremaining: 859ms\n",
      "870:\tlearn: 0.0870121\ttotal: 5.75s\tremaining: 852ms\n",
      "871:\tlearn: 0.0870032\ttotal: 5.76s\tremaining: 845ms\n",
      "872:\tlearn: 0.0869945\ttotal: 5.76s\tremaining: 839ms\n",
      "873:\tlearn: 0.0869748\ttotal: 5.77s\tremaining: 832ms\n",
      "874:\tlearn: 0.0869473\ttotal: 5.78s\tremaining: 825ms\n",
      "875:\tlearn: 0.0869398\ttotal: 5.78s\tremaining: 818ms\n",
      "876:\tlearn: 0.0869236\ttotal: 5.79s\tremaining: 812ms\n",
      "877:\tlearn: 0.0869117\ttotal: 5.79s\tremaining: 805ms\n",
      "878:\tlearn: 0.0869030\ttotal: 5.8s\tremaining: 798ms\n",
      "879:\tlearn: 0.0868811\ttotal: 5.8s\tremaining: 791ms\n",
      "880:\tlearn: 0.0868742\ttotal: 5.81s\tremaining: 785ms\n",
      "881:\tlearn: 0.0868587\ttotal: 5.82s\tremaining: 778ms\n",
      "882:\tlearn: 0.0868450\ttotal: 5.82s\tremaining: 771ms\n",
      "883:\tlearn: 0.0868292\ttotal: 5.83s\tremaining: 765ms\n",
      "884:\tlearn: 0.0868249\ttotal: 5.83s\tremaining: 758ms\n",
      "885:\tlearn: 0.0867899\ttotal: 5.84s\tremaining: 751ms\n",
      "886:\tlearn: 0.0867816\ttotal: 5.84s\tremaining: 745ms\n",
      "887:\tlearn: 0.0867757\ttotal: 5.85s\tremaining: 738ms\n",
      "888:\tlearn: 0.0867671\ttotal: 5.86s\tremaining: 732ms\n",
      "889:\tlearn: 0.0867534\ttotal: 5.86s\tremaining: 725ms\n",
      "890:\tlearn: 0.0867382\ttotal: 5.87s\tremaining: 718ms\n",
      "891:\tlearn: 0.0867203\ttotal: 5.88s\tremaining: 711ms\n",
      "892:\tlearn: 0.0867117\ttotal: 5.88s\tremaining: 705ms\n",
      "893:\tlearn: 0.0866945\ttotal: 5.89s\tremaining: 698ms\n",
      "894:\tlearn: 0.0866902\ttotal: 5.89s\tremaining: 692ms\n",
      "895:\tlearn: 0.0866813\ttotal: 5.9s\tremaining: 685ms\n",
      "896:\tlearn: 0.0866768\ttotal: 5.91s\tremaining: 679ms\n",
      "897:\tlearn: 0.0866558\ttotal: 5.92s\tremaining: 672ms\n",
      "898:\tlearn: 0.0866320\ttotal: 5.92s\tremaining: 665ms\n",
      "899:\tlearn: 0.0866244\ttotal: 5.93s\tremaining: 659ms\n",
      "900:\tlearn: 0.0866129\ttotal: 5.93s\tremaining: 652ms\n",
      "901:\tlearn: 0.0866040\ttotal: 5.94s\tremaining: 645ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902:\tlearn: 0.0865716\ttotal: 5.94s\tremaining: 638ms\n",
      "903:\tlearn: 0.0865635\ttotal: 5.95s\tremaining: 632ms\n",
      "904:\tlearn: 0.0865511\ttotal: 5.96s\tremaining: 625ms\n",
      "905:\tlearn: 0.0865368\ttotal: 5.96s\tremaining: 619ms\n",
      "906:\tlearn: 0.0865295\ttotal: 5.97s\tremaining: 612ms\n",
      "907:\tlearn: 0.0865206\ttotal: 5.98s\tremaining: 606ms\n",
      "908:\tlearn: 0.0865102\ttotal: 5.99s\tremaining: 599ms\n",
      "909:\tlearn: 0.0865018\ttotal: 5.99s\tremaining: 593ms\n",
      "910:\tlearn: 0.0864896\ttotal: 6s\tremaining: 586ms\n",
      "911:\tlearn: 0.0864798\ttotal: 6s\tremaining: 579ms\n",
      "912:\tlearn: 0.0864669\ttotal: 6.01s\tremaining: 573ms\n",
      "913:\tlearn: 0.0864323\ttotal: 6.01s\tremaining: 566ms\n",
      "914:\tlearn: 0.0864176\ttotal: 6.02s\tremaining: 559ms\n",
      "915:\tlearn: 0.0864083\ttotal: 6.02s\tremaining: 552ms\n",
      "916:\tlearn: 0.0863868\ttotal: 6.03s\tremaining: 546ms\n",
      "917:\tlearn: 0.0863481\ttotal: 6.03s\tremaining: 539ms\n",
      "918:\tlearn: 0.0863265\ttotal: 6.04s\tremaining: 532ms\n",
      "919:\tlearn: 0.0863126\ttotal: 6.05s\tremaining: 526ms\n",
      "920:\tlearn: 0.0862972\ttotal: 6.06s\tremaining: 520ms\n",
      "921:\tlearn: 0.0862876\ttotal: 6.07s\tremaining: 514ms\n",
      "922:\tlearn: 0.0862753\ttotal: 6.08s\tremaining: 508ms\n",
      "923:\tlearn: 0.0862688\ttotal: 6.1s\tremaining: 501ms\n",
      "924:\tlearn: 0.0862614\ttotal: 6.11s\tremaining: 495ms\n",
      "925:\tlearn: 0.0862538\ttotal: 6.12s\tremaining: 489ms\n",
      "926:\tlearn: 0.0862279\ttotal: 6.13s\tremaining: 483ms\n",
      "927:\tlearn: 0.0862201\ttotal: 6.13s\tremaining: 476ms\n",
      "928:\tlearn: 0.0862110\ttotal: 6.14s\tremaining: 469ms\n",
      "929:\tlearn: 0.0861888\ttotal: 6.14s\tremaining: 463ms\n",
      "930:\tlearn: 0.0861720\ttotal: 6.15s\tremaining: 456ms\n",
      "931:\tlearn: 0.0861652\ttotal: 6.16s\tremaining: 449ms\n",
      "932:\tlearn: 0.0861571\ttotal: 6.16s\tremaining: 442ms\n",
      "933:\tlearn: 0.0861371\ttotal: 6.17s\tremaining: 436ms\n",
      "934:\tlearn: 0.0861296\ttotal: 6.17s\tremaining: 429ms\n",
      "935:\tlearn: 0.0861211\ttotal: 6.18s\tremaining: 422ms\n",
      "936:\tlearn: 0.0861116\ttotal: 6.18s\tremaining: 416ms\n",
      "937:\tlearn: 0.0861043\ttotal: 6.19s\tremaining: 409ms\n",
      "938:\tlearn: 0.0860984\ttotal: 6.2s\tremaining: 403ms\n",
      "939:\tlearn: 0.0860814\ttotal: 6.21s\tremaining: 396ms\n",
      "940:\tlearn: 0.0860731\ttotal: 6.21s\tremaining: 390ms\n",
      "941:\tlearn: 0.0860543\ttotal: 6.22s\tremaining: 383ms\n",
      "942:\tlearn: 0.0860478\ttotal: 6.23s\tremaining: 377ms\n",
      "943:\tlearn: 0.0860381\ttotal: 6.24s\tremaining: 370ms\n",
      "944:\tlearn: 0.0860285\ttotal: 6.24s\tremaining: 363ms\n",
      "945:\tlearn: 0.0860211\ttotal: 6.25s\tremaining: 357ms\n",
      "946:\tlearn: 0.0860153\ttotal: 6.25s\tremaining: 350ms\n",
      "947:\tlearn: 0.0859907\ttotal: 6.26s\tremaining: 343ms\n",
      "948:\tlearn: 0.0859727\ttotal: 6.26s\tremaining: 337ms\n",
      "949:\tlearn: 0.0859695\ttotal: 6.27s\tremaining: 330ms\n",
      "950:\tlearn: 0.0859628\ttotal: 6.27s\tremaining: 323ms\n",
      "951:\tlearn: 0.0859478\ttotal: 6.28s\tremaining: 317ms\n",
      "952:\tlearn: 0.0859188\ttotal: 6.29s\tremaining: 310ms\n",
      "953:\tlearn: 0.0859024\ttotal: 6.29s\tremaining: 304ms\n",
      "954:\tlearn: 0.0858923\ttotal: 6.3s\tremaining: 297ms\n",
      "955:\tlearn: 0.0858876\ttotal: 6.31s\tremaining: 290ms\n",
      "956:\tlearn: 0.0858773\ttotal: 6.31s\tremaining: 284ms\n",
      "957:\tlearn: 0.0858695\ttotal: 6.32s\tremaining: 277ms\n",
      "958:\tlearn: 0.0858605\ttotal: 6.32s\tremaining: 270ms\n",
      "959:\tlearn: 0.0858500\ttotal: 6.33s\tremaining: 264ms\n",
      "960:\tlearn: 0.0858396\ttotal: 6.34s\tremaining: 257ms\n",
      "961:\tlearn: 0.0858204\ttotal: 6.34s\tremaining: 251ms\n",
      "962:\tlearn: 0.0858145\ttotal: 6.35s\tremaining: 244ms\n",
      "963:\tlearn: 0.0858042\ttotal: 6.35s\tremaining: 237ms\n",
      "964:\tlearn: 0.0857977\ttotal: 6.36s\tremaining: 231ms\n",
      "965:\tlearn: 0.0857820\ttotal: 6.36s\tremaining: 224ms\n",
      "966:\tlearn: 0.0857704\ttotal: 6.37s\tremaining: 217ms\n",
      "967:\tlearn: 0.0857524\ttotal: 6.37s\tremaining: 211ms\n",
      "968:\tlearn: 0.0857428\ttotal: 6.38s\tremaining: 204ms\n",
      "969:\tlearn: 0.0857393\ttotal: 6.38s\tremaining: 197ms\n",
      "970:\tlearn: 0.0857311\ttotal: 6.39s\tremaining: 191ms\n",
      "971:\tlearn: 0.0857270\ttotal: 6.39s\tremaining: 184ms\n",
      "972:\tlearn: 0.0857174\ttotal: 6.4s\tremaining: 178ms\n",
      "973:\tlearn: 0.0857063\ttotal: 6.41s\tremaining: 171ms\n",
      "974:\tlearn: 0.0856818\ttotal: 6.41s\tremaining: 164ms\n",
      "975:\tlearn: 0.0856564\ttotal: 6.42s\tremaining: 158ms\n",
      "976:\tlearn: 0.0856490\ttotal: 6.42s\tremaining: 151ms\n",
      "977:\tlearn: 0.0856400\ttotal: 6.43s\tremaining: 145ms\n",
      "978:\tlearn: 0.0856352\ttotal: 6.44s\tremaining: 138ms\n",
      "979:\tlearn: 0.0856214\ttotal: 6.45s\tremaining: 132ms\n",
      "980:\tlearn: 0.0856058\ttotal: 6.45s\tremaining: 125ms\n",
      "981:\tlearn: 0.0855994\ttotal: 6.46s\tremaining: 118ms\n",
      "982:\tlearn: 0.0855941\ttotal: 6.47s\tremaining: 112ms\n",
      "983:\tlearn: 0.0855789\ttotal: 6.48s\tremaining: 105ms\n",
      "984:\tlearn: 0.0855692\ttotal: 6.48s\tremaining: 98.8ms\n",
      "985:\tlearn: 0.0855612\ttotal: 6.49s\tremaining: 92.2ms\n",
      "986:\tlearn: 0.0855306\ttotal: 6.5s\tremaining: 85.6ms\n",
      "987:\tlearn: 0.0855245\ttotal: 6.5s\tremaining: 79ms\n",
      "988:\tlearn: 0.0855101\ttotal: 6.51s\tremaining: 72.4ms\n",
      "989:\tlearn: 0.0855013\ttotal: 6.51s\tremaining: 65.8ms\n",
      "990:\tlearn: 0.0854900\ttotal: 6.52s\tremaining: 59.2ms\n",
      "991:\tlearn: 0.0854833\ttotal: 6.53s\tremaining: 52.6ms\n",
      "992:\tlearn: 0.0854788\ttotal: 6.53s\tremaining: 46ms\n",
      "993:\tlearn: 0.0854639\ttotal: 6.54s\tremaining: 39.5ms\n",
      "994:\tlearn: 0.0854525\ttotal: 6.54s\tremaining: 32.9ms\n",
      "995:\tlearn: 0.0854367\ttotal: 6.55s\tremaining: 26.3ms\n",
      "996:\tlearn: 0.0854310\ttotal: 6.55s\tremaining: 19.7ms\n",
      "997:\tlearn: 0.0854230\ttotal: 6.56s\tremaining: 13.1ms\n",
      "998:\tlearn: 0.0854035\ttotal: 6.56s\tremaining: 6.57ms\n",
      "999:\tlearn: 0.0853989\ttotal: 6.57s\tremaining: 0us\n",
      "0:\tlearn: 0.4372226\ttotal: 5.31ms\tremaining: 5.3s\n",
      "1:\tlearn: 0.4264907\ttotal: 14.1ms\tremaining: 7.01s\n",
      "2:\tlearn: 0.4161370\ttotal: 24ms\tremaining: 7.98s\n",
      "3:\tlearn: 0.4062674\ttotal: 31.4ms\tremaining: 7.82s\n",
      "4:\tlearn: 0.3966883\ttotal: 39.1ms\tremaining: 7.78s\n",
      "5:\tlearn: 0.3874756\ttotal: 44.8ms\tremaining: 7.42s\n",
      "6:\tlearn: 0.3783537\ttotal: 50.4ms\tremaining: 7.14s\n",
      "7:\tlearn: 0.3699848\ttotal: 56.2ms\tremaining: 6.97s\n",
      "8:\tlearn: 0.3617828\ttotal: 65.8ms\tremaining: 7.25s\n",
      "9:\tlearn: 0.3538104\ttotal: 71.8ms\tremaining: 7.11s\n",
      "10:\tlearn: 0.3462601\ttotal: 78.8ms\tremaining: 7.08s\n",
      "11:\tlearn: 0.3385202\ttotal: 86ms\tremaining: 7.08s\n",
      "12:\tlearn: 0.3310348\ttotal: 91.4ms\tremaining: 6.94s\n",
      "13:\tlearn: 0.3238807\ttotal: 96.9ms\tremaining: 6.83s\n",
      "14:\tlearn: 0.3171998\ttotal: 103ms\tremaining: 6.75s\n",
      "15:\tlearn: 0.3102765\ttotal: 111ms\tremaining: 6.81s\n",
      "16:\tlearn: 0.3037226\ttotal: 116ms\tremaining: 6.73s\n",
      "17:\tlearn: 0.2976639\ttotal: 122ms\tremaining: 6.66s\n",
      "18:\tlearn: 0.2917654\ttotal: 128ms\tremaining: 6.61s\n",
      "19:\tlearn: 0.2858532\ttotal: 134ms\tremaining: 6.55s\n",
      "20:\tlearn: 0.2802451\ttotal: 140ms\tremaining: 6.54s\n",
      "21:\tlearn: 0.2747645\ttotal: 147ms\tremaining: 6.54s\n",
      "22:\tlearn: 0.2691889\ttotal: 153ms\tremaining: 6.49s\n",
      "23:\tlearn: 0.2642810\ttotal: 158ms\tremaining: 6.44s\n",
      "24:\tlearn: 0.2594196\ttotal: 164ms\tremaining: 6.4s\n",
      "25:\tlearn: 0.2548537\ttotal: 170ms\tremaining: 6.36s\n",
      "26:\tlearn: 0.2501182\ttotal: 175ms\tremaining: 6.32s\n",
      "27:\tlearn: 0.2455135\ttotal: 181ms\tremaining: 6.3s\n",
      "28:\tlearn: 0.2414000\ttotal: 187ms\tremaining: 6.27s\n",
      "29:\tlearn: 0.2370226\ttotal: 193ms\tremaining: 6.23s\n",
      "30:\tlearn: 0.2331064\ttotal: 199ms\tremaining: 6.22s\n",
      "31:\tlearn: 0.2290382\ttotal: 204ms\tremaining: 6.18s\n",
      "32:\tlearn: 0.2250573\ttotal: 210ms\tremaining: 6.15s\n",
      "33:\tlearn: 0.2214228\ttotal: 215ms\tremaining: 6.12s\n",
      "34:\tlearn: 0.2179541\ttotal: 222ms\tremaining: 6.12s\n",
      "35:\tlearn: 0.2146408\ttotal: 229ms\tremaining: 6.13s\n",
      "36:\tlearn: 0.2112132\ttotal: 234ms\tremaining: 6.1s\n",
      "37:\tlearn: 0.2078805\ttotal: 240ms\tremaining: 6.07s\n",
      "38:\tlearn: 0.2049040\ttotal: 245ms\tremaining: 6.04s\n",
      "39:\tlearn: 0.2020456\ttotal: 251ms\tremaining: 6.02s\n",
      "40:\tlearn: 0.1993194\ttotal: 257ms\tremaining: 6s\n",
      "41:\tlearn: 0.1966850\ttotal: 265ms\tremaining: 6.03s\n",
      "42:\tlearn: 0.1939097\ttotal: 271ms\tremaining: 6.02s\n",
      "43:\tlearn: 0.1914820\ttotal: 277ms\tremaining: 6.02s\n",
      "44:\tlearn: 0.1890148\ttotal: 283ms\tremaining: 6.01s\n",
      "45:\tlearn: 0.1868338\ttotal: 289ms\tremaining: 5.98s\n",
      "46:\tlearn: 0.1846685\ttotal: 294ms\tremaining: 5.96s\n",
      "47:\tlearn: 0.1825807\ttotal: 300ms\tremaining: 5.94s\n",
      "48:\tlearn: 0.1803910\ttotal: 305ms\tremaining: 5.93s\n",
      "49:\tlearn: 0.1781917\ttotal: 312ms\tremaining: 5.92s\n",
      "50:\tlearn: 0.1761856\ttotal: 319ms\tremaining: 5.93s\n",
      "51:\tlearn: 0.1744159\ttotal: 327ms\tremaining: 5.95s\n",
      "52:\tlearn: 0.1725239\ttotal: 334ms\tremaining: 5.96s\n",
      "53:\tlearn: 0.1708429\ttotal: 339ms\tremaining: 5.94s\n",
      "54:\tlearn: 0.1693006\ttotal: 345ms\tremaining: 5.92s\n",
      "55:\tlearn: 0.1674145\ttotal: 360ms\tremaining: 6.06s\n",
      "56:\tlearn: 0.1657132\ttotal: 374ms\tremaining: 6.18s\n",
      "57:\tlearn: 0.1640279\ttotal: 397ms\tremaining: 6.44s\n",
      "58:\tlearn: 0.1625285\ttotal: 420ms\tremaining: 6.7s\n",
      "59:\tlearn: 0.1611633\ttotal: 438ms\tremaining: 6.86s\n",
      "60:\tlearn: 0.1598950\ttotal: 447ms\tremaining: 6.89s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61:\tlearn: 0.1586390\ttotal: 454ms\tremaining: 6.86s\n",
      "62:\tlearn: 0.1573388\ttotal: 460ms\tremaining: 6.83s\n",
      "63:\tlearn: 0.1561976\ttotal: 465ms\tremaining: 6.8s\n",
      "64:\tlearn: 0.1548353\ttotal: 470ms\tremaining: 6.76s\n",
      "65:\tlearn: 0.1536223\ttotal: 475ms\tremaining: 6.72s\n",
      "66:\tlearn: 0.1523601\ttotal: 481ms\tremaining: 6.7s\n",
      "67:\tlearn: 0.1513194\ttotal: 486ms\tremaining: 6.67s\n",
      "68:\tlearn: 0.1502959\ttotal: 492ms\tremaining: 6.63s\n",
      "69:\tlearn: 0.1494071\ttotal: 500ms\tremaining: 6.65s\n",
      "70:\tlearn: 0.1484282\ttotal: 506ms\tremaining: 6.62s\n",
      "71:\tlearn: 0.1475423\ttotal: 512ms\tremaining: 6.6s\n",
      "72:\tlearn: 0.1465284\ttotal: 518ms\tremaining: 6.57s\n",
      "73:\tlearn: 0.1456124\ttotal: 523ms\tremaining: 6.54s\n",
      "74:\tlearn: 0.1447561\ttotal: 528ms\tremaining: 6.51s\n",
      "75:\tlearn: 0.1437109\ttotal: 540ms\tremaining: 6.57s\n",
      "76:\tlearn: 0.1430038\ttotal: 546ms\tremaining: 6.54s\n",
      "77:\tlearn: 0.1422957\ttotal: 551ms\tremaining: 6.52s\n",
      "78:\tlearn: 0.1416402\ttotal: 557ms\tremaining: 6.49s\n",
      "79:\tlearn: 0.1409915\ttotal: 563ms\tremaining: 6.47s\n",
      "80:\tlearn: 0.1403429\ttotal: 568ms\tremaining: 6.44s\n",
      "81:\tlearn: 0.1397362\ttotal: 575ms\tremaining: 6.44s\n",
      "82:\tlearn: 0.1391207\ttotal: 586ms\tremaining: 6.48s\n",
      "83:\tlearn: 0.1385146\ttotal: 594ms\tremaining: 6.47s\n",
      "84:\tlearn: 0.1379705\ttotal: 599ms\tremaining: 6.45s\n",
      "85:\tlearn: 0.1373477\ttotal: 604ms\tremaining: 6.42s\n",
      "86:\tlearn: 0.1367052\ttotal: 610ms\tremaining: 6.4s\n",
      "87:\tlearn: 0.1359913\ttotal: 617ms\tremaining: 6.4s\n",
      "88:\tlearn: 0.1353672\ttotal: 625ms\tremaining: 6.4s\n",
      "89:\tlearn: 0.1348197\ttotal: 631ms\tremaining: 6.38s\n",
      "90:\tlearn: 0.1341381\ttotal: 636ms\tremaining: 6.36s\n",
      "91:\tlearn: 0.1336451\ttotal: 641ms\tremaining: 6.33s\n",
      "92:\tlearn: 0.1331895\ttotal: 647ms\tremaining: 6.31s\n",
      "93:\tlearn: 0.1327642\ttotal: 653ms\tremaining: 6.29s\n",
      "94:\tlearn: 0.1322136\ttotal: 659ms\tremaining: 6.28s\n",
      "95:\tlearn: 0.1316942\ttotal: 673ms\tremaining: 6.34s\n",
      "96:\tlearn: 0.1313400\ttotal: 678ms\tremaining: 6.31s\n",
      "97:\tlearn: 0.1309755\ttotal: 684ms\tremaining: 6.29s\n",
      "98:\tlearn: 0.1305372\ttotal: 689ms\tremaining: 6.27s\n",
      "99:\tlearn: 0.1299990\ttotal: 696ms\tremaining: 6.26s\n",
      "100:\tlearn: 0.1296327\ttotal: 705ms\tremaining: 6.28s\n",
      "101:\tlearn: 0.1292046\ttotal: 720ms\tremaining: 6.34s\n",
      "102:\tlearn: 0.1288753\ttotal: 728ms\tremaining: 6.34s\n",
      "103:\tlearn: 0.1283724\ttotal: 734ms\tremaining: 6.33s\n",
      "104:\tlearn: 0.1280572\ttotal: 740ms\tremaining: 6.3s\n",
      "105:\tlearn: 0.1275502\ttotal: 752ms\tremaining: 6.34s\n",
      "106:\tlearn: 0.1271439\ttotal: 762ms\tremaining: 6.36s\n",
      "107:\tlearn: 0.1268162\ttotal: 767ms\tremaining: 6.34s\n",
      "108:\tlearn: 0.1264581\ttotal: 774ms\tremaining: 6.32s\n",
      "109:\tlearn: 0.1259315\ttotal: 779ms\tremaining: 6.3s\n",
      "110:\tlearn: 0.1256673\ttotal: 784ms\tremaining: 6.28s\n",
      "111:\tlearn: 0.1253099\ttotal: 794ms\tremaining: 6.29s\n",
      "112:\tlearn: 0.1249710\ttotal: 805ms\tremaining: 6.32s\n",
      "113:\tlearn: 0.1247241\ttotal: 811ms\tremaining: 6.3s\n",
      "114:\tlearn: 0.1244583\ttotal: 816ms\tremaining: 6.28s\n",
      "115:\tlearn: 0.1242245\ttotal: 822ms\tremaining: 6.26s\n",
      "116:\tlearn: 0.1239960\ttotal: 838ms\tremaining: 6.33s\n",
      "117:\tlearn: 0.1236418\ttotal: 848ms\tremaining: 6.34s\n",
      "118:\tlearn: 0.1232692\ttotal: 854ms\tremaining: 6.32s\n",
      "119:\tlearn: 0.1230759\ttotal: 860ms\tremaining: 6.3s\n",
      "120:\tlearn: 0.1228318\ttotal: 865ms\tremaining: 6.28s\n",
      "121:\tlearn: 0.1226514\ttotal: 870ms\tremaining: 6.26s\n",
      "122:\tlearn: 0.1223717\ttotal: 877ms\tremaining: 6.25s\n",
      "123:\tlearn: 0.1219093\ttotal: 893ms\tremaining: 6.31s\n",
      "124:\tlearn: 0.1216293\ttotal: 898ms\tremaining: 6.29s\n",
      "125:\tlearn: 0.1214427\ttotal: 904ms\tremaining: 6.27s\n",
      "126:\tlearn: 0.1211739\ttotal: 910ms\tremaining: 6.25s\n",
      "127:\tlearn: 0.1208200\ttotal: 916ms\tremaining: 6.24s\n",
      "128:\tlearn: 0.1205958\ttotal: 927ms\tremaining: 6.26s\n",
      "129:\tlearn: 0.1204067\ttotal: 938ms\tremaining: 6.28s\n",
      "130:\tlearn: 0.1201678\ttotal: 944ms\tremaining: 6.26s\n",
      "131:\tlearn: 0.1200095\ttotal: 950ms\tremaining: 6.25s\n",
      "132:\tlearn: 0.1196686\ttotal: 957ms\tremaining: 6.24s\n",
      "133:\tlearn: 0.1193516\ttotal: 963ms\tremaining: 6.22s\n",
      "134:\tlearn: 0.1191181\ttotal: 981ms\tremaining: 6.28s\n",
      "135:\tlearn: 0.1189302\ttotal: 986ms\tremaining: 6.26s\n",
      "136:\tlearn: 0.1187209\ttotal: 994ms\tremaining: 6.26s\n",
      "137:\tlearn: 0.1185793\ttotal: 1000ms\tremaining: 6.24s\n",
      "138:\tlearn: 0.1184482\ttotal: 1s\tremaining: 6.22s\n",
      "139:\tlearn: 0.1183300\ttotal: 1.02s\tremaining: 6.28s\n",
      "140:\tlearn: 0.1181967\ttotal: 1.03s\tremaining: 6.26s\n",
      "141:\tlearn: 0.1178413\ttotal: 1.03s\tremaining: 6.25s\n",
      "142:\tlearn: 0.1175782\ttotal: 1.04s\tremaining: 6.23s\n",
      "143:\tlearn: 0.1174537\ttotal: 1.04s\tremaining: 6.21s\n",
      "144:\tlearn: 0.1171827\ttotal: 1.06s\tremaining: 6.23s\n",
      "145:\tlearn: 0.1170672\ttotal: 1.07s\tremaining: 6.25s\n",
      "146:\tlearn: 0.1168961\ttotal: 1.07s\tremaining: 6.24s\n",
      "147:\tlearn: 0.1166844\ttotal: 1.08s\tremaining: 6.23s\n",
      "148:\tlearn: 0.1165203\ttotal: 1.09s\tremaining: 6.21s\n",
      "149:\tlearn: 0.1162407\ttotal: 1.09s\tremaining: 6.2s\n",
      "150:\tlearn: 0.1160258\ttotal: 1.11s\tremaining: 6.23s\n",
      "151:\tlearn: 0.1158452\ttotal: 1.11s\tremaining: 6.22s\n",
      "152:\tlearn: 0.1156543\ttotal: 1.12s\tremaining: 6.2s\n",
      "153:\tlearn: 0.1155066\ttotal: 1.13s\tremaining: 6.18s\n",
      "154:\tlearn: 0.1153791\ttotal: 1.13s\tremaining: 6.17s\n",
      "155:\tlearn: 0.1151384\ttotal: 1.14s\tremaining: 6.17s\n",
      "156:\tlearn: 0.1149807\ttotal: 1.15s\tremaining: 6.2s\n",
      "157:\tlearn: 0.1148753\ttotal: 1.16s\tremaining: 6.18s\n",
      "158:\tlearn: 0.1147349\ttotal: 1.17s\tremaining: 6.17s\n",
      "159:\tlearn: 0.1146131\ttotal: 1.17s\tremaining: 6.15s\n",
      "160:\tlearn: 0.1145079\ttotal: 1.18s\tremaining: 6.13s\n",
      "161:\tlearn: 0.1142882\ttotal: 1.19s\tremaining: 6.13s\n",
      "162:\tlearn: 0.1142163\ttotal: 1.2s\tremaining: 6.15s\n",
      "163:\tlearn: 0.1140096\ttotal: 1.2s\tremaining: 6.14s\n",
      "164:\tlearn: 0.1138182\ttotal: 1.21s\tremaining: 6.12s\n",
      "165:\tlearn: 0.1137053\ttotal: 1.22s\tremaining: 6.11s\n",
      "166:\tlearn: 0.1135088\ttotal: 1.22s\tremaining: 6.09s\n",
      "167:\tlearn: 0.1134199\ttotal: 1.23s\tremaining: 6.09s\n",
      "168:\tlearn: 0.1131845\ttotal: 1.24s\tremaining: 6.11s\n",
      "169:\tlearn: 0.1131013\ttotal: 1.25s\tremaining: 6.09s\n",
      "170:\tlearn: 0.1128546\ttotal: 1.25s\tremaining: 6.08s\n",
      "171:\tlearn: 0.1127071\ttotal: 1.26s\tremaining: 6.07s\n",
      "172:\tlearn: 0.1125214\ttotal: 1.27s\tremaining: 6.05s\n",
      "173:\tlearn: 0.1124468\ttotal: 1.28s\tremaining: 6.08s\n",
      "174:\tlearn: 0.1122578\ttotal: 1.29s\tremaining: 6.06s\n",
      "175:\tlearn: 0.1120832\ttotal: 1.29s\tremaining: 6.05s\n",
      "176:\tlearn: 0.1119203\ttotal: 1.3s\tremaining: 6.04s\n",
      "177:\tlearn: 0.1117225\ttotal: 1.31s\tremaining: 6.03s\n",
      "178:\tlearn: 0.1116019\ttotal: 1.33s\tremaining: 6.1s\n",
      "179:\tlearn: 0.1115030\ttotal: 1.34s\tremaining: 6.11s\n",
      "180:\tlearn: 0.1113860\ttotal: 1.35s\tremaining: 6.13s\n",
      "181:\tlearn: 0.1112113\ttotal: 1.38s\tremaining: 6.19s\n",
      "182:\tlearn: 0.1110805\ttotal: 1.39s\tremaining: 6.2s\n",
      "183:\tlearn: 0.1109977\ttotal: 1.4s\tremaining: 6.21s\n",
      "184:\tlearn: 0.1108447\ttotal: 1.42s\tremaining: 6.27s\n",
      "185:\tlearn: 0.1107335\ttotal: 1.43s\tremaining: 6.25s\n",
      "186:\tlearn: 0.1106194\ttotal: 1.44s\tremaining: 6.24s\n",
      "187:\tlearn: 0.1104855\ttotal: 1.44s\tremaining: 6.22s\n",
      "188:\tlearn: 0.1103237\ttotal: 1.45s\tremaining: 6.2s\n",
      "189:\tlearn: 0.1102112\ttotal: 1.46s\tremaining: 6.21s\n",
      "190:\tlearn: 0.1100964\ttotal: 1.47s\tremaining: 6.21s\n",
      "191:\tlearn: 0.1099485\ttotal: 1.47s\tremaining: 6.19s\n",
      "192:\tlearn: 0.1097589\ttotal: 1.48s\tremaining: 6.17s\n",
      "193:\tlearn: 0.1096576\ttotal: 1.48s\tremaining: 6.16s\n",
      "194:\tlearn: 0.1095093\ttotal: 1.49s\tremaining: 6.14s\n",
      "195:\tlearn: 0.1093869\ttotal: 1.49s\tremaining: 6.13s\n",
      "196:\tlearn: 0.1093403\ttotal: 1.5s\tremaining: 6.13s\n",
      "197:\tlearn: 0.1092442\ttotal: 1.51s\tremaining: 6.13s\n",
      "198:\tlearn: 0.1090542\ttotal: 1.52s\tremaining: 6.11s\n",
      "199:\tlearn: 0.1089893\ttotal: 1.52s\tremaining: 6.1s\n",
      "200:\tlearn: 0.1088825\ttotal: 1.53s\tremaining: 6.08s\n",
      "201:\tlearn: 0.1088255\ttotal: 1.54s\tremaining: 6.07s\n",
      "202:\tlearn: 0.1087778\ttotal: 1.55s\tremaining: 6.08s\n",
      "203:\tlearn: 0.1087217\ttotal: 1.55s\tremaining: 6.06s\n",
      "204:\tlearn: 0.1085760\ttotal: 1.56s\tremaining: 6.05s\n",
      "205:\tlearn: 0.1085089\ttotal: 1.56s\tremaining: 6.03s\n",
      "206:\tlearn: 0.1083728\ttotal: 1.57s\tremaining: 6.03s\n",
      "207:\tlearn: 0.1083085\ttotal: 1.58s\tremaining: 6.02s\n",
      "208:\tlearn: 0.1082471\ttotal: 1.59s\tremaining: 6.03s\n",
      "209:\tlearn: 0.1081218\ttotal: 1.6s\tremaining: 6.01s\n",
      "210:\tlearn: 0.1080688\ttotal: 1.6s\tremaining: 6s\n",
      "211:\tlearn: 0.1080101\ttotal: 1.61s\tremaining: 5.99s\n",
      "212:\tlearn: 0.1079478\ttotal: 1.61s\tremaining: 5.97s\n",
      "213:\tlearn: 0.1079071\ttotal: 1.63s\tremaining: 5.97s\n",
      "214:\tlearn: 0.1078618\ttotal: 1.64s\tremaining: 5.99s\n",
      "215:\tlearn: 0.1078066\ttotal: 1.65s\tremaining: 5.98s\n",
      "216:\tlearn: 0.1077421\ttotal: 1.65s\tremaining: 5.96s\n",
      "217:\tlearn: 0.1076512\ttotal: 1.66s\tremaining: 5.95s\n",
      "218:\tlearn: 0.1075887\ttotal: 1.66s\tremaining: 5.93s\n",
      "219:\tlearn: 0.1074621\ttotal: 1.67s\tremaining: 5.94s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220:\tlearn: 0.1073831\ttotal: 1.68s\tremaining: 5.92s\n",
      "221:\tlearn: 0.1073009\ttotal: 1.69s\tremaining: 5.91s\n",
      "222:\tlearn: 0.1072381\ttotal: 1.69s\tremaining: 5.9s\n",
      "223:\tlearn: 0.1071274\ttotal: 1.7s\tremaining: 5.89s\n",
      "224:\tlearn: 0.1070472\ttotal: 1.71s\tremaining: 5.89s\n",
      "225:\tlearn: 0.1070025\ttotal: 1.72s\tremaining: 5.89s\n",
      "226:\tlearn: 0.1069752\ttotal: 1.73s\tremaining: 5.88s\n",
      "227:\tlearn: 0.1068564\ttotal: 1.73s\tremaining: 5.87s\n",
      "228:\tlearn: 0.1067345\ttotal: 1.74s\tremaining: 5.85s\n",
      "229:\tlearn: 0.1066702\ttotal: 1.74s\tremaining: 5.84s\n",
      "230:\tlearn: 0.1065649\ttotal: 1.75s\tremaining: 5.83s\n",
      "231:\tlearn: 0.1065024\ttotal: 1.76s\tremaining: 5.82s\n",
      "232:\tlearn: 0.1064266\ttotal: 1.77s\tremaining: 5.83s\n",
      "233:\tlearn: 0.1063001\ttotal: 1.78s\tremaining: 5.82s\n",
      "234:\tlearn: 0.1062594\ttotal: 1.78s\tremaining: 5.8s\n",
      "235:\tlearn: 0.1061615\ttotal: 1.79s\tremaining: 5.79s\n",
      "236:\tlearn: 0.1061127\ttotal: 1.79s\tremaining: 5.78s\n",
      "237:\tlearn: 0.1059420\ttotal: 1.81s\tremaining: 5.79s\n",
      "238:\tlearn: 0.1058340\ttotal: 1.81s\tremaining: 5.78s\n",
      "239:\tlearn: 0.1057607\ttotal: 1.82s\tremaining: 5.76s\n",
      "240:\tlearn: 0.1056815\ttotal: 1.82s\tremaining: 5.75s\n",
      "241:\tlearn: 0.1056217\ttotal: 1.83s\tremaining: 5.74s\n",
      "242:\tlearn: 0.1055879\ttotal: 1.84s\tremaining: 5.72s\n",
      "243:\tlearn: 0.1055402\ttotal: 1.85s\tremaining: 5.74s\n",
      "244:\tlearn: 0.1054455\ttotal: 1.86s\tremaining: 5.73s\n",
      "245:\tlearn: 0.1053978\ttotal: 1.86s\tremaining: 5.71s\n",
      "246:\tlearn: 0.1053572\ttotal: 1.87s\tremaining: 5.7s\n",
      "247:\tlearn: 0.1052422\ttotal: 1.87s\tremaining: 5.68s\n",
      "248:\tlearn: 0.1051965\ttotal: 1.88s\tremaining: 5.67s\n",
      "249:\tlearn: 0.1051295\ttotal: 1.9s\tremaining: 5.69s\n",
      "250:\tlearn: 0.1050147\ttotal: 1.91s\tremaining: 5.71s\n",
      "251:\tlearn: 0.1049524\ttotal: 1.92s\tremaining: 5.71s\n",
      "252:\tlearn: 0.1048146\ttotal: 1.95s\tremaining: 5.75s\n",
      "253:\tlearn: 0.1047583\ttotal: 1.95s\tremaining: 5.74s\n",
      "254:\tlearn: 0.1047088\ttotal: 1.96s\tremaining: 5.72s\n",
      "255:\tlearn: 0.1046697\ttotal: 1.96s\tremaining: 5.71s\n",
      "256:\tlearn: 0.1045017\ttotal: 1.97s\tremaining: 5.7s\n",
      "257:\tlearn: 0.1044666\ttotal: 1.99s\tremaining: 5.72s\n",
      "258:\tlearn: 0.1043846\ttotal: 2s\tremaining: 5.71s\n",
      "259:\tlearn: 0.1042841\ttotal: 2s\tremaining: 5.7s\n",
      "260:\tlearn: 0.1041719\ttotal: 2.01s\tremaining: 5.69s\n",
      "261:\tlearn: 0.1040470\ttotal: 2.02s\tremaining: 5.7s\n",
      "262:\tlearn: 0.1040223\ttotal: 2.03s\tremaining: 5.7s\n",
      "263:\tlearn: 0.1039611\ttotal: 2.04s\tremaining: 5.68s\n",
      "264:\tlearn: 0.1038782\ttotal: 2.04s\tremaining: 5.67s\n",
      "265:\tlearn: 0.1037910\ttotal: 2.05s\tremaining: 5.66s\n",
      "266:\tlearn: 0.1037545\ttotal: 2.06s\tremaining: 5.64s\n",
      "267:\tlearn: 0.1037088\ttotal: 2.07s\tremaining: 5.64s\n",
      "268:\tlearn: 0.1036295\ttotal: 2.08s\tremaining: 5.65s\n",
      "269:\tlearn: 0.1035959\ttotal: 2.08s\tremaining: 5.63s\n",
      "270:\tlearn: 0.1035276\ttotal: 2.09s\tremaining: 5.62s\n",
      "271:\tlearn: 0.1034675\ttotal: 2.1s\tremaining: 5.61s\n",
      "272:\tlearn: 0.1034402\ttotal: 2.11s\tremaining: 5.61s\n",
      "273:\tlearn: 0.1033919\ttotal: 2.12s\tremaining: 5.62s\n",
      "274:\tlearn: 0.1033406\ttotal: 2.13s\tremaining: 5.62s\n",
      "275:\tlearn: 0.1032706\ttotal: 2.14s\tremaining: 5.61s\n",
      "276:\tlearn: 0.1031893\ttotal: 2.14s\tremaining: 5.6s\n",
      "277:\tlearn: 0.1030999\ttotal: 2.15s\tremaining: 5.59s\n",
      "278:\tlearn: 0.1029619\ttotal: 2.17s\tremaining: 5.6s\n",
      "279:\tlearn: 0.1029195\ttotal: 2.17s\tremaining: 5.58s\n",
      "280:\tlearn: 0.1028412\ttotal: 2.18s\tremaining: 5.59s\n",
      "281:\tlearn: 0.1027655\ttotal: 2.19s\tremaining: 5.58s\n",
      "282:\tlearn: 0.1027403\ttotal: 2.21s\tremaining: 5.59s\n",
      "283:\tlearn: 0.1026989\ttotal: 2.21s\tremaining: 5.58s\n",
      "284:\tlearn: 0.1025875\ttotal: 2.22s\tremaining: 5.57s\n",
      "285:\tlearn: 0.1025394\ttotal: 2.22s\tremaining: 5.55s\n",
      "286:\tlearn: 0.1024635\ttotal: 2.23s\tremaining: 5.54s\n",
      "287:\tlearn: 0.1024034\ttotal: 2.23s\tremaining: 5.53s\n",
      "288:\tlearn: 0.1023504\ttotal: 2.25s\tremaining: 5.53s\n",
      "289:\tlearn: 0.1022994\ttotal: 2.26s\tremaining: 5.53s\n",
      "290:\tlearn: 0.1022050\ttotal: 2.27s\tremaining: 5.53s\n",
      "291:\tlearn: 0.1021088\ttotal: 2.27s\tremaining: 5.52s\n",
      "292:\tlearn: 0.1020395\ttotal: 2.29s\tremaining: 5.52s\n",
      "293:\tlearn: 0.1019567\ttotal: 2.3s\tremaining: 5.53s\n",
      "294:\tlearn: 0.1019045\ttotal: 2.32s\tremaining: 5.54s\n",
      "295:\tlearn: 0.1018146\ttotal: 2.37s\tremaining: 5.63s\n",
      "296:\tlearn: 0.1017352\ttotal: 2.39s\tremaining: 5.66s\n",
      "297:\tlearn: 0.1016848\ttotal: 2.4s\tremaining: 5.66s\n",
      "298:\tlearn: 0.1016533\ttotal: 2.41s\tremaining: 5.66s\n",
      "299:\tlearn: 0.1016176\ttotal: 2.42s\tremaining: 5.65s\n",
      "300:\tlearn: 0.1015717\ttotal: 2.44s\tremaining: 5.66s\n",
      "301:\tlearn: 0.1014997\ttotal: 2.44s\tremaining: 5.65s\n",
      "302:\tlearn: 0.1014668\ttotal: 2.45s\tremaining: 5.63s\n",
      "303:\tlearn: 0.1014368\ttotal: 2.45s\tremaining: 5.62s\n",
      "304:\tlearn: 0.1013778\ttotal: 2.46s\tremaining: 5.61s\n",
      "305:\tlearn: 0.1013137\ttotal: 2.47s\tremaining: 5.6s\n",
      "306:\tlearn: 0.1012127\ttotal: 2.48s\tremaining: 5.59s\n",
      "307:\tlearn: 0.1011434\ttotal: 2.48s\tremaining: 5.58s\n",
      "308:\tlearn: 0.1011094\ttotal: 2.49s\tremaining: 5.57s\n",
      "309:\tlearn: 0.1010670\ttotal: 2.5s\tremaining: 5.56s\n",
      "310:\tlearn: 0.1010422\ttotal: 2.5s\tremaining: 5.54s\n",
      "311:\tlearn: 0.1010168\ttotal: 2.51s\tremaining: 5.54s\n",
      "312:\tlearn: 0.1009021\ttotal: 2.52s\tremaining: 5.54s\n",
      "313:\tlearn: 0.1008104\ttotal: 2.53s\tremaining: 5.52s\n",
      "314:\tlearn: 0.1007789\ttotal: 2.53s\tremaining: 5.51s\n",
      "315:\tlearn: 0.1007019\ttotal: 2.54s\tremaining: 5.5s\n",
      "316:\tlearn: 0.1006403\ttotal: 2.54s\tremaining: 5.48s\n",
      "317:\tlearn: 0.1005831\ttotal: 2.56s\tremaining: 5.49s\n",
      "318:\tlearn: 0.1005635\ttotal: 2.56s\tremaining: 5.48s\n",
      "319:\tlearn: 0.1004593\ttotal: 2.57s\tremaining: 5.46s\n",
      "320:\tlearn: 0.1003978\ttotal: 2.58s\tremaining: 5.45s\n",
      "321:\tlearn: 0.1003169\ttotal: 2.58s\tremaining: 5.44s\n",
      "322:\tlearn: 0.1002446\ttotal: 2.59s\tremaining: 5.43s\n",
      "323:\tlearn: 0.1002134\ttotal: 2.6s\tremaining: 5.44s\n",
      "324:\tlearn: 0.1001747\ttotal: 2.61s\tremaining: 5.42s\n",
      "325:\tlearn: 0.1000897\ttotal: 2.62s\tremaining: 5.41s\n",
      "326:\tlearn: 0.1000443\ttotal: 2.62s\tremaining: 5.4s\n",
      "327:\tlearn: 0.0999987\ttotal: 2.63s\tremaining: 5.38s\n",
      "328:\tlearn: 0.0999451\ttotal: 2.63s\tremaining: 5.37s\n",
      "329:\tlearn: 0.0998687\ttotal: 2.65s\tremaining: 5.38s\n",
      "330:\tlearn: 0.0998108\ttotal: 2.65s\tremaining: 5.37s\n",
      "331:\tlearn: 0.0997180\ttotal: 2.66s\tremaining: 5.35s\n",
      "332:\tlearn: 0.0996557\ttotal: 2.67s\tremaining: 5.34s\n",
      "333:\tlearn: 0.0996113\ttotal: 2.67s\tremaining: 5.33s\n",
      "334:\tlearn: 0.0995734\ttotal: 2.68s\tremaining: 5.32s\n",
      "335:\tlearn: 0.0994629\ttotal: 2.69s\tremaining: 5.33s\n",
      "336:\tlearn: 0.0994447\ttotal: 2.7s\tremaining: 5.32s\n",
      "337:\tlearn: 0.0993580\ttotal: 2.71s\tremaining: 5.31s\n",
      "338:\tlearn: 0.0993030\ttotal: 2.71s\tremaining: 5.29s\n",
      "339:\tlearn: 0.0992503\ttotal: 2.72s\tremaining: 5.28s\n",
      "340:\tlearn: 0.0991814\ttotal: 2.73s\tremaining: 5.28s\n",
      "341:\tlearn: 0.0991350\ttotal: 2.74s\tremaining: 5.27s\n",
      "342:\tlearn: 0.0990596\ttotal: 2.74s\tremaining: 5.25s\n",
      "343:\tlearn: 0.0990318\ttotal: 2.75s\tremaining: 5.24s\n",
      "344:\tlearn: 0.0989640\ttotal: 2.75s\tremaining: 5.23s\n",
      "345:\tlearn: 0.0988879\ttotal: 2.76s\tremaining: 5.22s\n",
      "346:\tlearn: 0.0988447\ttotal: 2.78s\tremaining: 5.23s\n",
      "347:\tlearn: 0.0987748\ttotal: 2.79s\tremaining: 5.22s\n",
      "348:\tlearn: 0.0987238\ttotal: 2.79s\tremaining: 5.21s\n",
      "349:\tlearn: 0.0986701\ttotal: 2.8s\tremaining: 5.2s\n",
      "350:\tlearn: 0.0986344\ttotal: 2.81s\tremaining: 5.19s\n",
      "351:\tlearn: 0.0985312\ttotal: 2.82s\tremaining: 5.19s\n",
      "352:\tlearn: 0.0984927\ttotal: 2.83s\tremaining: 5.18s\n",
      "353:\tlearn: 0.0984371\ttotal: 2.83s\tremaining: 5.17s\n",
      "354:\tlearn: 0.0983831\ttotal: 2.84s\tremaining: 5.16s\n",
      "355:\tlearn: 0.0983507\ttotal: 2.85s\tremaining: 5.15s\n",
      "356:\tlearn: 0.0982926\ttotal: 2.85s\tremaining: 5.13s\n",
      "357:\tlearn: 0.0982339\ttotal: 2.86s\tremaining: 5.14s\n",
      "358:\tlearn: 0.0982106\ttotal: 2.87s\tremaining: 5.13s\n",
      "359:\tlearn: 0.0981568\ttotal: 2.88s\tremaining: 5.11s\n",
      "360:\tlearn: 0.0980818\ttotal: 2.88s\tremaining: 5.1s\n",
      "361:\tlearn: 0.0980138\ttotal: 2.89s\tremaining: 5.09s\n",
      "362:\tlearn: 0.0979536\ttotal: 2.9s\tremaining: 5.08s\n",
      "363:\tlearn: 0.0979234\ttotal: 2.91s\tremaining: 5.08s\n",
      "364:\tlearn: 0.0978864\ttotal: 2.92s\tremaining: 5.07s\n",
      "365:\tlearn: 0.0978627\ttotal: 2.92s\tremaining: 5.06s\n",
      "366:\tlearn: 0.0978295\ttotal: 2.93s\tremaining: 5.05s\n",
      "367:\tlearn: 0.0977774\ttotal: 2.93s\tremaining: 5.04s\n",
      "368:\tlearn: 0.0976924\ttotal: 2.94s\tremaining: 5.02s\n",
      "369:\tlearn: 0.0976191\ttotal: 2.95s\tremaining: 5.03s\n",
      "370:\tlearn: 0.0975809\ttotal: 2.96s\tremaining: 5.01s\n",
      "371:\tlearn: 0.0975396\ttotal: 2.96s\tremaining: 5s\n",
      "372:\tlearn: 0.0974804\ttotal: 2.97s\tremaining: 4.99s\n",
      "373:\tlearn: 0.0974469\ttotal: 2.98s\tremaining: 4.98s\n",
      "374:\tlearn: 0.0973993\ttotal: 2.98s\tremaining: 4.97s\n",
      "375:\tlearn: 0.0973794\ttotal: 3s\tremaining: 4.97s\n",
      "376:\tlearn: 0.0973294\ttotal: 3s\tremaining: 4.96s\n",
      "377:\tlearn: 0.0972769\ttotal: 3.01s\tremaining: 4.95s\n",
      "378:\tlearn: 0.0972373\ttotal: 3.02s\tremaining: 4.94s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379:\tlearn: 0.0972005\ttotal: 3.02s\tremaining: 4.93s\n",
      "380:\tlearn: 0.0971830\ttotal: 3.03s\tremaining: 4.92s\n",
      "381:\tlearn: 0.0971541\ttotal: 3.05s\tremaining: 4.93s\n",
      "382:\tlearn: 0.0971039\ttotal: 3.05s\tremaining: 4.92s\n",
      "383:\tlearn: 0.0970669\ttotal: 3.06s\tremaining: 4.91s\n",
      "384:\tlearn: 0.0970041\ttotal: 3.08s\tremaining: 4.91s\n",
      "385:\tlearn: 0.0969482\ttotal: 3.09s\tremaining: 4.91s\n",
      "386:\tlearn: 0.0969236\ttotal: 3.1s\tremaining: 4.91s\n",
      "387:\tlearn: 0.0968948\ttotal: 3.1s\tremaining: 4.89s\n",
      "388:\tlearn: 0.0968461\ttotal: 3.11s\tremaining: 4.88s\n",
      "389:\tlearn: 0.0968055\ttotal: 3.11s\tremaining: 4.87s\n",
      "390:\tlearn: 0.0967746\ttotal: 3.13s\tremaining: 4.87s\n",
      "391:\tlearn: 0.0966982\ttotal: 3.13s\tremaining: 4.86s\n",
      "392:\tlearn: 0.0966740\ttotal: 3.14s\tremaining: 4.85s\n",
      "393:\tlearn: 0.0966542\ttotal: 3.15s\tremaining: 4.84s\n",
      "394:\tlearn: 0.0966231\ttotal: 3.15s\tremaining: 4.83s\n",
      "395:\tlearn: 0.0965778\ttotal: 3.16s\tremaining: 4.82s\n",
      "396:\tlearn: 0.0965421\ttotal: 3.17s\tremaining: 4.82s\n",
      "397:\tlearn: 0.0964697\ttotal: 3.19s\tremaining: 4.82s\n",
      "398:\tlearn: 0.0964044\ttotal: 3.19s\tremaining: 4.81s\n",
      "399:\tlearn: 0.0963647\ttotal: 3.2s\tremaining: 4.8s\n",
      "400:\tlearn: 0.0963319\ttotal: 3.22s\tremaining: 4.81s\n",
      "401:\tlearn: 0.0962968\ttotal: 3.23s\tremaining: 4.8s\n",
      "402:\tlearn: 0.0962670\ttotal: 3.23s\tremaining: 4.79s\n",
      "403:\tlearn: 0.0962417\ttotal: 3.24s\tremaining: 4.78s\n",
      "404:\tlearn: 0.0962078\ttotal: 3.24s\tremaining: 4.76s\n",
      "405:\tlearn: 0.0961185\ttotal: 3.25s\tremaining: 4.76s\n",
      "406:\tlearn: 0.0960848\ttotal: 3.26s\tremaining: 4.75s\n",
      "407:\tlearn: 0.0960163\ttotal: 3.27s\tremaining: 4.74s\n",
      "408:\tlearn: 0.0959942\ttotal: 3.27s\tremaining: 4.73s\n",
      "409:\tlearn: 0.0959616\ttotal: 3.28s\tremaining: 4.72s\n",
      "410:\tlearn: 0.0958978\ttotal: 3.29s\tremaining: 4.71s\n",
      "411:\tlearn: 0.0958483\ttotal: 3.31s\tremaining: 4.72s\n",
      "412:\tlearn: 0.0958189\ttotal: 3.32s\tremaining: 4.72s\n",
      "413:\tlearn: 0.0957605\ttotal: 3.36s\tremaining: 4.75s\n",
      "414:\tlearn: 0.0957360\ttotal: 3.37s\tremaining: 4.75s\n",
      "415:\tlearn: 0.0957040\ttotal: 3.38s\tremaining: 4.75s\n",
      "416:\tlearn: 0.0956802\ttotal: 3.4s\tremaining: 4.75s\n",
      "417:\tlearn: 0.0956386\ttotal: 3.4s\tremaining: 4.74s\n",
      "418:\tlearn: 0.0956022\ttotal: 3.41s\tremaining: 4.73s\n",
      "419:\tlearn: 0.0955881\ttotal: 3.41s\tremaining: 4.71s\n",
      "420:\tlearn: 0.0955696\ttotal: 3.42s\tremaining: 4.7s\n",
      "421:\tlearn: 0.0955482\ttotal: 3.42s\tremaining: 4.69s\n",
      "422:\tlearn: 0.0955098\ttotal: 3.44s\tremaining: 4.69s\n",
      "423:\tlearn: 0.0954838\ttotal: 3.45s\tremaining: 4.68s\n",
      "424:\tlearn: 0.0954511\ttotal: 3.45s\tremaining: 4.67s\n",
      "425:\tlearn: 0.0954159\ttotal: 3.46s\tremaining: 4.66s\n",
      "426:\tlearn: 0.0953602\ttotal: 3.46s\tremaining: 4.65s\n",
      "427:\tlearn: 0.0953209\ttotal: 3.48s\tremaining: 4.66s\n",
      "428:\tlearn: 0.0952969\ttotal: 3.49s\tremaining: 4.65s\n",
      "429:\tlearn: 0.0952616\ttotal: 3.5s\tremaining: 4.64s\n",
      "430:\tlearn: 0.0952324\ttotal: 3.5s\tremaining: 4.63s\n",
      "431:\tlearn: 0.0952055\ttotal: 3.51s\tremaining: 4.61s\n",
      "432:\tlearn: 0.0951874\ttotal: 3.52s\tremaining: 4.61s\n",
      "433:\tlearn: 0.0951677\ttotal: 3.53s\tremaining: 4.6s\n",
      "434:\tlearn: 0.0951540\ttotal: 3.53s\tremaining: 4.59s\n",
      "435:\tlearn: 0.0951140\ttotal: 3.54s\tremaining: 4.58s\n",
      "436:\tlearn: 0.0950975\ttotal: 3.54s\tremaining: 4.57s\n",
      "437:\tlearn: 0.0950680\ttotal: 3.55s\tremaining: 4.55s\n",
      "438:\tlearn: 0.0950525\ttotal: 3.56s\tremaining: 4.54s\n",
      "439:\tlearn: 0.0950006\ttotal: 3.57s\tremaining: 4.54s\n",
      "440:\tlearn: 0.0949392\ttotal: 3.58s\tremaining: 4.54s\n",
      "441:\tlearn: 0.0949178\ttotal: 3.58s\tremaining: 4.53s\n",
      "442:\tlearn: 0.0948914\ttotal: 3.59s\tremaining: 4.51s\n",
      "443:\tlearn: 0.0948749\ttotal: 3.6s\tremaining: 4.5s\n",
      "444:\tlearn: 0.0948210\ttotal: 3.61s\tremaining: 4.51s\n",
      "445:\tlearn: 0.0947796\ttotal: 3.62s\tremaining: 4.5s\n",
      "446:\tlearn: 0.0947469\ttotal: 3.63s\tremaining: 4.49s\n",
      "447:\tlearn: 0.0947315\ttotal: 3.63s\tremaining: 4.47s\n",
      "448:\tlearn: 0.0946966\ttotal: 3.64s\tremaining: 4.46s\n",
      "449:\tlearn: 0.0946437\ttotal: 3.64s\tremaining: 4.45s\n",
      "450:\tlearn: 0.0946250\ttotal: 3.66s\tremaining: 4.45s\n",
      "451:\tlearn: 0.0945769\ttotal: 3.66s\tremaining: 4.44s\n",
      "452:\tlearn: 0.0945623\ttotal: 3.67s\tremaining: 4.43s\n",
      "453:\tlearn: 0.0945282\ttotal: 3.67s\tremaining: 4.42s\n",
      "454:\tlearn: 0.0945020\ttotal: 3.68s\tremaining: 4.41s\n",
      "455:\tlearn: 0.0944639\ttotal: 3.69s\tremaining: 4.4s\n",
      "456:\tlearn: 0.0944101\ttotal: 3.7s\tremaining: 4.4s\n",
      "457:\tlearn: 0.0943767\ttotal: 3.71s\tremaining: 4.39s\n",
      "458:\tlearn: 0.0943440\ttotal: 3.72s\tremaining: 4.38s\n",
      "459:\tlearn: 0.0943082\ttotal: 3.73s\tremaining: 4.37s\n",
      "460:\tlearn: 0.0942804\ttotal: 3.74s\tremaining: 4.38s\n",
      "461:\tlearn: 0.0942616\ttotal: 3.75s\tremaining: 4.37s\n",
      "462:\tlearn: 0.0942470\ttotal: 3.76s\tremaining: 4.36s\n",
      "463:\tlearn: 0.0941944\ttotal: 3.76s\tremaining: 4.35s\n",
      "464:\tlearn: 0.0941595\ttotal: 3.77s\tremaining: 4.34s\n",
      "465:\tlearn: 0.0941365\ttotal: 3.79s\tremaining: 4.34s\n",
      "466:\tlearn: 0.0941124\ttotal: 3.8s\tremaining: 4.33s\n",
      "467:\tlearn: 0.0940895\ttotal: 3.8s\tremaining: 4.32s\n",
      "468:\tlearn: 0.0940720\ttotal: 3.81s\tremaining: 4.31s\n",
      "469:\tlearn: 0.0940511\ttotal: 3.81s\tremaining: 4.3s\n",
      "470:\tlearn: 0.0940262\ttotal: 3.82s\tremaining: 4.29s\n",
      "471:\tlearn: 0.0940063\ttotal: 3.84s\tremaining: 4.29s\n",
      "472:\tlearn: 0.0939833\ttotal: 3.84s\tremaining: 4.28s\n",
      "473:\tlearn: 0.0939690\ttotal: 3.85s\tremaining: 4.27s\n",
      "474:\tlearn: 0.0939483\ttotal: 3.85s\tremaining: 4.26s\n",
      "475:\tlearn: 0.0939205\ttotal: 3.86s\tremaining: 4.25s\n",
      "476:\tlearn: 0.0938698\ttotal: 3.87s\tremaining: 4.24s\n",
      "477:\tlearn: 0.0938536\ttotal: 3.88s\tremaining: 4.24s\n",
      "478:\tlearn: 0.0938271\ttotal: 3.89s\tremaining: 4.23s\n",
      "479:\tlearn: 0.0938073\ttotal: 3.89s\tremaining: 4.22s\n",
      "480:\tlearn: 0.0937619\ttotal: 3.9s\tremaining: 4.21s\n",
      "481:\tlearn: 0.0937120\ttotal: 3.9s\tremaining: 4.2s\n",
      "482:\tlearn: 0.0936843\ttotal: 3.92s\tremaining: 4.2s\n",
      "483:\tlearn: 0.0936419\ttotal: 3.93s\tremaining: 4.19s\n",
      "484:\tlearn: 0.0936207\ttotal: 3.93s\tremaining: 4.18s\n",
      "485:\tlearn: 0.0935763\ttotal: 3.94s\tremaining: 4.17s\n",
      "486:\tlearn: 0.0935523\ttotal: 3.94s\tremaining: 4.15s\n",
      "487:\tlearn: 0.0935303\ttotal: 3.97s\tremaining: 4.16s\n",
      "488:\tlearn: 0.0935024\ttotal: 3.98s\tremaining: 4.16s\n",
      "489:\tlearn: 0.0934713\ttotal: 3.98s\tremaining: 4.14s\n",
      "490:\tlearn: 0.0934618\ttotal: 3.99s\tremaining: 4.13s\n",
      "491:\tlearn: 0.0934292\ttotal: 3.99s\tremaining: 4.12s\n",
      "492:\tlearn: 0.0934054\ttotal: 4.01s\tremaining: 4.12s\n",
      "493:\tlearn: 0.0933959\ttotal: 4.01s\tremaining: 4.11s\n",
      "494:\tlearn: 0.0933758\ttotal: 4.02s\tremaining: 4.1s\n",
      "495:\tlearn: 0.0933388\ttotal: 4.03s\tremaining: 4.09s\n",
      "496:\tlearn: 0.0933038\ttotal: 4.03s\tremaining: 4.08s\n",
      "497:\tlearn: 0.0932779\ttotal: 4.04s\tremaining: 4.07s\n",
      "498:\tlearn: 0.0932595\ttotal: 4.05s\tremaining: 4.07s\n",
      "499:\tlearn: 0.0932348\ttotal: 4.06s\tremaining: 4.06s\n",
      "500:\tlearn: 0.0932175\ttotal: 4.06s\tremaining: 4.05s\n",
      "501:\tlearn: 0.0931792\ttotal: 4.07s\tremaining: 4.04s\n",
      "502:\tlearn: 0.0931307\ttotal: 4.08s\tremaining: 4.03s\n",
      "503:\tlearn: 0.0931138\ttotal: 4.08s\tremaining: 4.02s\n",
      "504:\tlearn: 0.0930795\ttotal: 4.1s\tremaining: 4.02s\n",
      "505:\tlearn: 0.0930604\ttotal: 4.11s\tremaining: 4.01s\n",
      "506:\tlearn: 0.0930219\ttotal: 4.11s\tremaining: 4s\n",
      "507:\tlearn: 0.0930039\ttotal: 4.12s\tremaining: 3.99s\n",
      "508:\tlearn: 0.0929747\ttotal: 4.12s\tremaining: 3.98s\n",
      "509:\tlearn: 0.0929421\ttotal: 4.13s\tremaining: 3.97s\n",
      "510:\tlearn: 0.0929166\ttotal: 4.14s\tremaining: 3.96s\n",
      "511:\tlearn: 0.0928962\ttotal: 4.14s\tremaining: 3.95s\n",
      "512:\tlearn: 0.0928477\ttotal: 4.15s\tremaining: 3.94s\n",
      "513:\tlearn: 0.0927894\ttotal: 4.16s\tremaining: 3.93s\n",
      "514:\tlearn: 0.0927744\ttotal: 4.16s\tremaining: 3.92s\n",
      "515:\tlearn: 0.0927451\ttotal: 4.17s\tremaining: 3.91s\n",
      "516:\tlearn: 0.0927065\ttotal: 4.19s\tremaining: 3.91s\n",
      "517:\tlearn: 0.0926686\ttotal: 4.19s\tremaining: 3.9s\n",
      "518:\tlearn: 0.0926570\ttotal: 4.2s\tremaining: 3.89s\n",
      "519:\tlearn: 0.0926445\ttotal: 4.2s\tremaining: 3.88s\n",
      "520:\tlearn: 0.0925843\ttotal: 4.21s\tremaining: 3.87s\n",
      "521:\tlearn: 0.0925665\ttotal: 4.22s\tremaining: 3.87s\n",
      "522:\tlearn: 0.0925470\ttotal: 4.23s\tremaining: 3.86s\n",
      "523:\tlearn: 0.0925147\ttotal: 4.24s\tremaining: 3.85s\n",
      "524:\tlearn: 0.0924919\ttotal: 4.24s\tremaining: 3.84s\n",
      "525:\tlearn: 0.0924679\ttotal: 4.25s\tremaining: 3.83s\n",
      "526:\tlearn: 0.0924539\ttotal: 4.25s\tremaining: 3.82s\n",
      "527:\tlearn: 0.0923934\ttotal: 4.26s\tremaining: 3.81s\n",
      "528:\tlearn: 0.0923659\ttotal: 4.28s\tremaining: 3.81s\n",
      "529:\tlearn: 0.0923357\ttotal: 4.29s\tremaining: 3.8s\n",
      "530:\tlearn: 0.0923052\ttotal: 4.3s\tremaining: 3.8s\n",
      "531:\tlearn: 0.0922879\ttotal: 4.32s\tremaining: 3.8s\n",
      "532:\tlearn: 0.0922406\ttotal: 4.33s\tremaining: 3.79s\n",
      "533:\tlearn: 0.0922007\ttotal: 4.34s\tremaining: 3.79s\n",
      "534:\tlearn: 0.0921824\ttotal: 4.36s\tremaining: 3.79s\n",
      "535:\tlearn: 0.0921449\ttotal: 4.36s\tremaining: 3.77s\n",
      "536:\tlearn: 0.0921287\ttotal: 4.37s\tremaining: 3.77s\n",
      "537:\tlearn: 0.0921039\ttotal: 4.37s\tremaining: 3.75s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538:\tlearn: 0.0920862\ttotal: 4.38s\tremaining: 3.75s\n",
      "539:\tlearn: 0.0920608\ttotal: 4.39s\tremaining: 3.74s\n",
      "540:\tlearn: 0.0920489\ttotal: 4.4s\tremaining: 3.73s\n",
      "541:\tlearn: 0.0920230\ttotal: 4.4s\tremaining: 3.72s\n",
      "542:\tlearn: 0.0919876\ttotal: 4.41s\tremaining: 3.71s\n",
      "543:\tlearn: 0.0919459\ttotal: 4.41s\tremaining: 3.7s\n",
      "544:\tlearn: 0.0919166\ttotal: 4.42s\tremaining: 3.69s\n",
      "545:\tlearn: 0.0918930\ttotal: 4.42s\tremaining: 3.68s\n",
      "546:\tlearn: 0.0918525\ttotal: 4.43s\tremaining: 3.67s\n",
      "547:\tlearn: 0.0918297\ttotal: 4.44s\tremaining: 3.66s\n",
      "548:\tlearn: 0.0918101\ttotal: 4.45s\tremaining: 3.65s\n",
      "549:\tlearn: 0.0917818\ttotal: 4.45s\tremaining: 3.64s\n",
      "550:\tlearn: 0.0917624\ttotal: 4.46s\tremaining: 3.64s\n",
      "551:\tlearn: 0.0917396\ttotal: 4.47s\tremaining: 3.63s\n",
      "552:\tlearn: 0.0916759\ttotal: 4.48s\tremaining: 3.62s\n",
      "553:\tlearn: 0.0916391\ttotal: 4.49s\tremaining: 3.61s\n",
      "554:\tlearn: 0.0916272\ttotal: 4.49s\tremaining: 3.6s\n",
      "555:\tlearn: 0.0916141\ttotal: 4.5s\tremaining: 3.59s\n",
      "556:\tlearn: 0.0916015\ttotal: 4.5s\tremaining: 3.58s\n",
      "557:\tlearn: 0.0915901\ttotal: 4.51s\tremaining: 3.57s\n",
      "558:\tlearn: 0.0915646\ttotal: 4.52s\tremaining: 3.56s\n",
      "559:\tlearn: 0.0915521\ttotal: 4.52s\tremaining: 3.55s\n",
      "560:\tlearn: 0.0915405\ttotal: 4.53s\tremaining: 3.54s\n",
      "561:\tlearn: 0.0915293\ttotal: 4.54s\tremaining: 3.54s\n",
      "562:\tlearn: 0.0915017\ttotal: 4.54s\tremaining: 3.53s\n",
      "563:\tlearn: 0.0914921\ttotal: 4.57s\tremaining: 3.53s\n",
      "564:\tlearn: 0.0914709\ttotal: 4.58s\tremaining: 3.52s\n",
      "565:\tlearn: 0.0914397\ttotal: 4.58s\tremaining: 3.52s\n",
      "566:\tlearn: 0.0914283\ttotal: 4.59s\tremaining: 3.5s\n",
      "567:\tlearn: 0.0914124\ttotal: 4.59s\tremaining: 3.5s\n",
      "568:\tlearn: 0.0913992\ttotal: 4.61s\tremaining: 3.49s\n",
      "569:\tlearn: 0.0913776\ttotal: 4.62s\tremaining: 3.48s\n",
      "570:\tlearn: 0.0913625\ttotal: 4.63s\tremaining: 3.47s\n",
      "571:\tlearn: 0.0913458\ttotal: 4.63s\tremaining: 3.46s\n",
      "572:\tlearn: 0.0913020\ttotal: 4.64s\tremaining: 3.46s\n",
      "573:\tlearn: 0.0912831\ttotal: 4.65s\tremaining: 3.45s\n",
      "574:\tlearn: 0.0912511\ttotal: 4.66s\tremaining: 3.44s\n",
      "575:\tlearn: 0.0912042\ttotal: 4.67s\tremaining: 3.44s\n",
      "576:\tlearn: 0.0911902\ttotal: 4.67s\tremaining: 3.42s\n",
      "577:\tlearn: 0.0911827\ttotal: 4.68s\tremaining: 3.42s\n",
      "578:\tlearn: 0.0911739\ttotal: 4.68s\tremaining: 3.4s\n",
      "579:\tlearn: 0.0911383\ttotal: 4.7s\tremaining: 3.4s\n",
      "580:\tlearn: 0.0911138\ttotal: 4.7s\tremaining: 3.39s\n",
      "581:\tlearn: 0.0910885\ttotal: 4.71s\tremaining: 3.38s\n",
      "582:\tlearn: 0.0910606\ttotal: 4.71s\tremaining: 3.37s\n",
      "583:\tlearn: 0.0910476\ttotal: 4.72s\tremaining: 3.36s\n",
      "584:\tlearn: 0.0910371\ttotal: 4.73s\tremaining: 3.35s\n",
      "585:\tlearn: 0.0909990\ttotal: 4.74s\tremaining: 3.35s\n",
      "586:\tlearn: 0.0909754\ttotal: 4.74s\tremaining: 3.34s\n",
      "587:\tlearn: 0.0909612\ttotal: 4.75s\tremaining: 3.33s\n",
      "588:\tlearn: 0.0909489\ttotal: 4.75s\tremaining: 3.32s\n",
      "589:\tlearn: 0.0909449\ttotal: 4.76s\tremaining: 3.31s\n",
      "590:\tlearn: 0.0909132\ttotal: 4.76s\tremaining: 3.3s\n",
      "591:\tlearn: 0.0908877\ttotal: 4.77s\tremaining: 3.29s\n",
      "592:\tlearn: 0.0908746\ttotal: 4.78s\tremaining: 3.28s\n",
      "593:\tlearn: 0.0908379\ttotal: 4.79s\tremaining: 3.27s\n",
      "594:\tlearn: 0.0908191\ttotal: 4.79s\tremaining: 3.26s\n",
      "595:\tlearn: 0.0907896\ttotal: 4.8s\tremaining: 3.25s\n",
      "596:\tlearn: 0.0907306\ttotal: 4.8s\tremaining: 3.24s\n",
      "597:\tlearn: 0.0907056\ttotal: 4.81s\tremaining: 3.23s\n",
      "598:\tlearn: 0.0906733\ttotal: 4.82s\tremaining: 3.23s\n",
      "599:\tlearn: 0.0906474\ttotal: 4.83s\tremaining: 3.22s\n",
      "600:\tlearn: 0.0906359\ttotal: 4.83s\tremaining: 3.21s\n",
      "601:\tlearn: 0.0906192\ttotal: 4.84s\tremaining: 3.2s\n",
      "602:\tlearn: 0.0905973\ttotal: 4.84s\tremaining: 3.19s\n",
      "603:\tlearn: 0.0905859\ttotal: 4.85s\tremaining: 3.18s\n",
      "604:\tlearn: 0.0905670\ttotal: 4.86s\tremaining: 3.17s\n",
      "605:\tlearn: 0.0905328\ttotal: 4.87s\tremaining: 3.17s\n",
      "606:\tlearn: 0.0905070\ttotal: 4.88s\tremaining: 3.16s\n",
      "607:\tlearn: 0.0904971\ttotal: 4.88s\tremaining: 3.15s\n",
      "608:\tlearn: 0.0904898\ttotal: 4.89s\tremaining: 3.14s\n",
      "609:\tlearn: 0.0904802\ttotal: 4.89s\tremaining: 3.13s\n",
      "610:\tlearn: 0.0904598\ttotal: 4.9s\tremaining: 3.12s\n",
      "611:\tlearn: 0.0904042\ttotal: 4.91s\tremaining: 3.11s\n",
      "612:\tlearn: 0.0903838\ttotal: 4.92s\tremaining: 3.1s\n",
      "613:\tlearn: 0.0903737\ttotal: 4.92s\tremaining: 3.09s\n",
      "614:\tlearn: 0.0903270\ttotal: 4.93s\tremaining: 3.09s\n",
      "615:\tlearn: 0.0903033\ttotal: 4.93s\tremaining: 3.08s\n",
      "616:\tlearn: 0.0902512\ttotal: 4.94s\tremaining: 3.07s\n",
      "617:\tlearn: 0.0902343\ttotal: 4.96s\tremaining: 3.07s\n",
      "618:\tlearn: 0.0902005\ttotal: 4.97s\tremaining: 3.06s\n",
      "619:\tlearn: 0.0901828\ttotal: 4.97s\tremaining: 3.05s\n",
      "620:\tlearn: 0.0901425\ttotal: 4.98s\tremaining: 3.04s\n",
      "621:\tlearn: 0.0901318\ttotal: 4.98s\tremaining: 3.03s\n",
      "622:\tlearn: 0.0901221\ttotal: 4.99s\tremaining: 3.02s\n",
      "623:\tlearn: 0.0900892\ttotal: 5s\tremaining: 3.01s\n",
      "624:\tlearn: 0.0900599\ttotal: 5.01s\tremaining: 3s\n",
      "625:\tlearn: 0.0900238\ttotal: 5.01s\tremaining: 2.99s\n",
      "626:\tlearn: 0.0899974\ttotal: 5.02s\tremaining: 2.98s\n",
      "627:\tlearn: 0.0899831\ttotal: 5.02s\tremaining: 2.98s\n",
      "628:\tlearn: 0.0899662\ttotal: 5.03s\tremaining: 2.96s\n",
      "629:\tlearn: 0.0899329\ttotal: 5.04s\tremaining: 2.96s\n",
      "630:\tlearn: 0.0899142\ttotal: 5.04s\tremaining: 2.95s\n",
      "631:\tlearn: 0.0898813\ttotal: 5.05s\tremaining: 2.94s\n",
      "632:\tlearn: 0.0898332\ttotal: 5.06s\tremaining: 2.93s\n",
      "633:\tlearn: 0.0898257\ttotal: 5.06s\tremaining: 2.92s\n",
      "634:\tlearn: 0.0897896\ttotal: 5.07s\tremaining: 2.91s\n",
      "635:\tlearn: 0.0897661\ttotal: 5.08s\tremaining: 2.9s\n",
      "636:\tlearn: 0.0897583\ttotal: 5.08s\tremaining: 2.9s\n",
      "637:\tlearn: 0.0897278\ttotal: 5.09s\tremaining: 2.89s\n",
      "638:\tlearn: 0.0897088\ttotal: 5.1s\tremaining: 2.88s\n",
      "639:\tlearn: 0.0896842\ttotal: 5.1s\tremaining: 2.87s\n",
      "640:\tlearn: 0.0896463\ttotal: 5.11s\tremaining: 2.86s\n",
      "641:\tlearn: 0.0896349\ttotal: 5.12s\tremaining: 2.85s\n",
      "642:\tlearn: 0.0896085\ttotal: 5.12s\tremaining: 2.84s\n",
      "643:\tlearn: 0.0895876\ttotal: 5.13s\tremaining: 2.84s\n",
      "644:\tlearn: 0.0895676\ttotal: 5.14s\tremaining: 2.83s\n",
      "645:\tlearn: 0.0895345\ttotal: 5.14s\tremaining: 2.82s\n",
      "646:\tlearn: 0.0895245\ttotal: 5.15s\tremaining: 2.81s\n",
      "647:\tlearn: 0.0894893\ttotal: 5.15s\tremaining: 2.8s\n",
      "648:\tlearn: 0.0894593\ttotal: 5.16s\tremaining: 2.79s\n",
      "649:\tlearn: 0.0894276\ttotal: 5.17s\tremaining: 2.78s\n",
      "650:\tlearn: 0.0894110\ttotal: 5.18s\tremaining: 2.77s\n",
      "651:\tlearn: 0.0893876\ttotal: 5.18s\tremaining: 2.77s\n",
      "652:\tlearn: 0.0893823\ttotal: 5.19s\tremaining: 2.76s\n",
      "653:\tlearn: 0.0893633\ttotal: 5.19s\tremaining: 2.75s\n",
      "654:\tlearn: 0.0893455\ttotal: 5.2s\tremaining: 2.74s\n",
      "655:\tlearn: 0.0893336\ttotal: 5.21s\tremaining: 2.73s\n",
      "656:\tlearn: 0.0893253\ttotal: 5.23s\tremaining: 2.73s\n",
      "657:\tlearn: 0.0892976\ttotal: 5.24s\tremaining: 2.72s\n",
      "658:\tlearn: 0.0892517\ttotal: 5.26s\tremaining: 2.72s\n",
      "659:\tlearn: 0.0892366\ttotal: 5.27s\tremaining: 2.72s\n",
      "660:\tlearn: 0.0891924\ttotal: 5.28s\tremaining: 2.71s\n",
      "661:\tlearn: 0.0891633\ttotal: 5.31s\tremaining: 2.71s\n",
      "662:\tlearn: 0.0891347\ttotal: 5.32s\tremaining: 2.7s\n",
      "663:\tlearn: 0.0891145\ttotal: 5.32s\tremaining: 2.69s\n",
      "664:\tlearn: 0.0891018\ttotal: 5.33s\tremaining: 2.68s\n",
      "665:\tlearn: 0.0890879\ttotal: 5.33s\tremaining: 2.67s\n",
      "666:\tlearn: 0.0890797\ttotal: 5.34s\tremaining: 2.67s\n",
      "667:\tlearn: 0.0890530\ttotal: 5.35s\tremaining: 2.66s\n",
      "668:\tlearn: 0.0890358\ttotal: 5.36s\tremaining: 2.65s\n",
      "669:\tlearn: 0.0890160\ttotal: 5.36s\tremaining: 2.64s\n",
      "670:\tlearn: 0.0890002\ttotal: 5.37s\tremaining: 2.63s\n",
      "671:\tlearn: 0.0889874\ttotal: 5.37s\tremaining: 2.62s\n",
      "672:\tlearn: 0.0889727\ttotal: 5.38s\tremaining: 2.61s\n",
      "673:\tlearn: 0.0889456\ttotal: 5.39s\tremaining: 2.61s\n",
      "674:\tlearn: 0.0889364\ttotal: 5.4s\tremaining: 2.6s\n",
      "675:\tlearn: 0.0888961\ttotal: 5.4s\tremaining: 2.59s\n",
      "676:\tlearn: 0.0888727\ttotal: 5.41s\tremaining: 2.58s\n",
      "677:\tlearn: 0.0888599\ttotal: 5.41s\tremaining: 2.57s\n",
      "678:\tlearn: 0.0888489\ttotal: 5.42s\tremaining: 2.56s\n",
      "679:\tlearn: 0.0888269\ttotal: 5.43s\tremaining: 2.56s\n",
      "680:\tlearn: 0.0887967\ttotal: 5.44s\tremaining: 2.55s\n",
      "681:\tlearn: 0.0887687\ttotal: 5.45s\tremaining: 2.54s\n",
      "682:\tlearn: 0.0887543\ttotal: 5.45s\tremaining: 2.53s\n",
      "683:\tlearn: 0.0887354\ttotal: 5.46s\tremaining: 2.52s\n",
      "684:\tlearn: 0.0887213\ttotal: 5.46s\tremaining: 2.51s\n",
      "685:\tlearn: 0.0887065\ttotal: 5.47s\tremaining: 2.51s\n",
      "686:\tlearn: 0.0886903\ttotal: 5.48s\tremaining: 2.5s\n",
      "687:\tlearn: 0.0886789\ttotal: 5.49s\tremaining: 2.49s\n",
      "688:\tlearn: 0.0886525\ttotal: 5.49s\tremaining: 2.48s\n",
      "689:\tlearn: 0.0886216\ttotal: 5.5s\tremaining: 2.47s\n",
      "690:\tlearn: 0.0885801\ttotal: 5.5s\tremaining: 2.46s\n",
      "691:\tlearn: 0.0885725\ttotal: 5.51s\tremaining: 2.45s\n",
      "692:\tlearn: 0.0885509\ttotal: 5.52s\tremaining: 2.45s\n",
      "693:\tlearn: 0.0885397\ttotal: 5.53s\tremaining: 2.44s\n",
      "694:\tlearn: 0.0885219\ttotal: 5.54s\tremaining: 2.43s\n",
      "695:\tlearn: 0.0884978\ttotal: 5.54s\tremaining: 2.42s\n",
      "696:\tlearn: 0.0884950\ttotal: 5.55s\tremaining: 2.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697:\tlearn: 0.0884801\ttotal: 5.56s\tremaining: 2.41s\n",
      "698:\tlearn: 0.0884596\ttotal: 5.57s\tremaining: 2.4s\n",
      "699:\tlearn: 0.0884388\ttotal: 5.58s\tremaining: 2.39s\n",
      "700:\tlearn: 0.0884109\ttotal: 5.58s\tremaining: 2.38s\n",
      "701:\tlearn: 0.0883938\ttotal: 5.59s\tremaining: 2.37s\n",
      "702:\tlearn: 0.0883788\ttotal: 5.59s\tremaining: 2.36s\n",
      "703:\tlearn: 0.0883662\ttotal: 5.61s\tremaining: 2.36s\n",
      "704:\tlearn: 0.0883415\ttotal: 5.61s\tremaining: 2.35s\n",
      "705:\tlearn: 0.0883141\ttotal: 5.62s\tremaining: 2.34s\n",
      "706:\tlearn: 0.0882798\ttotal: 5.63s\tremaining: 2.33s\n",
      "707:\tlearn: 0.0882666\ttotal: 5.63s\tremaining: 2.32s\n",
      "708:\tlearn: 0.0882457\ttotal: 5.65s\tremaining: 2.32s\n",
      "709:\tlearn: 0.0882156\ttotal: 5.66s\tremaining: 2.31s\n",
      "710:\tlearn: 0.0881930\ttotal: 5.66s\tremaining: 2.3s\n",
      "711:\tlearn: 0.0881777\ttotal: 5.67s\tremaining: 2.29s\n",
      "712:\tlearn: 0.0881679\ttotal: 5.67s\tremaining: 2.28s\n",
      "713:\tlearn: 0.0881498\ttotal: 5.69s\tremaining: 2.28s\n",
      "714:\tlearn: 0.0881372\ttotal: 5.7s\tremaining: 2.27s\n",
      "715:\tlearn: 0.0881215\ttotal: 5.71s\tremaining: 2.26s\n",
      "716:\tlearn: 0.0881032\ttotal: 5.71s\tremaining: 2.25s\n",
      "717:\tlearn: 0.0880849\ttotal: 5.72s\tremaining: 2.25s\n",
      "718:\tlearn: 0.0880748\ttotal: 5.72s\tremaining: 2.24s\n",
      "719:\tlearn: 0.0880617\ttotal: 5.74s\tremaining: 2.23s\n",
      "720:\tlearn: 0.0880458\ttotal: 5.74s\tremaining: 2.22s\n",
      "721:\tlearn: 0.0880349\ttotal: 5.75s\tremaining: 2.21s\n",
      "722:\tlearn: 0.0880322\ttotal: 5.76s\tremaining: 2.21s\n",
      "723:\tlearn: 0.0880261\ttotal: 5.76s\tremaining: 2.2s\n",
      "724:\tlearn: 0.0879959\ttotal: 5.77s\tremaining: 2.19s\n",
      "725:\tlearn: 0.0879870\ttotal: 5.78s\tremaining: 2.18s\n",
      "726:\tlearn: 0.0879592\ttotal: 5.79s\tremaining: 2.17s\n",
      "727:\tlearn: 0.0879449\ttotal: 5.79s\tremaining: 2.17s\n",
      "728:\tlearn: 0.0879328\ttotal: 5.8s\tremaining: 2.16s\n",
      "729:\tlearn: 0.0879099\ttotal: 5.81s\tremaining: 2.15s\n",
      "730:\tlearn: 0.0878792\ttotal: 5.81s\tremaining: 2.14s\n",
      "731:\tlearn: 0.0878617\ttotal: 5.83s\tremaining: 2.13s\n",
      "732:\tlearn: 0.0878416\ttotal: 5.84s\tremaining: 2.13s\n",
      "733:\tlearn: 0.0878264\ttotal: 5.84s\tremaining: 2.12s\n",
      "734:\tlearn: 0.0878111\ttotal: 5.85s\tremaining: 2.11s\n",
      "735:\tlearn: 0.0877957\ttotal: 5.85s\tremaining: 2.1s\n",
      "736:\tlearn: 0.0877875\ttotal: 5.87s\tremaining: 2.09s\n",
      "737:\tlearn: 0.0877684\ttotal: 5.88s\tremaining: 2.09s\n",
      "738:\tlearn: 0.0877601\ttotal: 5.88s\tremaining: 2.08s\n",
      "739:\tlearn: 0.0877512\ttotal: 5.89s\tremaining: 2.07s\n",
      "740:\tlearn: 0.0877265\ttotal: 5.89s\tremaining: 2.06s\n",
      "741:\tlearn: 0.0877186\ttotal: 5.9s\tremaining: 2.05s\n",
      "742:\tlearn: 0.0877070\ttotal: 5.91s\tremaining: 2.04s\n",
      "743:\tlearn: 0.0876903\ttotal: 5.92s\tremaining: 2.04s\n",
      "744:\tlearn: 0.0876846\ttotal: 5.93s\tremaining: 2.03s\n",
      "745:\tlearn: 0.0876694\ttotal: 5.93s\tremaining: 2.02s\n",
      "746:\tlearn: 0.0876420\ttotal: 5.94s\tremaining: 2.01s\n",
      "747:\tlearn: 0.0876394\ttotal: 5.95s\tremaining: 2s\n",
      "748:\tlearn: 0.0876201\ttotal: 5.96s\tremaining: 2s\n",
      "749:\tlearn: 0.0876011\ttotal: 5.96s\tremaining: 1.99s\n",
      "750:\tlearn: 0.0875933\ttotal: 5.97s\tremaining: 1.98s\n",
      "751:\tlearn: 0.0875763\ttotal: 5.98s\tremaining: 1.97s\n",
      "752:\tlearn: 0.0875681\ttotal: 5.98s\tremaining: 1.96s\n",
      "753:\tlearn: 0.0875405\ttotal: 5.99s\tremaining: 1.95s\n",
      "754:\tlearn: 0.0875234\ttotal: 6s\tremaining: 1.95s\n",
      "755:\tlearn: 0.0875092\ttotal: 6s\tremaining: 1.94s\n",
      "756:\tlearn: 0.0874954\ttotal: 6.01s\tremaining: 1.93s\n",
      "757:\tlearn: 0.0874780\ttotal: 6.01s\tremaining: 1.92s\n",
      "758:\tlearn: 0.0874563\ttotal: 6.02s\tremaining: 1.91s\n",
      "759:\tlearn: 0.0874406\ttotal: 6.02s\tremaining: 1.9s\n",
      "760:\tlearn: 0.0874322\ttotal: 6.03s\tremaining: 1.89s\n",
      "761:\tlearn: 0.0874196\ttotal: 6.04s\tremaining: 1.89s\n",
      "762:\tlearn: 0.0873962\ttotal: 6.04s\tremaining: 1.88s\n",
      "763:\tlearn: 0.0873886\ttotal: 6.05s\tremaining: 1.87s\n",
      "764:\tlearn: 0.0873808\ttotal: 6.05s\tremaining: 1.86s\n",
      "765:\tlearn: 0.0873655\ttotal: 6.06s\tremaining: 1.85s\n",
      "766:\tlearn: 0.0873500\ttotal: 6.07s\tremaining: 1.84s\n",
      "767:\tlearn: 0.0873303\ttotal: 6.07s\tremaining: 1.83s\n",
      "768:\tlearn: 0.0873148\ttotal: 6.08s\tremaining: 1.83s\n",
      "769:\tlearn: 0.0872959\ttotal: 6.09s\tremaining: 1.82s\n",
      "770:\tlearn: 0.0872883\ttotal: 6.09s\tremaining: 1.81s\n",
      "771:\tlearn: 0.0872802\ttotal: 6.1s\tremaining: 1.8s\n",
      "772:\tlearn: 0.0872712\ttotal: 6.1s\tremaining: 1.79s\n",
      "773:\tlearn: 0.0872636\ttotal: 6.11s\tremaining: 1.78s\n",
      "774:\tlearn: 0.0872543\ttotal: 6.11s\tremaining: 1.77s\n",
      "775:\tlearn: 0.0872472\ttotal: 6.13s\tremaining: 1.77s\n",
      "776:\tlearn: 0.0872247\ttotal: 6.13s\tremaining: 1.76s\n",
      "777:\tlearn: 0.0872024\ttotal: 6.14s\tremaining: 1.75s\n",
      "778:\tlearn: 0.0871857\ttotal: 6.14s\tremaining: 1.74s\n",
      "779:\tlearn: 0.0871436\ttotal: 6.15s\tremaining: 1.73s\n",
      "780:\tlearn: 0.0871216\ttotal: 6.15s\tremaining: 1.73s\n",
      "781:\tlearn: 0.0871063\ttotal: 6.16s\tremaining: 1.72s\n",
      "782:\tlearn: 0.0870807\ttotal: 6.17s\tremaining: 1.71s\n",
      "783:\tlearn: 0.0870733\ttotal: 6.18s\tremaining: 1.7s\n",
      "784:\tlearn: 0.0870534\ttotal: 6.19s\tremaining: 1.7s\n",
      "785:\tlearn: 0.0870491\ttotal: 6.2s\tremaining: 1.69s\n",
      "786:\tlearn: 0.0870467\ttotal: 6.22s\tremaining: 1.68s\n",
      "787:\tlearn: 0.0870245\ttotal: 6.24s\tremaining: 1.68s\n",
      "788:\tlearn: 0.0870205\ttotal: 6.25s\tremaining: 1.67s\n",
      "789:\tlearn: 0.0869942\ttotal: 6.27s\tremaining: 1.67s\n",
      "790:\tlearn: 0.0869823\ttotal: 6.28s\tremaining: 1.66s\n",
      "791:\tlearn: 0.0869807\ttotal: 6.28s\tremaining: 1.65s\n",
      "792:\tlearn: 0.0869674\ttotal: 6.29s\tremaining: 1.64s\n",
      "793:\tlearn: 0.0869318\ttotal: 6.29s\tremaining: 1.63s\n",
      "794:\tlearn: 0.0869229\ttotal: 6.3s\tremaining: 1.62s\n",
      "795:\tlearn: 0.0869095\ttotal: 6.3s\tremaining: 1.61s\n",
      "796:\tlearn: 0.0868973\ttotal: 6.31s\tremaining: 1.61s\n",
      "797:\tlearn: 0.0868886\ttotal: 6.32s\tremaining: 1.6s\n",
      "798:\tlearn: 0.0868759\ttotal: 6.32s\tremaining: 1.59s\n",
      "799:\tlearn: 0.0868688\ttotal: 6.33s\tremaining: 1.58s\n",
      "800:\tlearn: 0.0868613\ttotal: 6.33s\tremaining: 1.57s\n",
      "801:\tlearn: 0.0868443\ttotal: 6.34s\tremaining: 1.56s\n",
      "802:\tlearn: 0.0868247\ttotal: 6.35s\tremaining: 1.56s\n",
      "803:\tlearn: 0.0868106\ttotal: 6.35s\tremaining: 1.55s\n",
      "804:\tlearn: 0.0867928\ttotal: 6.36s\tremaining: 1.54s\n",
      "805:\tlearn: 0.0867735\ttotal: 6.36s\tremaining: 1.53s\n",
      "806:\tlearn: 0.0867483\ttotal: 6.37s\tremaining: 1.52s\n",
      "807:\tlearn: 0.0867336\ttotal: 6.38s\tremaining: 1.51s\n",
      "808:\tlearn: 0.0867270\ttotal: 6.38s\tremaining: 1.51s\n",
      "809:\tlearn: 0.0867246\ttotal: 6.39s\tremaining: 1.5s\n",
      "810:\tlearn: 0.0867126\ttotal: 6.4s\tremaining: 1.49s\n",
      "811:\tlearn: 0.0866901\ttotal: 6.41s\tremaining: 1.48s\n",
      "812:\tlearn: 0.0866775\ttotal: 6.41s\tremaining: 1.47s\n",
      "813:\tlearn: 0.0866563\ttotal: 6.42s\tremaining: 1.47s\n",
      "814:\tlearn: 0.0866504\ttotal: 6.42s\tremaining: 1.46s\n",
      "815:\tlearn: 0.0866206\ttotal: 6.43s\tremaining: 1.45s\n",
      "816:\tlearn: 0.0866127\ttotal: 6.44s\tremaining: 1.44s\n",
      "817:\tlearn: 0.0866062\ttotal: 6.44s\tremaining: 1.43s\n",
      "818:\tlearn: 0.0865899\ttotal: 6.45s\tremaining: 1.43s\n",
      "819:\tlearn: 0.0865679\ttotal: 6.45s\tremaining: 1.42s\n",
      "820:\tlearn: 0.0865493\ttotal: 6.46s\tremaining: 1.41s\n",
      "821:\tlearn: 0.0865391\ttotal: 6.46s\tremaining: 1.4s\n",
      "822:\tlearn: 0.0865237\ttotal: 6.47s\tremaining: 1.39s\n",
      "823:\tlearn: 0.0865129\ttotal: 6.48s\tremaining: 1.38s\n",
      "824:\tlearn: 0.0865082\ttotal: 6.49s\tremaining: 1.38s\n",
      "825:\tlearn: 0.0864937\ttotal: 6.49s\tremaining: 1.37s\n",
      "826:\tlearn: 0.0864816\ttotal: 6.5s\tremaining: 1.36s\n",
      "827:\tlearn: 0.0864697\ttotal: 6.5s\tremaining: 1.35s\n",
      "828:\tlearn: 0.0864624\ttotal: 6.51s\tremaining: 1.34s\n",
      "829:\tlearn: 0.0864541\ttotal: 6.52s\tremaining: 1.33s\n",
      "830:\tlearn: 0.0864266\ttotal: 6.53s\tremaining: 1.33s\n",
      "831:\tlearn: 0.0863988\ttotal: 6.53s\tremaining: 1.32s\n",
      "832:\tlearn: 0.0863784\ttotal: 6.54s\tremaining: 1.31s\n",
      "833:\tlearn: 0.0863579\ttotal: 6.55s\tremaining: 1.3s\n",
      "834:\tlearn: 0.0863444\ttotal: 6.57s\tremaining: 1.3s\n",
      "835:\tlearn: 0.0863419\ttotal: 6.58s\tremaining: 1.29s\n",
      "836:\tlearn: 0.0863169\ttotal: 6.58s\tremaining: 1.28s\n",
      "837:\tlearn: 0.0862763\ttotal: 6.59s\tremaining: 1.27s\n",
      "838:\tlearn: 0.0862714\ttotal: 6.59s\tremaining: 1.26s\n",
      "839:\tlearn: 0.0862413\ttotal: 6.6s\tremaining: 1.26s\n",
      "840:\tlearn: 0.0862356\ttotal: 6.61s\tremaining: 1.25s\n",
      "841:\tlearn: 0.0862096\ttotal: 6.62s\tremaining: 1.24s\n",
      "842:\tlearn: 0.0862022\ttotal: 6.62s\tremaining: 1.23s\n",
      "843:\tlearn: 0.0861828\ttotal: 6.63s\tremaining: 1.23s\n",
      "844:\tlearn: 0.0861661\ttotal: 6.63s\tremaining: 1.22s\n",
      "845:\tlearn: 0.0861529\ttotal: 6.64s\tremaining: 1.21s\n",
      "846:\tlearn: 0.0861331\ttotal: 6.64s\tremaining: 1.2s\n",
      "847:\tlearn: 0.0861213\ttotal: 6.65s\tremaining: 1.19s\n",
      "848:\tlearn: 0.0861143\ttotal: 6.66s\tremaining: 1.18s\n",
      "849:\tlearn: 0.0861050\ttotal: 6.66s\tremaining: 1.18s\n",
      "850:\tlearn: 0.0860861\ttotal: 6.67s\tremaining: 1.17s\n",
      "851:\tlearn: 0.0860742\ttotal: 6.67s\tremaining: 1.16s\n",
      "852:\tlearn: 0.0860679\ttotal: 6.67s\tremaining: 1.15s\n",
      "853:\tlearn: 0.0860464\ttotal: 6.68s\tremaining: 1.14s\n",
      "854:\tlearn: 0.0860297\ttotal: 6.69s\tremaining: 1.13s\n",
      "855:\tlearn: 0.0860113\ttotal: 6.7s\tremaining: 1.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856:\tlearn: 0.0860001\ttotal: 6.71s\tremaining: 1.12s\n",
      "857:\tlearn: 0.0859665\ttotal: 6.72s\tremaining: 1.11s\n",
      "858:\tlearn: 0.0859529\ttotal: 6.73s\tremaining: 1.1s\n",
      "859:\tlearn: 0.0859434\ttotal: 6.74s\tremaining: 1.1s\n",
      "860:\tlearn: 0.0859322\ttotal: 6.75s\tremaining: 1.09s\n",
      "861:\tlearn: 0.0859305\ttotal: 6.75s\tremaining: 1.08s\n",
      "862:\tlearn: 0.0859268\ttotal: 6.76s\tremaining: 1.07s\n",
      "863:\tlearn: 0.0859046\ttotal: 6.77s\tremaining: 1.06s\n",
      "864:\tlearn: 0.0858927\ttotal: 6.77s\tremaining: 1.06s\n",
      "865:\tlearn: 0.0858718\ttotal: 6.78s\tremaining: 1.05s\n",
      "866:\tlearn: 0.0858628\ttotal: 6.78s\tremaining: 1.04s\n",
      "867:\tlearn: 0.0858478\ttotal: 6.79s\tremaining: 1.03s\n",
      "868:\tlearn: 0.0858384\ttotal: 6.79s\tremaining: 1.02s\n",
      "869:\tlearn: 0.0858344\ttotal: 6.8s\tremaining: 1.01s\n",
      "870:\tlearn: 0.0858229\ttotal: 6.8s\tremaining: 1.01s\n",
      "871:\tlearn: 0.0858116\ttotal: 6.81s\tremaining: 1000ms\n",
      "872:\tlearn: 0.0857990\ttotal: 6.82s\tremaining: 992ms\n",
      "873:\tlearn: 0.0857907\ttotal: 6.82s\tremaining: 984ms\n",
      "874:\tlearn: 0.0857769\ttotal: 6.83s\tremaining: 976ms\n",
      "875:\tlearn: 0.0857482\ttotal: 6.83s\tremaining: 967ms\n",
      "876:\tlearn: 0.0857405\ttotal: 6.84s\tremaining: 960ms\n",
      "877:\tlearn: 0.0857222\ttotal: 6.85s\tremaining: 951ms\n",
      "878:\tlearn: 0.0857104\ttotal: 6.85s\tremaining: 943ms\n",
      "879:\tlearn: 0.0856987\ttotal: 6.86s\tremaining: 935ms\n",
      "880:\tlearn: 0.0856969\ttotal: 6.86s\tremaining: 927ms\n",
      "881:\tlearn: 0.0856902\ttotal: 6.87s\tremaining: 919ms\n",
      "882:\tlearn: 0.0856824\ttotal: 6.87s\tremaining: 911ms\n",
      "883:\tlearn: 0.0856744\ttotal: 6.88s\tremaining: 903ms\n",
      "884:\tlearn: 0.0856608\ttotal: 6.88s\tremaining: 895ms\n",
      "885:\tlearn: 0.0856463\ttotal: 6.89s\tremaining: 887ms\n",
      "886:\tlearn: 0.0856270\ttotal: 6.9s\tremaining: 879ms\n",
      "887:\tlearn: 0.0856162\ttotal: 6.91s\tremaining: 871ms\n",
      "888:\tlearn: 0.0856008\ttotal: 6.91s\tremaining: 863ms\n",
      "889:\tlearn: 0.0855892\ttotal: 6.92s\tremaining: 855ms\n",
      "890:\tlearn: 0.0855751\ttotal: 6.92s\tremaining: 847ms\n",
      "891:\tlearn: 0.0855603\ttotal: 6.93s\tremaining: 839ms\n",
      "892:\tlearn: 0.0855484\ttotal: 6.93s\tremaining: 831ms\n",
      "893:\tlearn: 0.0855473\ttotal: 6.94s\tremaining: 823ms\n",
      "894:\tlearn: 0.0855366\ttotal: 6.94s\tremaining: 815ms\n",
      "895:\tlearn: 0.0855278\ttotal: 6.95s\tremaining: 807ms\n",
      "896:\tlearn: 0.0855014\ttotal: 6.95s\tremaining: 799ms\n",
      "897:\tlearn: 0.0854849\ttotal: 6.96s\tremaining: 791ms\n",
      "898:\tlearn: 0.0854659\ttotal: 6.96s\tremaining: 783ms\n",
      "899:\tlearn: 0.0854505\ttotal: 6.97s\tremaining: 774ms\n",
      "900:\tlearn: 0.0854340\ttotal: 6.98s\tremaining: 767ms\n",
      "901:\tlearn: 0.0854231\ttotal: 6.98s\tremaining: 759ms\n",
      "902:\tlearn: 0.0854084\ttotal: 6.99s\tremaining: 751ms\n",
      "903:\tlearn: 0.0853774\ttotal: 7s\tremaining: 743ms\n",
      "904:\tlearn: 0.0853587\ttotal: 7s\tremaining: 735ms\n",
      "905:\tlearn: 0.0853465\ttotal: 7s\tremaining: 727ms\n",
      "906:\tlearn: 0.0853365\ttotal: 7.01s\tremaining: 719ms\n",
      "907:\tlearn: 0.0853295\ttotal: 7.02s\tremaining: 711ms\n",
      "908:\tlearn: 0.0853151\ttotal: 7.02s\tremaining: 703ms\n",
      "909:\tlearn: 0.0853046\ttotal: 7.03s\tremaining: 695ms\n",
      "910:\tlearn: 0.0852988\ttotal: 7.03s\tremaining: 687ms\n",
      "911:\tlearn: 0.0852821\ttotal: 7.04s\tremaining: 679ms\n",
      "912:\tlearn: 0.0852691\ttotal: 7.04s\tremaining: 671ms\n",
      "913:\tlearn: 0.0852601\ttotal: 7.05s\tremaining: 663ms\n",
      "914:\tlearn: 0.0852538\ttotal: 7.06s\tremaining: 655ms\n",
      "915:\tlearn: 0.0852414\ttotal: 7.06s\tremaining: 648ms\n",
      "916:\tlearn: 0.0852306\ttotal: 7.07s\tremaining: 640ms\n",
      "917:\tlearn: 0.0852288\ttotal: 7.08s\tremaining: 632ms\n",
      "918:\tlearn: 0.0852231\ttotal: 7.08s\tremaining: 624ms\n",
      "919:\tlearn: 0.0852098\ttotal: 7.09s\tremaining: 616ms\n",
      "920:\tlearn: 0.0852014\ttotal: 7.09s\tremaining: 609ms\n",
      "921:\tlearn: 0.0851979\ttotal: 7.1s\tremaining: 601ms\n",
      "922:\tlearn: 0.0851900\ttotal: 7.11s\tremaining: 593ms\n",
      "923:\tlearn: 0.0851770\ttotal: 7.11s\tremaining: 585ms\n",
      "924:\tlearn: 0.0851728\ttotal: 7.12s\tremaining: 577ms\n",
      "925:\tlearn: 0.0851628\ttotal: 7.12s\tremaining: 569ms\n",
      "926:\tlearn: 0.0851551\ttotal: 7.13s\tremaining: 562ms\n",
      "927:\tlearn: 0.0851453\ttotal: 7.14s\tremaining: 554ms\n",
      "928:\tlearn: 0.0851356\ttotal: 7.16s\tremaining: 547ms\n",
      "929:\tlearn: 0.0851063\ttotal: 7.18s\tremaining: 540ms\n",
      "930:\tlearn: 0.0851015\ttotal: 7.19s\tremaining: 533ms\n",
      "931:\tlearn: 0.0850943\ttotal: 7.2s\tremaining: 526ms\n",
      "932:\tlearn: 0.0850764\ttotal: 7.21s\tremaining: 518ms\n",
      "933:\tlearn: 0.0850537\ttotal: 7.22s\tremaining: 511ms\n",
      "934:\tlearn: 0.0850452\ttotal: 7.23s\tremaining: 503ms\n",
      "935:\tlearn: 0.0850299\ttotal: 7.24s\tremaining: 495ms\n",
      "936:\tlearn: 0.0850191\ttotal: 7.24s\tremaining: 487ms\n",
      "937:\tlearn: 0.0850082\ttotal: 7.25s\tremaining: 479ms\n",
      "938:\tlearn: 0.0850052\ttotal: 7.25s\tremaining: 471ms\n",
      "939:\tlearn: 0.0849943\ttotal: 7.26s\tremaining: 463ms\n",
      "940:\tlearn: 0.0849775\ttotal: 7.26s\tremaining: 456ms\n",
      "941:\tlearn: 0.0849479\ttotal: 7.27s\tremaining: 448ms\n",
      "942:\tlearn: 0.0849318\ttotal: 7.28s\tremaining: 440ms\n",
      "943:\tlearn: 0.0849205\ttotal: 7.28s\tremaining: 432ms\n",
      "944:\tlearn: 0.0849118\ttotal: 7.29s\tremaining: 424ms\n",
      "945:\tlearn: 0.0848810\ttotal: 7.3s\tremaining: 417ms\n",
      "946:\tlearn: 0.0848721\ttotal: 7.31s\tremaining: 409ms\n",
      "947:\tlearn: 0.0848555\ttotal: 7.31s\tremaining: 401ms\n",
      "948:\tlearn: 0.0848472\ttotal: 7.32s\tremaining: 393ms\n",
      "949:\tlearn: 0.0848254\ttotal: 7.32s\tremaining: 386ms\n",
      "950:\tlearn: 0.0848144\ttotal: 7.33s\tremaining: 378ms\n",
      "951:\tlearn: 0.0848036\ttotal: 7.34s\tremaining: 370ms\n",
      "952:\tlearn: 0.0847721\ttotal: 7.34s\tremaining: 362ms\n",
      "953:\tlearn: 0.0847634\ttotal: 7.35s\tremaining: 354ms\n",
      "954:\tlearn: 0.0847552\ttotal: 7.35s\tremaining: 346ms\n",
      "955:\tlearn: 0.0847253\ttotal: 7.36s\tremaining: 339ms\n",
      "956:\tlearn: 0.0847144\ttotal: 7.36s\tremaining: 331ms\n",
      "957:\tlearn: 0.0847042\ttotal: 7.37s\tremaining: 323ms\n",
      "958:\tlearn: 0.0846957\ttotal: 7.38s\tremaining: 315ms\n",
      "959:\tlearn: 0.0846865\ttotal: 7.38s\tremaining: 308ms\n",
      "960:\tlearn: 0.0846711\ttotal: 7.39s\tremaining: 300ms\n",
      "961:\tlearn: 0.0846609\ttotal: 7.39s\tremaining: 292ms\n",
      "962:\tlearn: 0.0846460\ttotal: 7.4s\tremaining: 284ms\n",
      "963:\tlearn: 0.0846290\ttotal: 7.41s\tremaining: 277ms\n",
      "964:\tlearn: 0.0846232\ttotal: 7.41s\tremaining: 269ms\n",
      "965:\tlearn: 0.0846174\ttotal: 7.42s\tremaining: 261ms\n",
      "966:\tlearn: 0.0846055\ttotal: 7.42s\tremaining: 253ms\n",
      "967:\tlearn: 0.0845937\ttotal: 7.43s\tremaining: 246ms\n",
      "968:\tlearn: 0.0845800\ttotal: 7.43s\tremaining: 238ms\n",
      "969:\tlearn: 0.0845690\ttotal: 7.44s\tremaining: 230ms\n",
      "970:\tlearn: 0.0845589\ttotal: 7.45s\tremaining: 222ms\n",
      "971:\tlearn: 0.0845448\ttotal: 7.45s\tremaining: 215ms\n",
      "972:\tlearn: 0.0845352\ttotal: 7.46s\tremaining: 207ms\n",
      "973:\tlearn: 0.0845270\ttotal: 7.46s\tremaining: 199ms\n",
      "974:\tlearn: 0.0845234\ttotal: 7.47s\tremaining: 192ms\n",
      "975:\tlearn: 0.0845168\ttotal: 7.48s\tremaining: 184ms\n",
      "976:\tlearn: 0.0845047\ttotal: 7.48s\tremaining: 176ms\n",
      "977:\tlearn: 0.0844955\ttotal: 7.49s\tremaining: 168ms\n",
      "978:\tlearn: 0.0844879\ttotal: 7.49s\tremaining: 161ms\n",
      "979:\tlearn: 0.0844756\ttotal: 7.5s\tremaining: 153ms\n",
      "980:\tlearn: 0.0844684\ttotal: 7.5s\tremaining: 145ms\n",
      "981:\tlearn: 0.0844629\ttotal: 7.51s\tremaining: 138ms\n",
      "982:\tlearn: 0.0844576\ttotal: 7.52s\tremaining: 130ms\n",
      "983:\tlearn: 0.0844490\ttotal: 7.53s\tremaining: 122ms\n",
      "984:\tlearn: 0.0844409\ttotal: 7.53s\tremaining: 115ms\n",
      "985:\tlearn: 0.0844270\ttotal: 7.54s\tremaining: 107ms\n",
      "986:\tlearn: 0.0844152\ttotal: 7.54s\tremaining: 99.3ms\n",
      "987:\tlearn: 0.0844094\ttotal: 7.55s\tremaining: 91.7ms\n",
      "988:\tlearn: 0.0843998\ttotal: 7.56s\tremaining: 84.1ms\n",
      "989:\tlearn: 0.0843728\ttotal: 7.57s\tremaining: 76.4ms\n",
      "990:\tlearn: 0.0843660\ttotal: 7.57s\tremaining: 68.8ms\n",
      "991:\tlearn: 0.0843589\ttotal: 7.58s\tremaining: 61.1ms\n",
      "992:\tlearn: 0.0843511\ttotal: 7.58s\tremaining: 53.5ms\n",
      "993:\tlearn: 0.0843415\ttotal: 7.59s\tremaining: 45.8ms\n",
      "994:\tlearn: 0.0843333\ttotal: 7.6s\tremaining: 38.2ms\n",
      "995:\tlearn: 0.0843283\ttotal: 7.61s\tremaining: 30.5ms\n",
      "996:\tlearn: 0.0843151\ttotal: 7.61s\tremaining: 22.9ms\n",
      "997:\tlearn: 0.0842989\ttotal: 7.62s\tremaining: 15.3ms\n",
      "998:\tlearn: 0.0842896\ttotal: 7.62s\tremaining: 7.63ms\n",
      "999:\tlearn: 0.0842825\ttotal: 7.63s\tremaining: 0us\n",
      "0:\tlearn: 0.4384688\ttotal: 4.15ms\tremaining: 4.14s\n",
      "1:\tlearn: 0.4278376\ttotal: 12.4ms\tremaining: 6.19s\n",
      "2:\tlearn: 0.4175165\ttotal: 22.2ms\tremaining: 7.39s\n",
      "3:\tlearn: 0.4076102\ttotal: 27.5ms\tremaining: 6.85s\n",
      "4:\tlearn: 0.3980056\ttotal: 33ms\tremaining: 6.58s\n",
      "5:\tlearn: 0.3887279\ttotal: 38.5ms\tremaining: 6.38s\n",
      "6:\tlearn: 0.3795599\ttotal: 44.4ms\tremaining: 6.29s\n",
      "7:\tlearn: 0.3710484\ttotal: 49.8ms\tremaining: 6.18s\n",
      "8:\tlearn: 0.3626121\ttotal: 59ms\tremaining: 6.5s\n",
      "9:\tlearn: 0.3546003\ttotal: 66.6ms\tremaining: 6.6s\n",
      "10:\tlearn: 0.3468823\ttotal: 72.7ms\tremaining: 6.53s\n",
      "11:\tlearn: 0.3393193\ttotal: 79.9ms\tremaining: 6.58s\n",
      "12:\tlearn: 0.3318386\ttotal: 85.5ms\tremaining: 6.49s\n",
      "13:\tlearn: 0.3248020\ttotal: 90.9ms\tremaining: 6.4s\n",
      "14:\tlearn: 0.3181690\ttotal: 97.6ms\tremaining: 6.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:\tlearn: 0.3112177\ttotal: 107ms\tremaining: 6.57s\n",
      "16:\tlearn: 0.3048908\ttotal: 113ms\tremaining: 6.51s\n",
      "17:\tlearn: 0.2985039\ttotal: 117ms\tremaining: 6.4s\n",
      "18:\tlearn: 0.2925707\ttotal: 123ms\tremaining: 6.33s\n",
      "19:\tlearn: 0.2868378\ttotal: 128ms\tremaining: 6.29s\n",
      "20:\tlearn: 0.2812698\ttotal: 134ms\tremaining: 6.24s\n",
      "21:\tlearn: 0.2759224\ttotal: 143ms\tremaining: 6.37s\n",
      "22:\tlearn: 0.2702797\ttotal: 152ms\tremaining: 6.46s\n",
      "23:\tlearn: 0.2650901\ttotal: 157ms\tremaining: 6.4s\n",
      "24:\tlearn: 0.2600141\ttotal: 164ms\tremaining: 6.39s\n",
      "25:\tlearn: 0.2554404\ttotal: 169ms\tremaining: 6.33s\n",
      "26:\tlearn: 0.2508557\ttotal: 175ms\tremaining: 6.3s\n",
      "27:\tlearn: 0.2462296\ttotal: 182ms\tremaining: 6.31s\n",
      "28:\tlearn: 0.2420949\ttotal: 196ms\tremaining: 6.58s\n",
      "29:\tlearn: 0.2380778\ttotal: 202ms\tremaining: 6.53s\n",
      "30:\tlearn: 0.2337253\ttotal: 208ms\tremaining: 6.49s\n",
      "31:\tlearn: 0.2296541\ttotal: 214ms\tremaining: 6.47s\n",
      "32:\tlearn: 0.2256442\ttotal: 220ms\tremaining: 6.43s\n",
      "33:\tlearn: 0.2220030\ttotal: 229ms\tremaining: 6.5s\n",
      "34:\tlearn: 0.2186555\ttotal: 238ms\tremaining: 6.55s\n",
      "35:\tlearn: 0.2151325\ttotal: 243ms\tremaining: 6.5s\n",
      "36:\tlearn: 0.2117166\ttotal: 249ms\tremaining: 6.48s\n",
      "37:\tlearn: 0.2083830\ttotal: 254ms\tremaining: 6.43s\n",
      "38:\tlearn: 0.2053727\ttotal: 260ms\tremaining: 6.42s\n",
      "39:\tlearn: 0.2023710\ttotal: 267ms\tremaining: 6.42s\n",
      "40:\tlearn: 0.1996753\ttotal: 289ms\tremaining: 6.77s\n",
      "41:\tlearn: 0.1970411\ttotal: 331ms\tremaining: 7.54s\n",
      "42:\tlearn: 0.1946303\ttotal: 336ms\tremaining: 7.48s\n",
      "43:\tlearn: 0.1919767\ttotal: 342ms\tremaining: 7.42s\n",
      "44:\tlearn: 0.1895635\ttotal: 347ms\tremaining: 7.36s\n",
      "45:\tlearn: 0.1873740\ttotal: 352ms\tremaining: 7.3s\n",
      "46:\tlearn: 0.1851049\ttotal: 357ms\tremaining: 7.25s\n",
      "47:\tlearn: 0.1829735\ttotal: 370ms\tremaining: 7.34s\n",
      "48:\tlearn: 0.1809776\ttotal: 378ms\tremaining: 7.33s\n",
      "49:\tlearn: 0.1787470\ttotal: 388ms\tremaining: 7.36s\n",
      "50:\tlearn: 0.1767914\ttotal: 400ms\tremaining: 7.45s\n",
      "51:\tlearn: 0.1749951\ttotal: 411ms\tremaining: 7.5s\n",
      "52:\tlearn: 0.1732926\ttotal: 424ms\tremaining: 7.58s\n",
      "53:\tlearn: 0.1713354\ttotal: 435ms\tremaining: 7.62s\n",
      "54:\tlearn: 0.1696931\ttotal: 445ms\tremaining: 7.64s\n",
      "55:\tlearn: 0.1682156\ttotal: 464ms\tremaining: 7.81s\n",
      "56:\tlearn: 0.1667381\ttotal: 475ms\tremaining: 7.86s\n",
      "57:\tlearn: 0.1651201\ttotal: 485ms\tremaining: 7.87s\n",
      "58:\tlearn: 0.1636277\ttotal: 491ms\tremaining: 7.83s\n",
      "59:\tlearn: 0.1622027\ttotal: 499ms\tremaining: 7.81s\n",
      "60:\tlearn: 0.1609023\ttotal: 506ms\tremaining: 7.79s\n",
      "61:\tlearn: 0.1596372\ttotal: 511ms\tremaining: 7.74s\n",
      "62:\tlearn: 0.1583726\ttotal: 517ms\tremaining: 7.68s\n",
      "63:\tlearn: 0.1571023\ttotal: 522ms\tremaining: 7.64s\n",
      "64:\tlearn: 0.1557121\ttotal: 527ms\tremaining: 7.59s\n",
      "65:\tlearn: 0.1546105\ttotal: 533ms\tremaining: 7.54s\n",
      "66:\tlearn: 0.1531850\ttotal: 539ms\tremaining: 7.51s\n",
      "67:\tlearn: 0.1521240\ttotal: 545ms\tremaining: 7.47s\n",
      "68:\tlearn: 0.1511215\ttotal: 550ms\tremaining: 7.42s\n",
      "69:\tlearn: 0.1501826\ttotal: 555ms\tremaining: 7.38s\n",
      "70:\tlearn: 0.1490719\ttotal: 560ms\tremaining: 7.33s\n",
      "71:\tlearn: 0.1482218\ttotal: 566ms\tremaining: 7.29s\n",
      "72:\tlearn: 0.1473600\ttotal: 573ms\tremaining: 7.27s\n",
      "73:\tlearn: 0.1463674\ttotal: 578ms\tremaining: 7.23s\n",
      "74:\tlearn: 0.1455458\ttotal: 584ms\tremaining: 7.2s\n",
      "75:\tlearn: 0.1446694\ttotal: 594ms\tremaining: 7.23s\n",
      "76:\tlearn: 0.1439793\ttotal: 603ms\tremaining: 7.23s\n",
      "77:\tlearn: 0.1431408\ttotal: 611ms\tremaining: 7.22s\n",
      "78:\tlearn: 0.1423606\ttotal: 618ms\tremaining: 7.2s\n",
      "79:\tlearn: 0.1417035\ttotal: 624ms\tremaining: 7.17s\n",
      "80:\tlearn: 0.1410508\ttotal: 630ms\tremaining: 7.15s\n",
      "81:\tlearn: 0.1404654\ttotal: 637ms\tremaining: 7.13s\n",
      "82:\tlearn: 0.1398454\ttotal: 642ms\tremaining: 7.09s\n",
      "83:\tlearn: 0.1391525\ttotal: 647ms\tremaining: 7.05s\n",
      "84:\tlearn: 0.1385920\ttotal: 653ms\tremaining: 7.03s\n",
      "85:\tlearn: 0.1380561\ttotal: 664ms\tremaining: 7.05s\n",
      "86:\tlearn: 0.1374621\ttotal: 685ms\tremaining: 7.19s\n",
      "87:\tlearn: 0.1370200\ttotal: 695ms\tremaining: 7.21s\n",
      "88:\tlearn: 0.1363009\ttotal: 703ms\tremaining: 7.2s\n",
      "89:\tlearn: 0.1356482\ttotal: 709ms\tremaining: 7.17s\n",
      "90:\tlearn: 0.1349529\ttotal: 716ms\tremaining: 7.15s\n",
      "91:\tlearn: 0.1342413\ttotal: 726ms\tremaining: 7.16s\n",
      "92:\tlearn: 0.1336908\ttotal: 731ms\tremaining: 7.13s\n",
      "93:\tlearn: 0.1332572\ttotal: 737ms\tremaining: 7.11s\n",
      "94:\tlearn: 0.1326968\ttotal: 742ms\tremaining: 7.07s\n",
      "95:\tlearn: 0.1323150\ttotal: 748ms\tremaining: 7.05s\n",
      "96:\tlearn: 0.1319645\ttotal: 753ms\tremaining: 7.01s\n",
      "97:\tlearn: 0.1315591\ttotal: 758ms\tremaining: 6.98s\n",
      "98:\tlearn: 0.1310375\ttotal: 763ms\tremaining: 6.95s\n",
      "99:\tlearn: 0.1305396\ttotal: 769ms\tremaining: 6.92s\n",
      "100:\tlearn: 0.1301772\ttotal: 775ms\tremaining: 6.89s\n",
      "101:\tlearn: 0.1298290\ttotal: 780ms\tremaining: 6.87s\n",
      "102:\tlearn: 0.1293436\ttotal: 785ms\tremaining: 6.84s\n",
      "103:\tlearn: 0.1290035\ttotal: 790ms\tremaining: 6.81s\n",
      "104:\tlearn: 0.1286663\ttotal: 799ms\tremaining: 6.81s\n",
      "105:\tlearn: 0.1282634\ttotal: 805ms\tremaining: 6.79s\n",
      "106:\tlearn: 0.1277982\ttotal: 810ms\tremaining: 6.76s\n",
      "107:\tlearn: 0.1275096\ttotal: 815ms\tremaining: 6.73s\n",
      "108:\tlearn: 0.1271405\ttotal: 821ms\tremaining: 6.71s\n",
      "109:\tlearn: 0.1267248\ttotal: 826ms\tremaining: 6.68s\n",
      "110:\tlearn: 0.1264439\ttotal: 832ms\tremaining: 6.66s\n",
      "111:\tlearn: 0.1260181\ttotal: 838ms\tremaining: 6.64s\n",
      "112:\tlearn: 0.1257073\ttotal: 843ms\tremaining: 6.61s\n",
      "113:\tlearn: 0.1253692\ttotal: 848ms\tremaining: 6.59s\n",
      "114:\tlearn: 0.1250321\ttotal: 859ms\tremaining: 6.61s\n",
      "115:\tlearn: 0.1247832\ttotal: 865ms\tremaining: 6.59s\n",
      "116:\tlearn: 0.1245590\ttotal: 872ms\tremaining: 6.58s\n",
      "117:\tlearn: 0.1239867\ttotal: 877ms\tremaining: 6.55s\n",
      "118:\tlearn: 0.1235915\ttotal: 883ms\tremaining: 6.54s\n",
      "119:\tlearn: 0.1233848\ttotal: 888ms\tremaining: 6.51s\n",
      "120:\tlearn: 0.1230319\ttotal: 893ms\tremaining: 6.49s\n",
      "121:\tlearn: 0.1228329\ttotal: 899ms\tremaining: 6.47s\n",
      "122:\tlearn: 0.1224713\ttotal: 904ms\tremaining: 6.45s\n",
      "123:\tlearn: 0.1221037\ttotal: 910ms\tremaining: 6.43s\n",
      "124:\tlearn: 0.1218200\ttotal: 915ms\tremaining: 6.41s\n",
      "125:\tlearn: 0.1216575\ttotal: 921ms\tremaining: 6.39s\n",
      "126:\tlearn: 0.1214752\ttotal: 926ms\tremaining: 6.37s\n",
      "127:\tlearn: 0.1211823\ttotal: 932ms\tremaining: 6.35s\n",
      "128:\tlearn: 0.1209182\ttotal: 937ms\tremaining: 6.33s\n",
      "129:\tlearn: 0.1206706\ttotal: 943ms\tremaining: 6.31s\n",
      "130:\tlearn: 0.1203184\ttotal: 948ms\tremaining: 6.29s\n",
      "131:\tlearn: 0.1199866\ttotal: 954ms\tremaining: 6.27s\n",
      "132:\tlearn: 0.1195624\ttotal: 960ms\tremaining: 6.26s\n",
      "133:\tlearn: 0.1192421\ttotal: 969ms\tremaining: 6.26s\n",
      "134:\tlearn: 0.1190261\ttotal: 975ms\tremaining: 6.25s\n",
      "135:\tlearn: 0.1188477\ttotal: 980ms\tremaining: 6.22s\n",
      "136:\tlearn: 0.1186774\ttotal: 985ms\tremaining: 6.21s\n",
      "137:\tlearn: 0.1185311\ttotal: 990ms\tremaining: 6.19s\n",
      "138:\tlearn: 0.1183982\ttotal: 996ms\tremaining: 6.17s\n",
      "139:\tlearn: 0.1182710\ttotal: 1s\tremaining: 6.15s\n",
      "140:\tlearn: 0.1181565\ttotal: 1.01s\tremaining: 6.13s\n",
      "141:\tlearn: 0.1177991\ttotal: 1.01s\tremaining: 6.12s\n",
      "142:\tlearn: 0.1176490\ttotal: 1.02s\tremaining: 6.1s\n",
      "143:\tlearn: 0.1175422\ttotal: 1.02s\tremaining: 6.08s\n",
      "144:\tlearn: 0.1172633\ttotal: 1.03s\tremaining: 6.06s\n",
      "145:\tlearn: 0.1171505\ttotal: 1.03s\tremaining: 6.04s\n",
      "146:\tlearn: 0.1169753\ttotal: 1.04s\tremaining: 6.03s\n",
      "147:\tlearn: 0.1167420\ttotal: 1.04s\tremaining: 6.01s\n",
      "148:\tlearn: 0.1164264\ttotal: 1.05s\tremaining: 6.01s\n",
      "149:\tlearn: 0.1161547\ttotal: 1.06s\tremaining: 5.99s\n",
      "150:\tlearn: 0.1159294\ttotal: 1.06s\tremaining: 5.98s\n",
      "151:\tlearn: 0.1157364\ttotal: 1.07s\tremaining: 5.96s\n",
      "152:\tlearn: 0.1155257\ttotal: 1.07s\tremaining: 5.94s\n",
      "153:\tlearn: 0.1153813\ttotal: 1.08s\tremaining: 5.93s\n",
      "154:\tlearn: 0.1152092\ttotal: 1.08s\tremaining: 5.91s\n",
      "155:\tlearn: 0.1149899\ttotal: 1.09s\tremaining: 5.9s\n",
      "156:\tlearn: 0.1148676\ttotal: 1.1s\tremaining: 5.88s\n",
      "157:\tlearn: 0.1146028\ttotal: 1.1s\tremaining: 5.89s\n",
      "158:\tlearn: 0.1144819\ttotal: 1.11s\tremaining: 5.88s\n",
      "159:\tlearn: 0.1143553\ttotal: 1.12s\tremaining: 5.86s\n",
      "160:\tlearn: 0.1142642\ttotal: 1.12s\tremaining: 5.85s\n",
      "161:\tlearn: 0.1141441\ttotal: 1.13s\tremaining: 5.83s\n",
      "162:\tlearn: 0.1140627\ttotal: 1.13s\tremaining: 5.82s\n",
      "163:\tlearn: 0.1138653\ttotal: 1.14s\tremaining: 5.81s\n",
      "164:\tlearn: 0.1135126\ttotal: 1.15s\tremaining: 5.81s\n",
      "165:\tlearn: 0.1133317\ttotal: 1.15s\tremaining: 5.8s\n",
      "166:\tlearn: 0.1131147\ttotal: 1.16s\tremaining: 5.79s\n",
      "167:\tlearn: 0.1130317\ttotal: 1.17s\tremaining: 5.77s\n",
      "168:\tlearn: 0.1127945\ttotal: 1.17s\tremaining: 5.75s\n",
      "169:\tlearn: 0.1127056\ttotal: 1.18s\tremaining: 5.74s\n",
      "170:\tlearn: 0.1124690\ttotal: 1.18s\tremaining: 5.74s\n",
      "171:\tlearn: 0.1123056\ttotal: 1.19s\tremaining: 5.73s\n",
      "172:\tlearn: 0.1121491\ttotal: 1.2s\tremaining: 5.72s\n",
      "173:\tlearn: 0.1120599\ttotal: 1.2s\tremaining: 5.71s\n",
      "174:\tlearn: 0.1119694\ttotal: 1.21s\tremaining: 5.69s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175:\tlearn: 0.1117704\ttotal: 1.21s\tremaining: 5.68s\n",
      "176:\tlearn: 0.1116433\ttotal: 1.22s\tremaining: 5.67s\n",
      "177:\tlearn: 0.1113862\ttotal: 1.23s\tremaining: 5.67s\n",
      "178:\tlearn: 0.1112647\ttotal: 1.25s\tremaining: 5.71s\n",
      "179:\tlearn: 0.1111812\ttotal: 1.25s\tremaining: 5.72s\n",
      "180:\tlearn: 0.1110142\ttotal: 1.26s\tremaining: 5.71s\n",
      "181:\tlearn: 0.1109155\ttotal: 1.27s\tremaining: 5.7s\n",
      "182:\tlearn: 0.1108165\ttotal: 1.28s\tremaining: 5.72s\n",
      "183:\tlearn: 0.1107307\ttotal: 1.29s\tremaining: 5.71s\n",
      "184:\tlearn: 0.1106509\ttotal: 1.29s\tremaining: 5.7s\n",
      "185:\tlearn: 0.1105320\ttotal: 1.3s\tremaining: 5.68s\n",
      "186:\tlearn: 0.1104164\ttotal: 1.3s\tremaining: 5.67s\n",
      "187:\tlearn: 0.1102445\ttotal: 1.31s\tremaining: 5.65s\n",
      "188:\tlearn: 0.1101062\ttotal: 1.32s\tremaining: 5.65s\n",
      "189:\tlearn: 0.1099832\ttotal: 1.32s\tremaining: 5.65s\n",
      "190:\tlearn: 0.1099230\ttotal: 1.33s\tremaining: 5.63s\n",
      "191:\tlearn: 0.1097329\ttotal: 1.33s\tremaining: 5.62s\n",
      "192:\tlearn: 0.1095611\ttotal: 1.34s\tremaining: 5.61s\n",
      "193:\tlearn: 0.1094391\ttotal: 1.35s\tremaining: 5.62s\n",
      "194:\tlearn: 0.1093670\ttotal: 1.37s\tremaining: 5.67s\n",
      "195:\tlearn: 0.1092356\ttotal: 1.38s\tremaining: 5.67s\n",
      "196:\tlearn: 0.1091761\ttotal: 1.4s\tremaining: 5.69s\n",
      "197:\tlearn: 0.1090658\ttotal: 1.42s\tremaining: 5.73s\n",
      "198:\tlearn: 0.1089853\ttotal: 1.42s\tremaining: 5.73s\n",
      "199:\tlearn: 0.1089220\ttotal: 1.44s\tremaining: 5.76s\n",
      "200:\tlearn: 0.1087844\ttotal: 1.45s\tremaining: 5.77s\n",
      "201:\tlearn: 0.1086839\ttotal: 1.46s\tremaining: 5.78s\n",
      "202:\tlearn: 0.1086147\ttotal: 1.47s\tremaining: 5.77s\n",
      "203:\tlearn: 0.1085621\ttotal: 1.47s\tremaining: 5.75s\n",
      "204:\tlearn: 0.1084081\ttotal: 1.48s\tremaining: 5.74s\n",
      "205:\tlearn: 0.1083314\ttotal: 1.49s\tremaining: 5.72s\n",
      "206:\tlearn: 0.1082516\ttotal: 1.49s\tremaining: 5.72s\n",
      "207:\tlearn: 0.1081851\ttotal: 1.5s\tremaining: 5.72s\n",
      "208:\tlearn: 0.1081276\ttotal: 1.51s\tremaining: 5.71s\n",
      "209:\tlearn: 0.1080557\ttotal: 1.51s\tremaining: 5.7s\n",
      "210:\tlearn: 0.1078828\ttotal: 1.52s\tremaining: 5.69s\n",
      "211:\tlearn: 0.1077149\ttotal: 1.52s\tremaining: 5.67s\n",
      "212:\tlearn: 0.1075915\ttotal: 1.53s\tremaining: 5.66s\n",
      "213:\tlearn: 0.1075508\ttotal: 1.54s\tremaining: 5.66s\n",
      "214:\tlearn: 0.1075018\ttotal: 1.55s\tremaining: 5.65s\n",
      "215:\tlearn: 0.1074370\ttotal: 1.55s\tremaining: 5.63s\n",
      "216:\tlearn: 0.1073707\ttotal: 1.56s\tremaining: 5.62s\n",
      "217:\tlearn: 0.1073055\ttotal: 1.56s\tremaining: 5.61s\n",
      "218:\tlearn: 0.1071699\ttotal: 1.57s\tremaining: 5.6s\n",
      "219:\tlearn: 0.1070461\ttotal: 1.57s\tremaining: 5.59s\n",
      "220:\tlearn: 0.1069588\ttotal: 1.58s\tremaining: 5.58s\n",
      "221:\tlearn: 0.1068991\ttotal: 1.59s\tremaining: 5.58s\n",
      "222:\tlearn: 0.1068361\ttotal: 1.6s\tremaining: 5.56s\n",
      "223:\tlearn: 0.1067021\ttotal: 1.6s\tremaining: 5.55s\n",
      "224:\tlearn: 0.1066174\ttotal: 1.61s\tremaining: 5.54s\n",
      "225:\tlearn: 0.1064810\ttotal: 1.61s\tremaining: 5.52s\n",
      "226:\tlearn: 0.1063704\ttotal: 1.62s\tremaining: 5.51s\n",
      "227:\tlearn: 0.1062647\ttotal: 1.63s\tremaining: 5.51s\n",
      "228:\tlearn: 0.1061678\ttotal: 1.63s\tremaining: 5.5s\n",
      "229:\tlearn: 0.1061322\ttotal: 1.64s\tremaining: 5.48s\n",
      "230:\tlearn: 0.1060149\ttotal: 1.64s\tremaining: 5.48s\n",
      "231:\tlearn: 0.1058834\ttotal: 1.65s\tremaining: 5.47s\n",
      "232:\tlearn: 0.1058006\ttotal: 1.66s\tremaining: 5.46s\n",
      "233:\tlearn: 0.1056979\ttotal: 1.67s\tremaining: 5.46s\n",
      "234:\tlearn: 0.1055330\ttotal: 1.67s\tremaining: 5.45s\n",
      "235:\tlearn: 0.1054199\ttotal: 1.68s\tremaining: 5.44s\n",
      "236:\tlearn: 0.1053570\ttotal: 1.69s\tremaining: 5.43s\n",
      "237:\tlearn: 0.1052035\ttotal: 1.69s\tremaining: 5.42s\n",
      "238:\tlearn: 0.1051132\ttotal: 1.7s\tremaining: 5.4s\n",
      "239:\tlearn: 0.1049897\ttotal: 1.7s\tremaining: 5.39s\n",
      "240:\tlearn: 0.1048971\ttotal: 1.71s\tremaining: 5.39s\n",
      "241:\tlearn: 0.1048314\ttotal: 1.72s\tremaining: 5.38s\n",
      "242:\tlearn: 0.1047935\ttotal: 1.72s\tremaining: 5.37s\n",
      "243:\tlearn: 0.1046429\ttotal: 1.73s\tremaining: 5.36s\n",
      "244:\tlearn: 0.1045858\ttotal: 1.74s\tremaining: 5.35s\n",
      "245:\tlearn: 0.1044717\ttotal: 1.74s\tremaining: 5.34s\n",
      "246:\tlearn: 0.1044309\ttotal: 1.75s\tremaining: 5.34s\n",
      "247:\tlearn: 0.1043630\ttotal: 1.76s\tremaining: 5.34s\n",
      "248:\tlearn: 0.1043052\ttotal: 1.77s\tremaining: 5.33s\n",
      "249:\tlearn: 0.1042827\ttotal: 1.77s\tremaining: 5.32s\n",
      "250:\tlearn: 0.1042587\ttotal: 1.78s\tremaining: 5.3s\n",
      "251:\tlearn: 0.1041374\ttotal: 1.78s\tremaining: 5.29s\n",
      "252:\tlearn: 0.1040327\ttotal: 1.79s\tremaining: 5.28s\n",
      "253:\tlearn: 0.1039663\ttotal: 1.8s\tremaining: 5.28s\n",
      "254:\tlearn: 0.1039220\ttotal: 1.8s\tremaining: 5.27s\n",
      "255:\tlearn: 0.1038863\ttotal: 1.81s\tremaining: 5.26s\n",
      "256:\tlearn: 0.1038568\ttotal: 1.81s\tremaining: 5.25s\n",
      "257:\tlearn: 0.1038094\ttotal: 1.82s\tremaining: 5.24s\n",
      "258:\tlearn: 0.1037047\ttotal: 1.82s\tremaining: 5.22s\n",
      "259:\tlearn: 0.1036583\ttotal: 1.83s\tremaining: 5.21s\n",
      "260:\tlearn: 0.1035589\ttotal: 1.84s\tremaining: 5.2s\n",
      "261:\tlearn: 0.1034491\ttotal: 1.84s\tremaining: 5.2s\n",
      "262:\tlearn: 0.1034163\ttotal: 1.85s\tremaining: 5.19s\n",
      "263:\tlearn: 0.1033480\ttotal: 1.86s\tremaining: 5.18s\n",
      "264:\tlearn: 0.1033067\ttotal: 1.86s\tremaining: 5.16s\n",
      "265:\tlearn: 0.1032188\ttotal: 1.87s\tremaining: 5.15s\n",
      "266:\tlearn: 0.1031336\ttotal: 1.87s\tremaining: 5.14s\n",
      "267:\tlearn: 0.1030445\ttotal: 1.88s\tremaining: 5.13s\n",
      "268:\tlearn: 0.1029194\ttotal: 1.89s\tremaining: 5.13s\n",
      "269:\tlearn: 0.1028893\ttotal: 1.89s\tremaining: 5.12s\n",
      "270:\tlearn: 0.1028488\ttotal: 1.9s\tremaining: 5.11s\n",
      "271:\tlearn: 0.1027947\ttotal: 1.9s\tremaining: 5.09s\n",
      "272:\tlearn: 0.1027585\ttotal: 1.91s\tremaining: 5.09s\n",
      "273:\tlearn: 0.1027343\ttotal: 1.92s\tremaining: 5.08s\n",
      "274:\tlearn: 0.1027058\ttotal: 1.92s\tremaining: 5.07s\n",
      "275:\tlearn: 0.1026323\ttotal: 1.93s\tremaining: 5.06s\n",
      "276:\tlearn: 0.1025661\ttotal: 1.93s\tremaining: 5.05s\n",
      "277:\tlearn: 0.1025171\ttotal: 1.94s\tremaining: 5.04s\n",
      "278:\tlearn: 0.1024668\ttotal: 1.95s\tremaining: 5.03s\n",
      "279:\tlearn: 0.1023411\ttotal: 1.95s\tremaining: 5.02s\n",
      "280:\tlearn: 0.1022565\ttotal: 1.96s\tremaining: 5.01s\n",
      "281:\tlearn: 0.1022147\ttotal: 1.96s\tremaining: 5s\n",
      "282:\tlearn: 0.1021761\ttotal: 1.97s\tremaining: 4.99s\n",
      "283:\tlearn: 0.1020794\ttotal: 1.97s\tremaining: 4.98s\n",
      "284:\tlearn: 0.1019545\ttotal: 1.98s\tremaining: 4.97s\n",
      "285:\tlearn: 0.1019087\ttotal: 1.99s\tremaining: 4.96s\n",
      "286:\tlearn: 0.1018502\ttotal: 1.99s\tremaining: 4.95s\n",
      "287:\tlearn: 0.1017798\ttotal: 2s\tremaining: 4.94s\n",
      "288:\tlearn: 0.1017538\ttotal: 2s\tremaining: 4.92s\n",
      "289:\tlearn: 0.1016855\ttotal: 2.01s\tremaining: 4.92s\n",
      "290:\tlearn: 0.1016274\ttotal: 2.02s\tremaining: 4.91s\n",
      "291:\tlearn: 0.1015320\ttotal: 2.02s\tremaining: 4.9s\n",
      "292:\tlearn: 0.1014766\ttotal: 2.03s\tremaining: 4.89s\n",
      "293:\tlearn: 0.1013880\ttotal: 2.03s\tremaining: 4.88s\n",
      "294:\tlearn: 0.1013422\ttotal: 2.04s\tremaining: 4.88s\n",
      "295:\tlearn: 0.1012542\ttotal: 2.05s\tremaining: 4.87s\n",
      "296:\tlearn: 0.1011986\ttotal: 2.06s\tremaining: 4.87s\n",
      "297:\tlearn: 0.1011536\ttotal: 2.06s\tremaining: 4.86s\n",
      "298:\tlearn: 0.1010879\ttotal: 2.07s\tremaining: 4.86s\n",
      "299:\tlearn: 0.1009751\ttotal: 2.08s\tremaining: 4.85s\n",
      "300:\tlearn: 0.1009117\ttotal: 2.08s\tremaining: 4.83s\n",
      "301:\tlearn: 0.1008355\ttotal: 2.09s\tremaining: 4.82s\n",
      "302:\tlearn: 0.1007337\ttotal: 2.09s\tremaining: 4.81s\n",
      "303:\tlearn: 0.1006724\ttotal: 2.1s\tremaining: 4.8s\n",
      "304:\tlearn: 0.1006427\ttotal: 2.1s\tremaining: 4.79s\n",
      "305:\tlearn: 0.1006019\ttotal: 2.11s\tremaining: 4.78s\n",
      "306:\tlearn: 0.1005371\ttotal: 2.12s\tremaining: 4.77s\n",
      "307:\tlearn: 0.1004559\ttotal: 2.12s\tremaining: 4.76s\n",
      "308:\tlearn: 0.1004114\ttotal: 2.13s\tremaining: 4.76s\n",
      "309:\tlearn: 0.1003510\ttotal: 2.13s\tremaining: 4.75s\n",
      "310:\tlearn: 0.1003162\ttotal: 2.14s\tremaining: 4.74s\n",
      "311:\tlearn: 0.1002285\ttotal: 2.15s\tremaining: 4.74s\n",
      "312:\tlearn: 0.1001137\ttotal: 2.15s\tremaining: 4.72s\n",
      "313:\tlearn: 0.1000086\ttotal: 2.16s\tremaining: 4.71s\n",
      "314:\tlearn: 0.0999658\ttotal: 2.16s\tremaining: 4.7s\n",
      "315:\tlearn: 0.0999253\ttotal: 2.17s\tremaining: 4.69s\n",
      "316:\tlearn: 0.0998219\ttotal: 2.17s\tremaining: 4.68s\n",
      "317:\tlearn: 0.0997493\ttotal: 2.18s\tremaining: 4.68s\n",
      "318:\tlearn: 0.0997291\ttotal: 2.19s\tremaining: 4.67s\n",
      "319:\tlearn: 0.0996290\ttotal: 2.19s\tremaining: 4.66s\n",
      "320:\tlearn: 0.0995753\ttotal: 2.2s\tremaining: 4.65s\n",
      "321:\tlearn: 0.0995209\ttotal: 2.21s\tremaining: 4.64s\n",
      "322:\tlearn: 0.0994691\ttotal: 2.21s\tremaining: 4.63s\n",
      "323:\tlearn: 0.0994059\ttotal: 2.22s\tremaining: 4.63s\n",
      "324:\tlearn: 0.0993602\ttotal: 2.22s\tremaining: 4.62s\n",
      "325:\tlearn: 0.0993352\ttotal: 2.23s\tremaining: 4.61s\n",
      "326:\tlearn: 0.0992652\ttotal: 2.24s\tremaining: 4.6s\n",
      "327:\tlearn: 0.0992229\ttotal: 2.24s\tremaining: 4.59s\n",
      "328:\tlearn: 0.0991546\ttotal: 2.25s\tremaining: 4.58s\n",
      "329:\tlearn: 0.0991180\ttotal: 2.25s\tremaining: 4.57s\n",
      "330:\tlearn: 0.0990859\ttotal: 2.26s\tremaining: 4.56s\n",
      "331:\tlearn: 0.0990039\ttotal: 2.26s\tremaining: 4.55s\n",
      "332:\tlearn: 0.0989473\ttotal: 2.27s\tremaining: 4.54s\n",
      "333:\tlearn: 0.0989020\ttotal: 2.27s\tremaining: 4.54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334:\tlearn: 0.0988590\ttotal: 2.28s\tremaining: 4.53s\n",
      "335:\tlearn: 0.0988429\ttotal: 2.29s\tremaining: 4.51s\n",
      "336:\tlearn: 0.0988037\ttotal: 2.29s\tremaining: 4.51s\n",
      "337:\tlearn: 0.0987447\ttotal: 2.3s\tremaining: 4.51s\n",
      "338:\tlearn: 0.0986879\ttotal: 2.31s\tremaining: 4.5s\n",
      "339:\tlearn: 0.0985970\ttotal: 2.34s\tremaining: 4.54s\n",
      "340:\tlearn: 0.0985367\ttotal: 2.35s\tremaining: 4.53s\n",
      "341:\tlearn: 0.0984806\ttotal: 2.36s\tremaining: 4.55s\n",
      "342:\tlearn: 0.0983881\ttotal: 2.37s\tremaining: 4.55s\n",
      "343:\tlearn: 0.0983579\ttotal: 2.38s\tremaining: 4.55s\n",
      "344:\tlearn: 0.0983200\ttotal: 2.39s\tremaining: 4.54s\n",
      "345:\tlearn: 0.0982573\ttotal: 2.41s\tremaining: 4.55s\n",
      "346:\tlearn: 0.0982182\ttotal: 2.42s\tremaining: 4.55s\n",
      "347:\tlearn: 0.0981728\ttotal: 2.42s\tremaining: 4.54s\n",
      "348:\tlearn: 0.0981526\ttotal: 2.43s\tremaining: 4.53s\n",
      "349:\tlearn: 0.0981091\ttotal: 2.43s\tremaining: 4.52s\n",
      "350:\tlearn: 0.0980711\ttotal: 2.44s\tremaining: 4.51s\n",
      "351:\tlearn: 0.0980284\ttotal: 2.44s\tremaining: 4.5s\n",
      "352:\tlearn: 0.0979711\ttotal: 2.45s\tremaining: 4.49s\n",
      "353:\tlearn: 0.0979297\ttotal: 2.45s\tremaining: 4.48s\n",
      "354:\tlearn: 0.0979079\ttotal: 2.46s\tremaining: 4.47s\n",
      "355:\tlearn: 0.0978496\ttotal: 2.46s\tremaining: 4.46s\n",
      "356:\tlearn: 0.0977933\ttotal: 2.47s\tremaining: 4.45s\n",
      "357:\tlearn: 0.0977293\ttotal: 2.49s\tremaining: 4.46s\n",
      "358:\tlearn: 0.0976990\ttotal: 2.49s\tremaining: 4.45s\n",
      "359:\tlearn: 0.0976469\ttotal: 2.5s\tremaining: 4.44s\n",
      "360:\tlearn: 0.0976013\ttotal: 2.5s\tremaining: 4.43s\n",
      "361:\tlearn: 0.0975675\ttotal: 2.51s\tremaining: 4.42s\n",
      "362:\tlearn: 0.0975199\ttotal: 2.52s\tremaining: 4.41s\n",
      "363:\tlearn: 0.0974820\ttotal: 2.52s\tremaining: 4.4s\n",
      "364:\tlearn: 0.0974430\ttotal: 2.53s\tremaining: 4.39s\n",
      "365:\tlearn: 0.0973934\ttotal: 2.53s\tremaining: 4.38s\n",
      "366:\tlearn: 0.0972921\ttotal: 2.54s\tremaining: 4.37s\n",
      "367:\tlearn: 0.0972559\ttotal: 2.54s\tremaining: 4.37s\n",
      "368:\tlearn: 0.0972158\ttotal: 2.55s\tremaining: 4.36s\n",
      "369:\tlearn: 0.0971816\ttotal: 2.55s\tremaining: 4.34s\n",
      "370:\tlearn: 0.0971575\ttotal: 2.56s\tremaining: 4.34s\n",
      "371:\tlearn: 0.0970939\ttotal: 2.56s\tremaining: 4.33s\n",
      "372:\tlearn: 0.0970572\ttotal: 2.57s\tremaining: 4.32s\n",
      "373:\tlearn: 0.0969724\ttotal: 2.58s\tremaining: 4.31s\n",
      "374:\tlearn: 0.0969332\ttotal: 2.58s\tremaining: 4.3s\n",
      "375:\tlearn: 0.0968772\ttotal: 2.59s\tremaining: 4.3s\n",
      "376:\tlearn: 0.0968439\ttotal: 2.59s\tremaining: 4.29s\n",
      "377:\tlearn: 0.0968067\ttotal: 2.6s\tremaining: 4.28s\n",
      "378:\tlearn: 0.0967544\ttotal: 2.61s\tremaining: 4.27s\n",
      "379:\tlearn: 0.0966887\ttotal: 2.62s\tremaining: 4.27s\n",
      "380:\tlearn: 0.0966647\ttotal: 2.63s\tremaining: 4.27s\n",
      "381:\tlearn: 0.0966357\ttotal: 2.64s\tremaining: 4.27s\n",
      "382:\tlearn: 0.0965700\ttotal: 2.65s\tremaining: 4.27s\n",
      "383:\tlearn: 0.0965304\ttotal: 2.66s\tremaining: 4.27s\n",
      "384:\tlearn: 0.0965099\ttotal: 2.67s\tremaining: 4.27s\n",
      "385:\tlearn: 0.0964597\ttotal: 2.68s\tremaining: 4.27s\n",
      "386:\tlearn: 0.0964325\ttotal: 2.69s\tremaining: 4.26s\n",
      "387:\tlearn: 0.0963879\ttotal: 2.7s\tremaining: 4.25s\n",
      "388:\tlearn: 0.0963456\ttotal: 2.71s\tremaining: 4.25s\n",
      "389:\tlearn: 0.0963102\ttotal: 2.71s\tremaining: 4.24s\n",
      "390:\tlearn: 0.0962477\ttotal: 2.72s\tremaining: 4.24s\n",
      "391:\tlearn: 0.0962123\ttotal: 2.73s\tremaining: 4.24s\n",
      "392:\tlearn: 0.0961860\ttotal: 2.74s\tremaining: 4.23s\n",
      "393:\tlearn: 0.0961640\ttotal: 2.74s\tremaining: 4.22s\n",
      "394:\tlearn: 0.0961046\ttotal: 2.75s\tremaining: 4.21s\n",
      "395:\tlearn: 0.0960765\ttotal: 2.75s\tremaining: 4.2s\n",
      "396:\tlearn: 0.0960449\ttotal: 2.77s\tremaining: 4.2s\n",
      "397:\tlearn: 0.0960186\ttotal: 2.77s\tremaining: 4.19s\n",
      "398:\tlearn: 0.0959618\ttotal: 2.78s\tremaining: 4.19s\n",
      "399:\tlearn: 0.0959007\ttotal: 2.79s\tremaining: 4.18s\n",
      "400:\tlearn: 0.0958793\ttotal: 2.79s\tremaining: 4.17s\n",
      "401:\tlearn: 0.0958569\ttotal: 2.8s\tremaining: 4.17s\n",
      "402:\tlearn: 0.0958285\ttotal: 2.81s\tremaining: 4.16s\n",
      "403:\tlearn: 0.0958005\ttotal: 2.81s\tremaining: 4.15s\n",
      "404:\tlearn: 0.0957688\ttotal: 2.82s\tremaining: 4.14s\n",
      "405:\tlearn: 0.0957413\ttotal: 2.83s\tremaining: 4.14s\n",
      "406:\tlearn: 0.0957042\ttotal: 2.84s\tremaining: 4.13s\n",
      "407:\tlearn: 0.0956670\ttotal: 2.84s\tremaining: 4.12s\n",
      "408:\tlearn: 0.0956168\ttotal: 2.85s\tremaining: 4.12s\n",
      "409:\tlearn: 0.0955847\ttotal: 2.85s\tremaining: 4.11s\n",
      "410:\tlearn: 0.0955390\ttotal: 2.86s\tremaining: 4.1s\n",
      "411:\tlearn: 0.0955072\ttotal: 2.87s\tremaining: 4.09s\n",
      "412:\tlearn: 0.0954777\ttotal: 2.88s\tremaining: 4.09s\n",
      "413:\tlearn: 0.0954320\ttotal: 2.88s\tremaining: 4.08s\n",
      "414:\tlearn: 0.0953781\ttotal: 2.89s\tremaining: 4.07s\n",
      "415:\tlearn: 0.0953251\ttotal: 2.9s\tremaining: 4.06s\n",
      "416:\tlearn: 0.0952991\ttotal: 2.9s\tremaining: 4.06s\n",
      "417:\tlearn: 0.0952586\ttotal: 2.91s\tremaining: 4.05s\n",
      "418:\tlearn: 0.0952289\ttotal: 2.92s\tremaining: 4.04s\n",
      "419:\tlearn: 0.0951630\ttotal: 2.92s\tremaining: 4.04s\n",
      "420:\tlearn: 0.0951303\ttotal: 2.93s\tremaining: 4.03s\n",
      "421:\tlearn: 0.0951115\ttotal: 2.94s\tremaining: 4.03s\n",
      "422:\tlearn: 0.0950653\ttotal: 2.95s\tremaining: 4.02s\n",
      "423:\tlearn: 0.0950317\ttotal: 2.96s\tremaining: 4.02s\n",
      "424:\tlearn: 0.0949926\ttotal: 2.97s\tremaining: 4.02s\n",
      "425:\tlearn: 0.0949610\ttotal: 2.98s\tremaining: 4.01s\n",
      "426:\tlearn: 0.0949214\ttotal: 2.99s\tremaining: 4.01s\n",
      "427:\tlearn: 0.0948885\ttotal: 3.01s\tremaining: 4.02s\n",
      "428:\tlearn: 0.0948567\ttotal: 3.03s\tremaining: 4.03s\n",
      "429:\tlearn: 0.0948165\ttotal: 3.05s\tremaining: 4.04s\n",
      "430:\tlearn: 0.0947741\ttotal: 3.07s\tremaining: 4.05s\n",
      "431:\tlearn: 0.0947555\ttotal: 3.08s\tremaining: 4.06s\n",
      "432:\tlearn: 0.0947266\ttotal: 3.11s\tremaining: 4.08s\n",
      "433:\tlearn: 0.0946893\ttotal: 3.13s\tremaining: 4.08s\n",
      "434:\tlearn: 0.0946641\ttotal: 3.21s\tremaining: 4.17s\n",
      "435:\tlearn: 0.0946394\ttotal: 3.24s\tremaining: 4.2s\n",
      "436:\tlearn: 0.0946070\ttotal: 3.25s\tremaining: 4.19s\n",
      "437:\tlearn: 0.0945831\ttotal: 3.26s\tremaining: 4.19s\n",
      "438:\tlearn: 0.0945520\ttotal: 3.27s\tremaining: 4.18s\n",
      "439:\tlearn: 0.0945196\ttotal: 3.42s\tremaining: 4.35s\n",
      "440:\tlearn: 0.0944833\ttotal: 3.45s\tremaining: 4.37s\n",
      "441:\tlearn: 0.0944460\ttotal: 3.46s\tremaining: 4.37s\n",
      "442:\tlearn: 0.0944139\ttotal: 3.48s\tremaining: 4.38s\n",
      "443:\tlearn: 0.0943935\ttotal: 3.49s\tremaining: 4.37s\n",
      "444:\tlearn: 0.0943678\ttotal: 3.5s\tremaining: 4.36s\n",
      "445:\tlearn: 0.0943510\ttotal: 3.51s\tremaining: 4.36s\n",
      "446:\tlearn: 0.0943217\ttotal: 3.52s\tremaining: 4.35s\n",
      "447:\tlearn: 0.0942958\ttotal: 3.53s\tremaining: 4.35s\n",
      "448:\tlearn: 0.0942595\ttotal: 3.54s\tremaining: 4.34s\n",
      "449:\tlearn: 0.0942402\ttotal: 3.55s\tremaining: 4.33s\n",
      "450:\tlearn: 0.0942053\ttotal: 3.55s\tremaining: 4.33s\n",
      "451:\tlearn: 0.0941671\ttotal: 3.56s\tremaining: 4.32s\n",
      "452:\tlearn: 0.0941427\ttotal: 3.57s\tremaining: 4.31s\n",
      "453:\tlearn: 0.0941072\ttotal: 3.58s\tremaining: 4.3s\n",
      "454:\tlearn: 0.0940827\ttotal: 3.59s\tremaining: 4.29s\n",
      "455:\tlearn: 0.0940616\ttotal: 3.59s\tremaining: 4.28s\n",
      "456:\tlearn: 0.0940320\ttotal: 3.6s\tremaining: 4.27s\n",
      "457:\tlearn: 0.0939869\ttotal: 3.62s\tremaining: 4.28s\n",
      "458:\tlearn: 0.0939623\ttotal: 3.62s\tremaining: 4.27s\n",
      "459:\tlearn: 0.0939296\ttotal: 3.63s\tremaining: 4.26s\n",
      "460:\tlearn: 0.0938958\ttotal: 3.64s\tremaining: 4.26s\n",
      "461:\tlearn: 0.0938709\ttotal: 3.66s\tremaining: 4.26s\n",
      "462:\tlearn: 0.0938462\ttotal: 3.67s\tremaining: 4.26s\n",
      "463:\tlearn: 0.0938201\ttotal: 3.68s\tremaining: 4.25s\n",
      "464:\tlearn: 0.0937843\ttotal: 3.69s\tremaining: 4.24s\n",
      "465:\tlearn: 0.0937653\ttotal: 3.7s\tremaining: 4.25s\n",
      "466:\tlearn: 0.0937450\ttotal: 3.71s\tremaining: 4.24s\n",
      "467:\tlearn: 0.0937168\ttotal: 3.72s\tremaining: 4.23s\n",
      "468:\tlearn: 0.0936946\ttotal: 3.73s\tremaining: 4.22s\n",
      "469:\tlearn: 0.0936758\ttotal: 3.74s\tremaining: 4.22s\n",
      "470:\tlearn: 0.0936575\ttotal: 3.75s\tremaining: 4.22s\n",
      "471:\tlearn: 0.0936350\ttotal: 3.76s\tremaining: 4.21s\n",
      "472:\tlearn: 0.0936037\ttotal: 3.77s\tremaining: 4.2s\n",
      "473:\tlearn: 0.0935917\ttotal: 3.78s\tremaining: 4.19s\n",
      "474:\tlearn: 0.0935742\ttotal: 3.79s\tremaining: 4.19s\n",
      "475:\tlearn: 0.0935510\ttotal: 3.81s\tremaining: 4.19s\n",
      "476:\tlearn: 0.0935250\ttotal: 3.81s\tremaining: 4.18s\n",
      "477:\tlearn: 0.0934844\ttotal: 3.82s\tremaining: 4.17s\n",
      "478:\tlearn: 0.0934369\ttotal: 3.83s\tremaining: 4.17s\n",
      "479:\tlearn: 0.0934092\ttotal: 3.84s\tremaining: 4.17s\n",
      "480:\tlearn: 0.0933975\ttotal: 3.85s\tremaining: 4.16s\n",
      "481:\tlearn: 0.0933262\ttotal: 3.86s\tremaining: 4.15s\n",
      "482:\tlearn: 0.0932968\ttotal: 3.86s\tremaining: 4.14s\n",
      "483:\tlearn: 0.0932775\ttotal: 3.88s\tremaining: 4.14s\n",
      "484:\tlearn: 0.0932390\ttotal: 3.89s\tremaining: 4.13s\n",
      "485:\tlearn: 0.0932118\ttotal: 3.9s\tremaining: 4.12s\n",
      "486:\tlearn: 0.0931647\ttotal: 3.9s\tremaining: 4.11s\n",
      "487:\tlearn: 0.0931371\ttotal: 3.91s\tremaining: 4.11s\n",
      "488:\tlearn: 0.0931143\ttotal: 3.93s\tremaining: 4.1s\n",
      "489:\tlearn: 0.0930856\ttotal: 3.94s\tremaining: 4.09s\n",
      "490:\tlearn: 0.0930468\ttotal: 3.94s\tremaining: 4.08s\n",
      "491:\tlearn: 0.0930274\ttotal: 3.95s\tremaining: 4.08s\n",
      "492:\tlearn: 0.0930009\ttotal: 3.95s\tremaining: 4.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493:\tlearn: 0.0929824\ttotal: 3.97s\tremaining: 4.07s\n",
      "494:\tlearn: 0.0929616\ttotal: 3.98s\tremaining: 4.06s\n",
      "495:\tlearn: 0.0929375\ttotal: 3.99s\tremaining: 4.05s\n",
      "496:\tlearn: 0.0928699\ttotal: 4s\tremaining: 4.04s\n",
      "497:\tlearn: 0.0928480\ttotal: 4.01s\tremaining: 4.04s\n",
      "498:\tlearn: 0.0928193\ttotal: 4.02s\tremaining: 4.03s\n",
      "499:\tlearn: 0.0928039\ttotal: 4.03s\tremaining: 4.03s\n",
      "500:\tlearn: 0.0927673\ttotal: 4.05s\tremaining: 4.03s\n",
      "501:\tlearn: 0.0927281\ttotal: 4.06s\tremaining: 4.03s\n",
      "502:\tlearn: 0.0927113\ttotal: 4.07s\tremaining: 4.02s\n",
      "503:\tlearn: 0.0926724\ttotal: 4.08s\tremaining: 4.02s\n",
      "504:\tlearn: 0.0926593\ttotal: 4.11s\tremaining: 4.03s\n",
      "505:\tlearn: 0.0926293\ttotal: 4.12s\tremaining: 4.02s\n",
      "506:\tlearn: 0.0925716\ttotal: 4.14s\tremaining: 4.03s\n",
      "507:\tlearn: 0.0925307\ttotal: 4.16s\tremaining: 4.03s\n",
      "508:\tlearn: 0.0924997\ttotal: 4.17s\tremaining: 4.02s\n",
      "509:\tlearn: 0.0924760\ttotal: 4.18s\tremaining: 4.02s\n",
      "510:\tlearn: 0.0924425\ttotal: 4.2s\tremaining: 4.02s\n",
      "511:\tlearn: 0.0924234\ttotal: 4.21s\tremaining: 4.01s\n",
      "512:\tlearn: 0.0923779\ttotal: 4.22s\tremaining: 4s\n",
      "513:\tlearn: 0.0923561\ttotal: 4.22s\tremaining: 3.99s\n",
      "514:\tlearn: 0.0923351\ttotal: 4.24s\tremaining: 4s\n",
      "515:\tlearn: 0.0923139\ttotal: 4.25s\tremaining: 3.99s\n",
      "516:\tlearn: 0.0922971\ttotal: 4.26s\tremaining: 3.98s\n",
      "517:\tlearn: 0.0922373\ttotal: 4.27s\tremaining: 3.97s\n",
      "518:\tlearn: 0.0921984\ttotal: 4.29s\tremaining: 3.97s\n",
      "519:\tlearn: 0.0921566\ttotal: 4.3s\tremaining: 3.97s\n",
      "520:\tlearn: 0.0921308\ttotal: 4.33s\tremaining: 3.98s\n",
      "521:\tlearn: 0.0920963\ttotal: 4.34s\tremaining: 3.97s\n",
      "522:\tlearn: 0.0920797\ttotal: 4.46s\tremaining: 4.07s\n",
      "523:\tlearn: 0.0920352\ttotal: 4.65s\tremaining: 4.22s\n",
      "524:\tlearn: 0.0920089\ttotal: 4.68s\tremaining: 4.23s\n",
      "525:\tlearn: 0.0919818\ttotal: 4.71s\tremaining: 4.25s\n",
      "526:\tlearn: 0.0919652\ttotal: 4.76s\tremaining: 4.28s\n",
      "527:\tlearn: 0.0919380\ttotal: 4.79s\tremaining: 4.28s\n",
      "528:\tlearn: 0.0918930\ttotal: 4.8s\tremaining: 4.27s\n",
      "529:\tlearn: 0.0918732\ttotal: 4.82s\tremaining: 4.27s\n",
      "530:\tlearn: 0.0918578\ttotal: 4.83s\tremaining: 4.26s\n",
      "531:\tlearn: 0.0918011\ttotal: 4.84s\tremaining: 4.26s\n",
      "532:\tlearn: 0.0917851\ttotal: 4.86s\tremaining: 4.25s\n",
      "533:\tlearn: 0.0917457\ttotal: 4.87s\tremaining: 4.25s\n",
      "534:\tlearn: 0.0917180\ttotal: 4.88s\tremaining: 4.24s\n",
      "535:\tlearn: 0.0917039\ttotal: 4.89s\tremaining: 4.23s\n",
      "536:\tlearn: 0.0916548\ttotal: 4.9s\tremaining: 4.23s\n",
      "537:\tlearn: 0.0916441\ttotal: 4.92s\tremaining: 4.22s\n",
      "538:\tlearn: 0.0916307\ttotal: 4.93s\tremaining: 4.22s\n",
      "539:\tlearn: 0.0915972\ttotal: 4.95s\tremaining: 4.21s\n",
      "540:\tlearn: 0.0915651\ttotal: 4.97s\tremaining: 4.22s\n",
      "541:\tlearn: 0.0915179\ttotal: 4.99s\tremaining: 4.22s\n",
      "542:\tlearn: 0.0914911\ttotal: 5s\tremaining: 4.21s\n",
      "543:\tlearn: 0.0914676\ttotal: 5.02s\tremaining: 4.21s\n",
      "544:\tlearn: 0.0914450\ttotal: 5.03s\tremaining: 4.2s\n",
      "545:\tlearn: 0.0914147\ttotal: 5.05s\tremaining: 4.2s\n",
      "546:\tlearn: 0.0914044\ttotal: 5.06s\tremaining: 4.19s\n",
      "547:\tlearn: 0.0913803\ttotal: 5.07s\tremaining: 4.18s\n",
      "548:\tlearn: 0.0913681\ttotal: 5.09s\tremaining: 4.18s\n",
      "549:\tlearn: 0.0913369\ttotal: 5.1s\tremaining: 4.17s\n",
      "550:\tlearn: 0.0913102\ttotal: 5.11s\tremaining: 4.17s\n",
      "551:\tlearn: 0.0912939\ttotal: 5.12s\tremaining: 4.16s\n",
      "552:\tlearn: 0.0912732\ttotal: 5.14s\tremaining: 4.15s\n",
      "553:\tlearn: 0.0912482\ttotal: 5.15s\tremaining: 4.14s\n",
      "554:\tlearn: 0.0912239\ttotal: 5.16s\tremaining: 4.13s\n",
      "555:\tlearn: 0.0911957\ttotal: 5.17s\tremaining: 4.13s\n",
      "556:\tlearn: 0.0911745\ttotal: 5.18s\tremaining: 4.12s\n",
      "557:\tlearn: 0.0911529\ttotal: 5.2s\tremaining: 4.12s\n",
      "558:\tlearn: 0.0910995\ttotal: 5.21s\tremaining: 4.11s\n",
      "559:\tlearn: 0.0910809\ttotal: 5.21s\tremaining: 4.1s\n",
      "560:\tlearn: 0.0910361\ttotal: 5.23s\tremaining: 4.09s\n",
      "561:\tlearn: 0.0910154\ttotal: 5.24s\tremaining: 4.08s\n",
      "562:\tlearn: 0.0910000\ttotal: 5.26s\tremaining: 4.08s\n",
      "563:\tlearn: 0.0909850\ttotal: 5.27s\tremaining: 4.07s\n",
      "564:\tlearn: 0.0909492\ttotal: 5.28s\tremaining: 4.06s\n",
      "565:\tlearn: 0.0909193\ttotal: 5.29s\tremaining: 4.06s\n",
      "566:\tlearn: 0.0909059\ttotal: 5.3s\tremaining: 4.05s\n",
      "567:\tlearn: 0.0908922\ttotal: 5.32s\tremaining: 4.04s\n",
      "568:\tlearn: 0.0908415\ttotal: 5.33s\tremaining: 4.04s\n",
      "569:\tlearn: 0.0908189\ttotal: 5.34s\tremaining: 4.03s\n",
      "570:\tlearn: 0.0907950\ttotal: 5.35s\tremaining: 4.02s\n",
      "571:\tlearn: 0.0907436\ttotal: 5.36s\tremaining: 4.01s\n",
      "572:\tlearn: 0.0907288\ttotal: 5.37s\tremaining: 4s\n",
      "573:\tlearn: 0.0907150\ttotal: 5.38s\tremaining: 4s\n",
      "574:\tlearn: 0.0907051\ttotal: 5.39s\tremaining: 3.99s\n",
      "575:\tlearn: 0.0906777\ttotal: 5.41s\tremaining: 3.98s\n",
      "576:\tlearn: 0.0906467\ttotal: 5.43s\tremaining: 3.98s\n",
      "577:\tlearn: 0.0906188\ttotal: 5.44s\tremaining: 3.97s\n",
      "578:\tlearn: 0.0906040\ttotal: 5.45s\tremaining: 3.96s\n",
      "579:\tlearn: 0.0905918\ttotal: 5.46s\tremaining: 3.95s\n",
      "580:\tlearn: 0.0905768\ttotal: 5.47s\tremaining: 3.94s\n",
      "581:\tlearn: 0.0905538\ttotal: 5.48s\tremaining: 3.94s\n",
      "582:\tlearn: 0.0905349\ttotal: 5.5s\tremaining: 3.93s\n",
      "583:\tlearn: 0.0905069\ttotal: 5.51s\tremaining: 3.92s\n",
      "584:\tlearn: 0.0904844\ttotal: 5.52s\tremaining: 3.92s\n",
      "585:\tlearn: 0.0904638\ttotal: 5.54s\tremaining: 3.91s\n",
      "586:\tlearn: 0.0904401\ttotal: 5.57s\tremaining: 3.92s\n",
      "587:\tlearn: 0.0904155\ttotal: 5.59s\tremaining: 3.92s\n",
      "588:\tlearn: 0.0904015\ttotal: 5.63s\tremaining: 3.92s\n",
      "589:\tlearn: 0.0903844\ttotal: 5.65s\tremaining: 3.93s\n",
      "590:\tlearn: 0.0903655\ttotal: 5.66s\tremaining: 3.92s\n",
      "591:\tlearn: 0.0903493\ttotal: 5.68s\tremaining: 3.91s\n",
      "592:\tlearn: 0.0903222\ttotal: 5.69s\tremaining: 3.9s\n",
      "593:\tlearn: 0.0902967\ttotal: 5.7s\tremaining: 3.9s\n",
      "594:\tlearn: 0.0902588\ttotal: 5.71s\tremaining: 3.89s\n",
      "595:\tlearn: 0.0902433\ttotal: 5.72s\tremaining: 3.88s\n",
      "596:\tlearn: 0.0902182\ttotal: 5.73s\tremaining: 3.87s\n",
      "597:\tlearn: 0.0902062\ttotal: 5.74s\tremaining: 3.86s\n",
      "598:\tlearn: 0.0901908\ttotal: 5.75s\tremaining: 3.85s\n",
      "599:\tlearn: 0.0901709\ttotal: 5.76s\tremaining: 3.84s\n",
      "600:\tlearn: 0.0901351\ttotal: 5.78s\tremaining: 3.84s\n",
      "601:\tlearn: 0.0901178\ttotal: 5.79s\tremaining: 3.83s\n",
      "602:\tlearn: 0.0901033\ttotal: 5.8s\tremaining: 3.82s\n",
      "603:\tlearn: 0.0900922\ttotal: 5.81s\tremaining: 3.81s\n",
      "604:\tlearn: 0.0900776\ttotal: 5.82s\tremaining: 3.8s\n",
      "605:\tlearn: 0.0900600\ttotal: 5.83s\tremaining: 3.79s\n",
      "606:\tlearn: 0.0900307\ttotal: 5.84s\tremaining: 3.78s\n",
      "607:\tlearn: 0.0900080\ttotal: 5.85s\tremaining: 3.77s\n",
      "608:\tlearn: 0.0899955\ttotal: 5.86s\tremaining: 3.76s\n",
      "609:\tlearn: 0.0899776\ttotal: 5.87s\tremaining: 3.75s\n",
      "610:\tlearn: 0.0899642\ttotal: 5.89s\tremaining: 3.75s\n",
      "611:\tlearn: 0.0899546\ttotal: 5.9s\tremaining: 3.74s\n",
      "612:\tlearn: 0.0899318\ttotal: 5.92s\tremaining: 3.73s\n",
      "613:\tlearn: 0.0899070\ttotal: 5.93s\tremaining: 3.73s\n",
      "614:\tlearn: 0.0898974\ttotal: 5.94s\tremaining: 3.72s\n",
      "615:\tlearn: 0.0898661\ttotal: 5.95s\tremaining: 3.71s\n",
      "616:\tlearn: 0.0898406\ttotal: 5.96s\tremaining: 3.7s\n",
      "617:\tlearn: 0.0898260\ttotal: 5.98s\tremaining: 3.69s\n",
      "618:\tlearn: 0.0898152\ttotal: 5.99s\tremaining: 3.69s\n",
      "619:\tlearn: 0.0897836\ttotal: 6s\tremaining: 3.68s\n",
      "620:\tlearn: 0.0897669\ttotal: 6.01s\tremaining: 3.67s\n",
      "621:\tlearn: 0.0897471\ttotal: 6.03s\tremaining: 3.66s\n",
      "622:\tlearn: 0.0897247\ttotal: 6.04s\tremaining: 3.66s\n",
      "623:\tlearn: 0.0897040\ttotal: 6.05s\tremaining: 3.65s\n",
      "624:\tlearn: 0.0896918\ttotal: 6.07s\tremaining: 3.64s\n",
      "625:\tlearn: 0.0896551\ttotal: 6.08s\tremaining: 3.63s\n",
      "626:\tlearn: 0.0896440\ttotal: 6.1s\tremaining: 3.63s\n",
      "627:\tlearn: 0.0896262\ttotal: 6.11s\tremaining: 3.62s\n",
      "628:\tlearn: 0.0896142\ttotal: 6.12s\tremaining: 3.61s\n",
      "629:\tlearn: 0.0895978\ttotal: 6.13s\tremaining: 3.6s\n",
      "630:\tlearn: 0.0895789\ttotal: 6.14s\tremaining: 3.59s\n",
      "631:\tlearn: 0.0895493\ttotal: 6.16s\tremaining: 3.58s\n",
      "632:\tlearn: 0.0895193\ttotal: 6.17s\tremaining: 3.58s\n",
      "633:\tlearn: 0.0895073\ttotal: 6.19s\tremaining: 3.57s\n",
      "634:\tlearn: 0.0894899\ttotal: 6.2s\tremaining: 3.56s\n",
      "635:\tlearn: 0.0894479\ttotal: 6.21s\tremaining: 3.56s\n",
      "636:\tlearn: 0.0894328\ttotal: 6.23s\tremaining: 3.55s\n",
      "637:\tlearn: 0.0894198\ttotal: 6.24s\tremaining: 3.54s\n",
      "638:\tlearn: 0.0894098\ttotal: 6.25s\tremaining: 3.53s\n",
      "639:\tlearn: 0.0893645\ttotal: 6.27s\tremaining: 3.52s\n",
      "640:\tlearn: 0.0893536\ttotal: 6.28s\tremaining: 3.52s\n",
      "641:\tlearn: 0.0893353\ttotal: 6.29s\tremaining: 3.51s\n",
      "642:\tlearn: 0.0893235\ttotal: 6.3s\tremaining: 3.5s\n",
      "643:\tlearn: 0.0893110\ttotal: 6.32s\tremaining: 3.49s\n",
      "644:\tlearn: 0.0892796\ttotal: 6.33s\tremaining: 3.48s\n",
      "645:\tlearn: 0.0892489\ttotal: 6.35s\tremaining: 3.48s\n",
      "646:\tlearn: 0.0892378\ttotal: 6.36s\tremaining: 3.47s\n",
      "647:\tlearn: 0.0892061\ttotal: 6.38s\tremaining: 3.47s\n",
      "648:\tlearn: 0.0891839\ttotal: 6.39s\tremaining: 3.46s\n",
      "649:\tlearn: 0.0891662\ttotal: 6.41s\tremaining: 3.45s\n",
      "650:\tlearn: 0.0891505\ttotal: 6.42s\tremaining: 3.44s\n",
      "651:\tlearn: 0.0891372\ttotal: 6.44s\tremaining: 3.44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652:\tlearn: 0.0890908\ttotal: 6.45s\tremaining: 3.43s\n",
      "653:\tlearn: 0.0890816\ttotal: 6.46s\tremaining: 3.42s\n",
      "654:\tlearn: 0.0890673\ttotal: 6.48s\tremaining: 3.41s\n",
      "655:\tlearn: 0.0890356\ttotal: 6.49s\tremaining: 3.4s\n",
      "656:\tlearn: 0.0889922\ttotal: 6.5s\tremaining: 3.39s\n",
      "657:\tlearn: 0.0889634\ttotal: 6.51s\tremaining: 3.38s\n",
      "658:\tlearn: 0.0889487\ttotal: 6.53s\tremaining: 3.38s\n",
      "659:\tlearn: 0.0889210\ttotal: 6.54s\tremaining: 3.37s\n",
      "660:\tlearn: 0.0889064\ttotal: 6.56s\tremaining: 3.36s\n",
      "661:\tlearn: 0.0888904\ttotal: 6.58s\tremaining: 3.36s\n",
      "662:\tlearn: 0.0888787\ttotal: 6.61s\tremaining: 3.36s\n",
      "663:\tlearn: 0.0888647\ttotal: 6.64s\tremaining: 3.36s\n",
      "664:\tlearn: 0.0888509\ttotal: 6.68s\tremaining: 3.37s\n",
      "665:\tlearn: 0.0888441\ttotal: 6.7s\tremaining: 3.36s\n",
      "666:\tlearn: 0.0888126\ttotal: 6.71s\tremaining: 3.35s\n",
      "667:\tlearn: 0.0887815\ttotal: 6.73s\tremaining: 3.34s\n",
      "668:\tlearn: 0.0887754\ttotal: 6.74s\tremaining: 3.33s\n",
      "669:\tlearn: 0.0887683\ttotal: 6.75s\tremaining: 3.32s\n",
      "670:\tlearn: 0.0887554\ttotal: 6.76s\tremaining: 3.31s\n",
      "671:\tlearn: 0.0887291\ttotal: 6.78s\tremaining: 3.31s\n",
      "672:\tlearn: 0.0887136\ttotal: 6.79s\tremaining: 3.3s\n",
      "673:\tlearn: 0.0887007\ttotal: 6.8s\tremaining: 3.29s\n",
      "674:\tlearn: 0.0886842\ttotal: 6.81s\tremaining: 3.28s\n",
      "675:\tlearn: 0.0886611\ttotal: 6.84s\tremaining: 3.28s\n",
      "676:\tlearn: 0.0886532\ttotal: 6.85s\tremaining: 3.27s\n",
      "677:\tlearn: 0.0886294\ttotal: 6.86s\tremaining: 3.26s\n",
      "678:\tlearn: 0.0885890\ttotal: 6.87s\tremaining: 3.25s\n",
      "679:\tlearn: 0.0885762\ttotal: 6.88s\tremaining: 3.24s\n",
      "680:\tlearn: 0.0885604\ttotal: 6.9s\tremaining: 3.23s\n",
      "681:\tlearn: 0.0885449\ttotal: 6.92s\tremaining: 3.22s\n",
      "682:\tlearn: 0.0885327\ttotal: 6.93s\tremaining: 3.22s\n",
      "683:\tlearn: 0.0885236\ttotal: 6.95s\tremaining: 3.21s\n",
      "684:\tlearn: 0.0885071\ttotal: 6.96s\tremaining: 3.2s\n",
      "685:\tlearn: 0.0884991\ttotal: 6.97s\tremaining: 3.19s\n",
      "686:\tlearn: 0.0884661\ttotal: 6.99s\tremaining: 3.18s\n",
      "687:\tlearn: 0.0884531\ttotal: 7s\tremaining: 3.17s\n",
      "688:\tlearn: 0.0884242\ttotal: 7.01s\tremaining: 3.16s\n",
      "689:\tlearn: 0.0884144\ttotal: 7.03s\tremaining: 3.16s\n",
      "690:\tlearn: 0.0884012\ttotal: 7.04s\tremaining: 3.15s\n",
      "691:\tlearn: 0.0883930\ttotal: 7.05s\tremaining: 3.14s\n",
      "692:\tlearn: 0.0883757\ttotal: 7.06s\tremaining: 3.13s\n",
      "693:\tlearn: 0.0883572\ttotal: 7.08s\tremaining: 3.12s\n",
      "694:\tlearn: 0.0883389\ttotal: 7.08s\tremaining: 3.11s\n",
      "695:\tlearn: 0.0883197\ttotal: 7.09s\tremaining: 3.1s\n",
      "696:\tlearn: 0.0883021\ttotal: 7.11s\tremaining: 3.09s\n",
      "697:\tlearn: 0.0882896\ttotal: 7.12s\tremaining: 3.08s\n",
      "698:\tlearn: 0.0882780\ttotal: 7.13s\tremaining: 3.07s\n",
      "699:\tlearn: 0.0882618\ttotal: 7.15s\tremaining: 3.06s\n",
      "700:\tlearn: 0.0882413\ttotal: 7.16s\tremaining: 3.05s\n",
      "701:\tlearn: 0.0882330\ttotal: 7.17s\tremaining: 3.04s\n",
      "702:\tlearn: 0.0881949\ttotal: 7.18s\tremaining: 3.04s\n",
      "703:\tlearn: 0.0881848\ttotal: 7.2s\tremaining: 3.02s\n",
      "704:\tlearn: 0.0881667\ttotal: 7.2s\tremaining: 3.01s\n",
      "705:\tlearn: 0.0881465\ttotal: 7.21s\tremaining: 3s\n",
      "706:\tlearn: 0.0881270\ttotal: 7.22s\tremaining: 2.99s\n",
      "707:\tlearn: 0.0881192\ttotal: 7.24s\tremaining: 2.98s\n",
      "708:\tlearn: 0.0881033\ttotal: 7.25s\tremaining: 2.97s\n",
      "709:\tlearn: 0.0880757\ttotal: 7.26s\tremaining: 2.96s\n",
      "710:\tlearn: 0.0880535\ttotal: 7.27s\tremaining: 2.96s\n",
      "711:\tlearn: 0.0880490\ttotal: 7.29s\tremaining: 2.95s\n",
      "712:\tlearn: 0.0880359\ttotal: 7.3s\tremaining: 2.94s\n",
      "713:\tlearn: 0.0880021\ttotal: 7.31s\tremaining: 2.93s\n",
      "714:\tlearn: 0.0879941\ttotal: 7.33s\tremaining: 2.92s\n",
      "715:\tlearn: 0.0879831\ttotal: 7.34s\tremaining: 2.91s\n",
      "716:\tlearn: 0.0879655\ttotal: 7.35s\tremaining: 2.9s\n",
      "717:\tlearn: 0.0879583\ttotal: 7.37s\tremaining: 2.9s\n",
      "718:\tlearn: 0.0879488\ttotal: 7.38s\tremaining: 2.88s\n",
      "719:\tlearn: 0.0879420\ttotal: 7.39s\tremaining: 2.87s\n",
      "720:\tlearn: 0.0879221\ttotal: 7.41s\tremaining: 2.87s\n",
      "721:\tlearn: 0.0878976\ttotal: 7.44s\tremaining: 2.86s\n",
      "722:\tlearn: 0.0878883\ttotal: 7.45s\tremaining: 2.85s\n",
      "723:\tlearn: 0.0878700\ttotal: 7.46s\tremaining: 2.85s\n",
      "724:\tlearn: 0.0878565\ttotal: 7.48s\tremaining: 2.84s\n",
      "725:\tlearn: 0.0878235\ttotal: 7.49s\tremaining: 2.83s\n",
      "726:\tlearn: 0.0877970\ttotal: 7.5s\tremaining: 2.82s\n",
      "727:\tlearn: 0.0877921\ttotal: 7.51s\tremaining: 2.81s\n",
      "728:\tlearn: 0.0877725\ttotal: 7.53s\tremaining: 2.8s\n",
      "729:\tlearn: 0.0877602\ttotal: 7.54s\tremaining: 2.79s\n",
      "730:\tlearn: 0.0877487\ttotal: 7.56s\tremaining: 2.78s\n",
      "731:\tlearn: 0.0877313\ttotal: 7.58s\tremaining: 2.77s\n",
      "732:\tlearn: 0.0876911\ttotal: 7.62s\tremaining: 2.78s\n",
      "733:\tlearn: 0.0876786\ttotal: 7.67s\tremaining: 2.78s\n",
      "734:\tlearn: 0.0876473\ttotal: 7.7s\tremaining: 2.78s\n",
      "735:\tlearn: 0.0876319\ttotal: 7.73s\tremaining: 2.77s\n",
      "736:\tlearn: 0.0876210\ttotal: 7.75s\tremaining: 2.77s\n",
      "737:\tlearn: 0.0876058\ttotal: 7.76s\tremaining: 2.76s\n",
      "738:\tlearn: 0.0875835\ttotal: 7.79s\tremaining: 2.75s\n",
      "739:\tlearn: 0.0875523\ttotal: 7.8s\tremaining: 2.74s\n",
      "740:\tlearn: 0.0875294\ttotal: 7.81s\tremaining: 2.73s\n",
      "741:\tlearn: 0.0875090\ttotal: 7.82s\tremaining: 2.72s\n",
      "742:\tlearn: 0.0874884\ttotal: 7.83s\tremaining: 2.71s\n",
      "743:\tlearn: 0.0874770\ttotal: 7.84s\tremaining: 2.7s\n",
      "744:\tlearn: 0.0874589\ttotal: 7.85s\tremaining: 2.69s\n",
      "745:\tlearn: 0.0874423\ttotal: 7.86s\tremaining: 2.68s\n",
      "746:\tlearn: 0.0874289\ttotal: 7.87s\tremaining: 2.67s\n",
      "747:\tlearn: 0.0874145\ttotal: 7.88s\tremaining: 2.66s\n",
      "748:\tlearn: 0.0873991\ttotal: 7.89s\tremaining: 2.65s\n",
      "749:\tlearn: 0.0873928\ttotal: 7.91s\tremaining: 2.64s\n",
      "750:\tlearn: 0.0873655\ttotal: 7.92s\tremaining: 2.63s\n",
      "751:\tlearn: 0.0873453\ttotal: 7.94s\tremaining: 2.62s\n",
      "752:\tlearn: 0.0873340\ttotal: 7.96s\tremaining: 2.61s\n",
      "753:\tlearn: 0.0873145\ttotal: 7.97s\tremaining: 2.6s\n",
      "754:\tlearn: 0.0873093\ttotal: 7.98s\tremaining: 2.59s\n",
      "755:\tlearn: 0.0872977\ttotal: 8s\tremaining: 2.58s\n",
      "756:\tlearn: 0.0872586\ttotal: 8.01s\tremaining: 2.57s\n",
      "757:\tlearn: 0.0872330\ttotal: 8.02s\tremaining: 2.56s\n",
      "758:\tlearn: 0.0872191\ttotal: 8.03s\tremaining: 2.55s\n",
      "759:\tlearn: 0.0872037\ttotal: 8.04s\tremaining: 2.54s\n",
      "760:\tlearn: 0.0871795\ttotal: 8.06s\tremaining: 2.53s\n",
      "761:\tlearn: 0.0871661\ttotal: 8.07s\tremaining: 2.52s\n",
      "762:\tlearn: 0.0871567\ttotal: 8.08s\tremaining: 2.51s\n",
      "763:\tlearn: 0.0871233\ttotal: 8.1s\tremaining: 2.5s\n",
      "764:\tlearn: 0.0871151\ttotal: 8.11s\tremaining: 2.49s\n",
      "765:\tlearn: 0.0871047\ttotal: 8.12s\tremaining: 2.48s\n",
      "766:\tlearn: 0.0870875\ttotal: 8.13s\tremaining: 2.47s\n",
      "767:\tlearn: 0.0870753\ttotal: 8.14s\tremaining: 2.46s\n",
      "768:\tlearn: 0.0870657\ttotal: 8.15s\tremaining: 2.45s\n",
      "769:\tlearn: 0.0870478\ttotal: 8.16s\tremaining: 2.44s\n",
      "770:\tlearn: 0.0870374\ttotal: 8.17s\tremaining: 2.43s\n",
      "771:\tlearn: 0.0870275\ttotal: 8.18s\tremaining: 2.42s\n",
      "772:\tlearn: 0.0870107\ttotal: 8.2s\tremaining: 2.41s\n",
      "773:\tlearn: 0.0869940\ttotal: 8.21s\tremaining: 2.4s\n",
      "774:\tlearn: 0.0869727\ttotal: 8.22s\tremaining: 2.39s\n",
      "775:\tlearn: 0.0869483\ttotal: 8.24s\tremaining: 2.38s\n",
      "776:\tlearn: 0.0869392\ttotal: 8.25s\tremaining: 2.37s\n",
      "777:\tlearn: 0.0869339\ttotal: 8.26s\tremaining: 2.36s\n",
      "778:\tlearn: 0.0869209\ttotal: 8.27s\tremaining: 2.35s\n",
      "779:\tlearn: 0.0869100\ttotal: 8.28s\tremaining: 2.34s\n",
      "780:\tlearn: 0.0868975\ttotal: 8.29s\tremaining: 2.33s\n",
      "781:\tlearn: 0.0868857\ttotal: 8.31s\tremaining: 2.32s\n",
      "782:\tlearn: 0.0868721\ttotal: 8.32s\tremaining: 2.31s\n",
      "783:\tlearn: 0.0868589\ttotal: 8.34s\tremaining: 2.3s\n",
      "784:\tlearn: 0.0868517\ttotal: 8.35s\tremaining: 2.29s\n",
      "785:\tlearn: 0.0868385\ttotal: 8.37s\tremaining: 2.28s\n",
      "786:\tlearn: 0.0868122\ttotal: 8.38s\tremaining: 2.27s\n",
      "787:\tlearn: 0.0867846\ttotal: 8.4s\tremaining: 2.26s\n",
      "788:\tlearn: 0.0867686\ttotal: 8.41s\tremaining: 2.25s\n",
      "789:\tlearn: 0.0867500\ttotal: 8.42s\tremaining: 2.24s\n",
      "790:\tlearn: 0.0867456\ttotal: 8.46s\tremaining: 2.23s\n",
      "791:\tlearn: 0.0867391\ttotal: 8.48s\tremaining: 2.23s\n",
      "792:\tlearn: 0.0867101\ttotal: 8.5s\tremaining: 2.22s\n",
      "793:\tlearn: 0.0866983\ttotal: 8.51s\tremaining: 2.21s\n",
      "794:\tlearn: 0.0866727\ttotal: 8.53s\tremaining: 2.2s\n",
      "795:\tlearn: 0.0866477\ttotal: 8.54s\tremaining: 2.19s\n",
      "796:\tlearn: 0.0866307\ttotal: 8.55s\tremaining: 2.18s\n",
      "797:\tlearn: 0.0866082\ttotal: 8.56s\tremaining: 2.17s\n",
      "798:\tlearn: 0.0865972\ttotal: 8.58s\tremaining: 2.16s\n",
      "799:\tlearn: 0.0865736\ttotal: 8.6s\tremaining: 2.15s\n",
      "800:\tlearn: 0.0865669\ttotal: 8.61s\tremaining: 2.14s\n",
      "801:\tlearn: 0.0865440\ttotal: 8.62s\tremaining: 2.13s\n",
      "802:\tlearn: 0.0865202\ttotal: 8.64s\tremaining: 2.12s\n",
      "803:\tlearn: 0.0865110\ttotal: 8.68s\tremaining: 2.12s\n",
      "804:\tlearn: 0.0864762\ttotal: 8.7s\tremaining: 2.11s\n",
      "805:\tlearn: 0.0864545\ttotal: 8.77s\tremaining: 2.11s\n",
      "806:\tlearn: 0.0864442\ttotal: 8.81s\tremaining: 2.11s\n",
      "807:\tlearn: 0.0864323\ttotal: 8.83s\tremaining: 2.1s\n",
      "808:\tlearn: 0.0864189\ttotal: 8.84s\tremaining: 2.09s\n",
      "809:\tlearn: 0.0864087\ttotal: 8.85s\tremaining: 2.08s\n",
      "810:\tlearn: 0.0864005\ttotal: 8.87s\tremaining: 2.07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811:\tlearn: 0.0863957\ttotal: 8.88s\tremaining: 2.06s\n",
      "812:\tlearn: 0.0863867\ttotal: 8.89s\tremaining: 2.04s\n",
      "813:\tlearn: 0.0863650\ttotal: 8.91s\tremaining: 2.04s\n",
      "814:\tlearn: 0.0863533\ttotal: 8.92s\tremaining: 2.02s\n",
      "815:\tlearn: 0.0863320\ttotal: 8.93s\tremaining: 2.01s\n",
      "816:\tlearn: 0.0863215\ttotal: 8.95s\tremaining: 2s\n",
      "817:\tlearn: 0.0863112\ttotal: 8.96s\tremaining: 1.99s\n",
      "818:\tlearn: 0.0863065\ttotal: 8.98s\tremaining: 1.98s\n",
      "819:\tlearn: 0.0862956\ttotal: 8.99s\tremaining: 1.97s\n",
      "820:\tlearn: 0.0862746\ttotal: 9s\tremaining: 1.96s\n",
      "821:\tlearn: 0.0862490\ttotal: 9.01s\tremaining: 1.95s\n",
      "822:\tlearn: 0.0862388\ttotal: 9.03s\tremaining: 1.94s\n",
      "823:\tlearn: 0.0862254\ttotal: 9.04s\tremaining: 1.93s\n",
      "824:\tlearn: 0.0861830\ttotal: 9.05s\tremaining: 1.92s\n",
      "825:\tlearn: 0.0861783\ttotal: 9.07s\tremaining: 1.91s\n",
      "826:\tlearn: 0.0861645\ttotal: 9.08s\tremaining: 1.9s\n",
      "827:\tlearn: 0.0861563\ttotal: 9.1s\tremaining: 1.89s\n",
      "828:\tlearn: 0.0861366\ttotal: 9.11s\tremaining: 1.88s\n",
      "829:\tlearn: 0.0861217\ttotal: 9.13s\tremaining: 1.87s\n",
      "830:\tlearn: 0.0861136\ttotal: 9.14s\tremaining: 1.86s\n",
      "831:\tlearn: 0.0861073\ttotal: 9.15s\tremaining: 1.85s\n",
      "832:\tlearn: 0.0860976\ttotal: 9.16s\tremaining: 1.84s\n",
      "833:\tlearn: 0.0860919\ttotal: 9.17s\tremaining: 1.82s\n",
      "834:\tlearn: 0.0860836\ttotal: 9.19s\tremaining: 1.81s\n",
      "835:\tlearn: 0.0860790\ttotal: 9.2s\tremaining: 1.8s\n",
      "836:\tlearn: 0.0860701\ttotal: 9.21s\tremaining: 1.79s\n",
      "837:\tlearn: 0.0860555\ttotal: 9.23s\tremaining: 1.78s\n",
      "838:\tlearn: 0.0860374\ttotal: 9.24s\tremaining: 1.77s\n",
      "839:\tlearn: 0.0860089\ttotal: 9.25s\tremaining: 1.76s\n",
      "840:\tlearn: 0.0859943\ttotal: 9.26s\tremaining: 1.75s\n",
      "841:\tlearn: 0.0859792\ttotal: 9.28s\tremaining: 1.74s\n",
      "842:\tlearn: 0.0859613\ttotal: 9.29s\tremaining: 1.73s\n",
      "843:\tlearn: 0.0859507\ttotal: 9.31s\tremaining: 1.72s\n",
      "844:\tlearn: 0.0859393\ttotal: 9.32s\tremaining: 1.71s\n",
      "845:\tlearn: 0.0859266\ttotal: 9.34s\tremaining: 1.7s\n",
      "846:\tlearn: 0.0859152\ttotal: 9.36s\tremaining: 1.69s\n",
      "847:\tlearn: 0.0859066\ttotal: 9.38s\tremaining: 1.68s\n",
      "848:\tlearn: 0.0858895\ttotal: 9.39s\tremaining: 1.67s\n",
      "849:\tlearn: 0.0858797\ttotal: 9.4s\tremaining: 1.66s\n",
      "850:\tlearn: 0.0858728\ttotal: 9.41s\tremaining: 1.65s\n",
      "851:\tlearn: 0.0858642\ttotal: 9.44s\tremaining: 1.64s\n",
      "852:\tlearn: 0.0858513\ttotal: 9.45s\tremaining: 1.63s\n",
      "853:\tlearn: 0.0858385\ttotal: 9.47s\tremaining: 1.62s\n",
      "854:\tlearn: 0.0858296\ttotal: 9.48s\tremaining: 1.61s\n",
      "855:\tlearn: 0.0858206\ttotal: 9.49s\tremaining: 1.6s\n",
      "856:\tlearn: 0.0858118\ttotal: 9.51s\tremaining: 1.59s\n",
      "857:\tlearn: 0.0858029\ttotal: 9.53s\tremaining: 1.58s\n",
      "858:\tlearn: 0.0857951\ttotal: 9.54s\tremaining: 1.57s\n",
      "859:\tlearn: 0.0857852\ttotal: 9.55s\tremaining: 1.55s\n",
      "860:\tlearn: 0.0857761\ttotal: 9.56s\tremaining: 1.54s\n",
      "861:\tlearn: 0.0857577\ttotal: 9.57s\tremaining: 1.53s\n",
      "862:\tlearn: 0.0857483\ttotal: 9.59s\tremaining: 1.52s\n",
      "863:\tlearn: 0.0857425\ttotal: 9.6s\tremaining: 1.51s\n",
      "864:\tlearn: 0.0857265\ttotal: 9.61s\tremaining: 1.5s\n",
      "865:\tlearn: 0.0857230\ttotal: 9.62s\tremaining: 1.49s\n",
      "866:\tlearn: 0.0857039\ttotal: 9.63s\tremaining: 1.48s\n",
      "867:\tlearn: 0.0856955\ttotal: 9.64s\tremaining: 1.47s\n",
      "868:\tlearn: 0.0856791\ttotal: 9.66s\tremaining: 1.46s\n",
      "869:\tlearn: 0.0856642\ttotal: 9.68s\tremaining: 1.45s\n",
      "870:\tlearn: 0.0856397\ttotal: 9.69s\tremaining: 1.43s\n",
      "871:\tlearn: 0.0856299\ttotal: 9.7s\tremaining: 1.42s\n",
      "872:\tlearn: 0.0856072\ttotal: 9.77s\tremaining: 1.42s\n",
      "873:\tlearn: 0.0855967\ttotal: 9.82s\tremaining: 1.42s\n",
      "874:\tlearn: 0.0855823\ttotal: 9.86s\tremaining: 1.41s\n",
      "875:\tlearn: 0.0855712\ttotal: 9.88s\tremaining: 1.4s\n",
      "876:\tlearn: 0.0855584\ttotal: 9.89s\tremaining: 1.39s\n",
      "877:\tlearn: 0.0855500\ttotal: 9.91s\tremaining: 1.38s\n",
      "878:\tlearn: 0.0855395\ttotal: 9.92s\tremaining: 1.36s\n",
      "879:\tlearn: 0.0855315\ttotal: 9.93s\tremaining: 1.35s\n",
      "880:\tlearn: 0.0855048\ttotal: 9.95s\tremaining: 1.34s\n",
      "881:\tlearn: 0.0854897\ttotal: 9.96s\tremaining: 1.33s\n",
      "882:\tlearn: 0.0854607\ttotal: 9.97s\tremaining: 1.32s\n",
      "883:\tlearn: 0.0854452\ttotal: 9.98s\tremaining: 1.31s\n",
      "884:\tlearn: 0.0854356\ttotal: 10s\tremaining: 1.3s\n",
      "885:\tlearn: 0.0854302\ttotal: 10s\tremaining: 1.29s\n",
      "886:\tlearn: 0.0853973\ttotal: 10s\tremaining: 1.28s\n",
      "887:\tlearn: 0.0853832\ttotal: 10.1s\tremaining: 1.27s\n",
      "888:\tlearn: 0.0853752\ttotal: 10.1s\tremaining: 1.26s\n",
      "889:\tlearn: 0.0853668\ttotal: 10.1s\tremaining: 1.25s\n",
      "890:\tlearn: 0.0853519\ttotal: 10.1s\tremaining: 1.23s\n",
      "891:\tlearn: 0.0853461\ttotal: 10.1s\tremaining: 1.22s\n",
      "892:\tlearn: 0.0853344\ttotal: 10.1s\tremaining: 1.21s\n",
      "893:\tlearn: 0.0853283\ttotal: 10.1s\tremaining: 1.2s\n",
      "894:\tlearn: 0.0853174\ttotal: 10.2s\tremaining: 1.19s\n",
      "895:\tlearn: 0.0852998\ttotal: 10.2s\tremaining: 1.18s\n",
      "896:\tlearn: 0.0852894\ttotal: 10.2s\tremaining: 1.17s\n",
      "897:\tlearn: 0.0852842\ttotal: 10.2s\tremaining: 1.16s\n",
      "898:\tlearn: 0.0852757\ttotal: 10.2s\tremaining: 1.15s\n",
      "899:\tlearn: 0.0852663\ttotal: 10.2s\tremaining: 1.14s\n",
      "900:\tlearn: 0.0852590\ttotal: 10.2s\tremaining: 1.13s\n",
      "901:\tlearn: 0.0852506\ttotal: 10.3s\tremaining: 1.11s\n",
      "902:\tlearn: 0.0852275\ttotal: 10.3s\tremaining: 1.1s\n",
      "903:\tlearn: 0.0852134\ttotal: 10.3s\tremaining: 1.09s\n",
      "904:\tlearn: 0.0852052\ttotal: 10.3s\tremaining: 1.08s\n",
      "905:\tlearn: 0.0851977\ttotal: 10.3s\tremaining: 1.07s\n",
      "906:\tlearn: 0.0851879\ttotal: 10.3s\tremaining: 1.06s\n",
      "907:\tlearn: 0.0851608\ttotal: 10.4s\tremaining: 1.05s\n",
      "908:\tlearn: 0.0851508\ttotal: 10.4s\tremaining: 1.04s\n",
      "909:\tlearn: 0.0851335\ttotal: 10.4s\tremaining: 1.03s\n",
      "910:\tlearn: 0.0851225\ttotal: 10.4s\tremaining: 1.02s\n",
      "911:\tlearn: 0.0851169\ttotal: 10.4s\tremaining: 1s\n",
      "912:\tlearn: 0.0851088\ttotal: 10.4s\tremaining: 994ms\n",
      "913:\tlearn: 0.0850972\ttotal: 10.4s\tremaining: 983ms\n",
      "914:\tlearn: 0.0850886\ttotal: 10.5s\tremaining: 972ms\n",
      "915:\tlearn: 0.0850814\ttotal: 10.5s\tremaining: 961ms\n",
      "916:\tlearn: 0.0850712\ttotal: 10.5s\tremaining: 949ms\n",
      "917:\tlearn: 0.0850653\ttotal: 10.5s\tremaining: 938ms\n",
      "918:\tlearn: 0.0850412\ttotal: 10.5s\tremaining: 927ms\n",
      "919:\tlearn: 0.0850275\ttotal: 10.5s\tremaining: 916ms\n",
      "920:\tlearn: 0.0850046\ttotal: 10.5s\tremaining: 905ms\n",
      "921:\tlearn: 0.0849973\ttotal: 10.6s\tremaining: 893ms\n",
      "922:\tlearn: 0.0849846\ttotal: 10.6s\tremaining: 882ms\n",
      "923:\tlearn: 0.0849715\ttotal: 10.6s\tremaining: 871ms\n",
      "924:\tlearn: 0.0849560\ttotal: 10.6s\tremaining: 861ms\n",
      "925:\tlearn: 0.0849371\ttotal: 10.6s\tremaining: 851ms\n",
      "926:\tlearn: 0.0849129\ttotal: 10.7s\tremaining: 839ms\n",
      "927:\tlearn: 0.0849024\ttotal: 10.7s\tremaining: 828ms\n",
      "928:\tlearn: 0.0848908\ttotal: 10.7s\tremaining: 816ms\n",
      "929:\tlearn: 0.0848675\ttotal: 10.7s\tremaining: 805ms\n",
      "930:\tlearn: 0.0848587\ttotal: 10.7s\tremaining: 793ms\n",
      "931:\tlearn: 0.0848535\ttotal: 10.7s\tremaining: 782ms\n",
      "932:\tlearn: 0.0848458\ttotal: 10.7s\tremaining: 771ms\n",
      "933:\tlearn: 0.0848367\ttotal: 10.7s\tremaining: 759ms\n",
      "934:\tlearn: 0.0848289\ttotal: 10.8s\tremaining: 750ms\n",
      "935:\tlearn: 0.0848229\ttotal: 10.8s\tremaining: 740ms\n",
      "936:\tlearn: 0.0848163\ttotal: 10.9s\tremaining: 731ms\n",
      "937:\tlearn: 0.0848085\ttotal: 10.9s\tremaining: 723ms\n",
      "938:\tlearn: 0.0848031\ttotal: 11s\tremaining: 712ms\n",
      "939:\tlearn: 0.0847908\ttotal: 11s\tremaining: 700ms\n",
      "940:\tlearn: 0.0847840\ttotal: 11s\tremaining: 689ms\n",
      "941:\tlearn: 0.0847623\ttotal: 11s\tremaining: 677ms\n",
      "942:\tlearn: 0.0847535\ttotal: 11s\tremaining: 666ms\n",
      "943:\tlearn: 0.0847423\ttotal: 11s\tremaining: 654ms\n",
      "944:\tlearn: 0.0847278\ttotal: 11s\tremaining: 643ms\n",
      "945:\tlearn: 0.0847221\ttotal: 11.1s\tremaining: 631ms\n",
      "946:\tlearn: 0.0847110\ttotal: 11.1s\tremaining: 619ms\n",
      "947:\tlearn: 0.0847017\ttotal: 11.1s\tremaining: 608ms\n",
      "948:\tlearn: 0.0846864\ttotal: 11.1s\tremaining: 596ms\n",
      "949:\tlearn: 0.0846803\ttotal: 11.1s\tremaining: 585ms\n",
      "950:\tlearn: 0.0846669\ttotal: 11.1s\tremaining: 573ms\n",
      "951:\tlearn: 0.0846551\ttotal: 11.1s\tremaining: 562ms\n",
      "952:\tlearn: 0.0846464\ttotal: 11.2s\tremaining: 550ms\n",
      "953:\tlearn: 0.0846381\ttotal: 11.2s\tremaining: 538ms\n",
      "954:\tlearn: 0.0846259\ttotal: 11.2s\tremaining: 527ms\n",
      "955:\tlearn: 0.0846078\ttotal: 11.2s\tremaining: 515ms\n",
      "956:\tlearn: 0.0845844\ttotal: 11.2s\tremaining: 504ms\n",
      "957:\tlearn: 0.0845744\ttotal: 11.2s\tremaining: 492ms\n",
      "958:\tlearn: 0.0845594\ttotal: 11.2s\tremaining: 480ms\n",
      "959:\tlearn: 0.0845541\ttotal: 11.2s\tremaining: 468ms\n",
      "960:\tlearn: 0.0845390\ttotal: 11.3s\tremaining: 457ms\n",
      "961:\tlearn: 0.0845338\ttotal: 11.3s\tremaining: 445ms\n",
      "962:\tlearn: 0.0845226\ttotal: 11.3s\tremaining: 434ms\n",
      "963:\tlearn: 0.0845098\ttotal: 11.3s\tremaining: 422ms\n",
      "964:\tlearn: 0.0844984\ttotal: 11.3s\tremaining: 410ms\n",
      "965:\tlearn: 0.0844857\ttotal: 11.3s\tremaining: 399ms\n",
      "966:\tlearn: 0.0844826\ttotal: 11.3s\tremaining: 387ms\n",
      "967:\tlearn: 0.0844742\ttotal: 11.3s\tremaining: 375ms\n",
      "968:\tlearn: 0.0844682\ttotal: 11.4s\tremaining: 363ms\n",
      "969:\tlearn: 0.0844527\ttotal: 11.4s\tremaining: 352ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970:\tlearn: 0.0844338\ttotal: 11.4s\tremaining: 340ms\n",
      "971:\tlearn: 0.0844218\ttotal: 11.4s\tremaining: 328ms\n",
      "972:\tlearn: 0.0844067\ttotal: 11.4s\tremaining: 317ms\n",
      "973:\tlearn: 0.0843946\ttotal: 11.4s\tremaining: 305ms\n",
      "974:\tlearn: 0.0843714\ttotal: 11.4s\tremaining: 293ms\n",
      "975:\tlearn: 0.0843628\ttotal: 11.5s\tremaining: 282ms\n",
      "976:\tlearn: 0.0843471\ttotal: 11.5s\tremaining: 270ms\n",
      "977:\tlearn: 0.0843373\ttotal: 11.5s\tremaining: 258ms\n",
      "978:\tlearn: 0.0843258\ttotal: 11.5s\tremaining: 247ms\n",
      "979:\tlearn: 0.0843210\ttotal: 11.5s\tremaining: 235ms\n",
      "980:\tlearn: 0.0843108\ttotal: 11.5s\tremaining: 223ms\n",
      "981:\tlearn: 0.0843042\ttotal: 11.5s\tremaining: 212ms\n",
      "982:\tlearn: 0.0842975\ttotal: 11.6s\tremaining: 200ms\n",
      "983:\tlearn: 0.0842913\ttotal: 11.6s\tremaining: 188ms\n",
      "984:\tlearn: 0.0842795\ttotal: 11.6s\tremaining: 176ms\n",
      "985:\tlearn: 0.0842683\ttotal: 11.6s\tremaining: 165ms\n",
      "986:\tlearn: 0.0842630\ttotal: 11.6s\tremaining: 153ms\n",
      "987:\tlearn: 0.0842468\ttotal: 11.6s\tremaining: 141ms\n",
      "988:\tlearn: 0.0842343\ttotal: 11.6s\tremaining: 129ms\n",
      "989:\tlearn: 0.0842271\ttotal: 11.7s\tremaining: 118ms\n",
      "990:\tlearn: 0.0842137\ttotal: 11.7s\tremaining: 106ms\n",
      "991:\tlearn: 0.0842070\ttotal: 11.7s\tremaining: 94.2ms\n",
      "992:\tlearn: 0.0842028\ttotal: 11.7s\tremaining: 82.4ms\n",
      "993:\tlearn: 0.0841941\ttotal: 11.7s\tremaining: 70.6ms\n",
      "994:\tlearn: 0.0841862\ttotal: 11.7s\tremaining: 58.9ms\n",
      "995:\tlearn: 0.0841820\ttotal: 11.7s\tremaining: 47.1ms\n",
      "996:\tlearn: 0.0841753\ttotal: 11.7s\tremaining: 35.3ms\n",
      "997:\tlearn: 0.0841625\ttotal: 11.8s\tremaining: 23.6ms\n",
      "998:\tlearn: 0.0841422\ttotal: 11.8s\tremaining: 11.8ms\n",
      "999:\tlearn: 0.0841312\ttotal: 11.8s\tremaining: 0us\n",
      "logloss: 22.456608816630993\n",
      "logloss: 23.01920830552185\n",
      "logloss: 23.647632785783642\n",
      "logloss: 24.08705755688655\n",
      "logloss: 23.613945292672994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:21:01,782] Finished trial#0 resulted in value: 0.9596821253257586. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:17,306] Finished trial#1 resulted in value: 0.9357427256009416. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:26,867] Finished trial#2 resulted in value: 0.9584777410088359. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:36,894] Finished trial#3 resulted in value: 0.9579537621927047. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:50,571] Finished trial#4 resulted in value: 0.648516401617903. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:52,677] Finished trial#5 resulted in value: 0.9573641737156503. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:21:56,305] Finished trial#6 resulted in value: 0.9596423823642546. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:14,110] Finished trial#7 resulted in value: 0.5948357544565132. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:20,142] Finished trial#8 resulted in value: 0.9594828081272023. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:26,472] Finished trial#9 resulted in value: 0.9595579230989273. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:33,423] Finished trial#10 resulted in value: 0.9587793263170191. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:35,268] Finished trial#11 resulted in value: 0.9571652262772199. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:22:44,136] Finished trial#12 resulted in value: 0.9589549376439737. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:05,424] Finished trial#13 resulted in value: 0.9576818855597375. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:07,272] Finished trial#14 resulted in value: 0.957329107444334. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:14,126] Finished trial#15 resulted in value: 0.9583387282800757. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:23:17,801] Finished trial#16 resulted in value: 0.9595798259525491. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:10,070] Finished trial#17 resulted in value: 0.9585421293099341. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:17,927] Finished trial#18 resulted in value: 0.9595336878897737. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:24:23,968] Finished trial#19 resulted in value: 0.9581816747483132. Current best value is 0.9596821253257586 with parameters: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.10744345376421341, 'max_depth': 5, 'eta': 7.047148967751763e-08, 'gamma': 0.0007216534252362373, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[14:24:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:24:34,855] Finished trial#0 resulted in value: 0.9567937390387996. Current best value is 0.9567937390387996 with parameters: {'booster': 'gbtree', 'alpha': 0.00035533331680522024, 'max_depth': 5, 'eta': 1.718165234948709e-08, 'gamma': 0.9504890453596655, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:24:40,818] Finished trial#1 resulted in value: 0.9603484325624834. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:03,970] Finished trial#2 resulted in value: 0.9579566222829371. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:07,898] Finished trial#3 resulted in value: 0.9600643449239799. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:11,039] Finished trial#4 resulted in value: 0.9602398517336781. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:13,361] Finished trial#5 resulted in value: 0.9575723867962497. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:25:35,873] Finished trial#6 resulted in value: 0.9594287117089143. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:26:22,345] Finished trial#7 resulted in value: 0.9579335244043504. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:06,132] Finished trial#8 resulted in value: 0.957584134689377. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:09,424] Finished trial#9 resulted in value: 0.9599922028468754. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:28,104] Finished trial#10 resulted in value: 0.9588864120350189. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:41,191] Finished trial#11 resulted in value: 0.9596057990536393. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:43,660] Finished trial#12 resulted in value: 0.9577773932093221. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:48,644] Finished trial#13 resulted in value: 0.9599650886099005. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:27:56,247] Finished trial#14 resulted in value: 0.9588844162143957. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:02,494] Finished trial#15 resulted in value: 0.9594017727552282. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:05,619] Finished trial#16 resulted in value: 0.9580422124524294. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:21,031] Finished trial#17 resulted in value: 0.9599158399005996. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:22,825] Finished trial#18 resulted in value: 0.9577605341245341. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n",
      "[I 2019-11-20 14:28:31,134] Finished trial#19 resulted in value: 0.9599540919871477. Current best value is 0.9603484325624834 with parameters: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'gbtree', 'alpha': 0.001971358628058, 'max_depth': 3, 'eta': 0.15533447191453723, 'gamma': 1.0842006744095024e-08, 'grow_policy': 'depthwise'}\n",
      "\n",
      "[14:28:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-20 14:28:36,487] Finished trial#0 resulted in value: -5281.996101440747. Current best value is -5281.996101440747 with parameters: {'booster': 'dart', 'alpha': 0.21584386307181835, 'max_depth': 1, 'eta': 8.364620654813273e-06, 'gamma': 5.1354720884141934e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.6861711435019764, 'skip_drop': 9.218609379057158e-05}.\n",
      "[I 2019-11-20 14:28:44,457] Finished trial#1 resulted in value: 0.9596666807955507. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:29,147] Finished trial#2 resulted in value: 0.9586916824849825. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:36,082] Finished trial#3 resulted in value: 0.9591160819994915. Current best value is 0.9596666807955507 with parameters: {'booster': 'gbtree', 'alpha': 2.291084678414858e-07, 'max_depth': 12, 'eta': 0.00010543165466508392, 'gamma': 0.0016813997063672653, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-20 14:29:54,434] Finished trial#4 resulted in value: 0.9603118871310308. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n",
      "[I 2019-11-20 14:30:22,839] Finished trial#5 resulted in value: 0.960225519714872. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n",
      "[I 2019-11-20 14:31:00,639] Finished trial#6 resulted in value: 0.9590115921780233. Current best value is 0.9603118871310308 with parameters: {'booster': 'dart', 'alpha': 1.4037670821773404e-07, 'max_depth': 8, 'eta': 1.957873420680383e-08, 'gamma': 1.5203477792037407e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.415129103279935e-05, 'skip_drop': 1.6857619114743415e-07}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-410-4df09d1b0fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# pred_test_2は、2層目のモデルのテストデータの予測値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressorCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mpred_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2))}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-410-4df09d1b0fb0>\u001b[0m in \u001b[0;36mpredict_cv\u001b[0;34m(model, train_x, train_y, test_x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mva_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 self._optimize_sequential(func, n_trials, timeout, catch, callbacks,\n\u001b[0;32m--> 260\u001b[0;31m                                           gc_after_trial)\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 self._optimize_parallel(func, n_trials, timeout, n_jobs, catch, callbacks,\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                    rate_drop=rate_drop, skip_drop=skip_drop)\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-249-8cbc68599900>\u001b[0m in \u001b[0;36mkfold_cv\u001b[0;34m(self, model, splits)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.0/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoost\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "# tensorflowの警告抑制\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# スタッキング\n",
    "# ----------------------------------\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# models.pyにModel1Xgb, Model1NN, Model2Linearを定義しているものとする\n",
    "# 各クラスは、fitで学習し、predictで予測値の確率を出力する\n",
    "\n",
    "\n",
    "# 学習データに対する「目的変数を知らない」予測値と、テストデータに対する予測値を返す関数\n",
    "def predict_cv(model, train_x, train_y, test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "    # クロスバリデーションで学習・予測を行い、予測値とインデックスを保存する\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # テストデータに対する予測値の平均をとる\n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test\n",
    "\n",
    "\n",
    "# 1層目のモデル\n",
    "\n",
    "model_1a =xgb.XGBRegressor(**{'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06})\n",
    "pred_train_1a, pred_test_1a = predict_cv(model_1a, X, y, test)\n",
    "\n",
    "model_1b = lgb.LGBMRegressor(**{'iterations': 55, 'depth': 22, 'learning_rate': 0.15022735769525583, 'random_strength': 40, 'bagging_temperature': 0.03207087240546606, 'od_type': 'Iter', 'od_wait': 41})\n",
    "pred_train_1b, pred_test_1b = predict_cv(model_1b, X, y, test)\n",
    "\n",
    "model_1c = LGBRegressorCV(n_trials=10)\n",
    "pred_train_1c, pred_test_1c = predict_cv(model_1c, X, y, test)\n",
    "\n",
    "model_1d = RFR(n_jobs=-1, random_state=2525)\n",
    "pred_train_1d, pred_test_1d = predict_cv(model_1d, X, y, test)\n",
    "\n",
    "model_1e =  CatBoost()\n",
    "pred_train_1e, pred_test_1e = predict_cv(model_1e, X, y, test)\n",
    "\n",
    "# model_1f=\n",
    "\n",
    "# 1層目のモデルの評価\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1a))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1b))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1c))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1d))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_1e))}')\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成\n",
    "train_x_2 = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b,'pred_1c': pred_train_1c,'pred_1d': pred_train_1d,'pred_1e': pred_train_1e})\n",
    "test_x_2 = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b, 'pred_1c': pred_test_1c, 'pred_1d': pred_test_1d, 'pred_1e': pred_test_1e})\n",
    "\n",
    "\n",
    "# 2層目のモデル\n",
    "# pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n",
    "# pred_test_2は、2層目のモデルのテストデータの予測値\n",
    "model_2 = XGBRegressorCV(n_trials=20)\n",
    "pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, y, test_x_2)\n",
    "\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2))}')\n",
    "model_2b = lgb.LGBMRegressor()\n",
    "pred_train_2b, pred_test_2b = predict_cv(model_2b, train_x_2, y, test_x_2)\n",
    "train_x_3 = pd.DataFrame({'pred_2a': pred_train_2a, 'pred_2b': pred_train_2b})\n",
    "test_x_3 = pd.DataFrame({'pred_2a': pred_test_2a, 'pred_2b': pred_test_2b})\n",
    "\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2a))}')\n",
    "print(f'logloss: {mean_absolute_error(np.exp(y),np.exp( pred_train_2b))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.exp(pred_test_2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:23:37,643] Finished trial#0 resulted in value: 0.9305498191433168. Current best value is 0.9305498191433168 with parameters: {'booster': 'gbtree', 'alpha': 0.05398288589157677, 'max_depth': 3, 'eta': 0.7140800447741135, 'gamma': 0.00024190393122103457, 'grow_policy': 'lossguide'}.\n",
      "[I 2019-11-19 14:24:54,582] Finished trial#1 resulted in value: 0.9603226552134976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:17,469] Finished trial#2 resulted in value: 0.9596122329185025. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:25:38,640] Finished trial#3 resulted in value: 0.9598963266814737. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:01,822] Finished trial#4 resulted in value: 0.9445592847731454. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:25,405] Finished trial#5 resulted in value: 0.9587629061169235. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:41,400] Finished trial#6 resulted in value: -387.9041239755456. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:26:54,677] Finished trial#7 resulted in value: 0.9592436087634768. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:28:56,906] Finished trial#8 resulted in value: 0.9566239935926616. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:07,426] Finished trial#9 resulted in value: 0.9567988998760226. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:31:42,840] Finished trial#10 resulted in value: 0.9476482257292294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:32:27,596] Finished trial#11 resulted in value: 0.956698401395849. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:33:16,474] Finished trial#12 resulted in value: -4.055874265306564. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:14,085] Finished trial#13 resulted in value: 0.9601138026638152. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:34:59,910] Finished trial#14 resulted in value: 0.9565527450078622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:33,531] Finished trial#15 resulted in value: 0.9446207766477638. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:35:48,702] Finished trial#16 resulted in value: -1399.7011359154567. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:00,125] Finished trial#17 resulted in value: 0.958760245390202. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:36:30,110] Finished trial#18 resulted in value: 0.9574032702359846. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:37:53,151] Finished trial#19 resulted in value: 0.9581539820151755. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:00,500] Finished trial#20 resulted in value: 0.956370606897214. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:18,114] Finished trial#21 resulted in value: 0.9586995742275745. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:38:41,289] Finished trial#22 resulted in value: 0.9594710457867158. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:06,406] Finished trial#23 resulted in value: 0.9545935103825889. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:24,873] Finished trial#24 resulted in value: 0.9594131128640582. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:39:52,900] Finished trial#25 resulted in value: 0.9568289978095595. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:40:18,111] Finished trial#26 resulted in value: 0.9568401840594071. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:44:38,410] Finished trial#27 resulted in value: 0.37976798637976894. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:09,769] Finished trial#28 resulted in value: 0.9569202272257933. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:45:31,764] Finished trial#29 resulted in value: 0.9530933288937604. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:46:20,190] Finished trial#30 resulted in value: 0.9596207162459487. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:47:21,110] Finished trial#31 resulted in value: 0.9587299746964092. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:22,964] Finished trial#32 resulted in value: 0.37901072492524795. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:48:59,562] Finished trial#33 resulted in value: 0.9557504185473518. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:50:19,887] Finished trial#34 resulted in value: 0.6920560246130634. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:07,764] Finished trial#35 resulted in value: 0.9563269444075339. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:15,479] Finished trial#36 resulted in value: 0.8104693935211186. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:51:44,993] Finished trial#37 resulted in value: 0.9596979309846085. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 14:52:08,960] Finished trial#38 resulted in value: 0.9594897252942587. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:34,318] Finished trial#39 resulted in value: 0.9560476206065488. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:52:59,365] Finished trial#40 resulted in value: 0.9589511941685105. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:35,311] Finished trial#41 resulted in value: 0.9557815871347504. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:53:57,821] Finished trial#42 resulted in value: 0.9596402683393249. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:22,898] Finished trial#43 resulted in value: 0.9596506393831744. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:54:49,563] Finished trial#44 resulted in value: 0.9583432840218722. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:12,913] Finished trial#45 resulted in value: 0.9503219866901705. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:55:44,323] Finished trial#46 resulted in value: 0.9439230284396045. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:07,679] Finished trial#47 resulted in value: 0.9577484164792291. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:33,616] Finished trial#48 resulted in value: 0.9588307979464294. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:56:54,266] Finished trial#49 resulted in value: 0.9595208600484575. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:13,349] Finished trial#50 resulted in value: 0.958859235560175. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:57:34,458] Finished trial#51 resulted in value: 0.959801537313227. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:01,690] Finished trial#52 resulted in value: 0.9586469259847702. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:58:37,602] Finished trial#53 resulted in value: 0.9567041406226083. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:03,668] Finished trial#54 resulted in value: 0.9503185290450906. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:33,333] Finished trial#55 resulted in value: 0.9596033634027707. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 14:59:55,627] Finished trial#56 resulted in value: 0.9579607729400174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:30,560] Finished trial#57 resulted in value: 0.9559231059905114. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:00:56,760] Finished trial#58 resulted in value: 0.9597679573180903. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:23,527] Finished trial#59 resulted in value: 0.9580893053324463. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:01:52,486] Finished trial#60 resulted in value: 0.9557485563030305. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:11,578] Finished trial#61 resulted in value: 0.9594123430012976. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:34,128] Finished trial#62 resulted in value: 0.9598668026754119. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:02:58,324] Finished trial#63 resulted in value: 0.9596332658609208. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:19,827] Finished trial#64 resulted in value: 0.9590550944936261. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:03:51,268] Finished trial#65 resulted in value: 0.9564344930902546. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:17,973] Finished trial#66 resulted in value: 0.9529373023400145. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:04:36,048] Finished trial#67 resulted in value: 0.9574774739825571. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:12,644] Finished trial#68 resulted in value: 0.9426763980484459. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:05:31,281] Finished trial#69 resulted in value: 0.9593833804168816. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:00,170] Finished trial#70 resulted in value: 0.9591893243636337. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:22,291] Finished trial#71 resulted in value: 0.9595051111829601. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:06:44,957] Finished trial#72 resulted in value: 0.9593363815773163. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:11,092] Finished trial#73 resulted in value: 0.958386606182769. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:07:38,646] Finished trial#74 resulted in value: 0.9576461057484386. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:08:02,243] Finished trial#75 resulted in value: 0.9592386374748388. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-19 15:09:04,771] Finished trial#76 resulted in value: 0.9597318738254854. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:06,931] Finished trial#77 resulted in value: 0.95899052150256. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:10:50,430] Finished trial#78 resulted in value: 0.9497551679431391. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:11:37,312] Finished trial#79 resulted in value: 0.9535350937350803. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:13:11,867] Finished trial#80 resulted in value: 0.37795736793164736. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:14:17,509] Finished trial#81 resulted in value: 0.9600409927854597. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:15:17,231] Finished trial#82 resulted in value: 0.9601820948004374. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:16:51,760] Finished trial#83 resulted in value: 0.9599088646926296. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:17:55,563] Finished trial#84 resulted in value: 0.9582942766014618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:19:16,561] Finished trial#85 resulted in value: 0.9556363922125138. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:20:50,623] Finished trial#86 resulted in value: 0.9598415829020268. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:21:56,978] Finished trial#87 resulted in value: 0.9591565948983902. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:23:02,511] Finished trial#88 resulted in value: 0.9592573940724174. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:24:21,418] Finished trial#89 resulted in value: 0.9583596724830622. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:25:05,185] Finished trial#90 resulted in value: 0.9579814074019618. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:26:03,650] Finished trial#91 resulted in value: 0.9596818974789345. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:27:14,538] Finished trial#92 resulted in value: 0.9597103277990394. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:28:23,695] Finished trial#93 resulted in value: 0.9499916301977823. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:29:22,181] Finished trial#94 resulted in value: 0.9596422136661624. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:15,336] Finished trial#95 resulted in value: 0.9590829201283609. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:30:33,575] Finished trial#96 resulted in value: -498.1587888134253. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:19,094] Finished trial#97 resulted in value: 0.9524442506788089. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:31:49,887] Finished trial#98 resulted in value: 0.9479648390482927. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n",
      "[I 2019-11-19 15:33:28,437] Finished trial#99 resulted in value: 0.9563083963830048. Current best value is 0.9603226552134976 with parameters: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: 0.96\n",
      "Best params: {'booster': 'dart', 'alpha': 3.5339524056628113e-06, 'max_depth': 17, 'eta': 6.85408959344308e-05, 'gamma': 0.04080859363733414, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.001370478356241324, 'skip_drop': 3.5472091912288913e-06}\n",
      "\n",
      "[15:33:28] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# xgbr = XGBRegressorCV(n_trials=40)\n",
    "\n",
    "xgbr.fit(X, y)\n",
    "\n",
    "pred=np.exp(xgbr.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(pd.read_csv(\"test_data.csv\")['id'])\n",
    "sub[\"y\"] = list(pred)\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
